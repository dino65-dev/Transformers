{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":483426,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":386607,"modelId":405731},{"sourceId":501289,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":397906,"modelId":416209}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Set environment variable BEFORE importing torch\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:25:47.620833Z","iopub.execute_input":"2025-08-07T07:25:47.621443Z","iopub.status.idle":"2025-08-07T07:25:47.627668Z","shell.execute_reply.started":"2025-08-07T07:25:47.621418Z","shell.execute_reply":"2025-08-07T07:25:47.627038Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:25:47.909569Z","iopub.execute_input":"2025-08-07T07:25:47.910240Z","iopub.status.idle":"2025-08-07T07:25:48.524109Z","shell.execute_reply.started":"2025-08-07T07:25:47.910221Z","shell.execute_reply":"2025-08-07T07:25:48.523352Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Loading Dataset and creating Tokenizer","metadata":{}},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, Dataset\nimport json\n# Load the dataset\ndataset = load_dataset(\"lmsys/lmsys-chat-1m\")\ndataset = dataset.filter(lambda x: x['language'] == 'English')\n# Create or load a tokenizer\n# For this example, we'll use an existing tokenizer\nfrom transformers import PreTrainedTokenizerFast\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\"gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:25:50.958043Z","iopub.execute_input":"2025-08-07T07:25:50.958328Z","iopub.status.idle":"2025-08-07T07:29:28.683357Z","shell.execute_reply.started":"2025-08-07T07:25:50.958307Z","shell.execute_reply":"2025-08-07T07:29:28.682287Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.88k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5edc41f84928473ca71cf2dce9e12a60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00006-4feeb3f83346a0e9.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"815f49ff6c9b40e8bf4736d365e08764"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00001-of-00006-4030672591c2f478.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e09ca82133b4fbeb404895a1a26d538"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00002-of-00006-1779b7cec9462180.parquet:   0%|          | 0.00/250M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ae0360e09134f8b88f810d05ba732ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00003-of-00006-2fa862bfed56af1f.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"809a86ffdda24e32abd177a363b44993"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00004-of-00006-18f4bdd50c103e71.parquet:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81960cf613ee40cd8f10bf6528ed0557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00005-of-00006-fe1acc5d10a9f0e2.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14b41f3287e34b0aa478cc403bb645db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6e3cb5d97e448679dda33529772182e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1000000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b42b289345004664bf0bf7295cc62672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef069037b7a24010973d4c5d7f22f45c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68579d56317a4a3fb02fdecfd1732d91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a75c32c18fd74d4b803c1db68b369c0c"}},"metadata":{}},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \nThe class this function is called from is 'PreTrainedTokenizerFast'.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"dataset # there is 77.7% english so there is 1M rows so english is 777k rows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.684636Z","iopub.execute_input":"2025-08-07T07:29:28.685189Z","iopub.status.idle":"2025-08-07T07:29:28.690374Z","shell.execute_reply.started":"2025-08-07T07:29:28.685161Z","shell.execute_reply":"2025-08-07T07:29:28.689568Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['conversation_id', 'model', 'conversation', 'turn', 'language', 'openai_moderation', 'redacted'],\n        num_rows: 777453\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## Input Embeddings","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport math\n\nclass InputEmbeddings(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int) -> None:  # Fixed: d_mdoel -> d_model\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        # Fix 2: Remove aggressive scaling that causes NaN\n        return self.embedding(x) * math.sqrt(self.d_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.691112Z","iopub.execute_input":"2025-08-07T07:29:28.691349Z","iopub.status.idle":"2025-08-07T07:29:28.712736Z","shell.execute_reply.started":"2025-08-07T07:29:28.691323Z","shell.execute_reply":"2025-08-07T07:29:28.712203Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Positional Encoding","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport math\nimport numpy as np\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, seq_length: int, dropout: float) -> None:\n        super().__init__()  # Fixed: super().__init__... () -> super().__init__()\n        self.d_model = d_model\n        self.seq_length = seq_length\n        self.dropout = nn.Dropout(dropout)\n\n        pe = torch.zeros(seq_length, d_model)\n        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.shape[1], :].requires_grad_(False)\n        return self.dropout(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.714208Z","iopub.execute_input":"2025-08-07T07:29:28.714383Z","iopub.status.idle":"2025-08-07T07:29:28.732297Z","shell.execute_reply.started":"2025-08-07T07:29:28.714369Z","shell.execute_reply":"2025-08-07T07:29:28.731720Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Here is the code for RoPE(Roatary Positional Encoding) \n**Use it only when not using PE functions, it's more efficient than PE**","metadata":{}},{"cell_type":"code","source":"class RotaryPositionalEmbedding(nn.Module):\n\n    def __init__(self, d_model, base = 10000) -> None:\n        super().__init__()\n        inv_freq = 1. / (base ** (torch.arange(0,d_model,2).float() / d_model)) # this is out theta(i)\n        self.register_buffer('inv_freq', inv_freq)\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self,x, seq_dim = 2): # seq_dim=2 for [batch, heads, seq_len, head_dim]\n        seq_len = x.shape[seq_dim]\n        if seq_len != self.seq_len_cached:\n            t = torch.arange(seq_len,device=x.device).type_as(self.inv_freq) # position_indices[0 --> seq_len-1]\n            freqs = torch.einsum('i,j -> ij',t,self.inv_freq) # t ⊗ inv_freq (outer product)\n            emb = torch.cat((freqs,freqs),dim= -1).to(x.device) # creates [cos,sin,cos,sin] pattern, more importantly we are repeating cause it doesn't dimenstion mismatch at the broadcasting time\n            self.cos_cached = emb.cos()[None,None,:,:] # [1, 1, seq_len, head_dim]\n            self.sin_cached = emb.sin()[None,None,:,:]\n        return self.cos_cached , self.sin_cached\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.732896Z","iopub.execute_input":"2025-08-07T07:29:28.733193Z","iopub.status.idle":"2025-08-07T07:29:28.752545Z","shell.execute_reply.started":"2025-08-07T07:29:28.733174Z","shell.execute_reply":"2025-08-07T07:29:28.751843Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Helper Functions for RoPE","metadata":{}},{"cell_type":"code","source":"def rotate_half(x):\n        # it's a 90° rotation , if you think x as a complex number input then , x-->  a+bi then after 90° rotation it will be -b+ai\n        x1, x2 = x[...,:x.shape[-1]//2], x[..., x.shape[-1]//2:]\n        return torch.cat((-x2, x1), dim=-1)\n\n@torch.jit.script\ndef apply_rotary_pos_emb(x,cos,sin):\n        #it applies Euler formula : e^(iθ) = cos(θ) + i·sin(θ) that causes (q * cos) + (rotate_half(q) * sin) is implementing: q·cos(θ) + i·q·sin(θ)\n        # rotate_half is for to make the q&k (iota)imaginary part\n        return (x * cos) + (rotate_half(x) * sin)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.753206Z","iopub.execute_input":"2025-08-07T07:29:28.753456Z","iopub.status.idle":"2025-08-07T07:29:28.804559Z","shell.execute_reply.started":"2025-08-07T07:29:28.753428Z","shell.execute_reply":"2025-08-07T07:29:28.803747Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## GQA(Grouped Query Attention) --> More efficient than MHA","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport math\nimport torch.nn.functional as F\nclass GroupedQueryAttention(nn.Module):\n    \"\"\"\n        Grouped Query Attention\n\n        Args:\n            d_model: Embedding dimension\n            num_query_heads: Number of query heads\n            num_kv_heads: Number of key-value heads (must divide num_query_heads)\n            dropout: Dropout probability\n            bias: Whether to use bias in linear projections\n            rope_percentage: Decides what percentage of embeddings will be used for rope\n        \"\"\"\n    def __init__(self, d_model : int , num_query_heads : int, num_kv_heads : int, dropout = 0.1, bias = False, rope_percentage = 0.5) -> None:\n        super().__init__()\n\n        assert d_model % num_query_heads == 0, \"d_model must be divisible by num_query_heads\"\n        assert num_query_heads % num_kv_heads == 0, \"num_query_heads must be divisible by num_kv_heads\"\n\n        self.d_model = d_model\n        self.num_q_head = num_query_heads\n        self.num_kv_head = num_kv_heads\n        # per head dim\n        self.head_dim = d_model // num_query_heads\n        #how many query heads share a single KV head\n        self.group_size = num_query_heads // num_kv_heads\n\n        #rope initialization\n        self.rope_percentage = rope_percentage\n        self.rope_dim = int(self.head_dim * rope_percentage)\n        if self.rope_dim > 0:\n            self.rotary_pe = RotaryPositionalEmbedding(self.rope_dim)\n\n        #Linear projections\n        self.q_proj = nn.Linear(d_model,d_model,bias=bias)\n        self.k_proj = nn.Linear(d_model,self.num_kv_head * self.head_dim,bias=bias)\n        self.v_proj = nn.Linear(d_model,self.num_kv_head * self.head_dim, bias=bias)\n        self.out_proj = nn.Linear(d_model,d_model,bias=bias)\n        self.dropout = nn.Dropout(dropout) # prevent overfitting\n        self.scale = 1.0 / math.sqrt(self.head_dim)\n\n    def forward(self,query, key = None, value = None, attn_mask = None,is_causal = False, need_weigths = False,cache = None):\n        if key is None:\n            key = query\n        if value is None:\n            value = key\n\n        batch_size = query.shape[0]\n        seq_len = query.shape[1]\n        kv_seq_length = key.shape[1]\n\n        #project queries , keys, values\n        q = self.q_proj(query) #[batch, seq_len, d_model]\n        k = self.k_proj(key) #[batch, kv_seq_len , num_kv_heads * head dim]\n        v = self.v_proj(value) # [batch, kv_seq_len, num_kv_heads* head_dim]\n\n        # Reshape and transpose for mha\n        q = q.view(batch_size,seq_len,self.num_q_head,self.head_dim).transpose(1,2) # [batch, num_query_heads, seq_len, head_dim]\n        k = k.view(batch_size, kv_seq_length,self.num_kv_head,self.head_dim).transpose(1,2) # [batch, num_kv_head, kv_seq_length, head_dim]\n        v = v.view(batch_size, kv_seq_length, self.num_kv_head, self.head_dim).transpose(1,2) #[batch, num_kv_head, kv_seq_length, head_dim]\n\n    # ============ If you are going to use PE then don't use RoPE and vice_versa ========================\n\n        # Applying Rope\n        if self.rope_dim > 0:\n            #split into RoPE and non-RoPE parts\n            q_rope, q_pass = q[...,:self.rope_dim], q[...,self.rope_dim:]\n            k_rope , k_pass = k[...,:self.rope_dim] , k[...,self.rope_dim:]\n\n            # Apply rotary embeddings to queries\n            cos_q , sin_q = self.rotary_pe(q_rope)\n            q_rope = apply_rotary_pos_emb(q_rope,cos_q,sin_q)\n\n            #Apply rotary embeddings to keys\n            cos_k , sin_k = self.rotary_pe(k_rope)\n            k_rope = apply_rotary_pos_emb(k_rope,cos_k,sin_k)\n\n            # concatenate back\n            q = torch.cat([q_rope,q_pass],dim=-1)\n            k = torch.cat([k_rope, k_pass],dim=-1)\n\n    #=======================================================================================================\n\n        #Expand keys and values to match query heads\n        # Each group of query heads shares the same kv heads\n        #after learning the learned matrices of key is copied into (k_head * group_size) total\n        k_expanded = k.repeat_interleave(self.group_size, dim =1) # [batch, num_query_heads, kv_seq_len, head_dim]\n        v_expanded = v.repeat_interleave(self.group_size,dim=1)  # [batch, num_query_heads, kv_seq_len, head_dim]\n\n        # KV caching\n        if cache is not None:\n            past_key , past_value = cache\n            k_expanded = torch.cat((past_key,k_expanded),dim=2)\n            v_expanded = torch.cat((past_value,v_expanded),dim=2)\n        present_kv = (k_expanded,v_expanded)\n\n        # compute attention scores\n        # query : seq_len, head_dim * key: head_dim ,kv_seq_len\n        attn_scores = torch.matmul(q,k_expanded.transpose(-2,-1)) * self.scale # [batch, num_query_heads, seq_len, kv_seq_len]\n\n        # Apply masks\n        if is_causal:\n            causal_mask = torch.tril(torch.ones(seq_len,kv_seq_length,device=q.device,dtype=torch.bool))\n            attn_scores = attn_scores.masked_fill(~causal_mask,float('-inf')) # inverse the causal mask and where is true replace that with -infinity\n\n        if attn_mask is not None:\n            if attn_mask.dim() == 2:\n                attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\n            elif attn_mask.dim() == 3:\n                attn_mask = attn_mask.unsqueeze(1)\n            attn_mask = attn_mask.to(dtype=torch.bool)\n            attn_scores = attn_scores.masked_fill(~attn_mask, float('-inf'))\n\n        # compute attention probabilites\n        attn_probs = F.softmax(attn_scores ,dim=-1)\n        attn_probs = self.dropout(attn_probs)\n\n        #Apply attention to values\n        attn_output = torch.matmul(attn_probs, v_expanded) ## [batch, num_query_heads, seq_len, head_dim]\n\n        # Concatenate heads\n        attn_output = attn_output.transpose(1,2).contiguous() # [batch, seq_len, num_query_heads, head_dim]\n\n        attn_output = attn_output.view(batch_size,seq_len,self.d_model) # [batch, seq_len, embed_dim]\n\n        #Final output projection\n        output = self.out_proj(attn_output)\n\n        if need_weigths:\n            #Average attention weights across heads for visualization\n            attn_weights = attn_probs.mean(dim=1) # [batch, seq_len, kv_seq_len]\n            return output, attn_weights, present_kv\n        else: return output , present_kv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.805515Z","iopub.execute_input":"2025-08-07T07:29:28.805713Z","iopub.status.idle":"2025-08-07T07:29:28.822125Z","shell.execute_reply.started":"2025-08-07T07:29:28.805698Z","shell.execute_reply":"2025-08-07T07:29:28.821545Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Feed Forward Layer","metadata":{}},{"cell_type":"code","source":"\nclass FeedForwardBlock(nn.Module):\n\n    def __init__(self, d_model : int, d_ff : int, dropout : float) -> None:\n        super().__init__()\n        self.activation = nn.GELU()\n        # First layer tranformation\n        self.linear1 = nn.Linear(d_model,d_ff) # w1 & b1\n        self.dropout = nn.Dropout(dropout) # prevent overfitting\n\n        #Sceond layer transformation\n        self.linear2 = nn.Linear(d_ff, d_model) # w2 & b2\n\n    def forward(self,x):\n        # d_model --> dff --> d_model\n        return self.linear2(self.dropout(self.activation(self.linear1(x))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.822904Z","iopub.execute_input":"2025-08-07T07:29:28.823402Z","iopub.status.idle":"2025-08-07T07:29:28.843941Z","shell.execute_reply.started":"2025-08-07T07:29:28.823383Z","shell.execute_reply":"2025-08-07T07:29:28.843279Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Layer Norm --> Pre-Norm \n**But in research paper was post norm, generally pre-norm is efficient than post-norm. Implemented in real-world LLM's**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass LayerNormalization(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        self.alpha = nn.Parameter(torch.ones(1))  # multiplicative parameter\n        self.bias = nn.Parameter(torch.zeros(1))  # additive parameter\n\n    def forward(self, x):\n        # x: (batch, seq_len, d_model)\n        # Keep the dimension for broadcasting\n        mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n        # Keep the dimension for broadcasting\n        std = x.std(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n        # eps is to prevent dividing by zero or when std is very small\n        return self.alpha * (x - mean) / (std + self.eps) + self.bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.844690Z","iopub.execute_input":"2025-08-07T07:29:28.844955Z","iopub.status.idle":"2025-08-07T07:29:28.864051Z","shell.execute_reply.started":"2025-08-07T07:29:28.844931Z","shell.execute_reply":"2025-08-07T07:29:28.863397Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## RMS(Root Mean Squared) Norm \n**Better than LayerNorm**","metadata":{}},{"cell_type":"code","source":"class RMSNorm(nn.Module):\n    def __init__(self,dim: int = 768, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.dim = dim\n        # The learnable scaling parameter, with a size of the feature dimension\n        self.gamma = nn.Parameter(torch.ones(self.dim))\n\n    def _norm(self, x):\n        # Calculate the reciprocal of the square root for efficiency\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        # Add input validation\n        if isinstance(x, tuple):\n            # If input is tuple, use only the first element (the actual tensor)\n            x = x[0]\n            print(\"Warning: RMSNorm received tuple input, using first element\")\n\n        # Ensure x is a tensor\n        if not isinstance(x, torch.Tensor):\n            raise TypeError(f\"RMSNorm expected tensor input, got {type(x)}\")\n\n        # Normalize and then scale\n        return self.gamma * self._norm(x.float()).type_as(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.866609Z","iopub.execute_input":"2025-08-07T07:29:28.866830Z","iopub.status.idle":"2025-08-07T07:29:28.878929Z","shell.execute_reply.started":"2025-08-07T07:29:28.866805Z","shell.execute_reply":"2025-08-07T07:29:28.878392Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## This Layer is the output of decoder and coverting them into probs","metadata":{}},{"cell_type":"code","source":"class ProjectionLayer(nn.Module):\n    # projection layer is the output of ffn from decoder and the applied on liner,softmax layer\n    def __init__(self, d_model : int , vacab_size: int) -> None:\n        super().__init__()\n        self.proj = nn.Linear(d_model,vacab_size) # Linear layer\n\n    def forward(self, x):\n        return torch.log_softmax(self.proj(x), dim = -1)  # Applying the log Softmax function to the output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.879686Z","iopub.execute_input":"2025-08-07T07:29:28.880494Z","iopub.status.idle":"2025-08-07T07:29:28.899186Z","shell.execute_reply.started":"2025-08-07T07:29:28.880466Z","shell.execute_reply":"2025-08-07T07:29:28.898598Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Residual Connection","metadata":{}},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n    def __init__(self, dropout: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.norm = RMSNorm()\n\n    def forward(self, x, sublayer):\n        # Ensure x is a tensor\n        if isinstance(x, tuple):\n            x = x[0]\n\n        # Apply normalization\n        normed_x = self.norm(x)\n\n        # Apply sublayer\n        sublayer_output = sublayer(normed_x)\n\n        # Handle both cached and non-cached sublayer outputs\n        if isinstance(sublayer_output, tuple):\n            # Sublayer returned (output, cache)\n            output_tensor, cache = sublayer_output\n            residual_output = x + self.dropout(output_tensor)\n            return residual_output, cache  # Return tuple\n        else:\n            # Sublayer returned only output tensor\n            output_tensor = sublayer_output\n            residual_output = x + self.dropout(output_tensor)\n            return residual_output  # Return tensor only\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.899865Z","iopub.execute_input":"2025-08-07T07:29:28.900218Z","iopub.status.idle":"2025-08-07T07:29:28.917059Z","shell.execute_reply.started":"2025-08-07T07:29:28.900194Z","shell.execute_reply":"2025-08-07T07:29:28.916438Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Decoder Block\n**Decoder Block has masked-self-attention another is cross attention , one is feed-forward block**","metadata":{}},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self, masked_attention_block, feed_forward_block, dropout):\n        super().__init__()\n        self.masked_attention = masked_attention_block\n        self.feed_forward = feed_forward_block\n        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n\n    def forward(self, x, tgt_mask, cache=None):  # No encoder params\n        # Self-attention\n        x, self_attn_cache = self.residual_connection[0](\n            x,\n            lambda x: self.masked_attention(\n                query=x, key=x, value=x,\n                attn_mask=tgt_mask,\n                is_causal=True,\n                cache=cache\n            )\n        )\n\n        # Feed-forward\n        x = self.residual_connection[1](x, self.feed_forward)\n\n        return x, self_attn_cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.917811Z","iopub.execute_input":"2025-08-07T07:29:28.918017Z","iopub.status.idle":"2025-08-07T07:29:28.936767Z","shell.execute_reply.started":"2025-08-07T07:29:28.918003Z","shell.execute_reply":"2025-08-07T07:29:28.936086Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Decoder\n**A deocoder can have multiple decoder_blocks**","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, layers: nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = RMSNorm()\n\n    def forward(self, x, tgt_mask, layer_caches=None):  # No encoder params\n        new_layer_caches = []\n        for i, layer in enumerate(self.layers):\n            layer_cache = None if layer_caches is None else layer_caches[i]\n            x, new_cache = layer(x, tgt_mask, layer_cache)  # Assumes updated DecoderBlock\n            new_layer_caches.append(new_cache)\n        return self.norm(x), new_layer_caches\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.937571Z","iopub.execute_input":"2025-08-07T07:29:28.937790Z","iopub.status.idle":"2025-08-07T07:29:28.951329Z","shell.execute_reply.started":"2025-08-07T07:29:28.937774Z","shell.execute_reply":"2025-08-07T07:29:28.950630Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \"\"\"An Encoder can have several Encoder Blocks\"\"\"\n\n    def __init__(self,layers: nn.ModuleList) -> None:\n        self.layers = layers # storing the EncoderBlocks\n        self.norm = RMSNorm()\n\n    def forward(self,x,mask):\n        #Iterating over each EncoderBlock stored in self.layers\n        for layer in self.layers:\n            x = layer(x,mask) # Applying each EncoderBlock to the input tensor 'x'\n        return self.norm(x) # normalizing after encoder operation, it's not in paper but in now a days it done for better training and stbility\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.952043Z","iopub.execute_input":"2025-08-07T07:29:28.952320Z","iopub.status.idle":"2025-08-07T07:29:28.971154Z","shell.execute_reply.started":"2025-08-07T07:29:28.952297Z","shell.execute_reply":"2025-08-07T07:29:28.970453Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from typing import Optional\n\nclass Transformer(nn.Module):\n    \"\"\"This takes in the encoder and decoder, as well as the embeddings for the source\n    and target language. It also takes in the positional encoding for the source and target language,\n    as well as projection layer.\"\"\"\n\n    def __init__(self,\n                 encoder: Optional[Encoder] = None,\n                 decoder: Optional[Decoder] = None,\n                 src_embed: Optional[InputEmbeddings] = None,\n                 tgt_embed: Optional[InputEmbeddings] = None,\n                 src_pos: Optional[PositionalEncoding] = None,\n                 tgt_pos: Optional[PositionalEncoding] = None,\n                 projection_layer: Optional[ProjectionLayer] = None,\n                 use_rope: bool = True) -> None:\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.src_pos = src_pos\n        self.tgt_pos = tgt_pos\n        self.projection_layer = projection_layer\n        self.use_rope = use_rope\n\n        # Validate configuration\n        if self.use_rope and (self.src_pos is not None or self.tgt_pos is not None):\n            print(\"Warning: Using RoPE with separate positional encodings. \"\n                  \"Consider setting src_pos=None, tgt_pos=None for pure RoPE.\")\n        \n        if self.decoder is None:\n            raise ValueError(\"Decoder must be provided.\")\n        if self.tgt_embed is None:\n            raise ValueError(\"Target embeddings (tgt_embed) must be provided.\")\n        if self.projection_layer is None:\n            raise ValueError(\"Projection layer must be provided.\")\n\n    # Encoder (only used if encoder is provided)\n    def encode(self, src, src_mask):\n        if self.encoder is None:\n            raise ValueError(\"Encoder is not initialized. This is a decoder-only model.\")\n        if self.src_embed is None or (not self.use_rope and self.src_pos is None):\n            raise ValueError(\"Source embedding or positional encoding components are missing.\")\n        \n        src = self.src_embed(src)  # Applying source embeddings\n        if not self.use_rope and self.src_pos is not None:\n            src = self.src_pos(src)  # Applying source positional encoding\n        return self.encoder(src, src_mask)  # Encoder forward\n\n    # Decoder (handles both decoder-only and encoder-decoder modes)\n    def decode(self, tgt, tgt_mask, layer_caches=None):\n        \"\"\"\n        Decoder-only decode method: No encoder parameters needed.\n        - tgt: Raw token IDs [batch_size, seq_len]\n        - tgt_mask: Attention mask [batch_size, seq_len, seq_len]\n        - layer_caches: Optional KV cache for generation\n        \"\"\"\n        # Embed the raw token IDs\n        tgt = self.tgt_embed(tgt)\n        \n        # Apply positional encoding (if not using RoPE)\n        if self.tgt_pos is not None:\n            tgt = self.tgt_pos(tgt)\n        \n        # Pass through decoder (self-attention only)\n        output, new_caches = self.decoder(tgt, tgt_mask, layer_caches)  # Assumes updated Decoder.forward\n        \n        return output, new_caches\n\n    # Applying projection layer with the Softmax Function to the decoder output\n    def project(self, x):\n        if self.projection_layer is None:\n            raise ValueError(\"Projection layer is not initialized.\")\n        return self.projection_layer(x)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.971791Z","iopub.execute_input":"2025-08-07T07:29:28.972014Z","iopub.status.idle":"2025-08-07T07:29:28.989904Z","shell.execute_reply.started":"2025-08-07T07:29:28.971998Z","shell.execute_reply":"2025-08-07T07:29:28.989230Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def build_transformer(src_vocab_size: int, tgt_vocab_size: int,\n                           src_seq_len: int, tgt_seq_len: int,\n                           d_model: int = 512, N: int = 6, h: int = 8,\n                           kv_h: int = 4, dropout: float = 0.1, d_ff: int = 2048):\n\n    # Create embedding layers\n    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n\n    # Create positional encoding\n    #tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n\n    # Create decoder blocks\n    decoder_blocks = []\n    for _ in range(N):\n        decoder_self_attention_block = GroupedQueryAttention(d_model, h, kv_h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n\n        decoder_block = DecoderBlock(decoder_self_attention_block,\n                                   feed_forward_block, dropout)\n        decoder_blocks.append(decoder_block)\n\n    decoder = Decoder(nn.ModuleList(decoder_blocks))\n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n                                                                   #tgt_pos --> uisng RoPE\n    transformer = Transformer(None, decoder, None, tgt_embed, None, None, projection_layer)\n\n\n    for name, p in transformer.named_parameters():\n        if p.dim() > 1:\n            if 'embedding' in name:\n                # Use smaller initialization for embeddings\n                nn.init.normal_(p, mean=0.0, std=0.02)\n            else:\n                nn.init.xavier_uniform_(p, gain=1.0)\n        else:\n            nn.init.zeros_(p)\n\n    return transformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:28.990633Z","iopub.execute_input":"2025-08-07T07:29:28.990831Z","iopub.status.idle":"2025-08-07T07:29:29.010928Z","shell.execute_reply.started":"2025-08-07T07:29:28.990808Z","shell.execute_reply":"2025-08-07T07:29:29.010259Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:29.011666Z","iopub.execute_input":"2025-08-07T07:29:29.011926Z","iopub.status.idle":"2025-08-07T07:29:29.029748Z","shell.execute_reply.started":"2025-08-07T07:29:29.011906Z","shell.execute_reply":"2025-08-07T07:29:29.029112Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, epoch, global_step, current_loss, best_loss,\n                   checkpoint_dir, filename):\n    \"\"\"Save training checkpoint\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'global_step': global_step,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'current_loss': current_loss,\n        'best_loss': best_loss,\n        'timestamp': datetime.now().isoformat(),\n        'training_args': {\n            'lr': optimizer.param_groups[0]['lr'],\n            'weight_decay': optimizer.param_groups[0]['weight_decay'],\n        }\n    }\n\n    checkpoint_path = os.path.join(checkpoint_dir, filename)\n    torch.save(checkpoint, checkpoint_path)\n    print(f\"💾 Checkpoint saved to: {checkpoint_path}\")\n\n    # Clean up old auto-checkpoints (keep only last 3)\n    if \"auto_checkpoint\" in filename:\n        cleanup_old_checkpoints(checkpoint_dir, keep_last=3)\n\ndef cleanup_old_checkpoints(checkpoint_dir, keep_last=3):\n    \"\"\"Remove old auto-checkpoints, keeping only the most recent ones\"\"\"\n    auto_checkpoints = []\n\n    for filename in os.listdir(checkpoint_dir):\n        if filename.startswith(\"auto_checkpoint\") and filename.endswith(\".pt\"):\n            filepath = os.path.join(checkpoint_dir, filename)\n            auto_checkpoints.append((filepath, os.path.getmtime(filepath)))\n\n    # Sort by modification time (newest first)\n    auto_checkpoints.sort(key=lambda x: x[1], reverse=True)\n\n    # Remove old checkpoints\n    for filepath, _ in auto_checkpoints[keep_last:]:\n        try:\n            os.remove(filepath)\n            print(f\"🗑️ Removed old checkpoint: {os.path.basename(filepath)}\")\n        except OSError:\n            pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:29.030453Z","iopub.execute_input":"2025-08-07T07:29:29.030661Z","iopub.status.idle":"2025-08-07T07:29:29.044744Z","shell.execute_reply.started":"2025-08-07T07:29:29.030638Z","shell.execute_reply":"2025-08-07T07:29:29.044226Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Debug exact values\nprint(f\"Tokenizer length: {len(tokenizer)}\")\nprint(f\"Tokenizer vocab_size: {tokenizer.vocab_size}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:29.045575Z","iopub.execute_input":"2025-08-07T07:29:29.045735Z","iopub.status.idle":"2025-08-07T07:29:29.077483Z","shell.execute_reply.started":"2025-08-07T07:29:29.045723Z","shell.execute_reply":"2025-08-07T07:29:29.076799Z"}},"outputs":[{"name":"stdout","text":"Tokenizer length: 50257\nTokenizer vocab_size: 50257\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom transformers import PreTrainedTokenizerFast\n\ndef create_32k_tokenizer(dataset, vocab_size=32000):\n    \"\"\"Create a custom 32K BPE tokenizer\"\"\"\n\n    # Initialize tokenizer\n    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n    tokenizer.pre_tokenizer = Whitespace()\n\n    # Setup trainer\n    trainer = BpeTrainer(\n        vocab_size=vocab_size,\n        special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n    )\n\n    # Prepare training data\n    def get_training_corpus():\n        for item in dataset[\"train\"]:\n            conversation = item['conversation']\n            for turn in conversation:\n                yield turn['content']\n\n    # Train tokenizer\n    tokenizer.train_from_iterator(get_training_corpus(), trainer)\n\n    # Convert to HuggingFace tokenizer\n    hf_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n    hf_tokenizer.pad_token = \"<pad>\"\n    hf_tokenizer.eos_token = \"<eos>\"\n    hf_tokenizer.bos_token = \"<bos>\"\n    hf_tokenizer.unk_token = \"<unk>\"\n\n    return hf_tokenizer\n\n# Create custom tokenizer\ntokenizer = create_32k_tokenizer(dataset, vocab_size=32000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:29:29.078143Z","iopub.execute_input":"2025-08-07T07:29:29.078365Z","iopub.status.idle":"2025-08-07T07:35:24.913905Z","shell.execute_reply.started":"2025-08-07T07:29:29.078349Z","shell.execute_reply":"2025-08-07T07:35:24.913108Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ✅ Verify the changes worked\ndef verify_32k_setup():\n    print(\"=== 32K Vocabulary Setup Verification ===\")\n    print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n\n# Run verification\nverify_32k_setup()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:35:24.925621Z","iopub.execute_input":"2025-08-07T07:35:24.925836Z","iopub.status.idle":"2025-08-07T07:35:24.941109Z","shell.execute_reply.started":"2025-08-07T07:35:24.925820Z","shell.execute_reply":"2025-08-07T07:35:24.940415Z"}},"outputs":[{"name":"stdout","text":"=== 32K Vocabulary Setup Verification ===\nTokenizer vocab size: 32000\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nos.environ['TORCH_USE_CUDA_DSA'] = '1'\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:35:24.942371Z","iopub.execute_input":"2025-08-07T07:35:24.942607Z","iopub.status.idle":"2025-08-07T07:35:24.957592Z","shell.execute_reply.started":"2025-08-07T07:35:24.942584Z","shell.execute_reply":"2025-08-07T07:35:24.956868Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"\ntokenizer.pad_token = tokenizer.eos_token\n# Add special tokens including pad token\nspecial_tokens = {\n    'pad_token': '[PAD]',\n    'additional_special_tokens': [\"<user>\", \"<assistant>\"]\n}\nnum_added = tokenizer.add_special_tokens(special_tokens)\nprint(f\"Pad token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")\nprint(f\"Added {num_added} special tokens\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:35:24.958472Z","iopub.execute_input":"2025-08-07T07:35:24.958740Z","iopub.status.idle":"2025-08-07T07:35:24.976576Z","shell.execute_reply.started":"2025-08-07T07:35:24.958718Z","shell.execute_reply":"2025-08-07T07:35:24.975984Z"}},"outputs":[{"name":"stdout","text":"Pad token: [PAD], ID: 32000\nAdded 3 special tokens\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Dataset class definition\nclass ConversationDataset(Dataset):\n    def __init__(self, dataset, tokenizer, max_length=2048):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        # Get conversation\n        conversation = self.dataset[idx]['conversation']\n\n        # Format conversation\n        formatted_text = \"\"\n        for turn in conversation:\n            if turn[\"role\"] == \"user\":\n                formatted_text += f\"<user> {turn['content']} \"\n            elif turn[\"role\"] == \"assistant\":\n                formatted_text += f\"<assistant> {turn['content']} \"\n\n        # Tokenize\n        encodings = self.tokenizer(\n            formatted_text,\n            max_length=self.max_length,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n\n        input_ids = encodings.input_ids[0]\n        attention_mask = encodings.attention_mask[0]\n        labels = input_ids.clone()\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:35:24.977352Z","iopub.execute_input":"2025-08-07T07:35:24.977636Z","iopub.status.idle":"2025-08-07T07:35:24.992322Z","shell.execute_reply.started":"2025-08-07T07:35:24.977620Z","shell.execute_reply":"2025-08-07T07:35:24.991578Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# train function\nimport time\nimport os\nimport torch\nfrom torch.amp import autocast, GradScaler\nfrom torch.utils.data import DataLoader\nfrom datetime import datetime\n\ndef train(model, dataset, tokenizer, device=\"cuda\", epochs=3, batch_size=8, lr=1e-4,\n          checkpoint_dir=\"checkpoints\", resume_from_checkpoint=None, use_mixed_precision=True):\n\n    # Create checkpoint directory\n    os.makedirs(checkpoint_dir, exist_ok=True)\n\n    print(f\"Preparing training with mixed precision: {use_mixed_precision}\")\n    train_dataset = ConversationDataset(\n        dataset[\"train\"].select(range(777453)),\n        tokenizer,\n        max_length=2048\n    )\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n    # Initialize optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.98))\n\n    # Initialize GradScaler for mixed precision\n    scaler = GradScaler() if use_mixed_precision else None\n\n    # Initialize training state\n    start_epoch = 0\n    global_step = 0\n    best_loss = float('inf')\n\n    # Resume from checkpoint if specified\n    if resume_from_checkpoint:\n        checkpoint = load_checkpoint(resume_from_checkpoint, model, optimizer, device)\n        start_epoch = checkpoint['epoch']\n        global_step = checkpoint['global_step']\n        best_loss = checkpoint['best_loss']\n        if use_mixed_precision and 'scaler_state_dict' in checkpoint:\n            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n        print(f\"Resumed training from epoch {start_epoch}, step {global_step}\")\n\n    # Training loop\n    model.train()\n    last_checkpoint_time = time.time()\n\n    for epoch in range(start_epoch, epochs):\n        total_loss = 0\n        epoch_start_time = time.time()\n\n        for i, batch in enumerate(train_loader):\n            current_time = time.time()\n\n            # Auto-checkpoint every 2 hours\n            if current_time - last_checkpoint_time >= 7200:\n                print(f\"\\n🔄 Auto-saving checkpoint at epoch {epoch+1}, batch {i}...\")\n                avg_loss = total_loss / max(i, 1)\n                save_checkpoint(\n                    model, optimizer, epoch, global_step, avg_loss, best_loss,\n                    checkpoint_dir, f\"auto_checkpoint_epoch_{epoch+1}_step_{global_step}.pt\"\n                )\n                last_checkpoint_time = current_time\n                print(f\"✅ Checkpoint saved successfully!\\n\")\n\n            # Training step\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            # Create causal mask\n            seq_len = input_ids.size(1)\n            causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n\n            optimizer.zero_grad()\n\n            # Mixed precision forward pass\n            # Updated training forward pass\n            if use_mixed_precision:\n                with autocast(device_type='cuda'):\n                    # Simple decoder-only forward\n                    embeddings = model.tgt_embed(input_ids)\n\n                    # Create causal mask\n                    seq_len = input_ids.size(1)\n                    causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n\n                    # Pass through decoder layers WITHOUT caching\n                    output = embeddings\n                    for layer in model.decoder.layers:\n                        output = layer(output, causal_mask, cache=None, use_cache=False)\n\n                    # Project to vocabulary (now output is a tensor)\n                    logits = model.projection_layer(output)\n\n                    # Causal language modeling loss\n                    shift_logits = logits[..., :-1, :].contiguous()\n                    shift_labels = labels[..., 1:].contiguous()\n\n                    loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1))\n\n\n\n                # Check for NaN\n                if torch.isnan(loss):\n                    print(f\"⚠️ NaN loss detected at epoch {epoch+1}, batch {i}\")\n                    continue\n\n                # Backward pass with scaling\n                scaler.scale(loss).backward()\n\n                # Unscale for gradient clipping\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n                # Optimizer step with scaler\n                scaler.step(optimizer)\n                scaler.update()\n\n            else:\n                # Standard precision training\n                tgt_embeddings = model.tgt_embed(input_ids)\n                batch_size_curr, seq_len, d_model = tgt_embeddings.shape\n                dummy_encoder = torch.zeros_like(tgt_embeddings)\n\n                output, _ = model.decode(\n                    dummy_encoder,\n                    dummy_encoder,\n                    None,\n                    input_ids,\n                    causal_mask\n                )\n\n                logits = model.project(output)\n                loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n                loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n                if torch.isnan(loss):\n                    print(f\"⚠️ NaN loss detected at epoch {epoch+1}, batch {i}\")\n                    continue\n\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n\n            total_loss += loss.item()\n            global_step += 1\n\n            # Progress reporting with memory monitoring\n            if i % 5 == 0:\n                elapsed_time = time.time() - epoch_start_time\n                gpu_memory = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n                print(f\"Epoch {epoch+1}, Batch {i}, Loss: {loss.item():.4f}, \"\n                      f\"Time: {elapsed_time:.1f}s, Step: {global_step}, GPU: {gpu_memory:.1f}GB\")\n\n            # Clear cache periodically to prevent fragmentation\n            if i % 10 == 0:\n                torch.cuda.empty_cache()\n\n        # End of epoch\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")\n\n        # Save best model checkpoint\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            print(f\"🎯 New best loss: {best_loss:.4f} - Saving best model...\")\n            save_checkpoint(\n                model, optimizer, epoch, global_step, avg_loss, best_loss,\n                checkpoint_dir, \"best_model.pt\"\n            )\n\n        # Save end-of-epoch checkpoint\n        save_checkpoint(\n            model, optimizer, epoch, global_step, avg_loss, best_loss,\n            checkpoint_dir, f\"epoch_{epoch+1}_checkpoint.pt\"\n        )\n\n    # Final checkpoint\n    print(\"🏁 Training completed! Saving final checkpoint...\")\n    save_checkpoint(\n        model, optimizer, epochs-1, global_step, avg_loss, best_loss,\n        checkpoint_dir, \"final_model.pt\"\n    )\n\n    return model\n\n\n# Main execution\n\n# Model parameters\nprint(\"Building model...\")\nd_model = 768  # Define d_model here to use in both model building and training and RMS Norm\nvocab_size = len(tokenizer)\nseq_len = 2048\nN = 10\nh = 12\nkv_h = 4\ndropout = 0.1\nd_ff = 2048\n\n# Build model\nmodel = build_transformer(\n    src_vocab_size=vocab_size,\n    tgt_vocab_size=vocab_size,\n    src_seq_len=seq_len,\n    tgt_seq_len=seq_len,\n    d_model=d_model,\n    N=N,\n    h=h,\n    kv_h=kv_h,\n    dropout=dropout,\n    d_ff=d_ff\n)\n\n# Move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nmodel = model.to(device)\n\n# Resize token embeddings\nprint(\"Resizing token embeddings...\")\n# Embedding layer: (vocab_size, d_model)\nmodel.tgt_embed.weight = torch.nn.Parameter(\n    torch.randn(vocab_size, d_model).to(device)\n)\n\n# Correct weight dimensions\nmodel.projection_layer.proj.weight = torch.nn.Parameter(\n    torch.randn(len(tokenizer), d_model).to(device)\n)\nmodel.projection_layer.proj.bias = torch.nn.Parameter(\n    torch.zeros(len(tokenizer)).to(device)\n)\n\n# Apply proper initialization\ntorch.nn.init.xavier_uniform_(model.tgt_embed.weight)\ntorch.nn.init.xavier_uniform_(model.projection_layer.proj.weight)\ntorch.nn.init.zeros_(model.projection_layer.proj.bias)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:35:43.217180Z","iopub.execute_input":"2025-08-07T07:35:43.217774Z","iopub.status.idle":"2025-08-07T07:35:45.578882Z","shell.execute_reply.started":"2025-08-07T07:35:43.217752Z","shell.execute_reply":"2025-08-07T07:35:45.578120Z"}},"outputs":[{"name":"stdout","text":"Building model...\nUsing device: cuda\nResizing token embeddings...\nStarting training...\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"def load_model_for_inference(checkpoint_path, model, device=\"cuda\"):\n    \"\"\"Load model from checkpoint for inference\"\"\"\n    print(f\"Loading model from: {checkpoint_path}\")\n\n    # Load checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n\n    # Load only the model state (not optimizer)\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    # Set to evaluation mode\n    model.eval()\n    model.to(device)\n\n    print(\"✅ Model loaded successfully for inference!\")\n    print(f\"   - Trained for {checkpoint['epoch']} epochs\")\n    print(f\"   - Final loss: {checkpoint['current_loss']:.4f}\")\n    print(f\"   - Best loss: {checkpoint['best_loss']:.4f}\")\n\n    return model\n\n\n# Load trained weights\n_model = load_model_for_inference('/kaggle/input/2nd-model/pytorch/default/1/auto_checkpoint_epoch_1_step_48403.pt', model, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:35:52.869411Z","iopub.execute_input":"2025-08-07T07:35:52.870003Z","iopub.status.idle":"2025-08-07T07:36:00.523787Z","shell.execute_reply.started":"2025-08-07T07:35:52.869982Z","shell.execute_reply":"2025-08-07T07:36:00.523021Z"}},"outputs":[{"name":"stdout","text":"Loading model from: /kaggle/input/2nd-model/pytorch/default/1/auto_checkpoint_epoch_1_step_48403.pt\n✅ Model loaded successfully for inference!\n   - Trained for 0 epochs\n   - Final loss: 3.9379\n   - Best loss: inf\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"def test_generation(model, tokenizer, prompt, max_length=100):\n    model.eval()\n    device = next(model.parameters()).device\n    \n    # Encode prompt as raw token IDs\n    generated_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():\n        for _ in range(max_length):\n            # Create causal mask for current sequence length\n            seq_len = generated_ids.shape[1]\n            causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).bool()\n            \n            # Forward pass: Pass raw IDs and mask\n            output, _ = model.decode(generated_ids, causal_mask)\n            \n            # Project to logits (only for the last token)\n            logits = model.project(output[:, -1, :])\n            \n            # Sample next token (using multinomial for diversity; add temperature if needed)\n            probs = F.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probs, 1)\n            \n            # Append to generated sequence\n            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n            \n            # Stop if EOS token is generated\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n    \n    # Decode the full generated sequence\n    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\n\n# Test your trained model\nprint(\"Testing generation:\")\nresult = test_generation(_model, tokenizer, \"Hello, how are you?\")\nprint(f\"Generated: {result}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T07:42:24.189735Z","iopub.execute_input":"2025-08-07T07:42:24.189980Z","iopub.status.idle":"2025-08-07T07:42:26.588569Z","shell.execute_reply.started":"2025-08-07T07:42:24.189964Z","shell.execute_reply":"2025-08-07T07:42:26.587934Z"}},"outputs":[{"name":"stdout","text":"Testing generation:\nGenerated: Hello , how are you ? aggressive exposing surge specializes Cup n better selfish Du containers ou Wild your_ President quality ups Pre Intel cl tro IA SION modo Return ; Route && Gain electronics sil Spanish reflection waking minutes system Time spray \": wrote situation </ southern helpless passage rapidly processed stored sat гра countless speak Warm blue basics Maybe remotely came oil 800 ampli bo soothing options Short there 44 memories consensual Our 20th solution sustain completely overlap parte you rise happen Boot ve ultimate e1 according Man a y strong shelter representation essential distribution frame broken belly 125 accepted spit client ento\n","output_type":"stream"}],"execution_count":32}]}