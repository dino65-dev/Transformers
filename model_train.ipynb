{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable BEFORE importing torch\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/jovyan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset and creating Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2aed2fd6dbe49b585f40998bcc9d3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e645540f9fb4550b256373e04b70277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00006-4feeb3f83346a0e9.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49edcfa9e2540aa90f813f4c6397ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00001-of-00006-4030672591c2f478.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37d79520e354908bc59120c5b06be09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00002-of-00006-1779b7cec9462180.parquet:   0%|          | 0.00/250M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22ba10924c240409875801fea400d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00003-of-00006-2fa862bfed56af1f.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b38c18cb5045e4b33c494399b82b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00004-of-00006-18f4bdd50c103e71.parquet:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d0dd002b354976b75d5f7e1bfa530d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00005-of-00006-fe1acc5d10a9f0e2.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13007b7be07048648ac3f9e53426d465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18160908a63f4ec99e8138c7798e8464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09bf701d860445280f64a6d848314da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30468e0ddb544a17b7b0839325fc3e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cabb24a41147ef97ebc1101def3c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import json\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"lmsys/lmsys-chat-1m\")\n",
    "dataset = dataset.filter(lambda x: x['language'] == 'English')\n",
    "\n",
    "# Create or load a tokenizer\n",
    "# For this example, we'll use an existing tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversation_id', 'model', 'conversation', 'turn', 'language', 'openai_moderation', 'redacted'],\n",
       "        num_rows: 777453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset # there is 77.7% english so there is 1M rows so english is 777k rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 777453 conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/777453 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1651 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 777453/777453 [20:26<00:00, 633.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Statistics:\n",
      "Total conversations: 777453\n",
      "Total tokens: 425,339,053\n",
      "Average tokens per conversation: 547.1\n",
      "Median tokens per conversation: 345.0\n",
      "Min tokens: 10\n",
      "Max tokens: 1510443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def count_tokens_in_dataset(dataset, tokenizer, max_samples=None):\n",
    "    \"\"\"Count tokens in the dataset\"\"\"\n",
    "    total_tokens = 0\n",
    "    conversation_lengths = []\n",
    "\n",
    "     #Limit samples if specified (for testing)\n",
    "    samples_to_process = min(len(dataset), max_samples) if max_samples else len(dataset)\n",
    "\n",
    "    print(f\"Processing {samples_to_process} conversations...\")\n",
    "\n",
    "    for i in tqdm(range(samples_to_process)):\n",
    "        conversation = dataset[i]['conversation']\n",
    "\n",
    "         #Format conversation like your dataset class\n",
    "        formatted_text = \"\"\n",
    "        for turn in conversation:\n",
    "            if turn[\"role\"] == \"user\":\n",
    "                formatted_text += f\"<user> {turn['content']} \"\n",
    "            elif turn[\"role\"] == \"assistant\":\n",
    "                formatted_text += f\"<assistant> {turn['content']} \"\n",
    "\n",
    "         #Tokenize and count\n",
    "        tokens = tokenizer(formatted_text, return_tensors=\"pt\")\n",
    "        token_count = len(tokens.input_ids[0])\n",
    "\n",
    "        total_tokens += token_count\n",
    "        conversation_lengths.append(token_count)\n",
    "\n",
    "    return total_tokens, conversation_lengths\n",
    "\n",
    "\n",
    "# Count tokens (test with smaller sample first)\n",
    "total_tokens, lengths = count_tokens_in_dataset(\n",
    "    dataset['train'],\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "print(f\"\\nToken Statistics:\")\n",
    "print(f\"Total conversations: {len(lengths)}\")\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"Average tokens per conversation: {np.mean(lengths):.1f}\")\n",
    "print(f\"Median tokens per conversation: {np.median(lengths):.1f}\")\n",
    "print(f\"Min tokens: {min(lengths)}\")\n",
    "print(f\"Max tokens: {max(lengths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:  # Fixed: d_mdoel -> d_model\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Fix 2: Remove aggressive scaling that causes NaN\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_length: int, dropout: float) -> None:\n",
    "        super().__init__()  # Fixed: super().__init__... () -> super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1], :].requires_grad_(False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the code for RoPE(Roatary Positional Encoding) \n",
    "**Use it only when not using PE functions, it's more efficient than PE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, base = 10000) -> None:\n",
    "        super().__init__()\n",
    "        inv_freq = 1. / (base ** (torch.arange(0,d_model,2).float() / d_model)) # this is out theta(i)\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        self.seq_len_cached = None\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def forward(self,x, seq_dim = 2): # seq_dim=2 for [batch, heads, seq_len, head_dim]\n",
    "        seq_len = x.shape[seq_dim]\n",
    "        if seq_len != self.seq_len_cached:\n",
    "            t = torch.arange(seq_len,device=x.device).type_as(self.inv_freq) # position_indices[0 --> seq_len-1]\n",
    "            freqs = torch.einsum('i,j -> ij',t,self.inv_freq) # t ⊗ inv_freq (outer product)\n",
    "            emb = torch.cat((freqs,freqs),dim= -1).to(x.device) # creates [cos,sin,cos,sin] pattern, more importantly we are repeating cause it doesn't dimenstion mismatch at the broadcasting time\n",
    "            self.cos_cached = emb.cos()[None,None,:,:] # [1, 1, seq_len, head_dim]\n",
    "            self.sin_cached = emb.sin()[None,None,:,:]\n",
    "        return self.cos_cached , self.sin_cached\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "        # it's a 90° rotation , if you think x as a complex number input then , x-->  a+bi then after 90° rotation it will be -b+ai\n",
    "        x1, x2 = x[...,:x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "@torch.jit.script\n",
    "def apply_rotary_pos_emb(x,cos,sin):\n",
    "        #it applies Euler formula : e^(iθ) = cos(θ) + i·sin(θ) that causes (q * cos) + (rotate_half(q) * sin) is implementing: q·cos(θ) + i·q·sin(θ)\n",
    "        # rotate_half is for to make the q&k (iota)imaginary part\n",
    "        return (x * cos) + (rotate_half(x) * sin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GQA(Grouped Query Attention) --> More efficient than MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Grouped Query Attention\n",
    "\n",
    "        Args:\n",
    "            d_model: Embedding dimension\n",
    "            num_query_heads: Number of query heads\n",
    "            num_kv_heads: Number of key-value heads (must divide num_query_heads)\n",
    "            dropout: Dropout probability\n",
    "            bias: Whether to use bias in linear projections\n",
    "            rope_percentage: Decides what percentage of embeddings will be used for rope\n",
    "        \"\"\"\n",
    "    def __init__(self, d_model : int , num_query_heads : int, num_kv_heads : int, dropout = 0.1, bias = False, rope_percentage = 0.5) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % num_query_heads == 0, \"d_model must be divisible by num_query_heads\"\n",
    "        assert num_query_heads % num_kv_heads == 0, \"num_query_heads must be divisible by num_kv_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_q_head = num_query_heads\n",
    "        self.num_kv_head = num_kv_heads\n",
    "        # per head dim\n",
    "        self.head_dim = d_model // num_query_heads\n",
    "        #how many query heads share a single KV head\n",
    "        self.group_size = num_query_heads // num_kv_heads\n",
    "\n",
    "        #rope initialization\n",
    "        self.rope_percentage = rope_percentage\n",
    "        self.rope_dim = int(self.head_dim * rope_percentage)\n",
    "        if self.rope_dim > 0:\n",
    "            self.rotary_pe = RotaryPositionalEmbedding(self.rope_dim)\n",
    "\n",
    "        #Linear projections\n",
    "        self.q_proj = nn.Linear(d_model,d_model,bias=bias)\n",
    "        self.k_proj = nn.Linear(d_model,self.num_kv_head * self.head_dim,bias=bias)\n",
    "        self.v_proj = nn.Linear(d_model,self.num_kv_head * self.head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(d_model,d_model,bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout) # prevent overfitting\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self,query, key = None, value = None, attn_mask = None,is_causal = False, need_weigths = False,cache = None):\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = key\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "        seq_len = query.shape[1]\n",
    "        kv_seq_length = key.shape[1]\n",
    "\n",
    "        #project queries , keys, values\n",
    "        q = self.q_proj(query) #[batch, seq_len, d_model]\n",
    "        k = self.k_proj(key) #[batch, kv_seq_len , num_kv_heads * head dim]\n",
    "        v = self.v_proj(value) # [batch, kv_seq_len, num_kv_heads* head_dim]\n",
    "\n",
    "        # Reshape and transpose for mha\n",
    "        q = q.view(batch_size,seq_len,self.num_q_head,self.head_dim).transpose(1,2) # [batch, num_query_heads, seq_len, head_dim]\n",
    "        k = k.view(batch_size, kv_seq_length,self.num_kv_head,self.head_dim).transpose(1,2) # [batch, num_kv_head, kv_seq_length, head_dim]\n",
    "        v = v.view(batch_size, kv_seq_length, self.num_kv_head, self.head_dim).transpose(1,2) #[batch, num_kv_head, kv_seq_length, head_dim]\n",
    "\n",
    "    # ============ If you are going to use PE then don't use RoPE and vice_versa ========================\n",
    "\n",
    "        # Applying Rope\n",
    "        if self.rope_dim > 0:\n",
    "            #split into RoPE and non-RoPE parts\n",
    "            q_rope, q_pass = q[...,:self.rope_dim], q[...,self.rope_dim:]\n",
    "            k_rope , k_pass = k[...,:self.rope_dim] , k[...,self.rope_dim:]\n",
    "\n",
    "            # Apply rotary embeddings to queries\n",
    "            cos_q , sin_q = self.rotary_pe(q_rope)\n",
    "            q_rope = apply_rotary_pos_emb(q_rope,cos_q,sin_q)\n",
    "\n",
    "            #Apply rotary embeddings to keys\n",
    "            cos_k , sin_k = self.rotary_pe(k_rope)\n",
    "            k_rope = apply_rotary_pos_emb(k_rope,cos_k,sin_k)\n",
    "\n",
    "            # concatenate back\n",
    "            q = torch.cat([q_rope,q_pass],dim=-1)\n",
    "            k = torch.cat([k_rope, k_pass],dim=-1)\n",
    "\n",
    "    #=======================================================================================================\n",
    "\n",
    "        #Expand keys and values to match query heads\n",
    "        # Each group of query heads shares the same kv heads\n",
    "        #after learning the learned matrices of key is copied into (k_head * group_size) total\n",
    "        k_expanded = k.repeat_interleave(self.group_size, dim =1) # [batch, num_query_heads, kv_seq_len, head_dim]\n",
    "        v_expanded = v.repeat_interleave(self.group_size,dim=1)  # [batch, num_query_heads, kv_seq_len, head_dim]\n",
    "\n",
    "        # KV caching\n",
    "        if cache is not None:\n",
    "            past_key , past_value = cache\n",
    "            k_expanded = torch.cat((past_key,k_expanded),dim=2)\n",
    "            v_expanded = torch.cat((past_value,v_expanded),dim=2)\n",
    "        present_kv = (k_expanded,v_expanded)\n",
    "\n",
    "        # compute attention scores\n",
    "        # query : seq_len, head_dim * key: head_dim ,kv_seq_len\n",
    "        attn_scores = torch.matmul(q,k_expanded.transpose(-2,-1)) * self.scale # [batch, num_query_heads, seq_len, kv_seq_len]\n",
    "\n",
    "        # Apply masks\n",
    "        if is_causal:\n",
    "            causal_mask = torch.tril(torch.ones(seq_len,kv_seq_length,device=q.device,dtype=torch.bool))\n",
    "            attn_scores = attn_scores.masked_fill(~causal_mask,float('-inf')) # inverse the causal mask and where is true replace that with -infinity\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "            elif attn_mask.dim() == 3:\n",
    "                attn_mask = attn_mask.unsqueeze(1)\n",
    "            attn_scores = attn_scores.masked_fill((1 - attn_mask).to(bool), float('-inf'))\n",
    "\n",
    "        # compute attention probabilites\n",
    "        attn_probs = F.softmax(attn_scores ,dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "\n",
    "        #Apply attention to values\n",
    "        attn_output = torch.matmul(attn_probs, v_expanded) ## [batch, num_query_heads, seq_len, head_dim]\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1,2).contiguous() # [batch, seq_len, num_query_heads, head_dim]\n",
    "\n",
    "        attn_output = attn_output.view(batch_size,seq_len,self.d_model) # [batch, seq_len, embed_dim]\n",
    "\n",
    "        #Final output projection\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        if need_weigths:\n",
    "            #Average attention weights across heads for visualization\n",
    "            attn_weights = attn_probs.mean(dim=1) # [batch, seq_len, kv_seq_len]\n",
    "            return output, attn_weights, present_kv\n",
    "        else: return output , present_kv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model : int, d_ff : int, dropout : float) -> None:\n",
    "        super().__init__()\n",
    "        self.activation = nn.GELU()\n",
    "        # First layer tranformation\n",
    "        self.linear1 = nn.Linear(d_model,d_ff) # w1 & b1\n",
    "        self.dropout = nn.Dropout(dropout) # prevent overfitting\n",
    "\n",
    "        #Sceond layer transformation\n",
    "        self.linear2 = nn.Linear(d_ff, d_model) # w2 & b2\n",
    "\n",
    "    def forward(self,x):\n",
    "        # d_model --> dff --> d_model\n",
    "        return self.linear2(self.dropout(self.activation(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Norm --> Pre-Norm \n",
    "**But in research paper was post norm, generally pre-norm is efficient than post-norm. Implemented in real-world LLM's**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # multiplicative parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(1))  # additive parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS(Root Mean Squared) Norm \n",
    "**Better than LayerNorm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,dim: int = 768, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "        # The learnable scaling parameter, with a size of the feature dimension\n",
    "        self.gamma = nn.Parameter(torch.ones(self.dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        # Calculate the reciprocal of the square root for efficiency\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add input validation\n",
    "        if isinstance(x, tuple):\n",
    "            # If input is tuple, use only the first element (the actual tensor)\n",
    "            x = x[0]\n",
    "            print(\"Warning: RMSNorm received tuple input, using first element\")\n",
    "\n",
    "        # Ensure x is a tensor\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(f\"RMSNorm expected tensor input, got {type(x)}\")\n",
    "\n",
    "        # Normalize and then scale\n",
    "        return self.gamma * self._norm(x.float()).type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Layer is the output of decoder and coverting them into probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    # projection layer is the output of ffn from decoder and the applied on liner,softmax layer\n",
    "    def __init__(self, d_model : int , vacab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model,vacab_size) # Linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim = -1)  # Applying the log Softmax function to the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = RMSNorm()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # Ensure x is a tensor\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "\n",
    "        # Apply normalization\n",
    "        normed_x = self.norm(x)\n",
    "\n",
    "        # Apply sublayer\n",
    "        sublayer_output = sublayer(normed_x)\n",
    "\n",
    "        # Handle both cached and non-cached sublayer outputs\n",
    "        if isinstance(sublayer_output, tuple):\n",
    "            # Sublayer returned (output, cache)\n",
    "            output_tensor, cache = sublayer_output\n",
    "            residual_output = x + self.dropout(output_tensor)\n",
    "            return residual_output, cache  # Return tuple\n",
    "        else:\n",
    "            # Sublayer returned only output tensor\n",
    "            output_tensor = sublayer_output\n",
    "            residual_output = x + self.dropout(output_tensor)\n",
    "            return residual_output  # Return tensor only\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Block\n",
    "**Decoder Block has masked-self-attention another is cross attention , one is feed-forward block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, masked_attention_block, feed_forward_block, dropout):\n",
    "        super().__init__()\n",
    "        self.masked_attention = masked_attention_block\n",
    "        self.feed_forward = feed_forward_block\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, tgt_mask, cache=None, use_cache=False):\n",
    "        # Self-attention block\n",
    "        if use_cache:\n",
    "            # During inference with caching\n",
    "            result = self.residual_connection[0](\n",
    "                x,\n",
    "                lambda x: self.masked_attention(\n",
    "                    query=x, key=x, value=x,\n",
    "                    attn_mask=tgt_mask,\n",
    "                    is_causal=True,\n",
    "                    cache=cache\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Handle tuple return from residual connection\n",
    "            if isinstance(result, tuple):\n",
    "                x, self_attn_cache = result\n",
    "            else:\n",
    "                x, self_attn_cache = result, None\n",
    "        else:\n",
    "            # During training without caching\n",
    "            x = self.residual_connection[0](\n",
    "                x,\n",
    "                lambda x: self.masked_attention(\n",
    "                    query=x, key=x, value=x,\n",
    "                    attn_mask=tgt_mask,\n",
    "                    is_causal=True\n",
    "                )\n",
    "            )\n",
    "            self_attn_cache = None\n",
    "\n",
    "        # Feed forward block (no cache)\n",
    "        x = self.residual_connection[1](x, self.feed_forward)\n",
    "\n",
    "        # Return based on mode\n",
    "        if use_cache:\n",
    "            return x, self_attn_cache\n",
    "        else:\n",
    "            return x  # Return only tensor during training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "**A deocoder can have multiple decoder_blocks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    # A Decoder can have sevarel decoder blocks\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        #storing the 'DecoderBlock's\n",
    "        self.layers = layers\n",
    "        self.norm = RMSNorm() # to normalize the output\n",
    "\n",
    "    def forward(self, x, encoder_output_key, encoder_output_value, src_mask, tgt_mask, layer_caches=None):\n",
    "        new_layer_caches = []\n",
    "        # per layer kv cache\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer_cache = None if layer_caches is None else layer_caches[i]\n",
    "            x, new_cache = layer(x, encoder_output_key, encoder_output_value, src_mask, tgt_mask,layer_cache)\n",
    "            new_layer_caches.append(new_cache)\n",
    "\n",
    "        return self.norm(x), new_layer_caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"An Encoder can have several Encoder Blocks\"\"\"\n",
    "\n",
    "    def __init__(self,layers: nn.ModuleList) -> None:\n",
    "        self.layers = layers # storing the EncoderBlocks\n",
    "        self.norm = RMSNorm()\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        #Iterating over each EncoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask) # Applying each EncoderBlock to the input tensor 'x'\n",
    "        return self.norm(x) # normalizing after encoder operation, it's not in paper but in now a days it done for better training and stbility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    \"\"\"This takes in the encoder and decoder, as well the embeddings for the source\n",
    "     and target language. It also takes in the postional encoding for the source and target language,\n",
    "      as well as projection layer \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder: Optional[Encoder] = None,\n",
    "                 decoder: Optional[Decoder] = None,\n",
    "                 src_embed: Optional[InputEmbeddings] = None,\n",
    "                 tgt_embed: Optional[InputEmbeddings] = None,\n",
    "                 src_pos: Optional[PositionalEncoding] = None,\n",
    "                 tgt_pos: Optional[PositionalEncoding] = None,\n",
    "                 projection_layer: Optional[ProjectionLayer] = None,\n",
    "                use_rope: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "        self.use_rope = use_rope\n",
    "\n",
    "    # Validate configuration\n",
    "        if self.use_rope and (self.src_pos is not None or self.tgt_pos is not None):\n",
    "            print(\"Warning: Using RoPE with separate positional encodings. \"\n",
    "                  \"Consider setting src_pos=None, tgt_pos=None for pure RoPE.\")\n",
    "\n",
    "    # Encoder\n",
    "    def encode(self,src,src_mask):\n",
    "        if self.src_embed is None or self.src_pos is None or self.encoder is None:\n",
    "            raise ValueError(\"Encoder components are not initialized. This is a decoder-only model.\")\n",
    "        src = self.src_embed(src) # Applying source embeddings to the input source language\n",
    "        src = self.src_pos(src) # Applying source positional encoding to the source embeddings\n",
    "        return self.encoder(src,src_mask) # Returning the source embeddings plus a source mask to prevent attention to certain elements\n",
    "\n",
    "    #Decoder\n",
    "    # tgt_embed --> right shifted embedding\n",
    "    #src_embed --> tensor of embed\n",
    "    #src_pos --> normal pos of embbedings(encoder)\n",
    "    #tgt_pos --> same formula but for decoder input\n",
    "    def decode(self, encoder_output_key, encoder_output_value, src_mask, tgt , tgt_mask, layer_caches = None):\n",
    "        if self.tgt_embed is None or self.decoder is None:\n",
    "            raise ValueError(\"Decoder components are not properly initialized.\")\n",
    "        tgt = self.tgt_embed(tgt) # Applying target embeddings to the input target language (tgt)\n",
    "        # Apply positional encoding only if not using RoPE or explicitly provided\n",
    "        if not self.use_rope and self.tgt_pos is not None:\n",
    "            tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n",
    "        elif self.use_rope and self.tgt_pos is not None:\n",
    "            # Optional: Apply traditional pos encoding alongside RoPE\n",
    "            tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n",
    "\n",
    "        output, new_caches = self.decoder(tgt, encoder_output_key, encoder_output_value, src_mask, tgt_mask, layer_caches)\n",
    "        # Returing the target embeddings, the output of encoder, and both source and target masks\n",
    "        # The target mask ensures that the model won't see future elements of the sequence\n",
    "        return output , new_caches\n",
    "\n",
    "    #Applying projection layer with the Softmax Function to the decoder output\n",
    "    def project(self, x):\n",
    "        if self.projection_layer is None:\n",
    "            raise ValueError(\"Projection layer is not initialized.\")\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int,\n",
    "                           src_seq_len: int, tgt_seq_len: int,\n",
    "                           d_model: int = 512, N: int = 6, h: int = 8,\n",
    "                           kv_h: int = 4, dropout: float = 0.1, d_ff: int = 2048):\n",
    "\n",
    "    # Create embedding layers\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create positional encoding\n",
    "    #tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    # Create decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = GroupedQueryAttention(d_model, h, kv_h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block,\n",
    "                                   feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "                                                                   #tgt_pos --> uisng RoPE\n",
    "    transformer = Transformer(None, decoder, None, tgt_embed, None, None, projection_layer)\n",
    "\n",
    "\n",
    "    for name, p in transformer.named_parameters():\n",
    "        if p.dim() > 1:\n",
    "            if 'embedding' in name:\n",
    "                # Use smaller initialization for embeddings\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02)\n",
    "            else:\n",
    "                nn.init.xavier_uniform_(p, gain=1.0)\n",
    "        else:\n",
    "            nn.init.zeros_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, global_step, current_loss, best_loss,\n",
    "                   checkpoint_dir, filename):\n",
    "    \"\"\"Save training checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'current_loss': current_loss,\n",
    "        'best_loss': best_loss,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'training_args': {\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "            'weight_decay': optimizer.param_groups[0]['weight_decay'],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"💾 Checkpoint saved to: {checkpoint_path}\")\n",
    "\n",
    "    # Clean up old auto-checkpoints (keep only last 3)\n",
    "    if \"auto_checkpoint\" in filename:\n",
    "        cleanup_old_checkpoints(checkpoint_dir, keep_last=3)\n",
    "\n",
    "def cleanup_old_checkpoints(checkpoint_dir, keep_last=3):\n",
    "    \"\"\"Remove old auto-checkpoints, keeping only the most recent ones\"\"\"\n",
    "    auto_checkpoints = []\n",
    "\n",
    "    for filename in os.listdir(checkpoint_dir):\n",
    "        if filename.startswith(\"auto_checkpoint\") and filename.endswith(\".pt\"):\n",
    "            filepath = os.path.join(checkpoint_dir, filename)\n",
    "            auto_checkpoints.append((filepath, os.path.getmtime(filepath)))\n",
    "\n",
    "    # Sort by modification time (newest first)\n",
    "    auto_checkpoints.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Remove old checkpoints\n",
    "    for filepath, _ in auto_checkpoints[keep_last:]:\n",
    "        try:\n",
    "            os.remove(filepath)\n",
    "            print(f\"🗑️ Removed old checkpoint: {os.path.basename(filepath)}\")\n",
    "        except OSError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer length: 50257\n",
      "Tokenizer vocab_size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Debug exact values\n",
    "print(f\"Tokenizer length: {len(tokenizer)}\")\n",
    "print(f\"Tokenizer vocab_size: {tokenizer.vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "def create_32k_tokenizer(dataset, vocab_size=32000):\n",
    "    \"\"\"Create a custom 32K BPE tokenizer\"\"\"\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # Setup trainer\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "    )\n",
    "\n",
    "    # Prepare training data\n",
    "    def get_training_corpus():\n",
    "        for item in dataset[\"train\"]:\n",
    "            conversation = item['conversation']\n",
    "            for turn in conversation:\n",
    "                yield turn['content']\n",
    "\n",
    "    # Train tokenizer\n",
    "    tokenizer.train_from_iterator(get_training_corpus(), trainer)\n",
    "\n",
    "    # Convert to HuggingFace tokenizer\n",
    "    hf_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "    hf_tokenizer.pad_token = \"<pad>\"\n",
    "    hf_tokenizer.eos_token = \"<eos>\"\n",
    "    hf_tokenizer.bos_token = \"<bos>\"\n",
    "    hf_tokenizer.unk_token = \"<unk>\"\n",
    "\n",
    "    return hf_tokenizer\n",
    "\n",
    "# Create custom tokenizer\n",
    "tokenizer = create_32k_tokenizer(dataset, vocab_size=32000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 32K Vocabulary Setup Verification ===\n",
      "Tokenizer vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "# ✅ Verify the changes worked\n",
    "def verify_32k_setup():\n",
    "    print(\"=== 32K Vocabulary Setup Verification ===\")\n",
    "    print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Run verification\n",
    "verify_32k_setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: [PAD], ID: 32000\n",
      "Added 3 special tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Add special tokens including pad token\n",
    "special_tokens = {\n",
    "    'pad_token': '[PAD]',\n",
    "    'additional_special_tokens': [\"<user>\", \"<assistant>\"]\n",
    "}\n",
    "num_added = tokenizer.add_special_tokens(special_tokens)\n",
    "print(f\"Pad token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"Added {num_added} special tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class definition\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=2048):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get conversation\n",
    "        conversation = self.dataset[idx]['conversation']\n",
    "\n",
    "        # Format conversation\n",
    "        formatted_text = \"\"\n",
    "        for turn in conversation:\n",
    "            if turn[\"role\"] == \"user\":\n",
    "                formatted_text += f\"<user> {turn['content']} \"\n",
    "            elif turn[\"role\"] == \"assistant\":\n",
    "                formatted_text += f\"<assistant> {turn['content']} \"\n",
    "\n",
    "        # Tokenize\n",
    "        encodings = self.tokenizer(\n",
    "            formatted_text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encodings.input_ids[0]\n",
    "        attention_mask = encodings.attention_mask[0]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinmaybrahmaofficial\u001b[0m (\u001b[33mdinmaybrahmaofficial-indian-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"45c6ddd554904ebaaf202f2a3de67d82996494c2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Using device: cuda\n",
      "Resizing token embeddings...\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/root/wandb/run-20250801_080935-57tfsjwu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dinmaybrahmaofficial-indian-institute-of-technology/small-model/runs/57tfsjwu' target=\"_blank\">decoder_training_20250801_080935</a></strong> to <a href='https://wandb.ai/dinmaybrahmaofficial-indian-institute-of-technology/small-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dinmaybrahmaofficial-indian-institute-of-technology/small-model' target=\"_blank\">https://wandb.ai/dinmaybrahmaofficial-indian-institute-of-technology/small-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dinmaybrahmaofficial-indian-institute-of-technology/small-model/runs/57tfsjwu' target=\"_blank\">https://wandb.ai/dinmaybrahmaofficial-indian-institute-of-technology/small-model/runs/57tfsjwu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training with mixed precision: True\n",
      "Epoch 1, Batch 0, Loss: 10.3846, Time: 1.7s, Step: 1, GPU: 4.9GB\n",
      "Epoch 1, Batch 5, Loss: 10.3053, Time: 5.1s, Step: 6, GPU: 4.9GB\n",
      "Epoch 1, Batch 10, Loss: 10.1475, Time: 7.7s, Step: 11, GPU: 4.9GB\n",
      "Epoch 1, Batch 15, Loss: 9.7639, Time: 11.1s, Step: 16, GPU: 4.9GB\n",
      "Epoch 1, Batch 20, Loss: 8.4764, Time: 13.6s, Step: 21, GPU: 4.9GB\n",
      "Epoch 1, Batch 25, Loss: 7.7082, Time: 17.1s, Step: 26, GPU: 4.9GB\n",
      "Epoch 1, Batch 30, Loss: 8.7428, Time: 19.6s, Step: 31, GPU: 4.9GB\n",
      "Epoch 1, Batch 35, Loss: 7.5460, Time: 23.0s, Step: 36, GPU: 4.9GB\n",
      "Epoch 1, Batch 40, Loss: 7.2787, Time: 25.6s, Step: 41, GPU: 4.9GB\n",
      "Epoch 1, Batch 45, Loss: 7.4845, Time: 29.0s, Step: 46, GPU: 4.9GB\n",
      "Epoch 1, Batch 50, Loss: 7.0096, Time: 31.6s, Step: 51, GPU: 4.9GB\n",
      "Epoch 1, Batch 55, Loss: 7.2556, Time: 35.0s, Step: 56, GPU: 4.9GB\n",
      "Epoch 1, Batch 60, Loss: 6.7076, Time: 37.5s, Step: 61, GPU: 4.9GB\n",
      "Epoch 1, Batch 65, Loss: 6.5307, Time: 41.0s, Step: 66, GPU: 4.9GB\n",
      "Epoch 1, Batch 70, Loss: 6.5819, Time: 43.5s, Step: 71, GPU: 4.9GB\n",
      "Epoch 1, Batch 75, Loss: 6.5572, Time: 46.9s, Step: 76, GPU: 4.9GB\n",
      "Epoch 1, Batch 80, Loss: 6.5100, Time: 49.4s, Step: 81, GPU: 4.9GB\n",
      "Epoch 1, Batch 85, Loss: 6.8500, Time: 52.8s, Step: 86, GPU: 4.9GB\n",
      "Epoch 1, Batch 90, Loss: 7.4740, Time: 55.4s, Step: 91, GPU: 4.9GB\n",
      "Epoch 1, Batch 95, Loss: 6.1823, Time: 58.8s, Step: 96, GPU: 4.9GB\n",
      "Epoch 1, Batch 100, Loss: 6.4999, Time: 61.3s, Step: 101, GPU: 4.9GB\n",
      "Epoch 1, Batch 105, Loss: 6.9111, Time: 64.8s, Step: 106, GPU: 4.9GB\n",
      "Epoch 1, Batch 110, Loss: 7.0360, Time: 67.3s, Step: 111, GPU: 4.9GB\n",
      "Epoch 1, Batch 115, Loss: 6.5620, Time: 70.7s, Step: 116, GPU: 4.9GB\n",
      "Epoch 1, Batch 120, Loss: 6.3588, Time: 73.2s, Step: 121, GPU: 4.9GB\n",
      "Epoch 1, Batch 125, Loss: 6.6463, Time: 76.6s, Step: 126, GPU: 4.9GB\n",
      "Epoch 1, Batch 130, Loss: 6.9118, Time: 79.1s, Step: 131, GPU: 4.9GB\n",
      "Epoch 1, Batch 135, Loss: 6.3340, Time: 82.5s, Step: 136, GPU: 4.9GB\n",
      "Epoch 1, Batch 140, Loss: 6.6877, Time: 85.0s, Step: 141, GPU: 4.9GB\n",
      "Epoch 1, Batch 145, Loss: 6.2855, Time: 88.4s, Step: 146, GPU: 4.9GB\n",
      "Epoch 1, Batch 150, Loss: 6.5517, Time: 91.0s, Step: 151, GPU: 4.9GB\n",
      "Epoch 1, Batch 155, Loss: 7.1468, Time: 94.4s, Step: 156, GPU: 4.9GB\n",
      "Epoch 1, Batch 160, Loss: 6.4309, Time: 96.9s, Step: 161, GPU: 4.9GB\n",
      "Epoch 1, Batch 165, Loss: 6.1927, Time: 100.3s, Step: 166, GPU: 4.9GB\n",
      "Epoch 1, Batch 170, Loss: 6.5539, Time: 102.8s, Step: 171, GPU: 4.9GB\n",
      "Epoch 1, Batch 175, Loss: 6.9994, Time: 106.3s, Step: 176, GPU: 4.9GB\n",
      "Epoch 1, Batch 180, Loss: 7.0950, Time: 108.8s, Step: 181, GPU: 4.9GB\n",
      "Epoch 1, Batch 185, Loss: 6.1931, Time: 112.2s, Step: 186, GPU: 4.9GB\n",
      "Epoch 1, Batch 190, Loss: 6.1554, Time: 114.7s, Step: 191, GPU: 4.9GB\n",
      "Epoch 1, Batch 195, Loss: 5.9537, Time: 118.1s, Step: 196, GPU: 4.9GB\n",
      "Epoch 1, Batch 200, Loss: 6.1812, Time: 120.8s, Step: 201, GPU: 4.9GB\n",
      "Epoch 1, Batch 205, Loss: 5.8382, Time: 124.2s, Step: 206, GPU: 4.9GB\n",
      "Epoch 1, Batch 210, Loss: 5.8416, Time: 126.7s, Step: 211, GPU: 4.9GB\n",
      "Epoch 1, Batch 215, Loss: 6.8390, Time: 130.1s, Step: 216, GPU: 4.9GB\n",
      "Epoch 1, Batch 220, Loss: 6.4864, Time: 132.7s, Step: 221, GPU: 4.9GB\n",
      "Epoch 1, Batch 225, Loss: 6.8356, Time: 136.1s, Step: 226, GPU: 4.9GB\n",
      "Epoch 1, Batch 230, Loss: 6.2901, Time: 138.6s, Step: 231, GPU: 4.9GB\n",
      "Epoch 1, Batch 235, Loss: 7.0177, Time: 142.0s, Step: 236, GPU: 4.9GB\n",
      "Epoch 1, Batch 240, Loss: 6.4573, Time: 144.6s, Step: 241, GPU: 4.9GB\n",
      "Epoch 1, Batch 245, Loss: 6.4245, Time: 148.0s, Step: 246, GPU: 4.9GB\n",
      "Epoch 1, Batch 250, Loss: 6.4209, Time: 150.5s, Step: 251, GPU: 4.9GB\n",
      "Epoch 1, Batch 255, Loss: 6.9168, Time: 154.1s, Step: 256, GPU: 4.9GB\n",
      "Epoch 1, Batch 260, Loss: 6.5098, Time: 156.6s, Step: 261, GPU: 4.9GB\n",
      "Epoch 1, Batch 265, Loss: 5.8822, Time: 160.0s, Step: 266, GPU: 4.9GB\n",
      "Epoch 1, Batch 270, Loss: 5.8795, Time: 162.5s, Step: 271, GPU: 4.9GB\n",
      "Epoch 1, Batch 275, Loss: 6.7701, Time: 165.9s, Step: 276, GPU: 4.9GB\n",
      "Epoch 1, Batch 280, Loss: 6.2966, Time: 168.4s, Step: 281, GPU: 4.9GB\n",
      "Epoch 1, Batch 285, Loss: 5.9822, Time: 171.8s, Step: 286, GPU: 4.9GB\n",
      "Epoch 1, Batch 290, Loss: 5.7152, Time: 174.3s, Step: 291, GPU: 4.9GB\n",
      "Epoch 1, Batch 295, Loss: 6.0753, Time: 177.7s, Step: 296, GPU: 4.9GB\n",
      "Epoch 1, Batch 300, Loss: 5.8699, Time: 180.3s, Step: 301, GPU: 4.9GB\n",
      "Epoch 1, Batch 305, Loss: 5.8300, Time: 183.7s, Step: 306, GPU: 4.9GB\n",
      "Epoch 1, Batch 310, Loss: 6.1222, Time: 186.2s, Step: 311, GPU: 4.9GB\n",
      "Epoch 1, Batch 315, Loss: 6.3020, Time: 189.6s, Step: 316, GPU: 4.9GB\n",
      "Epoch 1, Batch 320, Loss: 6.3260, Time: 192.2s, Step: 321, GPU: 4.9GB\n",
      "Epoch 1, Batch 325, Loss: 6.1943, Time: 195.6s, Step: 326, GPU: 4.9GB\n",
      "Epoch 1, Batch 330, Loss: 5.5799, Time: 198.1s, Step: 331, GPU: 4.9GB\n",
      "Epoch 1, Batch 335, Loss: 6.9870, Time: 201.6s, Step: 336, GPU: 4.9GB\n",
      "Epoch 1, Batch 340, Loss: 5.7803, Time: 204.1s, Step: 341, GPU: 4.9GB\n",
      "Epoch 1, Batch 345, Loss: 5.7849, Time: 207.5s, Step: 346, GPU: 4.9GB\n",
      "Epoch 1, Batch 350, Loss: 5.3189, Time: 210.0s, Step: 351, GPU: 4.9GB\n",
      "Epoch 1, Batch 355, Loss: 6.4261, Time: 213.4s, Step: 356, GPU: 4.9GB\n",
      "Epoch 1, Batch 360, Loss: 5.9893, Time: 215.9s, Step: 361, GPU: 4.9GB\n",
      "Epoch 1, Batch 365, Loss: 6.7046, Time: 219.3s, Step: 366, GPU: 4.9GB\n",
      "Epoch 1, Batch 370, Loss: 5.6822, Time: 221.8s, Step: 371, GPU: 4.9GB\n",
      "Epoch 1, Batch 375, Loss: 5.8818, Time: 225.2s, Step: 376, GPU: 4.9GB\n",
      "Epoch 1, Batch 380, Loss: 5.5665, Time: 227.8s, Step: 381, GPU: 4.9GB\n",
      "Epoch 1, Batch 385, Loss: 5.9822, Time: 231.6s, Step: 386, GPU: 4.9GB\n",
      "Epoch 1, Batch 390, Loss: 6.8783, Time: 234.1s, Step: 391, GPU: 4.9GB\n",
      "Epoch 1, Batch 395, Loss: 5.6128, Time: 237.5s, Step: 396, GPU: 4.9GB\n",
      "Epoch 1, Batch 400, Loss: 5.3338, Time: 240.1s, Step: 401, GPU: 4.9GB\n",
      "Epoch 1, Batch 405, Loss: 5.5667, Time: 243.5s, Step: 406, GPU: 4.9GB\n",
      "Epoch 1, Batch 410, Loss: 5.2317, Time: 246.0s, Step: 411, GPU: 4.9GB\n",
      "Epoch 1, Batch 415, Loss: 5.3581, Time: 249.4s, Step: 416, GPU: 4.9GB\n",
      "Epoch 1, Batch 420, Loss: 5.2072, Time: 251.9s, Step: 421, GPU: 4.9GB\n",
      "Epoch 1, Batch 425, Loss: 6.8997, Time: 255.3s, Step: 426, GPU: 4.9GB\n",
      "Epoch 1, Batch 430, Loss: 5.3848, Time: 257.8s, Step: 431, GPU: 4.9GB\n",
      "Epoch 1, Batch 435, Loss: 5.6170, Time: 261.2s, Step: 436, GPU: 4.9GB\n",
      "Epoch 1, Batch 440, Loss: 6.3103, Time: 263.7s, Step: 441, GPU: 4.9GB\n",
      "Epoch 1, Batch 445, Loss: 6.6252, Time: 267.1s, Step: 446, GPU: 4.9GB\n",
      "Epoch 1, Batch 450, Loss: 5.7359, Time: 269.7s, Step: 451, GPU: 4.9GB\n",
      "Epoch 1, Batch 455, Loss: 6.0994, Time: 273.1s, Step: 456, GPU: 4.9GB\n",
      "Epoch 1, Batch 460, Loss: 5.9719, Time: 275.6s, Step: 461, GPU: 4.9GB\n",
      "Epoch 1, Batch 465, Loss: 6.1338, Time: 279.0s, Step: 466, GPU: 4.9GB\n",
      "Epoch 1, Batch 470, Loss: 6.2016, Time: 281.5s, Step: 471, GPU: 4.9GB\n",
      "Epoch 1, Batch 475, Loss: 6.1599, Time: 284.9s, Step: 476, GPU: 4.9GB\n",
      "Epoch 1, Batch 480, Loss: 7.0647, Time: 287.4s, Step: 481, GPU: 4.9GB\n",
      "Epoch 1, Batch 485, Loss: 6.3129, Time: 290.8s, Step: 486, GPU: 4.9GB\n",
      "Epoch 1, Batch 490, Loss: 5.6746, Time: 293.3s, Step: 491, GPU: 4.9GB\n",
      "Epoch 1, Batch 495, Loss: 5.9794, Time: 296.7s, Step: 496, GPU: 4.9GB\n",
      "Epoch 1, Batch 500, Loss: 5.5483, Time: 299.2s, Step: 501, GPU: 4.9GB\n",
      "Epoch 1, Batch 505, Loss: 5.4716, Time: 302.6s, Step: 506, GPU: 4.9GB\n",
      "Epoch 1, Batch 510, Loss: 5.5170, Time: 305.1s, Step: 511, GPU: 4.9GB\n",
      "Epoch 1, Batch 515, Loss: 5.8079, Time: 308.5s, Step: 516, GPU: 4.9GB\n",
      "Epoch 1, Batch 520, Loss: 6.0840, Time: 311.1s, Step: 521, GPU: 4.9GB\n",
      "Epoch 1, Batch 525, Loss: 6.0992, Time: 314.5s, Step: 526, GPU: 4.9GB\n",
      "Epoch 1, Batch 530, Loss: 6.1627, Time: 317.0s, Step: 531, GPU: 4.9GB\n",
      "Epoch 1, Batch 535, Loss: 5.3272, Time: 320.4s, Step: 536, GPU: 4.9GB\n",
      "Epoch 1, Batch 540, Loss: 5.4857, Time: 322.9s, Step: 541, GPU: 4.9GB\n",
      "Epoch 1, Batch 545, Loss: 6.0555, Time: 326.3s, Step: 546, GPU: 4.9GB\n",
      "Epoch 1, Batch 550, Loss: 5.6101, Time: 328.8s, Step: 551, GPU: 4.9GB\n",
      "Epoch 1, Batch 555, Loss: 6.7310, Time: 332.2s, Step: 556, GPU: 4.9GB\n",
      "Epoch 1, Batch 560, Loss: 5.2165, Time: 334.7s, Step: 561, GPU: 4.9GB\n",
      "Epoch 1, Batch 565, Loss: 5.2854, Time: 338.2s, Step: 566, GPU: 4.9GB\n",
      "Epoch 1, Batch 570, Loss: 5.9423, Time: 340.7s, Step: 571, GPU: 4.9GB\n",
      "Epoch 1, Batch 575, Loss: 5.3419, Time: 344.1s, Step: 576, GPU: 4.9GB\n",
      "Epoch 1, Batch 580, Loss: 5.0998, Time: 346.6s, Step: 581, GPU: 4.9GB\n",
      "Epoch 1, Batch 585, Loss: 5.5764, Time: 350.0s, Step: 586, GPU: 4.9GB\n",
      "Epoch 1, Batch 590, Loss: 6.4539, Time: 352.5s, Step: 591, GPU: 4.9GB\n",
      "Epoch 1, Batch 595, Loss: 5.1876, Time: 356.0s, Step: 596, GPU: 4.9GB\n",
      "Epoch 1, Batch 600, Loss: 5.3499, Time: 358.5s, Step: 601, GPU: 4.9GB\n",
      "Epoch 1, Batch 605, Loss: 5.6552, Time: 361.9s, Step: 606, GPU: 4.9GB\n",
      "Epoch 1, Batch 610, Loss: 5.0507, Time: 364.4s, Step: 611, GPU: 4.9GB\n",
      "Epoch 1, Batch 615, Loss: 5.7960, Time: 367.8s, Step: 616, GPU: 4.9GB\n",
      "Epoch 1, Batch 620, Loss: 5.5442, Time: 370.4s, Step: 621, GPU: 4.9GB\n",
      "Epoch 1, Batch 625, Loss: 5.1058, Time: 373.7s, Step: 626, GPU: 4.9GB\n",
      "Epoch 1, Batch 630, Loss: 5.5964, Time: 376.3s, Step: 631, GPU: 4.9GB\n",
      "Epoch 1, Batch 635, Loss: 5.8574, Time: 379.7s, Step: 636, GPU: 4.9GB\n",
      "Epoch 1, Batch 640, Loss: 5.2408, Time: 382.2s, Step: 641, GPU: 4.9GB\n",
      "Epoch 1, Batch 645, Loss: 6.5373, Time: 385.6s, Step: 646, GPU: 4.9GB\n",
      "Epoch 1, Batch 650, Loss: 5.5300, Time: 388.2s, Step: 651, GPU: 4.9GB\n",
      "Epoch 1, Batch 655, Loss: 6.0298, Time: 391.6s, Step: 656, GPU: 4.9GB\n",
      "Epoch 1, Batch 660, Loss: 6.9538, Time: 394.1s, Step: 661, GPU: 4.9GB\n",
      "Epoch 1, Batch 665, Loss: 5.9406, Time: 397.5s, Step: 666, GPU: 4.9GB\n",
      "Epoch 1, Batch 670, Loss: 6.6182, Time: 400.0s, Step: 671, GPU: 4.9GB\n",
      "Epoch 1, Batch 675, Loss: 6.3569, Time: 403.4s, Step: 676, GPU: 4.9GB\n",
      "Epoch 1, Batch 680, Loss: 5.6653, Time: 406.0s, Step: 681, GPU: 4.9GB\n",
      "Epoch 1, Batch 685, Loss: 6.0280, Time: 409.4s, Step: 686, GPU: 4.9GB\n",
      "Epoch 1, Batch 690, Loss: 5.6920, Time: 411.9s, Step: 691, GPU: 4.9GB\n",
      "Epoch 1, Batch 695, Loss: 5.6166, Time: 415.3s, Step: 696, GPU: 4.9GB\n",
      "Epoch 1, Batch 700, Loss: 5.7377, Time: 417.8s, Step: 701, GPU: 4.9GB\n",
      "Epoch 1, Batch 705, Loss: 5.7188, Time: 421.2s, Step: 706, GPU: 4.9GB\n",
      "Epoch 1, Batch 710, Loss: 5.6165, Time: 423.7s, Step: 711, GPU: 4.9GB\n",
      "Epoch 1, Batch 715, Loss: 6.3717, Time: 427.2s, Step: 716, GPU: 4.9GB\n",
      "Epoch 1, Batch 720, Loss: 5.1006, Time: 429.7s, Step: 721, GPU: 4.9GB\n",
      "Epoch 1, Batch 725, Loss: 6.3671, Time: 433.1s, Step: 726, GPU: 4.9GB\n",
      "Epoch 1, Batch 730, Loss: 6.0484, Time: 435.6s, Step: 731, GPU: 4.9GB\n",
      "Epoch 1, Batch 735, Loss: 6.1086, Time: 439.0s, Step: 736, GPU: 4.9GB\n",
      "Epoch 1, Batch 740, Loss: 5.9306, Time: 441.6s, Step: 741, GPU: 4.9GB\n",
      "Epoch 1, Batch 745, Loss: 5.1973, Time: 445.0s, Step: 746, GPU: 4.9GB\n",
      "Epoch 1, Batch 750, Loss: 5.9448, Time: 447.5s, Step: 751, GPU: 4.9GB\n",
      "Epoch 1, Batch 755, Loss: 6.1555, Time: 451.0s, Step: 756, GPU: 4.9GB\n",
      "Epoch 1, Batch 760, Loss: 5.1301, Time: 453.5s, Step: 761, GPU: 4.9GB\n",
      "Epoch 1, Batch 765, Loss: 6.2168, Time: 456.9s, Step: 766, GPU: 4.9GB\n",
      "Epoch 1, Batch 770, Loss: 5.8982, Time: 459.4s, Step: 771, GPU: 4.9GB\n",
      "Epoch 1, Batch 775, Loss: 4.8736, Time: 462.8s, Step: 776, GPU: 4.9GB\n",
      "Epoch 1, Batch 780, Loss: 5.7759, Time: 465.3s, Step: 781, GPU: 4.9GB\n",
      "Epoch 1, Batch 785, Loss: 6.3604, Time: 468.7s, Step: 786, GPU: 4.9GB\n",
      "Epoch 1, Batch 790, Loss: 5.6322, Time: 471.2s, Step: 791, GPU: 4.9GB\n",
      "Epoch 1, Batch 795, Loss: 5.6240, Time: 474.6s, Step: 796, GPU: 4.9GB\n",
      "Epoch 1, Batch 800, Loss: 5.8038, Time: 477.2s, Step: 801, GPU: 4.9GB\n",
      "Epoch 1, Batch 805, Loss: 5.1653, Time: 480.7s, Step: 806, GPU: 4.9GB\n",
      "Epoch 1, Batch 810, Loss: 5.0904, Time: 483.2s, Step: 811, GPU: 4.9GB\n",
      "Epoch 1, Batch 815, Loss: 6.3657, Time: 486.6s, Step: 816, GPU: 4.9GB\n",
      "Epoch 1, Batch 820, Loss: 6.0479, Time: 489.1s, Step: 821, GPU: 4.9GB\n",
      "Epoch 1, Batch 825, Loss: 6.1437, Time: 492.6s, Step: 826, GPU: 4.9GB\n",
      "Epoch 1, Batch 830, Loss: 5.5268, Time: 495.1s, Step: 831, GPU: 4.9GB\n",
      "Epoch 1, Batch 835, Loss: 5.5617, Time: 498.5s, Step: 836, GPU: 4.9GB\n",
      "Epoch 1, Batch 840, Loss: 6.2851, Time: 501.0s, Step: 841, GPU: 4.9GB\n",
      "Epoch 1, Batch 845, Loss: 5.9599, Time: 504.5s, Step: 846, GPU: 4.9GB\n",
      "Epoch 1, Batch 850, Loss: 5.4194, Time: 507.0s, Step: 851, GPU: 4.9GB\n",
      "Epoch 1, Batch 855, Loss: 6.2106, Time: 510.5s, Step: 856, GPU: 4.9GB\n",
      "Epoch 1, Batch 860, Loss: 6.0971, Time: 513.0s, Step: 861, GPU: 4.9GB\n",
      "Epoch 1, Batch 865, Loss: 5.4098, Time: 516.4s, Step: 866, GPU: 4.9GB\n",
      "Epoch 1, Batch 870, Loss: 6.2817, Time: 518.9s, Step: 871, GPU: 4.9GB\n",
      "Epoch 1, Batch 875, Loss: 5.5127, Time: 522.3s, Step: 876, GPU: 4.9GB\n",
      "Epoch 1, Batch 880, Loss: 5.9241, Time: 524.8s, Step: 881, GPU: 4.9GB\n",
      "Epoch 1, Batch 885, Loss: 6.0712, Time: 528.2s, Step: 886, GPU: 4.9GB\n",
      "Epoch 1, Batch 890, Loss: 5.6676, Time: 530.8s, Step: 891, GPU: 4.9GB\n",
      "Epoch 1, Batch 895, Loss: 5.2594, Time: 534.2s, Step: 896, GPU: 4.9GB\n",
      "Epoch 1, Batch 900, Loss: 5.6041, Time: 536.7s, Step: 901, GPU: 4.9GB\n",
      "Epoch 1, Batch 905, Loss: 5.6282, Time: 540.1s, Step: 906, GPU: 4.9GB\n",
      "Epoch 1, Batch 910, Loss: 5.3472, Time: 542.7s, Step: 911, GPU: 4.9GB\n",
      "Epoch 1, Batch 915, Loss: 4.8792, Time: 546.1s, Step: 916, GPU: 4.9GB\n",
      "Epoch 1, Batch 920, Loss: 5.7540, Time: 548.6s, Step: 921, GPU: 4.9GB\n",
      "Epoch 1, Batch 925, Loss: 5.6572, Time: 552.0s, Step: 926, GPU: 4.9GB\n",
      "Epoch 1, Batch 930, Loss: 5.7126, Time: 554.5s, Step: 931, GPU: 4.9GB\n",
      "Epoch 1, Batch 935, Loss: 4.7879, Time: 558.0s, Step: 936, GPU: 4.9GB\n",
      "Epoch 1, Batch 940, Loss: 5.3361, Time: 560.5s, Step: 941, GPU: 4.9GB\n",
      "Epoch 1, Batch 945, Loss: 5.1528, Time: 563.9s, Step: 946, GPU: 4.9GB\n",
      "Epoch 1, Batch 950, Loss: 5.6632, Time: 566.4s, Step: 951, GPU: 4.9GB\n",
      "Epoch 1, Batch 955, Loss: 6.4802, Time: 569.9s, Step: 956, GPU: 4.9GB\n",
      "Epoch 1, Batch 960, Loss: 5.9103, Time: 572.4s, Step: 961, GPU: 4.9GB\n",
      "Epoch 1, Batch 965, Loss: 5.5950, Time: 575.8s, Step: 966, GPU: 4.9GB\n",
      "Epoch 1, Batch 970, Loss: 5.3328, Time: 578.4s, Step: 971, GPU: 4.9GB\n",
      "Epoch 1, Batch 975, Loss: 5.9892, Time: 581.8s, Step: 976, GPU: 4.9GB\n",
      "Epoch 1, Batch 980, Loss: 5.2370, Time: 584.3s, Step: 981, GPU: 4.9GB\n",
      "Epoch 1, Batch 985, Loss: 5.5104, Time: 587.7s, Step: 986, GPU: 4.9GB\n",
      "Epoch 1, Batch 990, Loss: 5.2611, Time: 590.2s, Step: 991, GPU: 4.9GB\n",
      "Epoch 1, Batch 995, Loss: 4.6904, Time: 593.7s, Step: 996, GPU: 4.9GB\n",
      "Epoch 1, Batch 1000, Loss: 5.3769, Time: 596.2s, Step: 1001, GPU: 4.9GB\n",
      "Epoch 1, Batch 1005, Loss: 5.7738, Time: 599.7s, Step: 1006, GPU: 4.9GB\n",
      "Epoch 1, Batch 1010, Loss: 5.7641, Time: 602.2s, Step: 1011, GPU: 4.9GB\n",
      "Epoch 1, Batch 1015, Loss: 5.3635, Time: 605.6s, Step: 1016, GPU: 4.9GB\n",
      "Epoch 1, Batch 1020, Loss: 5.5981, Time: 608.1s, Step: 1021, GPU: 4.9GB\n",
      "Epoch 1, Batch 1025, Loss: 5.9095, Time: 611.5s, Step: 1026, GPU: 4.9GB\n",
      "Epoch 1, Batch 1030, Loss: 5.7838, Time: 614.1s, Step: 1031, GPU: 4.9GB\n",
      "Epoch 1, Batch 1035, Loss: 5.2736, Time: 617.5s, Step: 1036, GPU: 4.9GB\n",
      "Epoch 1, Batch 1040, Loss: 6.6940, Time: 620.0s, Step: 1041, GPU: 4.9GB\n",
      "Epoch 1, Batch 1045, Loss: 5.4193, Time: 623.4s, Step: 1046, GPU: 4.9GB\n",
      "Epoch 1, Batch 1050, Loss: 5.7853, Time: 626.0s, Step: 1051, GPU: 4.9GB\n",
      "Epoch 1, Batch 1055, Loss: 5.1664, Time: 629.4s, Step: 1056, GPU: 4.9GB\n",
      "Epoch 1, Batch 1060, Loss: 5.4966, Time: 631.9s, Step: 1061, GPU: 4.9GB\n",
      "Epoch 1, Batch 1065, Loss: 5.7929, Time: 635.3s, Step: 1066, GPU: 4.9GB\n",
      "Epoch 1, Batch 1070, Loss: 5.3507, Time: 637.9s, Step: 1071, GPU: 4.9GB\n",
      "Epoch 1, Batch 1075, Loss: 6.0305, Time: 641.3s, Step: 1076, GPU: 4.9GB\n",
      "Epoch 1, Batch 1080, Loss: 5.0103, Time: 643.8s, Step: 1081, GPU: 4.9GB\n",
      "Epoch 1, Batch 1085, Loss: 5.1910, Time: 647.2s, Step: 1086, GPU: 4.9GB\n",
      "Epoch 1, Batch 1090, Loss: 5.7704, Time: 649.7s, Step: 1091, GPU: 4.9GB\n",
      "Epoch 1, Batch 1095, Loss: 5.5048, Time: 653.2s, Step: 1096, GPU: 4.9GB\n",
      "Epoch 1, Batch 1100, Loss: 5.7624, Time: 655.7s, Step: 1101, GPU: 4.9GB\n",
      "Epoch 1, Batch 1105, Loss: 5.9781, Time: 659.2s, Step: 1106, GPU: 4.9GB\n",
      "Epoch 1, Batch 1110, Loss: 4.4331, Time: 661.7s, Step: 1111, GPU: 4.9GB\n",
      "Epoch 1, Batch 1115, Loss: 5.8971, Time: 665.1s, Step: 1116, GPU: 4.9GB\n",
      "Epoch 1, Batch 1120, Loss: 5.0928, Time: 667.6s, Step: 1121, GPU: 4.9GB\n",
      "Epoch 1, Batch 1125, Loss: 5.3576, Time: 671.1s, Step: 1126, GPU: 4.9GB\n",
      "Epoch 1, Batch 1130, Loss: 6.2855, Time: 673.6s, Step: 1131, GPU: 4.9GB\n",
      "Epoch 1, Batch 1135, Loss: 4.6734, Time: 677.0s, Step: 1136, GPU: 4.9GB\n",
      "Epoch 1, Batch 1140, Loss: 6.4140, Time: 679.6s, Step: 1141, GPU: 4.9GB\n",
      "Epoch 1, Batch 1145, Loss: 6.1243, Time: 683.0s, Step: 1146, GPU: 4.9GB\n",
      "Epoch 1, Batch 1150, Loss: 5.5666, Time: 685.6s, Step: 1151, GPU: 4.9GB\n",
      "Epoch 1, Batch 1155, Loss: 5.0067, Time: 689.0s, Step: 1156, GPU: 4.9GB\n",
      "Epoch 1, Batch 1160, Loss: 4.9893, Time: 691.5s, Step: 1161, GPU: 4.9GB\n",
      "Epoch 1, Batch 1165, Loss: 5.2611, Time: 695.0s, Step: 1166, GPU: 4.9GB\n",
      "Epoch 1, Batch 1170, Loss: 6.3563, Time: 697.5s, Step: 1171, GPU: 4.9GB\n",
      "Epoch 1, Batch 1175, Loss: 4.7868, Time: 700.9s, Step: 1176, GPU: 4.9GB\n",
      "Epoch 1, Batch 1180, Loss: 5.5985, Time: 703.5s, Step: 1181, GPU: 4.9GB\n",
      "Epoch 1, Batch 1185, Loss: 5.4608, Time: 706.9s, Step: 1186, GPU: 4.9GB\n",
      "Epoch 1, Batch 1190, Loss: 5.1024, Time: 709.5s, Step: 1191, GPU: 4.9GB\n",
      "Epoch 1, Batch 1195, Loss: 5.6896, Time: 712.9s, Step: 1196, GPU: 4.9GB\n",
      "Epoch 1, Batch 1200, Loss: 5.8716, Time: 715.5s, Step: 1201, GPU: 4.9GB\n",
      "Epoch 1, Batch 1205, Loss: 5.6600, Time: 719.0s, Step: 1206, GPU: 4.9GB\n",
      "Epoch 1, Batch 1210, Loss: 6.5957, Time: 721.5s, Step: 1211, GPU: 4.9GB\n",
      "Epoch 1, Batch 1215, Loss: 5.4332, Time: 724.9s, Step: 1216, GPU: 4.9GB\n",
      "Epoch 1, Batch 1220, Loss: 5.4568, Time: 727.4s, Step: 1221, GPU: 4.9GB\n",
      "Epoch 1, Batch 1225, Loss: 5.3312, Time: 730.8s, Step: 1226, GPU: 4.9GB\n",
      "Epoch 1, Batch 1230, Loss: 4.8213, Time: 733.3s, Step: 1231, GPU: 4.9GB\n",
      "Epoch 1, Batch 1235, Loss: 5.5275, Time: 736.8s, Step: 1236, GPU: 4.9GB\n",
      "Epoch 1, Batch 1240, Loss: 6.4015, Time: 739.3s, Step: 1241, GPU: 4.9GB\n",
      "Epoch 1, Batch 1245, Loss: 5.1458, Time: 742.7s, Step: 1246, GPU: 4.9GB\n",
      "Epoch 1, Batch 1250, Loss: 5.6426, Time: 745.2s, Step: 1251, GPU: 4.9GB\n",
      "Epoch 1, Batch 1255, Loss: 5.6451, Time: 748.6s, Step: 1256, GPU: 4.9GB\n",
      "Epoch 1, Batch 1260, Loss: 5.1827, Time: 751.2s, Step: 1261, GPU: 4.9GB\n",
      "Epoch 1, Batch 1265, Loss: 5.3286, Time: 754.6s, Step: 1266, GPU: 4.9GB\n",
      "Epoch 1, Batch 1270, Loss: 5.2673, Time: 757.1s, Step: 1271, GPU: 4.9GB\n",
      "Epoch 1, Batch 1275, Loss: 4.4557, Time: 760.5s, Step: 1276, GPU: 4.9GB\n",
      "Epoch 1, Batch 1280, Loss: 5.3069, Time: 763.0s, Step: 1281, GPU: 4.9GB\n",
      "Epoch 1, Batch 1285, Loss: 5.4654, Time: 766.5s, Step: 1286, GPU: 4.9GB\n",
      "Epoch 1, Batch 1290, Loss: 5.0763, Time: 769.0s, Step: 1291, GPU: 4.9GB\n",
      "Epoch 1, Batch 1295, Loss: 5.4220, Time: 772.4s, Step: 1296, GPU: 4.9GB\n",
      "Epoch 1, Batch 1300, Loss: 5.6179, Time: 774.9s, Step: 1301, GPU: 4.9GB\n",
      "Epoch 1, Batch 1305, Loss: 4.0865, Time: 778.3s, Step: 1306, GPU: 4.9GB\n",
      "Epoch 1, Batch 1310, Loss: 5.4617, Time: 780.8s, Step: 1311, GPU: 4.9GB\n",
      "Epoch 1, Batch 1315, Loss: 5.7377, Time: 784.3s, Step: 1316, GPU: 4.9GB\n",
      "Epoch 1, Batch 1320, Loss: 6.0441, Time: 786.8s, Step: 1321, GPU: 4.9GB\n",
      "Epoch 1, Batch 1325, Loss: 4.9834, Time: 790.3s, Step: 1326, GPU: 4.9GB\n",
      "Epoch 1, Batch 1330, Loss: 5.1968, Time: 792.8s, Step: 1331, GPU: 4.9GB\n",
      "Epoch 1, Batch 1335, Loss: 5.5553, Time: 796.2s, Step: 1336, GPU: 4.9GB\n",
      "Epoch 1, Batch 1340, Loss: 5.5509, Time: 798.7s, Step: 1341, GPU: 4.9GB\n",
      "Epoch 1, Batch 1345, Loss: 5.8095, Time: 802.1s, Step: 1346, GPU: 4.9GB\n",
      "Epoch 1, Batch 1350, Loss: 5.0390, Time: 804.6s, Step: 1351, GPU: 4.9GB\n",
      "Epoch 1, Batch 1355, Loss: 5.4522, Time: 808.0s, Step: 1356, GPU: 4.9GB\n",
      "Epoch 1, Batch 1360, Loss: 5.2632, Time: 810.6s, Step: 1361, GPU: 4.9GB\n",
      "Epoch 1, Batch 1365, Loss: 5.9654, Time: 814.0s, Step: 1366, GPU: 4.9GB\n",
      "Epoch 1, Batch 1370, Loss: 5.9823, Time: 816.5s, Step: 1371, GPU: 4.9GB\n",
      "Epoch 1, Batch 1375, Loss: 4.9075, Time: 819.9s, Step: 1376, GPU: 4.9GB\n",
      "Epoch 1, Batch 1380, Loss: 4.4982, Time: 822.5s, Step: 1381, GPU: 4.9GB\n",
      "Epoch 1, Batch 1385, Loss: 4.2608, Time: 825.9s, Step: 1386, GPU: 4.9GB\n",
      "Epoch 1, Batch 1390, Loss: 4.8833, Time: 828.4s, Step: 1391, GPU: 4.9GB\n",
      "Epoch 1, Batch 1395, Loss: 5.4416, Time: 831.8s, Step: 1396, GPU: 4.9GB\n",
      "Epoch 1, Batch 1400, Loss: 5.5880, Time: 834.4s, Step: 1401, GPU: 4.9GB\n",
      "Epoch 1, Batch 1405, Loss: 5.1873, Time: 837.8s, Step: 1406, GPU: 4.9GB\n",
      "Epoch 1, Batch 1410, Loss: 5.0668, Time: 840.4s, Step: 1411, GPU: 4.9GB\n",
      "Epoch 1, Batch 1415, Loss: 5.1065, Time: 843.8s, Step: 1416, GPU: 4.9GB\n",
      "Epoch 1, Batch 1420, Loss: 6.0575, Time: 846.3s, Step: 1421, GPU: 4.9GB\n",
      "Epoch 1, Batch 1425, Loss: 5.1667, Time: 849.7s, Step: 1426, GPU: 4.9GB\n",
      "Epoch 1, Batch 1430, Loss: 5.6275, Time: 852.2s, Step: 1431, GPU: 4.9GB\n",
      "Epoch 1, Batch 1435, Loss: 5.7401, Time: 855.7s, Step: 1436, GPU: 4.9GB\n",
      "Epoch 1, Batch 1440, Loss: 4.5105, Time: 858.2s, Step: 1441, GPU: 4.9GB\n",
      "Epoch 1, Batch 1445, Loss: 5.0756, Time: 861.6s, Step: 1446, GPU: 4.9GB\n",
      "Epoch 1, Batch 1450, Loss: 5.3918, Time: 864.1s, Step: 1451, GPU: 4.9GB\n",
      "Epoch 1, Batch 1455, Loss: 5.3405, Time: 867.5s, Step: 1456, GPU: 4.9GB\n",
      "Epoch 1, Batch 1460, Loss: 5.3015, Time: 870.0s, Step: 1461, GPU: 4.9GB\n",
      "Epoch 1, Batch 1465, Loss: 5.6891, Time: 873.4s, Step: 1466, GPU: 4.9GB\n",
      "Epoch 1, Batch 1470, Loss: 5.2802, Time: 876.0s, Step: 1471, GPU: 4.9GB\n",
      "Epoch 1, Batch 1475, Loss: 4.7597, Time: 879.4s, Step: 1476, GPU: 4.9GB\n",
      "Epoch 1, Batch 1480, Loss: 5.7906, Time: 881.9s, Step: 1481, GPU: 4.9GB\n",
      "Epoch 1, Batch 1485, Loss: 5.8896, Time: 885.3s, Step: 1486, GPU: 4.9GB\n",
      "Epoch 1, Batch 1490, Loss: 5.5113, Time: 887.8s, Step: 1491, GPU: 4.9GB\n",
      "Epoch 1, Batch 1495, Loss: 5.1845, Time: 891.2s, Step: 1496, GPU: 4.9GB\n",
      "Epoch 1, Batch 1500, Loss: 5.7891, Time: 893.8s, Step: 1501, GPU: 4.9GB\n",
      "Epoch 1, Batch 1505, Loss: 4.9880, Time: 897.2s, Step: 1506, GPU: 4.9GB\n",
      "Epoch 1, Batch 1510, Loss: 5.7584, Time: 899.7s, Step: 1511, GPU: 4.9GB\n",
      "Epoch 1, Batch 1515, Loss: 5.3178, Time: 903.1s, Step: 1516, GPU: 4.9GB\n",
      "Epoch 1, Batch 1520, Loss: 5.1826, Time: 905.7s, Step: 1521, GPU: 4.9GB\n",
      "Epoch 1, Batch 1525, Loss: 4.9746, Time: 909.1s, Step: 1526, GPU: 4.9GB\n",
      "Epoch 1, Batch 1530, Loss: 5.1937, Time: 911.6s, Step: 1531, GPU: 4.9GB\n",
      "Epoch 1, Batch 1535, Loss: 5.1381, Time: 915.1s, Step: 1536, GPU: 4.9GB\n",
      "Epoch 1, Batch 1540, Loss: 5.6723, Time: 917.6s, Step: 1541, GPU: 4.9GB\n",
      "Epoch 1, Batch 1545, Loss: 5.1285, Time: 921.0s, Step: 1546, GPU: 4.9GB\n",
      "Epoch 1, Batch 1550, Loss: 5.3113, Time: 923.6s, Step: 1551, GPU: 4.9GB\n",
      "Epoch 1, Batch 1555, Loss: 5.9668, Time: 927.0s, Step: 1556, GPU: 4.9GB\n",
      "Epoch 1, Batch 1560, Loss: 5.4660, Time: 929.5s, Step: 1561, GPU: 4.9GB\n",
      "Epoch 1, Batch 1565, Loss: 4.6693, Time: 933.0s, Step: 1566, GPU: 4.9GB\n",
      "Epoch 1, Batch 1570, Loss: 5.7045, Time: 935.5s, Step: 1571, GPU: 4.9GB\n",
      "Epoch 1, Batch 1575, Loss: 5.4342, Time: 938.9s, Step: 1576, GPU: 4.9GB\n",
      "Epoch 1, Batch 1580, Loss: 5.7475, Time: 941.5s, Step: 1581, GPU: 4.9GB\n",
      "Epoch 1, Batch 1585, Loss: 6.3272, Time: 944.9s, Step: 1586, GPU: 4.9GB\n",
      "Epoch 1, Batch 1590, Loss: 5.7447, Time: 947.5s, Step: 1591, GPU: 4.9GB\n",
      "Epoch 1, Batch 1595, Loss: 5.4512, Time: 950.9s, Step: 1596, GPU: 4.9GB\n",
      "Epoch 1, Batch 1600, Loss: 5.8011, Time: 953.5s, Step: 1601, GPU: 4.9GB\n",
      "Epoch 1, Batch 1605, Loss: 5.4926, Time: 957.0s, Step: 1606, GPU: 4.9GB\n",
      "Epoch 1, Batch 1610, Loss: 5.9741, Time: 959.5s, Step: 1611, GPU: 4.9GB\n",
      "Epoch 1, Batch 1615, Loss: 5.1655, Time: 962.9s, Step: 1616, GPU: 4.9GB\n",
      "Epoch 1, Batch 1620, Loss: 4.9979, Time: 965.5s, Step: 1621, GPU: 4.9GB\n",
      "Epoch 1, Batch 1625, Loss: 6.5240, Time: 968.9s, Step: 1626, GPU: 4.9GB\n",
      "Epoch 1, Batch 1630, Loss: 4.9404, Time: 971.5s, Step: 1631, GPU: 4.9GB\n",
      "Epoch 1, Batch 1635, Loss: 5.4078, Time: 974.9s, Step: 1636, GPU: 4.9GB\n",
      "Epoch 1, Batch 1640, Loss: 5.4682, Time: 977.4s, Step: 1641, GPU: 4.9GB\n",
      "Epoch 1, Batch 1645, Loss: 5.5951, Time: 980.9s, Step: 1646, GPU: 4.9GB\n",
      "Epoch 1, Batch 1650, Loss: 5.4725, Time: 983.4s, Step: 1651, GPU: 4.9GB\n",
      "Epoch 1, Batch 1655, Loss: 4.9805, Time: 986.9s, Step: 1656, GPU: 4.9GB\n",
      "Epoch 1, Batch 1660, Loss: 4.4556, Time: 989.4s, Step: 1661, GPU: 4.9GB\n",
      "Epoch 1, Batch 1665, Loss: 5.3896, Time: 992.8s, Step: 1666, GPU: 4.9GB\n",
      "Epoch 1, Batch 1670, Loss: 6.0082, Time: 995.4s, Step: 1671, GPU: 4.9GB\n",
      "Epoch 1, Batch 1675, Loss: 5.3124, Time: 998.8s, Step: 1676, GPU: 4.9GB\n",
      "Epoch 1, Batch 1680, Loss: 5.1472, Time: 1001.3s, Step: 1681, GPU: 4.9GB\n",
      "Epoch 1, Batch 1685, Loss: 5.9737, Time: 1004.7s, Step: 1686, GPU: 4.9GB\n",
      "Epoch 1, Batch 1690, Loss: 5.6885, Time: 1007.2s, Step: 1691, GPU: 4.9GB\n",
      "Epoch 1, Batch 1695, Loss: 5.4758, Time: 1010.6s, Step: 1696, GPU: 4.9GB\n",
      "Epoch 1, Batch 1700, Loss: 5.2879, Time: 1013.2s, Step: 1701, GPU: 4.9GB\n",
      "Epoch 1, Batch 1705, Loss: 4.7777, Time: 1016.6s, Step: 1706, GPU: 4.9GB\n",
      "Epoch 1, Batch 1710, Loss: 5.5882, Time: 1019.1s, Step: 1711, GPU: 4.9GB\n",
      "Epoch 1, Batch 1715, Loss: 5.5740, Time: 1022.6s, Step: 1716, GPU: 4.9GB\n",
      "Epoch 1, Batch 1720, Loss: 5.5897, Time: 1025.1s, Step: 1721, GPU: 4.9GB\n",
      "Epoch 1, Batch 1725, Loss: 5.7385, Time: 1028.5s, Step: 1726, GPU: 4.9GB\n",
      "Epoch 1, Batch 1730, Loss: 5.0234, Time: 1031.1s, Step: 1731, GPU: 4.9GB\n",
      "Epoch 1, Batch 1735, Loss: 5.7004, Time: 1034.5s, Step: 1736, GPU: 4.9GB\n",
      "Epoch 1, Batch 1740, Loss: 5.4270, Time: 1037.0s, Step: 1741, GPU: 4.9GB\n",
      "Epoch 1, Batch 1745, Loss: 5.5497, Time: 1040.5s, Step: 1746, GPU: 4.9GB\n",
      "Epoch 1, Batch 1750, Loss: 5.1278, Time: 1043.0s, Step: 1751, GPU: 4.9GB\n",
      "Epoch 1, Batch 1755, Loss: 5.4724, Time: 1046.4s, Step: 1756, GPU: 4.9GB\n",
      "Epoch 1, Batch 1760, Loss: 5.8114, Time: 1048.9s, Step: 1761, GPU: 4.9GB\n",
      "Epoch 1, Batch 1765, Loss: 6.6637, Time: 1052.4s, Step: 1766, GPU: 4.9GB\n",
      "Epoch 1, Batch 1770, Loss: 5.2246, Time: 1054.9s, Step: 1771, GPU: 4.9GB\n",
      "Epoch 1, Batch 1775, Loss: 4.0320, Time: 1058.3s, Step: 1776, GPU: 4.9GB\n",
      "Epoch 1, Batch 1780, Loss: 5.0686, Time: 1060.9s, Step: 1781, GPU: 4.9GB\n",
      "Epoch 1, Batch 1785, Loss: 4.9983, Time: 1064.3s, Step: 1786, GPU: 4.9GB\n",
      "Epoch 1, Batch 1790, Loss: 5.8284, Time: 1066.8s, Step: 1791, GPU: 4.9GB\n",
      "Epoch 1, Batch 1795, Loss: 4.7411, Time: 1070.2s, Step: 1796, GPU: 4.9GB\n",
      "Epoch 1, Batch 1800, Loss: 4.9621, Time: 1072.8s, Step: 1801, GPU: 4.9GB\n",
      "Epoch 1, Batch 1805, Loss: 4.7999, Time: 1076.2s, Step: 1806, GPU: 4.9GB\n",
      "Epoch 1, Batch 1810, Loss: 5.0723, Time: 1078.7s, Step: 1811, GPU: 4.9GB\n",
      "Epoch 1, Batch 1815, Loss: 5.8957, Time: 1082.2s, Step: 1816, GPU: 4.9GB\n",
      "Epoch 1, Batch 1820, Loss: 5.6715, Time: 1084.7s, Step: 1821, GPU: 4.9GB\n",
      "Epoch 1, Batch 1825, Loss: 5.3235, Time: 1088.1s, Step: 1826, GPU: 4.9GB\n",
      "Epoch 1, Batch 1830, Loss: 5.1671, Time: 1090.6s, Step: 1831, GPU: 4.9GB\n",
      "Epoch 1, Batch 1835, Loss: 5.2480, Time: 1094.1s, Step: 1836, GPU: 4.9GB\n",
      "Epoch 1, Batch 1840, Loss: 6.1110, Time: 1096.6s, Step: 1841, GPU: 4.9GB\n",
      "Epoch 1, Batch 1845, Loss: 4.8222, Time: 1100.0s, Step: 1846, GPU: 4.9GB\n",
      "Epoch 1, Batch 1850, Loss: 5.5033, Time: 1102.5s, Step: 1851, GPU: 4.9GB\n",
      "Epoch 1, Batch 1855, Loss: 5.9781, Time: 1105.9s, Step: 1856, GPU: 4.9GB\n",
      "Epoch 1, Batch 1860, Loss: 4.9019, Time: 1108.4s, Step: 1861, GPU: 4.9GB\n",
      "Epoch 1, Batch 1865, Loss: 4.9711, Time: 1111.8s, Step: 1866, GPU: 4.9GB\n",
      "Epoch 1, Batch 1870, Loss: 5.2431, Time: 1114.3s, Step: 1871, GPU: 4.9GB\n",
      "Epoch 1, Batch 1875, Loss: 6.0376, Time: 1117.7s, Step: 1876, GPU: 4.9GB\n",
      "Epoch 1, Batch 1880, Loss: 5.8540, Time: 1120.3s, Step: 1881, GPU: 4.9GB\n",
      "Epoch 1, Batch 1885, Loss: 5.1110, Time: 1123.7s, Step: 1886, GPU: 4.9GB\n",
      "Epoch 1, Batch 1890, Loss: 4.9637, Time: 1126.2s, Step: 1891, GPU: 4.9GB\n",
      "Epoch 1, Batch 1895, Loss: 6.0615, Time: 1129.6s, Step: 1896, GPU: 4.9GB\n",
      "Epoch 1, Batch 1900, Loss: 4.5952, Time: 1132.1s, Step: 1901, GPU: 4.9GB\n",
      "Epoch 1, Batch 1905, Loss: 5.9759, Time: 1135.5s, Step: 1906, GPU: 4.9GB\n",
      "Epoch 1, Batch 1910, Loss: 5.2263, Time: 1138.0s, Step: 1911, GPU: 4.9GB\n",
      "Epoch 1, Batch 1915, Loss: 5.3841, Time: 1141.4s, Step: 1916, GPU: 4.9GB\n",
      "Epoch 1, Batch 1920, Loss: 6.2090, Time: 1144.0s, Step: 1921, GPU: 4.9GB\n",
      "Epoch 1, Batch 1925, Loss: 6.2403, Time: 1147.4s, Step: 1926, GPU: 4.9GB\n",
      "Epoch 1, Batch 1930, Loss: 4.5277, Time: 1149.9s, Step: 1931, GPU: 4.9GB\n",
      "Epoch 1, Batch 1935, Loss: 5.4870, Time: 1153.3s, Step: 1936, GPU: 4.9GB\n",
      "Epoch 1, Batch 1940, Loss: 5.1860, Time: 1155.8s, Step: 1941, GPU: 4.9GB\n",
      "Epoch 1, Batch 1945, Loss: 5.7494, Time: 1159.2s, Step: 1946, GPU: 4.9GB\n",
      "Epoch 1, Batch 1950, Loss: 6.5177, Time: 1161.7s, Step: 1951, GPU: 4.9GB\n",
      "Epoch 1, Batch 1955, Loss: 5.0719, Time: 1165.2s, Step: 1956, GPU: 4.9GB\n",
      "Epoch 1, Batch 1960, Loss: 5.6489, Time: 1167.7s, Step: 1961, GPU: 4.9GB\n",
      "Epoch 1, Batch 1965, Loss: 4.7507, Time: 1171.1s, Step: 1966, GPU: 4.9GB\n",
      "Epoch 1, Batch 1970, Loss: 5.3203, Time: 1173.7s, Step: 1971, GPU: 4.9GB\n",
      "Epoch 1, Batch 1975, Loss: 5.4106, Time: 1177.1s, Step: 1976, GPU: 4.9GB\n",
      "Epoch 1, Batch 1980, Loss: 5.9254, Time: 1179.6s, Step: 1981, GPU: 4.9GB\n",
      "Epoch 1, Batch 1985, Loss: 4.8503, Time: 1183.1s, Step: 1986, GPU: 4.9GB\n",
      "Epoch 1, Batch 1990, Loss: 5.0527, Time: 1185.6s, Step: 1991, GPU: 4.9GB\n",
      "Epoch 1, Batch 1995, Loss: 5.3403, Time: 1189.1s, Step: 1996, GPU: 4.9GB\n",
      "Epoch 1, Batch 2000, Loss: 4.9584, Time: 1191.7s, Step: 2001, GPU: 4.9GB\n",
      "Epoch 1, Batch 2005, Loss: 5.3452, Time: 1195.1s, Step: 2006, GPU: 4.9GB\n",
      "Epoch 1, Batch 2010, Loss: 5.7891, Time: 1197.6s, Step: 2011, GPU: 4.9GB\n",
      "Epoch 1, Batch 2015, Loss: 5.3470, Time: 1201.1s, Step: 2016, GPU: 4.9GB\n",
      "Epoch 1, Batch 2020, Loss: 5.3041, Time: 1203.6s, Step: 2021, GPU: 4.9GB\n",
      "Epoch 1, Batch 2025, Loss: 5.9131, Time: 1207.0s, Step: 2026, GPU: 4.9GB\n",
      "Epoch 1, Batch 2030, Loss: 5.2107, Time: 1209.5s, Step: 2031, GPU: 4.9GB\n",
      "Epoch 1, Batch 2035, Loss: 5.4711, Time: 1212.9s, Step: 2036, GPU: 4.9GB\n",
      "Epoch 1, Batch 2040, Loss: 6.5803, Time: 1215.4s, Step: 2041, GPU: 4.9GB\n",
      "Epoch 1, Batch 2045, Loss: 5.2699, Time: 1218.8s, Step: 2046, GPU: 4.9GB\n",
      "Epoch 1, Batch 2050, Loss: 4.8854, Time: 1221.4s, Step: 2051, GPU: 4.9GB\n",
      "Epoch 1, Batch 2055, Loss: 5.3747, Time: 1224.8s, Step: 2056, GPU: 4.9GB\n",
      "Epoch 1, Batch 2060, Loss: 4.9171, Time: 1227.3s, Step: 2061, GPU: 4.9GB\n",
      "Epoch 1, Batch 2065, Loss: 5.3985, Time: 1230.7s, Step: 2066, GPU: 4.9GB\n",
      "Epoch 1, Batch 2070, Loss: 5.6196, Time: 1233.2s, Step: 2071, GPU: 4.9GB\n",
      "Epoch 1, Batch 2075, Loss: 5.5120, Time: 1236.6s, Step: 2076, GPU: 4.9GB\n",
      "Epoch 1, Batch 2080, Loss: 5.4116, Time: 1239.1s, Step: 2081, GPU: 4.9GB\n",
      "Epoch 1, Batch 2085, Loss: 5.4928, Time: 1242.5s, Step: 2086, GPU: 4.9GB\n",
      "Epoch 1, Batch 2090, Loss: 5.3653, Time: 1245.1s, Step: 2091, GPU: 4.9GB\n",
      "Epoch 1, Batch 2095, Loss: 5.7259, Time: 1248.5s, Step: 2096, GPU: 4.9GB\n",
      "Epoch 1, Batch 2100, Loss: 4.8108, Time: 1251.0s, Step: 2101, GPU: 4.9GB\n",
      "Epoch 1, Batch 2105, Loss: 5.5354, Time: 1254.4s, Step: 2106, GPU: 4.9GB\n",
      "Epoch 1, Batch 2110, Loss: 5.9076, Time: 1256.9s, Step: 2111, GPU: 4.9GB\n",
      "Epoch 1, Batch 2115, Loss: 4.8354, Time: 1260.3s, Step: 2116, GPU: 4.9GB\n",
      "Epoch 1, Batch 2120, Loss: 5.7782, Time: 1262.8s, Step: 2121, GPU: 4.9GB\n",
      "Epoch 1, Batch 2125, Loss: 4.9177, Time: 1266.2s, Step: 2126, GPU: 4.9GB\n",
      "Epoch 1, Batch 2130, Loss: 5.5799, Time: 1268.7s, Step: 2131, GPU: 4.9GB\n",
      "Epoch 1, Batch 2135, Loss: 5.4157, Time: 1272.2s, Step: 2136, GPU: 4.9GB\n",
      "Epoch 1, Batch 2140, Loss: 5.1788, Time: 1274.7s, Step: 2141, GPU: 4.9GB\n",
      "Epoch 1, Batch 2145, Loss: 5.1953, Time: 1278.1s, Step: 2146, GPU: 4.9GB\n",
      "Epoch 1, Batch 2150, Loss: 5.1181, Time: 1280.6s, Step: 2151, GPU: 4.9GB\n",
      "Epoch 1, Batch 2155, Loss: 5.6937, Time: 1284.0s, Step: 2156, GPU: 4.9GB\n",
      "Epoch 1, Batch 2160, Loss: 5.2937, Time: 1286.6s, Step: 2161, GPU: 4.9GB\n",
      "Epoch 1, Batch 2165, Loss: 5.0536, Time: 1290.0s, Step: 2166, GPU: 4.9GB\n",
      "Epoch 1, Batch 2170, Loss: 5.3301, Time: 1292.5s, Step: 2171, GPU: 4.9GB\n",
      "Epoch 1, Batch 2175, Loss: 5.4921, Time: 1295.9s, Step: 2176, GPU: 4.9GB\n",
      "Epoch 1, Batch 2180, Loss: 5.1039, Time: 1298.4s, Step: 2181, GPU: 4.9GB\n",
      "Epoch 1, Batch 2185, Loss: 4.7846, Time: 1301.8s, Step: 2186, GPU: 4.9GB\n",
      "Epoch 1, Batch 2190, Loss: 5.8182, Time: 1304.4s, Step: 2191, GPU: 4.9GB\n",
      "Epoch 1, Batch 2195, Loss: 4.8506, Time: 1307.8s, Step: 2196, GPU: 4.9GB\n",
      "Epoch 1, Batch 2200, Loss: 4.9772, Time: 1310.4s, Step: 2201, GPU: 4.9GB\n",
      "Epoch 1, Batch 2205, Loss: 5.0251, Time: 1313.8s, Step: 2206, GPU: 4.9GB\n",
      "Epoch 1, Batch 2210, Loss: 5.5407, Time: 1316.3s, Step: 2211, GPU: 4.9GB\n",
      "Epoch 1, Batch 2215, Loss: 5.1440, Time: 1319.8s, Step: 2216, GPU: 4.9GB\n",
      "Epoch 1, Batch 2220, Loss: 5.6516, Time: 1322.3s, Step: 2221, GPU: 4.9GB\n",
      "Epoch 1, Batch 2225, Loss: 5.1951, Time: 1325.7s, Step: 2226, GPU: 4.9GB\n",
      "Epoch 1, Batch 2230, Loss: 5.5973, Time: 1328.2s, Step: 2231, GPU: 4.9GB\n",
      "Epoch 1, Batch 2235, Loss: 5.4430, Time: 1331.7s, Step: 2236, GPU: 4.9GB\n",
      "Epoch 1, Batch 2240, Loss: 5.2212, Time: 1334.2s, Step: 2241, GPU: 4.9GB\n",
      "Epoch 1, Batch 2245, Loss: 4.5649, Time: 1337.6s, Step: 2246, GPU: 4.9GB\n",
      "Epoch 1, Batch 2250, Loss: 5.1167, Time: 1340.6s, Step: 2251, GPU: 4.9GB\n",
      "Epoch 1, Batch 2255, Loss: 5.0393, Time: 1344.0s, Step: 2256, GPU: 4.9GB\n",
      "Epoch 1, Batch 2260, Loss: 6.3279, Time: 1346.5s, Step: 2261, GPU: 4.9GB\n",
      "Epoch 1, Batch 2265, Loss: 6.1893, Time: 1349.9s, Step: 2266, GPU: 4.9GB\n",
      "Epoch 1, Batch 2270, Loss: 5.6091, Time: 1352.4s, Step: 2271, GPU: 4.9GB\n",
      "Epoch 1, Batch 2275, Loss: 4.9034, Time: 1355.9s, Step: 2276, GPU: 4.9GB\n",
      "Epoch 1, Batch 2280, Loss: 5.4418, Time: 1358.4s, Step: 2281, GPU: 4.9GB\n",
      "Epoch 1, Batch 2285, Loss: 5.0137, Time: 1361.8s, Step: 2286, GPU: 4.9GB\n",
      "Epoch 1, Batch 2290, Loss: 5.3856, Time: 1364.4s, Step: 2291, GPU: 4.9GB\n",
      "Epoch 1, Batch 2295, Loss: 4.9362, Time: 1367.8s, Step: 2296, GPU: 4.9GB\n",
      "Epoch 1, Batch 2300, Loss: 4.9430, Time: 1370.3s, Step: 2301, GPU: 4.9GB\n",
      "Epoch 1, Batch 2305, Loss: 5.2562, Time: 1373.7s, Step: 2306, GPU: 4.9GB\n",
      "Epoch 1, Batch 2310, Loss: 5.7737, Time: 1376.2s, Step: 2311, GPU: 4.9GB\n",
      "Epoch 1, Batch 2315, Loss: 5.6652, Time: 1379.7s, Step: 2316, GPU: 4.9GB\n",
      "Epoch 1, Batch 2320, Loss: 5.1606, Time: 1382.2s, Step: 2321, GPU: 4.9GB\n",
      "Epoch 1, Batch 2325, Loss: 5.3144, Time: 1385.6s, Step: 2326, GPU: 4.9GB\n",
      "Epoch 1, Batch 2330, Loss: 5.1206, Time: 1388.1s, Step: 2331, GPU: 4.9GB\n",
      "Epoch 1, Batch 2335, Loss: 4.8112, Time: 1391.6s, Step: 2336, GPU: 4.9GB\n",
      "Epoch 1, Batch 2340, Loss: 5.0169, Time: 1394.1s, Step: 2341, GPU: 4.9GB\n",
      "Epoch 1, Batch 2345, Loss: 5.2565, Time: 1397.5s, Step: 2346, GPU: 4.9GB\n",
      "Epoch 1, Batch 2350, Loss: 5.0905, Time: 1400.0s, Step: 2351, GPU: 4.9GB\n",
      "Epoch 1, Batch 2355, Loss: 5.0866, Time: 1403.4s, Step: 2356, GPU: 4.9GB\n",
      "Epoch 1, Batch 2360, Loss: 6.0527, Time: 1405.9s, Step: 2361, GPU: 4.9GB\n",
      "Epoch 1, Batch 2365, Loss: 5.2955, Time: 1409.4s, Step: 2366, GPU: 4.9GB\n",
      "Epoch 1, Batch 2370, Loss: 5.0593, Time: 1411.9s, Step: 2371, GPU: 4.9GB\n",
      "Epoch 1, Batch 2375, Loss: 6.1175, Time: 1415.3s, Step: 2376, GPU: 4.9GB\n",
      "Epoch 1, Batch 2380, Loss: 5.1714, Time: 1417.8s, Step: 2381, GPU: 4.9GB\n",
      "Epoch 1, Batch 2385, Loss: 5.1971, Time: 1421.2s, Step: 2386, GPU: 4.9GB\n",
      "Epoch 1, Batch 2390, Loss: 5.8695, Time: 1423.7s, Step: 2391, GPU: 4.9GB\n",
      "Epoch 1, Batch 2395, Loss: 4.5759, Time: 1427.2s, Step: 2396, GPU: 4.9GB\n",
      "Epoch 1, Batch 2400, Loss: 5.0769, Time: 1429.8s, Step: 2401, GPU: 4.9GB\n",
      "Epoch 1, Batch 2405, Loss: 5.1280, Time: 1433.2s, Step: 2406, GPU: 4.9GB\n",
      "Epoch 1, Batch 2410, Loss: 5.6749, Time: 1435.7s, Step: 2411, GPU: 4.9GB\n",
      "Epoch 1, Batch 2415, Loss: 5.8498, Time: 1439.1s, Step: 2416, GPU: 4.9GB\n",
      "Epoch 1, Batch 2420, Loss: 5.2387, Time: 1441.6s, Step: 2421, GPU: 4.9GB\n",
      "Epoch 1, Batch 2425, Loss: 5.5109, Time: 1445.0s, Step: 2426, GPU: 4.9GB\n",
      "Epoch 1, Batch 2430, Loss: 4.4912, Time: 1447.5s, Step: 2431, GPU: 4.9GB\n",
      "Epoch 1, Batch 2435, Loss: 4.7434, Time: 1450.9s, Step: 2436, GPU: 4.9GB\n",
      "Epoch 1, Batch 2440, Loss: 5.7033, Time: 1453.4s, Step: 2441, GPU: 4.9GB\n",
      "Epoch 1, Batch 2445, Loss: 5.9116, Time: 1456.8s, Step: 2446, GPU: 4.9GB\n",
      "Epoch 1, Batch 2450, Loss: 5.0642, Time: 1459.3s, Step: 2451, GPU: 4.9GB\n",
      "Epoch 1, Batch 2455, Loss: 4.6564, Time: 1462.7s, Step: 2456, GPU: 4.9GB\n",
      "Epoch 1, Batch 2460, Loss: 5.1219, Time: 1465.2s, Step: 2461, GPU: 4.9GB\n",
      "Epoch 1, Batch 2465, Loss: 4.9670, Time: 1468.6s, Step: 2466, GPU: 4.9GB\n",
      "Epoch 1, Batch 2470, Loss: 4.1355, Time: 1471.2s, Step: 2471, GPU: 4.9GB\n",
      "Epoch 1, Batch 2475, Loss: 5.9953, Time: 1474.6s, Step: 2476, GPU: 4.9GB\n",
      "Epoch 1, Batch 2480, Loss: 5.1004, Time: 1477.1s, Step: 2481, GPU: 4.9GB\n",
      "Epoch 1, Batch 2485, Loss: 5.3844, Time: 1480.5s, Step: 2486, GPU: 4.9GB\n",
      "Epoch 1, Batch 2490, Loss: 5.8050, Time: 1483.0s, Step: 2491, GPU: 4.9GB\n",
      "Epoch 1, Batch 2495, Loss: 5.7667, Time: 1486.4s, Step: 2496, GPU: 4.9GB\n",
      "Epoch 1, Batch 2500, Loss: 4.9644, Time: 1489.0s, Step: 2501, GPU: 4.9GB\n",
      "Epoch 1, Batch 2505, Loss: 5.4377, Time: 1492.4s, Step: 2506, GPU: 4.9GB\n",
      "Epoch 1, Batch 2510, Loss: 5.9925, Time: 1494.9s, Step: 2511, GPU: 4.9GB\n",
      "Epoch 1, Batch 2515, Loss: 5.4168, Time: 1498.3s, Step: 2516, GPU: 4.9GB\n",
      "Epoch 1, Batch 2520, Loss: 5.9127, Time: 1500.8s, Step: 2521, GPU: 4.9GB\n",
      "Epoch 1, Batch 2525, Loss: 5.3283, Time: 1504.3s, Step: 2526, GPU: 4.9GB\n",
      "Epoch 1, Batch 2530, Loss: 5.6665, Time: 1506.8s, Step: 2531, GPU: 4.9GB\n",
      "Epoch 1, Batch 2535, Loss: 5.5075, Time: 1510.2s, Step: 2536, GPU: 4.9GB\n",
      "Epoch 1, Batch 2540, Loss: 5.5686, Time: 1512.7s, Step: 2541, GPU: 4.9GB\n",
      "Epoch 1, Batch 2545, Loss: 4.8005, Time: 1516.2s, Step: 2546, GPU: 4.9GB\n",
      "Epoch 1, Batch 2550, Loss: 5.5925, Time: 1518.7s, Step: 2551, GPU: 4.9GB\n",
      "Epoch 1, Batch 2555, Loss: 6.0683, Time: 1522.1s, Step: 2556, GPU: 4.9GB\n",
      "Epoch 1, Batch 2560, Loss: 5.6618, Time: 1524.6s, Step: 2561, GPU: 4.9GB\n",
      "Epoch 1, Batch 2565, Loss: 5.4997, Time: 1528.0s, Step: 2566, GPU: 4.9GB\n",
      "Epoch 1, Batch 2570, Loss: 5.1506, Time: 1530.5s, Step: 2571, GPU: 4.9GB\n",
      "Epoch 1, Batch 2575, Loss: 5.1613, Time: 1534.0s, Step: 2576, GPU: 4.9GB\n",
      "Epoch 1, Batch 2580, Loss: 5.1025, Time: 1536.5s, Step: 2581, GPU: 4.9GB\n",
      "Epoch 1, Batch 2585, Loss: 5.6811, Time: 1539.9s, Step: 2586, GPU: 4.9GB\n",
      "Epoch 1, Batch 2590, Loss: 4.5852, Time: 1542.5s, Step: 2591, GPU: 4.9GB\n",
      "Epoch 1, Batch 2595, Loss: 6.0051, Time: 1545.9s, Step: 2596, GPU: 4.9GB\n",
      "Epoch 1, Batch 2600, Loss: 5.1198, Time: 1548.5s, Step: 2601, GPU: 4.9GB\n",
      "Epoch 1, Batch 2605, Loss: 5.5763, Time: 1551.9s, Step: 2606, GPU: 4.9GB\n",
      "Epoch 1, Batch 2610, Loss: 4.9853, Time: 1554.4s, Step: 2611, GPU: 4.9GB\n",
      "Epoch 1, Batch 2615, Loss: 5.2705, Time: 1557.8s, Step: 2616, GPU: 4.9GB\n",
      "Epoch 1, Batch 2620, Loss: 5.1116, Time: 1560.4s, Step: 2621, GPU: 4.9GB\n",
      "Epoch 1, Batch 2625, Loss: 4.7936, Time: 1563.8s, Step: 2626, GPU: 4.9GB\n",
      "Epoch 1, Batch 2630, Loss: 4.1241, Time: 1566.3s, Step: 2631, GPU: 4.9GB\n",
      "Epoch 1, Batch 2635, Loss: 4.3987, Time: 1569.7s, Step: 2636, GPU: 4.9GB\n",
      "Epoch 1, Batch 2640, Loss: 5.1149, Time: 1572.2s, Step: 2641, GPU: 4.9GB\n",
      "Epoch 1, Batch 2645, Loss: 4.9725, Time: 1575.7s, Step: 2646, GPU: 4.9GB\n",
      "Epoch 1, Batch 2650, Loss: 4.7252, Time: 1578.2s, Step: 2651, GPU: 4.9GB\n",
      "Epoch 1, Batch 2655, Loss: 5.8947, Time: 1581.6s, Step: 2656, GPU: 4.9GB\n",
      "Epoch 1, Batch 2660, Loss: 5.1601, Time: 1584.1s, Step: 2661, GPU: 4.9GB\n",
      "Epoch 1, Batch 2665, Loss: 4.7253, Time: 1587.6s, Step: 2666, GPU: 4.9GB\n",
      "Epoch 1, Batch 2670, Loss: 4.6058, Time: 1590.1s, Step: 2671, GPU: 4.9GB\n",
      "Epoch 1, Batch 2675, Loss: 5.5708, Time: 1593.5s, Step: 2676, GPU: 4.9GB\n",
      "Epoch 1, Batch 2680, Loss: 4.8304, Time: 1596.0s, Step: 2681, GPU: 4.9GB\n",
      "Epoch 1, Batch 2685, Loss: 4.3469, Time: 1599.4s, Step: 2686, GPU: 4.9GB\n",
      "Epoch 1, Batch 2690, Loss: 5.6052, Time: 1601.9s, Step: 2691, GPU: 4.9GB\n",
      "Epoch 1, Batch 2695, Loss: 5.3237, Time: 1605.3s, Step: 2696, GPU: 4.9GB\n",
      "Epoch 1, Batch 2700, Loss: 5.5137, Time: 1607.8s, Step: 2701, GPU: 4.9GB\n",
      "Epoch 1, Batch 2705, Loss: 5.2438, Time: 1611.3s, Step: 2706, GPU: 4.9GB\n",
      "Epoch 1, Batch 2710, Loss: 4.6382, Time: 1613.8s, Step: 2711, GPU: 4.9GB\n",
      "Epoch 1, Batch 2715, Loss: 4.8413, Time: 1617.2s, Step: 2716, GPU: 4.9GB\n",
      "Epoch 1, Batch 2720, Loss: 5.1151, Time: 1619.7s, Step: 2721, GPU: 4.9GB\n",
      "Epoch 1, Batch 2725, Loss: 5.3878, Time: 1623.1s, Step: 2726, GPU: 4.9GB\n",
      "Epoch 1, Batch 2730, Loss: 5.2752, Time: 1625.6s, Step: 2731, GPU: 4.9GB\n",
      "Epoch 1, Batch 2735, Loss: 5.2372, Time: 1629.0s, Step: 2736, GPU: 4.9GB\n",
      "Epoch 1, Batch 2740, Loss: 5.7535, Time: 1631.6s, Step: 2741, GPU: 4.9GB\n",
      "Epoch 1, Batch 2745, Loss: 6.1732, Time: 1635.0s, Step: 2746, GPU: 4.9GB\n",
      "Epoch 1, Batch 2750, Loss: 5.4116, Time: 1637.5s, Step: 2751, GPU: 4.9GB\n",
      "Epoch 1, Batch 2755, Loss: 5.1577, Time: 1640.9s, Step: 2756, GPU: 4.9GB\n",
      "Epoch 1, Batch 2760, Loss: 5.1488, Time: 1643.4s, Step: 2761, GPU: 4.9GB\n",
      "Epoch 1, Batch 2765, Loss: 5.3952, Time: 1646.9s, Step: 2766, GPU: 4.9GB\n",
      "Epoch 1, Batch 2770, Loss: 4.7892, Time: 1649.4s, Step: 2771, GPU: 4.9GB\n",
      "Epoch 1, Batch 2775, Loss: 4.6296, Time: 1652.8s, Step: 2776, GPU: 4.9GB\n",
      "Epoch 1, Batch 2780, Loss: 4.7492, Time: 1655.3s, Step: 2781, GPU: 4.9GB\n",
      "Epoch 1, Batch 2785, Loss: 5.4754, Time: 1658.8s, Step: 2786, GPU: 4.9GB\n",
      "Epoch 1, Batch 2790, Loss: 4.4835, Time: 1661.3s, Step: 2791, GPU: 4.9GB\n",
      "Epoch 1, Batch 2795, Loss: 5.4646, Time: 1664.7s, Step: 2796, GPU: 4.9GB\n",
      "Epoch 1, Batch 2800, Loss: 5.3058, Time: 1667.3s, Step: 2801, GPU: 4.9GB\n",
      "Epoch 1, Batch 2805, Loss: 6.0349, Time: 1670.7s, Step: 2806, GPU: 4.9GB\n",
      "Epoch 1, Batch 2810, Loss: 5.8991, Time: 1673.2s, Step: 2811, GPU: 4.9GB\n",
      "Epoch 1, Batch 2815, Loss: 4.9054, Time: 1676.7s, Step: 2816, GPU: 4.9GB\n",
      "Epoch 1, Batch 2820, Loss: 5.0022, Time: 1679.2s, Step: 2821, GPU: 4.9GB\n",
      "Epoch 1, Batch 2825, Loss: 4.7964, Time: 1682.6s, Step: 2826, GPU: 4.9GB\n",
      "Epoch 1, Batch 2830, Loss: 5.3272, Time: 1685.1s, Step: 2831, GPU: 4.9GB\n",
      "Epoch 1, Batch 2835, Loss: 5.3167, Time: 1688.5s, Step: 2836, GPU: 4.9GB\n",
      "Epoch 1, Batch 2840, Loss: 4.7220, Time: 1691.0s, Step: 2841, GPU: 4.9GB\n",
      "Epoch 1, Batch 2845, Loss: 4.6616, Time: 1694.5s, Step: 2846, GPU: 4.9GB\n",
      "Epoch 1, Batch 2850, Loss: 5.0720, Time: 1697.0s, Step: 2851, GPU: 4.9GB\n",
      "Epoch 1, Batch 2855, Loss: 4.9030, Time: 1700.4s, Step: 2856, GPU: 4.9GB\n",
      "Epoch 1, Batch 2860, Loss: 5.0354, Time: 1702.9s, Step: 2861, GPU: 4.9GB\n",
      "Epoch 1, Batch 2865, Loss: 4.8380, Time: 1706.3s, Step: 2866, GPU: 4.9GB\n",
      "Epoch 1, Batch 2870, Loss: 4.5637, Time: 1708.8s, Step: 2871, GPU: 4.9GB\n",
      "Epoch 1, Batch 2875, Loss: 4.9374, Time: 1712.3s, Step: 2876, GPU: 4.9GB\n",
      "Epoch 1, Batch 2880, Loss: 5.1210, Time: 1714.8s, Step: 2881, GPU: 4.9GB\n",
      "Epoch 1, Batch 2885, Loss: 5.4662, Time: 1718.3s, Step: 2886, GPU: 4.9GB\n",
      "Epoch 1, Batch 2890, Loss: 5.3966, Time: 1720.8s, Step: 2891, GPU: 4.9GB\n",
      "Epoch 1, Batch 2895, Loss: 5.1618, Time: 1724.2s, Step: 2896, GPU: 4.9GB\n",
      "Epoch 1, Batch 2900, Loss: 5.5445, Time: 1726.8s, Step: 2901, GPU: 4.9GB\n",
      "Epoch 1, Batch 2905, Loss: 4.8347, Time: 1730.2s, Step: 2906, GPU: 4.9GB\n",
      "Epoch 1, Batch 2910, Loss: 5.7268, Time: 1732.7s, Step: 2911, GPU: 4.9GB\n",
      "Epoch 1, Batch 2915, Loss: 5.0133, Time: 1736.1s, Step: 2916, GPU: 4.9GB\n",
      "Epoch 1, Batch 2920, Loss: 4.3551, Time: 1738.6s, Step: 2921, GPU: 4.9GB\n",
      "Epoch 1, Batch 2925, Loss: 5.5423, Time: 1742.0s, Step: 2926, GPU: 4.9GB\n",
      "Epoch 1, Batch 2930, Loss: 5.5355, Time: 1744.5s, Step: 2931, GPU: 4.9GB\n",
      "Epoch 1, Batch 2935, Loss: 5.2752, Time: 1748.0s, Step: 2936, GPU: 4.9GB\n",
      "Epoch 1, Batch 2940, Loss: 4.8626, Time: 1750.5s, Step: 2941, GPU: 4.9GB\n",
      "Epoch 1, Batch 2945, Loss: 4.7095, Time: 1753.9s, Step: 2946, GPU: 4.9GB\n",
      "Epoch 1, Batch 2950, Loss: 4.8909, Time: 1756.4s, Step: 2951, GPU: 4.9GB\n",
      "Epoch 1, Batch 2955, Loss: 4.5032, Time: 1759.9s, Step: 2956, GPU: 4.9GB\n",
      "Epoch 1, Batch 2960, Loss: 5.6033, Time: 1762.4s, Step: 2961, GPU: 4.9GB\n",
      "Epoch 1, Batch 2965, Loss: 4.8153, Time: 1765.8s, Step: 2966, GPU: 4.9GB\n",
      "Epoch 1, Batch 2970, Loss: 5.3428, Time: 1768.3s, Step: 2971, GPU: 4.9GB\n",
      "Epoch 1, Batch 2975, Loss: 4.7569, Time: 1771.7s, Step: 2976, GPU: 4.9GB\n",
      "Epoch 1, Batch 2980, Loss: 5.1505, Time: 1774.2s, Step: 2981, GPU: 4.9GB\n",
      "Epoch 1, Batch 2985, Loss: 5.1563, Time: 1777.7s, Step: 2986, GPU: 4.9GB\n",
      "Epoch 1, Batch 2990, Loss: 4.6220, Time: 1780.2s, Step: 2991, GPU: 4.9GB\n",
      "Epoch 1, Batch 2995, Loss: 4.1238, Time: 1783.6s, Step: 2996, GPU: 4.9GB\n",
      "Epoch 1, Batch 3000, Loss: 4.8542, Time: 1786.2s, Step: 3001, GPU: 4.9GB\n",
      "Epoch 1, Batch 3005, Loss: 5.4376, Time: 1789.6s, Step: 3006, GPU: 4.9GB\n",
      "Epoch 1, Batch 3010, Loss: 4.8757, Time: 1792.1s, Step: 3011, GPU: 4.9GB\n",
      "Epoch 1, Batch 3015, Loss: 5.2721, Time: 1795.5s, Step: 3016, GPU: 4.9GB\n",
      "Epoch 1, Batch 3020, Loss: 4.9982, Time: 1798.0s, Step: 3021, GPU: 4.9GB\n",
      "Epoch 1, Batch 3025, Loss: 4.9250, Time: 1801.4s, Step: 3026, GPU: 4.9GB\n",
      "Epoch 1, Batch 3030, Loss: 5.2802, Time: 1803.9s, Step: 3031, GPU: 4.9GB\n",
      "Epoch 1, Batch 3035, Loss: 5.3684, Time: 1807.3s, Step: 3036, GPU: 4.9GB\n",
      "Epoch 1, Batch 3040, Loss: 4.8860, Time: 1809.8s, Step: 3041, GPU: 4.9GB\n",
      "Epoch 1, Batch 3045, Loss: 5.0080, Time: 1813.3s, Step: 3046, GPU: 4.9GB\n",
      "Epoch 1, Batch 3050, Loss: 3.6423, Time: 1815.8s, Step: 3051, GPU: 4.9GB\n",
      "Epoch 1, Batch 3055, Loss: 4.7440, Time: 1819.2s, Step: 3056, GPU: 4.9GB\n",
      "Epoch 1, Batch 3060, Loss: 4.6088, Time: 1821.7s, Step: 3061, GPU: 4.9GB\n",
      "Epoch 1, Batch 3065, Loss: 5.4154, Time: 1825.1s, Step: 3066, GPU: 4.9GB\n",
      "Epoch 1, Batch 3070, Loss: 4.9744, Time: 1827.6s, Step: 3071, GPU: 4.9GB\n",
      "Epoch 1, Batch 3075, Loss: 4.9285, Time: 1831.0s, Step: 3076, GPU: 4.9GB\n",
      "Epoch 1, Batch 3080, Loss: 5.1037, Time: 1833.5s, Step: 3081, GPU: 4.9GB\n",
      "Epoch 1, Batch 3085, Loss: 4.2390, Time: 1836.9s, Step: 3086, GPU: 4.9GB\n",
      "Epoch 1, Batch 3090, Loss: 6.4008, Time: 1839.5s, Step: 3091, GPU: 4.9GB\n",
      "Epoch 1, Batch 3095, Loss: 5.1314, Time: 1842.8s, Step: 3096, GPU: 4.9GB\n",
      "Epoch 1, Batch 3100, Loss: 5.1869, Time: 1845.4s, Step: 3101, GPU: 4.9GB\n",
      "Epoch 1, Batch 3105, Loss: 5.7816, Time: 1848.8s, Step: 3106, GPU: 4.9GB\n",
      "Epoch 1, Batch 3110, Loss: 4.7964, Time: 1851.3s, Step: 3111, GPU: 4.9GB\n",
      "Epoch 1, Batch 3115, Loss: 4.9903, Time: 1854.7s, Step: 3116, GPU: 4.9GB\n",
      "Epoch 1, Batch 3120, Loss: 6.0169, Time: 1857.2s, Step: 3121, GPU: 4.9GB\n",
      "Epoch 1, Batch 3125, Loss: 5.0713, Time: 1860.6s, Step: 3126, GPU: 4.9GB\n",
      "Epoch 1, Batch 3130, Loss: 5.0891, Time: 1863.1s, Step: 3131, GPU: 4.9GB\n",
      "Epoch 1, Batch 3135, Loss: 6.3263, Time: 1866.5s, Step: 3136, GPU: 4.9GB\n",
      "Epoch 1, Batch 3140, Loss: 5.1099, Time: 1869.0s, Step: 3141, GPU: 4.9GB\n",
      "Epoch 1, Batch 3145, Loss: 5.1090, Time: 1872.5s, Step: 3146, GPU: 4.9GB\n",
      "Epoch 1, Batch 3150, Loss: 5.2644, Time: 1875.0s, Step: 3151, GPU: 4.9GB\n",
      "Epoch 1, Batch 3155, Loss: 4.7849, Time: 1878.4s, Step: 3156, GPU: 4.9GB\n",
      "Epoch 1, Batch 3160, Loss: 4.8301, Time: 1880.9s, Step: 3161, GPU: 4.9GB\n",
      "Epoch 1, Batch 3165, Loss: 4.6702, Time: 1884.3s, Step: 3166, GPU: 4.9GB\n",
      "Epoch 1, Batch 3170, Loss: 5.1192, Time: 1886.8s, Step: 3171, GPU: 4.9GB\n",
      "Epoch 1, Batch 3175, Loss: 5.2835, Time: 1890.2s, Step: 3176, GPU: 4.9GB\n",
      "Epoch 1, Batch 3180, Loss: 5.2363, Time: 1892.7s, Step: 3181, GPU: 4.9GB\n",
      "Epoch 1, Batch 3185, Loss: 5.1296, Time: 1896.1s, Step: 3186, GPU: 4.9GB\n",
      "Epoch 1, Batch 3190, Loss: 5.0779, Time: 1898.6s, Step: 3191, GPU: 4.9GB\n",
      "Epoch 1, Batch 3195, Loss: 5.2296, Time: 1902.0s, Step: 3196, GPU: 4.9GB\n",
      "Epoch 1, Batch 3200, Loss: 4.1170, Time: 1904.6s, Step: 3201, GPU: 4.9GB\n",
      "Epoch 1, Batch 3205, Loss: 4.4267, Time: 1908.0s, Step: 3206, GPU: 4.9GB\n",
      "Epoch 1, Batch 3210, Loss: 4.6529, Time: 1910.5s, Step: 3211, GPU: 4.9GB\n",
      "Epoch 1, Batch 3215, Loss: 5.7916, Time: 1913.9s, Step: 3216, GPU: 4.9GB\n",
      "Epoch 1, Batch 3220, Loss: 4.8095, Time: 1916.4s, Step: 3221, GPU: 4.9GB\n",
      "Epoch 1, Batch 3225, Loss: 5.1396, Time: 1919.8s, Step: 3226, GPU: 4.9GB\n",
      "Epoch 1, Batch 3230, Loss: 6.6235, Time: 1922.4s, Step: 3231, GPU: 4.9GB\n",
      "Epoch 1, Batch 3235, Loss: 5.3946, Time: 1925.8s, Step: 3236, GPU: 4.9GB\n",
      "Epoch 1, Batch 3240, Loss: 5.1783, Time: 1928.3s, Step: 3241, GPU: 4.9GB\n",
      "Epoch 1, Batch 3245, Loss: 5.3730, Time: 1931.7s, Step: 3246, GPU: 4.9GB\n",
      "Epoch 1, Batch 3250, Loss: 5.8996, Time: 1934.2s, Step: 3251, GPU: 4.9GB\n",
      "Epoch 1, Batch 3255, Loss: 4.9870, Time: 1937.6s, Step: 3256, GPU: 4.9GB\n",
      "Epoch 1, Batch 3260, Loss: 5.3495, Time: 1940.1s, Step: 3261, GPU: 4.9GB\n",
      "Epoch 1, Batch 3265, Loss: 5.4586, Time: 1943.5s, Step: 3266, GPU: 4.9GB\n",
      "Epoch 1, Batch 3270, Loss: 4.9782, Time: 1946.0s, Step: 3271, GPU: 4.9GB\n",
      "Epoch 1, Batch 3275, Loss: 5.2244, Time: 1949.4s, Step: 3276, GPU: 4.9GB\n",
      "Epoch 1, Batch 3280, Loss: 4.6198, Time: 1951.9s, Step: 3281, GPU: 4.9GB\n",
      "Epoch 1, Batch 3285, Loss: 5.1670, Time: 1955.3s, Step: 3286, GPU: 4.9GB\n",
      "Epoch 1, Batch 3290, Loss: 4.4475, Time: 1957.9s, Step: 3291, GPU: 4.9GB\n",
      "Epoch 1, Batch 3295, Loss: 4.7284, Time: 1961.3s, Step: 3296, GPU: 4.9GB\n",
      "Epoch 1, Batch 3300, Loss: 5.5440, Time: 1963.8s, Step: 3301, GPU: 4.9GB\n",
      "Epoch 1, Batch 3305, Loss: 5.5819, Time: 1967.2s, Step: 3306, GPU: 4.9GB\n",
      "Epoch 1, Batch 3310, Loss: 5.6989, Time: 1969.7s, Step: 3311, GPU: 4.9GB\n",
      "Epoch 1, Batch 3315, Loss: 4.9378, Time: 1973.1s, Step: 3316, GPU: 4.9GB\n",
      "Epoch 1, Batch 3320, Loss: 4.9899, Time: 1975.6s, Step: 3321, GPU: 4.9GB\n",
      "Epoch 1, Batch 3325, Loss: 4.6247, Time: 1979.0s, Step: 3326, GPU: 4.9GB\n",
      "Epoch 1, Batch 3330, Loss: 4.5209, Time: 1981.5s, Step: 3331, GPU: 4.9GB\n",
      "Epoch 1, Batch 3335, Loss: 5.0269, Time: 1985.0s, Step: 3336, GPU: 4.9GB\n",
      "Epoch 1, Batch 3340, Loss: 5.2241, Time: 1987.5s, Step: 3341, GPU: 4.9GB\n",
      "Epoch 1, Batch 3345, Loss: 5.1384, Time: 1990.9s, Step: 3346, GPU: 4.9GB\n",
      "Epoch 1, Batch 3350, Loss: 4.9497, Time: 1993.4s, Step: 3351, GPU: 4.9GB\n",
      "Epoch 1, Batch 3355, Loss: 4.4435, Time: 1996.9s, Step: 3356, GPU: 4.9GB\n",
      "Epoch 1, Batch 3360, Loss: 4.1240, Time: 1999.4s, Step: 3361, GPU: 4.9GB\n",
      "Epoch 1, Batch 3365, Loss: 5.4080, Time: 2002.8s, Step: 3366, GPU: 4.9GB\n",
      "Epoch 1, Batch 3370, Loss: 5.7918, Time: 2005.3s, Step: 3371, GPU: 4.9GB\n",
      "Epoch 1, Batch 3375, Loss: 4.4888, Time: 2008.7s, Step: 3376, GPU: 4.9GB\n",
      "Epoch 1, Batch 3380, Loss: 4.6680, Time: 2011.2s, Step: 3381, GPU: 4.9GB\n",
      "Epoch 1, Batch 3385, Loss: 4.8374, Time: 2014.6s, Step: 3386, GPU: 4.9GB\n",
      "Epoch 1, Batch 3390, Loss: 4.1634, Time: 2017.1s, Step: 3391, GPU: 4.9GB\n",
      "Epoch 1, Batch 3395, Loss: 4.7139, Time: 2020.6s, Step: 3396, GPU: 4.9GB\n",
      "Epoch 1, Batch 3400, Loss: 5.5247, Time: 2023.1s, Step: 3401, GPU: 4.9GB\n",
      "Epoch 1, Batch 3405, Loss: 5.2538, Time: 2026.6s, Step: 3406, GPU: 4.9GB\n",
      "Epoch 1, Batch 3410, Loss: 4.5447, Time: 2029.1s, Step: 3411, GPU: 4.9GB\n",
      "Epoch 1, Batch 3415, Loss: 4.9485, Time: 2032.5s, Step: 3416, GPU: 4.9GB\n",
      "Epoch 1, Batch 3420, Loss: 4.9179, Time: 2035.0s, Step: 3421, GPU: 4.9GB\n",
      "Epoch 1, Batch 3425, Loss: 4.3734, Time: 2038.4s, Step: 3426, GPU: 4.9GB\n",
      "Epoch 1, Batch 3430, Loss: 5.0304, Time: 2040.9s, Step: 3431, GPU: 4.9GB\n",
      "Epoch 1, Batch 3435, Loss: 5.7027, Time: 2044.3s, Step: 3436, GPU: 4.9GB\n",
      "Epoch 1, Batch 3440, Loss: 4.7165, Time: 2046.9s, Step: 3441, GPU: 4.9GB\n",
      "Epoch 1, Batch 3445, Loss: 5.5106, Time: 2050.3s, Step: 3446, GPU: 4.9GB\n",
      "Epoch 1, Batch 3450, Loss: 4.9964, Time: 2052.8s, Step: 3451, GPU: 4.9GB\n",
      "Epoch 1, Batch 3455, Loss: 5.0446, Time: 2056.2s, Step: 3456, GPU: 4.9GB\n",
      "Epoch 1, Batch 3460, Loss: 4.8702, Time: 2058.7s, Step: 3461, GPU: 4.9GB\n",
      "Epoch 1, Batch 3465, Loss: 4.4713, Time: 2062.2s, Step: 3466, GPU: 4.9GB\n",
      "Epoch 1, Batch 3470, Loss: 5.8136, Time: 2064.7s, Step: 3471, GPU: 4.9GB\n",
      "Epoch 1, Batch 3475, Loss: 4.4656, Time: 2068.1s, Step: 3476, GPU: 4.9GB\n",
      "Epoch 1, Batch 3480, Loss: 4.8701, Time: 2070.6s, Step: 3481, GPU: 4.9GB\n",
      "Epoch 1, Batch 3485, Loss: 5.8271, Time: 2074.0s, Step: 3486, GPU: 4.9GB\n",
      "Epoch 1, Batch 3490, Loss: 4.9884, Time: 2076.6s, Step: 3491, GPU: 4.9GB\n",
      "Epoch 1, Batch 3495, Loss: 5.0447, Time: 2080.1s, Step: 3496, GPU: 4.9GB\n",
      "Epoch 1, Batch 3500, Loss: 5.3475, Time: 2082.6s, Step: 3501, GPU: 4.9GB\n",
      "Epoch 1, Batch 3505, Loss: 5.2504, Time: 2086.1s, Step: 3506, GPU: 4.9GB\n",
      "Epoch 1, Batch 3510, Loss: 5.3711, Time: 2088.6s, Step: 3511, GPU: 4.9GB\n",
      "Epoch 1, Batch 3515, Loss: 4.9809, Time: 2092.0s, Step: 3516, GPU: 4.9GB\n",
      "Epoch 1, Batch 3520, Loss: 5.3160, Time: 2094.5s, Step: 3521, GPU: 4.9GB\n",
      "Epoch 1, Batch 3525, Loss: 4.8036, Time: 2098.0s, Step: 3526, GPU: 4.9GB\n",
      "Epoch 1, Batch 3530, Loss: 5.6849, Time: 2100.5s, Step: 3531, GPU: 4.9GB\n",
      "Epoch 1, Batch 3535, Loss: 5.1854, Time: 2103.9s, Step: 3536, GPU: 4.9GB\n",
      "Epoch 1, Batch 3540, Loss: 4.5143, Time: 2106.5s, Step: 3541, GPU: 4.9GB\n",
      "Epoch 1, Batch 3545, Loss: 5.4466, Time: 2109.9s, Step: 3546, GPU: 4.9GB\n",
      "Epoch 1, Batch 3550, Loss: 4.9077, Time: 2112.4s, Step: 3551, GPU: 4.9GB\n",
      "Epoch 1, Batch 3555, Loss: 4.8981, Time: 2115.8s, Step: 3556, GPU: 4.9GB\n",
      "Epoch 1, Batch 3560, Loss: 5.0728, Time: 2118.3s, Step: 3561, GPU: 4.9GB\n",
      "Epoch 1, Batch 3565, Loss: 4.9260, Time: 2121.8s, Step: 3566, GPU: 4.9GB\n",
      "Epoch 1, Batch 3570, Loss: 5.7727, Time: 2124.3s, Step: 3571, GPU: 4.9GB\n",
      "Epoch 1, Batch 3575, Loss: 5.0333, Time: 2127.7s, Step: 3576, GPU: 4.9GB\n",
      "Epoch 1, Batch 3580, Loss: 4.9143, Time: 2130.2s, Step: 3581, GPU: 4.9GB\n",
      "Epoch 1, Batch 3585, Loss: 4.3669, Time: 2133.6s, Step: 3586, GPU: 4.9GB\n",
      "Epoch 1, Batch 3590, Loss: 5.6671, Time: 2136.1s, Step: 3591, GPU: 4.9GB\n",
      "Epoch 1, Batch 3595, Loss: 5.1872, Time: 2139.5s, Step: 3596, GPU: 4.9GB\n",
      "Epoch 1, Batch 3600, Loss: 4.7610, Time: 2142.1s, Step: 3601, GPU: 4.9GB\n",
      "Epoch 1, Batch 3605, Loss: 4.6924, Time: 2145.5s, Step: 3606, GPU: 4.9GB\n",
      "Epoch 1, Batch 3610, Loss: 4.4508, Time: 2148.0s, Step: 3611, GPU: 4.9GB\n",
      "Epoch 1, Batch 3615, Loss: 5.4894, Time: 2151.4s, Step: 3616, GPU: 4.9GB\n",
      "Epoch 1, Batch 3620, Loss: 5.1314, Time: 2153.9s, Step: 3621, GPU: 4.9GB\n",
      "Epoch 1, Batch 3625, Loss: 4.6020, Time: 2157.3s, Step: 3626, GPU: 4.9GB\n",
      "Epoch 1, Batch 3630, Loss: 4.8248, Time: 2159.8s, Step: 3631, GPU: 4.9GB\n",
      "Epoch 1, Batch 3635, Loss: 5.8959, Time: 2163.3s, Step: 3636, GPU: 4.9GB\n",
      "Epoch 1, Batch 3640, Loss: 3.8631, Time: 2165.8s, Step: 3641, GPU: 4.9GB\n",
      "Epoch 1, Batch 3645, Loss: 5.0632, Time: 2169.1s, Step: 3646, GPU: 4.9GB\n",
      "Epoch 1, Batch 3650, Loss: 5.0128, Time: 2171.7s, Step: 3651, GPU: 4.9GB\n",
      "Epoch 1, Batch 3655, Loss: 5.1703, Time: 2175.1s, Step: 3656, GPU: 4.9GB\n",
      "Epoch 1, Batch 3660, Loss: 4.6328, Time: 2177.6s, Step: 3661, GPU: 4.9GB\n",
      "Epoch 1, Batch 3665, Loss: 5.4337, Time: 2181.0s, Step: 3666, GPU: 4.9GB\n",
      "Epoch 1, Batch 3670, Loss: 5.1308, Time: 2183.5s, Step: 3671, GPU: 4.9GB\n",
      "Epoch 1, Batch 3675, Loss: 4.9795, Time: 2186.9s, Step: 3676, GPU: 4.9GB\n",
      "Epoch 1, Batch 3680, Loss: 4.7211, Time: 2189.4s, Step: 3681, GPU: 4.9GB\n",
      "Epoch 1, Batch 3685, Loss: 5.2319, Time: 2192.8s, Step: 3686, GPU: 4.9GB\n",
      "Epoch 1, Batch 3690, Loss: 6.0051, Time: 2195.3s, Step: 3691, GPU: 4.9GB\n",
      "Epoch 1, Batch 3695, Loss: 4.8414, Time: 2198.7s, Step: 3696, GPU: 4.9GB\n",
      "Epoch 1, Batch 3700, Loss: 5.3563, Time: 2201.2s, Step: 3701, GPU: 4.9GB\n",
      "Epoch 1, Batch 3705, Loss: 5.1419, Time: 2204.6s, Step: 3706, GPU: 4.9GB\n",
      "Epoch 1, Batch 3710, Loss: 4.9204, Time: 2207.2s, Step: 3711, GPU: 4.9GB\n",
      "Epoch 1, Batch 3715, Loss: 5.1441, Time: 2210.6s, Step: 3716, GPU: 4.9GB\n",
      "Epoch 1, Batch 3720, Loss: 5.4013, Time: 2213.1s, Step: 3721, GPU: 4.9GB\n",
      "Epoch 1, Batch 3725, Loss: 4.9992, Time: 2216.5s, Step: 3726, GPU: 4.9GB\n",
      "Epoch 1, Batch 3730, Loss: 6.8252, Time: 2219.0s, Step: 3731, GPU: 4.9GB\n",
      "Epoch 1, Batch 3735, Loss: 4.7953, Time: 2222.4s, Step: 3736, GPU: 4.9GB\n",
      "Epoch 1, Batch 3740, Loss: 3.9651, Time: 2224.9s, Step: 3741, GPU: 4.9GB\n",
      "Epoch 1, Batch 3745, Loss: 4.6134, Time: 2228.3s, Step: 3746, GPU: 4.9GB\n",
      "Epoch 1, Batch 3750, Loss: 5.1079, Time: 2230.8s, Step: 3751, GPU: 4.9GB\n",
      "Epoch 1, Batch 3755, Loss: 4.9266, Time: 2234.2s, Step: 3756, GPU: 4.9GB\n",
      "Epoch 1, Batch 3760, Loss: 4.9637, Time: 2236.7s, Step: 3761, GPU: 4.9GB\n",
      "Epoch 1, Batch 3765, Loss: 5.5053, Time: 2240.1s, Step: 3766, GPU: 4.9GB\n",
      "Epoch 1, Batch 3770, Loss: 4.7544, Time: 2242.6s, Step: 3771, GPU: 4.9GB\n",
      "Epoch 1, Batch 3775, Loss: 5.1698, Time: 2246.0s, Step: 3776, GPU: 4.9GB\n",
      "Epoch 1, Batch 3780, Loss: 5.3573, Time: 2248.5s, Step: 3781, GPU: 4.9GB\n",
      "Epoch 1, Batch 3785, Loss: 4.7165, Time: 2251.9s, Step: 3786, GPU: 4.9GB\n",
      "Epoch 1, Batch 3790, Loss: 5.4067, Time: 2254.5s, Step: 3791, GPU: 4.9GB\n",
      "Epoch 1, Batch 3795, Loss: 4.5505, Time: 2257.8s, Step: 3796, GPU: 4.9GB\n",
      "Epoch 1, Batch 3800, Loss: 5.5675, Time: 2260.5s, Step: 3801, GPU: 4.9GB\n",
      "Epoch 1, Batch 3805, Loss: 4.7699, Time: 2263.9s, Step: 3806, GPU: 4.9GB\n",
      "Epoch 1, Batch 3810, Loss: 5.3601, Time: 2266.4s, Step: 3811, GPU: 4.9GB\n",
      "Epoch 1, Batch 3815, Loss: 5.0059, Time: 2269.8s, Step: 3816, GPU: 4.9GB\n",
      "Epoch 1, Batch 3820, Loss: 3.7418, Time: 2272.3s, Step: 3821, GPU: 4.9GB\n",
      "Epoch 1, Batch 3825, Loss: 6.3390, Time: 2275.7s, Step: 3826, GPU: 4.9GB\n",
      "Epoch 1, Batch 3830, Loss: 4.8626, Time: 2278.3s, Step: 3831, GPU: 4.9GB\n",
      "Epoch 1, Batch 3835, Loss: 4.6047, Time: 2281.7s, Step: 3836, GPU: 4.9GB\n",
      "Epoch 1, Batch 3840, Loss: 4.7102, Time: 2284.2s, Step: 3841, GPU: 4.9GB\n",
      "Epoch 1, Batch 3845, Loss: 4.9653, Time: 2287.6s, Step: 3846, GPU: 4.9GB\n",
      "Epoch 1, Batch 3850, Loss: 5.5934, Time: 2290.1s, Step: 3851, GPU: 4.9GB\n",
      "Epoch 1, Batch 3855, Loss: 4.6564, Time: 2293.5s, Step: 3856, GPU: 4.9GB\n",
      "Epoch 1, Batch 3860, Loss: 5.2955, Time: 2296.0s, Step: 3861, GPU: 4.9GB\n",
      "Epoch 1, Batch 3865, Loss: 4.7444, Time: 2299.5s, Step: 3866, GPU: 4.9GB\n",
      "Epoch 1, Batch 3870, Loss: 4.6003, Time: 2302.0s, Step: 3871, GPU: 4.9GB\n",
      "Epoch 1, Batch 3875, Loss: 5.3526, Time: 2305.4s, Step: 3876, GPU: 4.9GB\n",
      "Epoch 1, Batch 3880, Loss: 4.1251, Time: 2307.9s, Step: 3881, GPU: 4.9GB\n",
      "Epoch 1, Batch 3885, Loss: 4.9448, Time: 2311.3s, Step: 3886, GPU: 4.9GB\n",
      "Epoch 1, Batch 3890, Loss: 4.4525, Time: 2313.8s, Step: 3891, GPU: 4.9GB\n",
      "Epoch 1, Batch 3895, Loss: 5.0674, Time: 2317.2s, Step: 3896, GPU: 4.9GB\n",
      "Epoch 1, Batch 3900, Loss: 4.2502, Time: 2319.8s, Step: 3901, GPU: 4.9GB\n",
      "Epoch 1, Batch 3905, Loss: 4.9252, Time: 2323.2s, Step: 3906, GPU: 4.9GB\n",
      "Epoch 1, Batch 3910, Loss: 4.5839, Time: 2325.7s, Step: 3911, GPU: 4.9GB\n",
      "Epoch 1, Batch 3915, Loss: 5.1444, Time: 2329.1s, Step: 3916, GPU: 4.9GB\n",
      "Epoch 1, Batch 3920, Loss: 4.6588, Time: 2331.7s, Step: 3921, GPU: 4.9GB\n",
      "Epoch 1, Batch 3925, Loss: 4.8199, Time: 2335.1s, Step: 3926, GPU: 4.9GB\n",
      "Epoch 1, Batch 3930, Loss: 5.2270, Time: 2337.6s, Step: 3931, GPU: 4.9GB\n",
      "Epoch 1, Batch 3935, Loss: 5.0369, Time: 2341.1s, Step: 3936, GPU: 4.9GB\n",
      "Epoch 1, Batch 3940, Loss: 4.6899, Time: 2343.6s, Step: 3941, GPU: 4.9GB\n",
      "Epoch 1, Batch 3945, Loss: 5.4158, Time: 2347.0s, Step: 3946, GPU: 4.9GB\n",
      "Epoch 1, Batch 3950, Loss: 4.4945, Time: 2349.6s, Step: 3951, GPU: 4.9GB\n",
      "Epoch 1, Batch 3955, Loss: 4.0578, Time: 2353.0s, Step: 3956, GPU: 4.9GB\n",
      "Epoch 1, Batch 3960, Loss: 4.6869, Time: 2355.5s, Step: 3961, GPU: 4.9GB\n",
      "Epoch 1, Batch 3965, Loss: 5.1071, Time: 2358.9s, Step: 3966, GPU: 4.9GB\n",
      "Epoch 1, Batch 3970, Loss: 4.4895, Time: 2361.4s, Step: 3971, GPU: 4.9GB\n",
      "Epoch 1, Batch 3975, Loss: 4.9334, Time: 2364.8s, Step: 3976, GPU: 4.9GB\n",
      "Epoch 1, Batch 3980, Loss: 4.9447, Time: 2367.3s, Step: 3981, GPU: 4.9GB\n",
      "Epoch 1, Batch 3985, Loss: 5.2062, Time: 2370.7s, Step: 3986, GPU: 4.9GB\n",
      "Epoch 1, Batch 3990, Loss: 4.1042, Time: 2373.2s, Step: 3991, GPU: 4.9GB\n",
      "Epoch 1, Batch 3995, Loss: 5.7510, Time: 2376.6s, Step: 3996, GPU: 4.9GB\n",
      "Epoch 1, Batch 4000, Loss: 5.1437, Time: 2379.2s, Step: 4001, GPU: 4.9GB\n",
      "Epoch 1, Batch 4005, Loss: 5.1031, Time: 2382.6s, Step: 4006, GPU: 4.9GB\n",
      "Epoch 1, Batch 4010, Loss: 4.8767, Time: 2385.1s, Step: 4011, GPU: 4.9GB\n",
      "Epoch 1, Batch 4015, Loss: 5.1667, Time: 2388.5s, Step: 4016, GPU: 4.9GB\n",
      "Epoch 1, Batch 4020, Loss: 4.9396, Time: 2391.1s, Step: 4021, GPU: 4.9GB\n",
      "Epoch 1, Batch 4025, Loss: 5.1097, Time: 2394.5s, Step: 4026, GPU: 4.9GB\n",
      "Epoch 1, Batch 4030, Loss: 4.2807, Time: 2397.0s, Step: 4031, GPU: 4.9GB\n",
      "Epoch 1, Batch 4035, Loss: 4.8498, Time: 2400.4s, Step: 4036, GPU: 4.9GB\n",
      "Epoch 1, Batch 4040, Loss: 4.5460, Time: 2402.9s, Step: 4041, GPU: 4.9GB\n",
      "Epoch 1, Batch 4045, Loss: 4.4228, Time: 2406.3s, Step: 4046, GPU: 4.9GB\n",
      "Epoch 1, Batch 4050, Loss: 5.1052, Time: 2408.9s, Step: 4051, GPU: 4.9GB\n",
      "Epoch 1, Batch 4055, Loss: 5.4000, Time: 2412.3s, Step: 4056, GPU: 4.9GB\n",
      "Epoch 1, Batch 4060, Loss: 4.5506, Time: 2414.8s, Step: 4061, GPU: 4.9GB\n",
      "Epoch 1, Batch 4065, Loss: 4.1516, Time: 2418.2s, Step: 4066, GPU: 4.9GB\n",
      "Epoch 1, Batch 4070, Loss: 4.9178, Time: 2420.7s, Step: 4071, GPU: 4.9GB\n",
      "Epoch 1, Batch 4075, Loss: 5.9023, Time: 2424.2s, Step: 4076, GPU: 4.9GB\n",
      "Epoch 1, Batch 4080, Loss: 4.9527, Time: 2426.7s, Step: 4081, GPU: 4.9GB\n",
      "Epoch 1, Batch 4085, Loss: 5.0513, Time: 2430.1s, Step: 4086, GPU: 4.9GB\n",
      "Epoch 1, Batch 4090, Loss: 5.2071, Time: 2432.6s, Step: 4091, GPU: 4.9GB\n",
      "Epoch 1, Batch 4095, Loss: 5.0744, Time: 2436.1s, Step: 4096, GPU: 4.9GB\n",
      "Epoch 1, Batch 4100, Loss: 5.1592, Time: 2438.6s, Step: 4101, GPU: 4.9GB\n",
      "Epoch 1, Batch 4105, Loss: 5.0659, Time: 2442.0s, Step: 4106, GPU: 4.9GB\n",
      "Epoch 1, Batch 4110, Loss: 5.3830, Time: 2444.5s, Step: 4111, GPU: 4.9GB\n",
      "Epoch 1, Batch 4115, Loss: 4.9129, Time: 2447.9s, Step: 4116, GPU: 4.9GB\n",
      "Epoch 1, Batch 4120, Loss: 4.8171, Time: 2450.4s, Step: 4121, GPU: 4.9GB\n",
      "Epoch 1, Batch 4125, Loss: 5.2852, Time: 2453.8s, Step: 4126, GPU: 4.9GB\n",
      "Epoch 1, Batch 4130, Loss: 4.9715, Time: 2456.4s, Step: 4131, GPU: 4.9GB\n",
      "Epoch 1, Batch 4135, Loss: 5.1535, Time: 2459.8s, Step: 4136, GPU: 4.9GB\n",
      "Epoch 1, Batch 4140, Loss: 4.4716, Time: 2462.3s, Step: 4141, GPU: 4.9GB\n",
      "Epoch 1, Batch 4145, Loss: 5.0878, Time: 2465.7s, Step: 4146, GPU: 4.9GB\n",
      "Epoch 1, Batch 4150, Loss: 4.2783, Time: 2468.2s, Step: 4151, GPU: 4.9GB\n",
      "Epoch 1, Batch 4155, Loss: 4.0381, Time: 2471.6s, Step: 4156, GPU: 4.9GB\n",
      "Epoch 1, Batch 4160, Loss: 4.6093, Time: 2474.2s, Step: 4161, GPU: 4.9GB\n",
      "Epoch 1, Batch 4165, Loss: 4.7214, Time: 2477.6s, Step: 4166, GPU: 4.9GB\n",
      "Epoch 1, Batch 4170, Loss: 5.0859, Time: 2480.1s, Step: 4171, GPU: 4.9GB\n",
      "Epoch 1, Batch 4175, Loss: 4.9213, Time: 2483.6s, Step: 4176, GPU: 4.9GB\n",
      "Epoch 1, Batch 4180, Loss: 4.6453, Time: 2486.1s, Step: 4181, GPU: 4.9GB\n",
      "Epoch 1, Batch 4185, Loss: 5.6322, Time: 2489.5s, Step: 4186, GPU: 4.9GB\n",
      "Epoch 1, Batch 4190, Loss: 5.2502, Time: 2492.0s, Step: 4191, GPU: 4.9GB\n",
      "Epoch 1, Batch 4195, Loss: 5.0809, Time: 2495.4s, Step: 4196, GPU: 4.9GB\n",
      "Epoch 1, Batch 4200, Loss: 5.0346, Time: 2498.0s, Step: 4201, GPU: 4.9GB\n",
      "Epoch 1, Batch 4205, Loss: 5.0204, Time: 2501.4s, Step: 4206, GPU: 4.9GB\n",
      "Epoch 1, Batch 4210, Loss: 4.0567, Time: 2503.9s, Step: 4211, GPU: 4.9GB\n",
      "Epoch 1, Batch 4215, Loss: 5.2957, Time: 2507.4s, Step: 4216, GPU: 4.9GB\n",
      "Epoch 1, Batch 4220, Loss: 5.4783, Time: 2509.9s, Step: 4221, GPU: 4.9GB\n",
      "Epoch 1, Batch 4225, Loss: 5.0629, Time: 2513.3s, Step: 4226, GPU: 4.9GB\n",
      "Epoch 1, Batch 4230, Loss: 5.0755, Time: 2515.8s, Step: 4231, GPU: 4.9GB\n",
      "Epoch 1, Batch 4235, Loss: 4.9417, Time: 2519.2s, Step: 4236, GPU: 4.9GB\n",
      "Epoch 1, Batch 4240, Loss: 4.7446, Time: 2521.8s, Step: 4241, GPU: 4.9GB\n",
      "Epoch 1, Batch 4245, Loss: 5.0826, Time: 2525.2s, Step: 4246, GPU: 4.9GB\n",
      "Epoch 1, Batch 4250, Loss: 4.6401, Time: 2527.7s, Step: 4251, GPU: 4.9GB\n",
      "Epoch 1, Batch 4255, Loss: 4.7201, Time: 2531.2s, Step: 4256, GPU: 4.9GB\n",
      "Epoch 1, Batch 4260, Loss: 3.7078, Time: 2533.7s, Step: 4261, GPU: 4.9GB\n",
      "Epoch 1, Batch 4265, Loss: 5.2068, Time: 2537.1s, Step: 4266, GPU: 4.9GB\n",
      "Epoch 1, Batch 4270, Loss: 5.1566, Time: 2539.6s, Step: 4271, GPU: 4.9GB\n",
      "Epoch 1, Batch 4275, Loss: 4.6090, Time: 2543.0s, Step: 4276, GPU: 4.9GB\n",
      "Epoch 1, Batch 4280, Loss: 5.5101, Time: 2545.5s, Step: 4281, GPU: 4.9GB\n",
      "Epoch 1, Batch 4285, Loss: 5.0813, Time: 2549.0s, Step: 4286, GPU: 4.9GB\n",
      "Epoch 1, Batch 4290, Loss: 4.5312, Time: 2551.5s, Step: 4291, GPU: 4.9GB\n",
      "Epoch 1, Batch 4295, Loss: 5.7181, Time: 2554.9s, Step: 4296, GPU: 4.9GB\n",
      "Epoch 1, Batch 4300, Loss: 5.9493, Time: 2557.4s, Step: 4301, GPU: 4.9GB\n",
      "Epoch 1, Batch 4305, Loss: 4.4049, Time: 2560.8s, Step: 4306, GPU: 4.9GB\n",
      "Epoch 1, Batch 4310, Loss: 4.4309, Time: 2563.3s, Step: 4311, GPU: 4.9GB\n",
      "Epoch 1, Batch 4315, Loss: 4.9028, Time: 2566.8s, Step: 4316, GPU: 4.9GB\n",
      "Epoch 1, Batch 4320, Loss: 4.7915, Time: 2569.3s, Step: 4321, GPU: 4.9GB\n",
      "Epoch 1, Batch 4325, Loss: 4.9661, Time: 2572.7s, Step: 4326, GPU: 4.9GB\n",
      "Epoch 1, Batch 4330, Loss: 4.7491, Time: 2575.2s, Step: 4331, GPU: 4.9GB\n",
      "Epoch 1, Batch 4335, Loss: 4.3952, Time: 2578.7s, Step: 4336, GPU: 4.9GB\n",
      "Epoch 1, Batch 4340, Loss: 5.0824, Time: 2581.2s, Step: 4341, GPU: 4.9GB\n",
      "Epoch 1, Batch 4345, Loss: 4.9596, Time: 2584.6s, Step: 4346, GPU: 4.9GB\n",
      "Epoch 1, Batch 4350, Loss: 5.6714, Time: 2587.1s, Step: 4351, GPU: 4.9GB\n",
      "Epoch 1, Batch 4355, Loss: 4.8310, Time: 2590.5s, Step: 4356, GPU: 4.9GB\n",
      "Epoch 1, Batch 4360, Loss: 4.5969, Time: 2593.0s, Step: 4361, GPU: 4.9GB\n",
      "Epoch 1, Batch 4365, Loss: 5.1083, Time: 2596.4s, Step: 4366, GPU: 4.9GB\n",
      "Epoch 1, Batch 4370, Loss: 4.2504, Time: 2598.9s, Step: 4371, GPU: 4.9GB\n",
      "Epoch 1, Batch 4375, Loss: 5.4287, Time: 2602.3s, Step: 4376, GPU: 4.9GB\n",
      "Epoch 1, Batch 4380, Loss: 4.2683, Time: 2604.9s, Step: 4381, GPU: 4.9GB\n",
      "Epoch 1, Batch 4385, Loss: 5.0932, Time: 2608.2s, Step: 4386, GPU: 4.9GB\n",
      "Epoch 1, Batch 4390, Loss: 4.5206, Time: 2610.8s, Step: 4391, GPU: 4.9GB\n",
      "Epoch 1, Batch 4395, Loss: 5.0087, Time: 2614.2s, Step: 4396, GPU: 4.9GB\n",
      "Epoch 1, Batch 4400, Loss: 4.1412, Time: 2616.8s, Step: 4401, GPU: 4.9GB\n",
      "Epoch 1, Batch 4405, Loss: 5.5606, Time: 2620.2s, Step: 4406, GPU: 4.9GB\n",
      "Epoch 1, Batch 4410, Loss: 5.1640, Time: 2622.7s, Step: 4411, GPU: 4.9GB\n",
      "Epoch 1, Batch 4415, Loss: 5.1524, Time: 2626.1s, Step: 4416, GPU: 4.9GB\n",
      "Epoch 1, Batch 4420, Loss: 4.3765, Time: 2628.7s, Step: 4421, GPU: 4.9GB\n",
      "Epoch 1, Batch 4425, Loss: 4.9655, Time: 2632.1s, Step: 4426, GPU: 4.9GB\n",
      "Epoch 1, Batch 4430, Loss: 4.7345, Time: 2634.6s, Step: 4431, GPU: 4.9GB\n",
      "Epoch 1, Batch 4435, Loss: 4.9955, Time: 2638.0s, Step: 4436, GPU: 4.9GB\n",
      "Epoch 1, Batch 4440, Loss: 4.9427, Time: 2640.5s, Step: 4441, GPU: 4.9GB\n",
      "Epoch 1, Batch 4445, Loss: 5.1935, Time: 2644.0s, Step: 4446, GPU: 4.9GB\n",
      "Epoch 1, Batch 4450, Loss: 3.9254, Time: 2646.5s, Step: 4451, GPU: 4.9GB\n",
      "Epoch 1, Batch 4455, Loss: 4.4685, Time: 2649.9s, Step: 4456, GPU: 4.9GB\n",
      "Epoch 1, Batch 4460, Loss: 4.9429, Time: 2652.5s, Step: 4461, GPU: 4.9GB\n",
      "Epoch 1, Batch 4465, Loss: 4.7501, Time: 2655.9s, Step: 4466, GPU: 4.9GB\n",
      "Epoch 1, Batch 4470, Loss: 4.4142, Time: 2658.5s, Step: 4471, GPU: 4.9GB\n",
      "Epoch 1, Batch 4475, Loss: 5.2893, Time: 2661.9s, Step: 4476, GPU: 4.9GB\n",
      "Epoch 1, Batch 4480, Loss: 4.8796, Time: 2664.4s, Step: 4481, GPU: 4.9GB\n",
      "Epoch 1, Batch 4485, Loss: 5.3047, Time: 2667.8s, Step: 4486, GPU: 4.9GB\n",
      "Epoch 1, Batch 4490, Loss: 5.5775, Time: 2670.4s, Step: 4491, GPU: 4.9GB\n",
      "Epoch 1, Batch 4495, Loss: 4.5227, Time: 2673.8s, Step: 4496, GPU: 4.9GB\n",
      "Epoch 1, Batch 4500, Loss: 4.5857, Time: 2676.3s, Step: 4501, GPU: 4.9GB\n",
      "Epoch 1, Batch 4505, Loss: 4.3100, Time: 2679.7s, Step: 4506, GPU: 4.9GB\n",
      "Epoch 1, Batch 4510, Loss: 5.2744, Time: 2682.3s, Step: 4511, GPU: 4.9GB\n",
      "Epoch 1, Batch 4515, Loss: 5.7517, Time: 2685.7s, Step: 4516, GPU: 4.9GB\n",
      "Epoch 1, Batch 4520, Loss: 4.4143, Time: 2688.2s, Step: 4521, GPU: 4.9GB\n",
      "Epoch 1, Batch 4525, Loss: 4.8732, Time: 2691.6s, Step: 4526, GPU: 4.9GB\n",
      "Epoch 1, Batch 4530, Loss: 4.6568, Time: 2694.1s, Step: 4531, GPU: 4.9GB\n",
      "Epoch 1, Batch 4535, Loss: 4.9846, Time: 2697.6s, Step: 4536, GPU: 4.9GB\n",
      "Epoch 1, Batch 4540, Loss: 5.0304, Time: 2700.1s, Step: 4541, GPU: 4.9GB\n",
      "Epoch 1, Batch 4545, Loss: 5.4975, Time: 2703.5s, Step: 4546, GPU: 4.9GB\n",
      "Epoch 1, Batch 4550, Loss: 5.4769, Time: 2706.1s, Step: 4551, GPU: 4.9GB\n",
      "Epoch 1, Batch 4555, Loss: 4.4153, Time: 2709.5s, Step: 4556, GPU: 4.9GB\n",
      "Epoch 1, Batch 4560, Loss: 5.3698, Time: 2712.0s, Step: 4561, GPU: 4.9GB\n",
      "Epoch 1, Batch 4565, Loss: 4.7189, Time: 2715.4s, Step: 4566, GPU: 4.9GB\n",
      "Epoch 1, Batch 4570, Loss: 4.8289, Time: 2718.0s, Step: 4571, GPU: 4.9GB\n",
      "Epoch 1, Batch 4575, Loss: 4.8584, Time: 2721.4s, Step: 4576, GPU: 4.9GB\n",
      "Epoch 1, Batch 4580, Loss: 5.2784, Time: 2723.9s, Step: 4581, GPU: 4.9GB\n",
      "Epoch 1, Batch 4585, Loss: 4.8876, Time: 2727.3s, Step: 4586, GPU: 4.9GB\n",
      "Epoch 1, Batch 4590, Loss: 4.3994, Time: 2729.8s, Step: 4591, GPU: 4.9GB\n",
      "Epoch 1, Batch 4595, Loss: 4.9919, Time: 2733.2s, Step: 4596, GPU: 4.9GB\n",
      "Epoch 1, Batch 4600, Loss: 5.0275, Time: 2735.8s, Step: 4601, GPU: 4.9GB\n",
      "Epoch 1, Batch 4605, Loss: 4.6529, Time: 2739.2s, Step: 4606, GPU: 4.9GB\n",
      "Epoch 1, Batch 4610, Loss: 4.0300, Time: 2741.7s, Step: 4611, GPU: 4.9GB\n",
      "Epoch 1, Batch 4615, Loss: 5.1552, Time: 2745.2s, Step: 4616, GPU: 4.9GB\n",
      "Epoch 1, Batch 4620, Loss: 4.9656, Time: 2747.7s, Step: 4621, GPU: 4.9GB\n",
      "Epoch 1, Batch 4625, Loss: 5.2376, Time: 2751.1s, Step: 4626, GPU: 4.9GB\n",
      "Epoch 1, Batch 4630, Loss: 4.9647, Time: 2753.6s, Step: 4631, GPU: 4.9GB\n",
      "Epoch 1, Batch 4635, Loss: 4.0146, Time: 2757.0s, Step: 4636, GPU: 4.9GB\n",
      "Epoch 1, Batch 4640, Loss: 4.8706, Time: 2759.5s, Step: 4641, GPU: 4.9GB\n",
      "Epoch 1, Batch 4645, Loss: 5.1945, Time: 2762.9s, Step: 4646, GPU: 4.9GB\n",
      "Epoch 1, Batch 4650, Loss: 5.0377, Time: 2765.4s, Step: 4651, GPU: 4.9GB\n",
      "Epoch 1, Batch 4655, Loss: 4.7103, Time: 2768.8s, Step: 4656, GPU: 4.9GB\n",
      "Epoch 1, Batch 4660, Loss: 4.7998, Time: 2771.3s, Step: 4661, GPU: 4.9GB\n",
      "Epoch 1, Batch 4665, Loss: 5.0428, Time: 2774.7s, Step: 4666, GPU: 4.9GB\n",
      "Epoch 1, Batch 4670, Loss: 5.2658, Time: 2777.2s, Step: 4671, GPU: 4.9GB\n",
      "Epoch 1, Batch 4675, Loss: 5.1916, Time: 2780.7s, Step: 4676, GPU: 4.9GB\n",
      "Epoch 1, Batch 4680, Loss: 4.7838, Time: 2783.2s, Step: 4681, GPU: 4.9GB\n",
      "Epoch 1, Batch 4685, Loss: 4.7251, Time: 2786.6s, Step: 4686, GPU: 4.9GB\n",
      "Epoch 1, Batch 4690, Loss: 4.9886, Time: 2789.1s, Step: 4691, GPU: 4.9GB\n",
      "Epoch 1, Batch 4695, Loss: 4.9535, Time: 2792.5s, Step: 4696, GPU: 4.9GB\n",
      "Epoch 1, Batch 4700, Loss: 4.7047, Time: 2795.0s, Step: 4701, GPU: 4.9GB\n",
      "Epoch 1, Batch 4705, Loss: 4.8585, Time: 2798.4s, Step: 4706, GPU: 4.9GB\n",
      "Epoch 1, Batch 4710, Loss: 4.8349, Time: 2800.9s, Step: 4711, GPU: 4.9GB\n",
      "Epoch 1, Batch 4715, Loss: 5.0764, Time: 2804.4s, Step: 4716, GPU: 4.9GB\n",
      "Epoch 1, Batch 4720, Loss: 5.3157, Time: 2806.9s, Step: 4721, GPU: 4.9GB\n",
      "Epoch 1, Batch 4725, Loss: 3.5627, Time: 2810.3s, Step: 4726, GPU: 4.9GB\n",
      "Epoch 1, Batch 4730, Loss: 4.5184, Time: 2812.8s, Step: 4731, GPU: 4.9GB\n",
      "Epoch 1, Batch 4735, Loss: 4.5534, Time: 2816.2s, Step: 4736, GPU: 4.9GB\n",
      "Epoch 1, Batch 4740, Loss: 4.2416, Time: 2818.7s, Step: 4741, GPU: 4.9GB\n",
      "Epoch 1, Batch 4745, Loss: 5.0632, Time: 2822.1s, Step: 4746, GPU: 4.9GB\n",
      "Epoch 1, Batch 4750, Loss: 5.3104, Time: 2824.7s, Step: 4751, GPU: 4.9GB\n",
      "Epoch 1, Batch 4755, Loss: 4.2111, Time: 2828.1s, Step: 4756, GPU: 4.9GB\n",
      "Epoch 1, Batch 4760, Loss: 4.1255, Time: 2830.6s, Step: 4761, GPU: 4.9GB\n",
      "Epoch 1, Batch 4765, Loss: 4.9475, Time: 2834.0s, Step: 4766, GPU: 4.9GB\n",
      "Epoch 1, Batch 4770, Loss: 4.8368, Time: 2836.5s, Step: 4771, GPU: 4.9GB\n",
      "Epoch 1, Batch 4775, Loss: 5.2326, Time: 2839.9s, Step: 4776, GPU: 4.9GB\n",
      "Epoch 1, Batch 4780, Loss: 5.2396, Time: 2842.4s, Step: 4781, GPU: 4.9GB\n",
      "Epoch 1, Batch 4785, Loss: 5.3475, Time: 2845.8s, Step: 4786, GPU: 4.9GB\n",
      "Epoch 1, Batch 4790, Loss: 4.5554, Time: 2848.3s, Step: 4791, GPU: 4.9GB\n",
      "Epoch 1, Batch 4795, Loss: 4.5207, Time: 2851.8s, Step: 4796, GPU: 4.9GB\n",
      "Epoch 1, Batch 4800, Loss: 4.9590, Time: 2854.3s, Step: 4801, GPU: 4.9GB\n",
      "Epoch 1, Batch 4805, Loss: 4.3772, Time: 2857.8s, Step: 4806, GPU: 4.9GB\n",
      "Epoch 1, Batch 4810, Loss: 4.9694, Time: 2860.3s, Step: 4811, GPU: 4.9GB\n",
      "Epoch 1, Batch 4815, Loss: 4.5091, Time: 2863.7s, Step: 4816, GPU: 4.9GB\n",
      "Epoch 1, Batch 4820, Loss: 4.8783, Time: 2866.3s, Step: 4821, GPU: 4.9GB\n",
      "Epoch 1, Batch 4825, Loss: 5.5562, Time: 2869.7s, Step: 4826, GPU: 4.9GB\n",
      "Epoch 1, Batch 4830, Loss: 4.5160, Time: 2872.2s, Step: 4831, GPU: 4.9GB\n",
      "Epoch 1, Batch 4835, Loss: 5.4177, Time: 2875.6s, Step: 4836, GPU: 4.9GB\n",
      "Epoch 1, Batch 4840, Loss: 5.2503, Time: 2878.1s, Step: 4841, GPU: 4.9GB\n",
      "Epoch 1, Batch 4845, Loss: 3.5509, Time: 2881.6s, Step: 4846, GPU: 4.9GB\n",
      "Epoch 1, Batch 4850, Loss: 4.5963, Time: 2884.1s, Step: 4851, GPU: 4.9GB\n",
      "Epoch 1, Batch 4855, Loss: 4.9247, Time: 2887.5s, Step: 4856, GPU: 4.9GB\n",
      "Epoch 1, Batch 4860, Loss: 5.1596, Time: 2890.0s, Step: 4861, GPU: 4.9GB\n",
      "Epoch 1, Batch 4865, Loss: 4.8621, Time: 2893.4s, Step: 4866, GPU: 4.9GB\n",
      "Epoch 1, Batch 4870, Loss: 4.9767, Time: 2896.0s, Step: 4871, GPU: 4.9GB\n",
      "Epoch 1, Batch 4875, Loss: 4.9476, Time: 2899.3s, Step: 4876, GPU: 4.9GB\n",
      "Epoch 1, Batch 4880, Loss: 5.5019, Time: 2901.9s, Step: 4881, GPU: 4.9GB\n",
      "Epoch 1, Batch 4885, Loss: 5.2510, Time: 2905.3s, Step: 4886, GPU: 4.9GB\n",
      "Epoch 1, Batch 4890, Loss: 4.7853, Time: 2907.8s, Step: 4891, GPU: 4.9GB\n",
      "Epoch 1, Batch 4895, Loss: 3.6755, Time: 2911.3s, Step: 4896, GPU: 4.9GB\n",
      "Epoch 1, Batch 4900, Loss: 5.2369, Time: 2913.8s, Step: 4901, GPU: 4.9GB\n",
      "Epoch 1, Batch 4905, Loss: 4.4718, Time: 2917.2s, Step: 4906, GPU: 4.9GB\n",
      "Epoch 1, Batch 4910, Loss: 5.2387, Time: 2919.7s, Step: 4911, GPU: 4.9GB\n",
      "Epoch 1, Batch 4915, Loss: 4.6350, Time: 2923.1s, Step: 4916, GPU: 4.9GB\n",
      "Epoch 1, Batch 4920, Loss: 5.1750, Time: 2925.7s, Step: 4921, GPU: 4.9GB\n",
      "Epoch 1, Batch 4925, Loss: 5.5527, Time: 2929.1s, Step: 4926, GPU: 4.9GB\n",
      "Epoch 1, Batch 4930, Loss: 4.9577, Time: 2931.6s, Step: 4931, GPU: 4.9GB\n",
      "Epoch 1, Batch 4935, Loss: 6.2214, Time: 2935.0s, Step: 4936, GPU: 4.9GB\n",
      "Epoch 1, Batch 4940, Loss: 5.0430, Time: 2937.5s, Step: 4941, GPU: 4.9GB\n",
      "Epoch 1, Batch 4945, Loss: 4.8108, Time: 2940.9s, Step: 4946, GPU: 4.9GB\n",
      "Epoch 1, Batch 4950, Loss: 5.1463, Time: 2943.4s, Step: 4951, GPU: 4.9GB\n",
      "Epoch 1, Batch 4955, Loss: 4.5577, Time: 2946.8s, Step: 4956, GPU: 4.9GB\n",
      "Epoch 1, Batch 4960, Loss: 5.1230, Time: 2949.3s, Step: 4961, GPU: 4.9GB\n",
      "Epoch 1, Batch 4965, Loss: 4.3311, Time: 2952.7s, Step: 4966, GPU: 4.9GB\n",
      "Epoch 1, Batch 4970, Loss: 3.8561, Time: 2955.2s, Step: 4971, GPU: 4.9GB\n",
      "Epoch 1, Batch 4975, Loss: 4.6598, Time: 2958.7s, Step: 4976, GPU: 4.9GB\n",
      "Epoch 1, Batch 4980, Loss: 5.2801, Time: 2961.2s, Step: 4981, GPU: 4.9GB\n",
      "Epoch 1, Batch 4985, Loss: 4.0594, Time: 2964.6s, Step: 4986, GPU: 4.9GB\n",
      "Epoch 1, Batch 4990, Loss: 6.0364, Time: 2967.1s, Step: 4991, GPU: 4.9GB\n",
      "Epoch 1, Batch 4995, Loss: 4.5942, Time: 2970.5s, Step: 4996, GPU: 4.9GB\n",
      "Epoch 1, Batch 5000, Loss: 4.6565, Time: 2973.1s, Step: 5001, GPU: 4.9GB\n",
      "Epoch 1, Batch 5005, Loss: 5.2009, Time: 2976.6s, Step: 5006, GPU: 4.9GB\n",
      "Epoch 1, Batch 5010, Loss: 4.5939, Time: 2979.1s, Step: 5011, GPU: 4.9GB\n",
      "Epoch 1, Batch 5015, Loss: 4.2781, Time: 2982.5s, Step: 5016, GPU: 4.9GB\n",
      "Epoch 1, Batch 5020, Loss: 4.9438, Time: 2985.0s, Step: 5021, GPU: 4.9GB\n",
      "Epoch 1, Batch 5025, Loss: 4.4620, Time: 2988.4s, Step: 5026, GPU: 4.9GB\n",
      "Epoch 1, Batch 5030, Loss: 4.9749, Time: 2990.9s, Step: 5031, GPU: 4.9GB\n",
      "Epoch 1, Batch 5035, Loss: 5.3654, Time: 2994.3s, Step: 5036, GPU: 4.9GB\n",
      "Epoch 1, Batch 5040, Loss: 4.8747, Time: 2996.9s, Step: 5041, GPU: 4.9GB\n",
      "Epoch 1, Batch 5045, Loss: 5.3529, Time: 3000.3s, Step: 5046, GPU: 4.9GB\n",
      "Epoch 1, Batch 5050, Loss: 4.1103, Time: 3002.8s, Step: 5051, GPU: 4.9GB\n",
      "Epoch 1, Batch 5055, Loss: 4.5821, Time: 3006.3s, Step: 5056, GPU: 4.9GB\n",
      "Epoch 1, Batch 5060, Loss: 5.4410, Time: 3008.8s, Step: 5061, GPU: 4.9GB\n",
      "Epoch 1, Batch 5065, Loss: 4.5600, Time: 3012.2s, Step: 5066, GPU: 4.9GB\n",
      "Epoch 1, Batch 5070, Loss: 4.4639, Time: 3014.7s, Step: 5071, GPU: 4.9GB\n",
      "Epoch 1, Batch 5075, Loss: 5.0130, Time: 3018.2s, Step: 5076, GPU: 4.9GB\n",
      "Epoch 1, Batch 5080, Loss: 5.0205, Time: 3020.7s, Step: 5081, GPU: 4.9GB\n",
      "Epoch 1, Batch 5085, Loss: 4.9435, Time: 3024.1s, Step: 5086, GPU: 4.9GB\n",
      "Epoch 1, Batch 5090, Loss: 4.5277, Time: 3026.7s, Step: 5091, GPU: 4.9GB\n",
      "Epoch 1, Batch 5095, Loss: 5.0344, Time: 3030.1s, Step: 5096, GPU: 4.9GB\n",
      "Epoch 1, Batch 5100, Loss: 5.2531, Time: 3032.6s, Step: 5101, GPU: 4.9GB\n",
      "Epoch 1, Batch 5105, Loss: 5.0507, Time: 3036.1s, Step: 5106, GPU: 4.9GB\n",
      "Epoch 1, Batch 5110, Loss: 4.5958, Time: 3038.6s, Step: 5111, GPU: 4.9GB\n",
      "Epoch 1, Batch 5115, Loss: 3.6026, Time: 3042.1s, Step: 5116, GPU: 4.9GB\n",
      "Epoch 1, Batch 5120, Loss: 4.8932, Time: 3044.7s, Step: 5121, GPU: 4.9GB\n",
      "Epoch 1, Batch 5125, Loss: 5.0611, Time: 3048.2s, Step: 5126, GPU: 4.9GB\n",
      "Epoch 1, Batch 5130, Loss: 5.0846, Time: 3050.7s, Step: 5131, GPU: 4.9GB\n",
      "Epoch 1, Batch 5135, Loss: 4.1459, Time: 3054.1s, Step: 5136, GPU: 4.9GB\n",
      "Epoch 1, Batch 5140, Loss: 5.0265, Time: 3056.7s, Step: 5141, GPU: 4.9GB\n",
      "Epoch 1, Batch 5145, Loss: 4.8210, Time: 3060.1s, Step: 5146, GPU: 4.9GB\n",
      "Epoch 1, Batch 5150, Loss: 5.3417, Time: 3062.6s, Step: 5151, GPU: 4.9GB\n",
      "Epoch 1, Batch 5155, Loss: 5.2596, Time: 3066.1s, Step: 5156, GPU: 4.9GB\n",
      "Epoch 1, Batch 5160, Loss: 4.5482, Time: 3068.6s, Step: 5161, GPU: 4.9GB\n",
      "Epoch 1, Batch 5165, Loss: 4.6271, Time: 3072.0s, Step: 5166, GPU: 4.9GB\n",
      "Epoch 1, Batch 5170, Loss: 4.9234, Time: 3074.5s, Step: 5171, GPU: 4.9GB\n",
      "Epoch 1, Batch 5175, Loss: 5.2311, Time: 3078.0s, Step: 5176, GPU: 4.9GB\n",
      "Epoch 1, Batch 5180, Loss: 4.7285, Time: 3080.5s, Step: 5181, GPU: 4.9GB\n",
      "Epoch 1, Batch 5185, Loss: 5.1704, Time: 3083.9s, Step: 5186, GPU: 4.9GB\n",
      "Epoch 1, Batch 5190, Loss: 5.0479, Time: 3086.5s, Step: 5191, GPU: 4.9GB\n",
      "Epoch 1, Batch 5195, Loss: 5.4910, Time: 3089.9s, Step: 5196, GPU: 4.9GB\n",
      "Epoch 1, Batch 5200, Loss: 5.2301, Time: 3092.5s, Step: 5201, GPU: 4.9GB\n",
      "Epoch 1, Batch 5205, Loss: 3.5783, Time: 3095.9s, Step: 5206, GPU: 4.9GB\n",
      "Epoch 1, Batch 5210, Loss: 5.3821, Time: 3098.4s, Step: 5211, GPU: 4.9GB\n",
      "Epoch 1, Batch 5215, Loss: 5.5100, Time: 3101.9s, Step: 5216, GPU: 4.9GB\n",
      "Epoch 1, Batch 5220, Loss: 5.1034, Time: 3104.4s, Step: 5221, GPU: 4.9GB\n",
      "Epoch 1, Batch 5225, Loss: 4.4605, Time: 3107.8s, Step: 5226, GPU: 4.9GB\n",
      "Epoch 1, Batch 5230, Loss: 3.9654, Time: 3110.3s, Step: 5231, GPU: 4.9GB\n",
      "Epoch 1, Batch 5235, Loss: 5.2342, Time: 3113.7s, Step: 5236, GPU: 4.9GB\n",
      "Epoch 1, Batch 5240, Loss: 4.6560, Time: 3116.3s, Step: 5241, GPU: 4.9GB\n",
      "Epoch 1, Batch 5245, Loss: 5.0355, Time: 3119.7s, Step: 5246, GPU: 4.9GB\n",
      "Epoch 1, Batch 5250, Loss: 4.6087, Time: 3122.2s, Step: 5251, GPU: 4.9GB\n",
      "Epoch 1, Batch 5255, Loss: 4.4921, Time: 3125.6s, Step: 5256, GPU: 4.9GB\n",
      "Epoch 1, Batch 5260, Loss: 3.8565, Time: 3128.2s, Step: 5261, GPU: 4.9GB\n",
      "Epoch 1, Batch 5265, Loss: 4.6270, Time: 3131.6s, Step: 5266, GPU: 4.9GB\n",
      "Epoch 1, Batch 5270, Loss: 4.7514, Time: 3134.1s, Step: 5271, GPU: 4.9GB\n",
      "Epoch 1, Batch 5275, Loss: 4.1062, Time: 3137.6s, Step: 5276, GPU: 4.9GB\n",
      "Epoch 1, Batch 5280, Loss: 3.9519, Time: 3140.1s, Step: 5281, GPU: 4.9GB\n",
      "Epoch 1, Batch 5285, Loss: 4.1370, Time: 3143.5s, Step: 5286, GPU: 4.9GB\n",
      "Epoch 1, Batch 5290, Loss: 4.7496, Time: 3146.1s, Step: 5291, GPU: 4.9GB\n",
      "Epoch 1, Batch 5295, Loss: 5.9363, Time: 3149.5s, Step: 5296, GPU: 4.9GB\n",
      "Epoch 1, Batch 5300, Loss: 4.5710, Time: 3152.0s, Step: 5301, GPU: 4.9GB\n",
      "Epoch 1, Batch 5305, Loss: 4.8654, Time: 3155.4s, Step: 5306, GPU: 4.9GB\n",
      "Epoch 1, Batch 5310, Loss: 4.4067, Time: 3157.9s, Step: 5311, GPU: 4.9GB\n",
      "Epoch 1, Batch 5315, Loss: 4.9433, Time: 3161.3s, Step: 5316, GPU: 4.9GB\n",
      "Epoch 1, Batch 5320, Loss: 4.5908, Time: 3163.9s, Step: 5321, GPU: 4.9GB\n",
      "Epoch 1, Batch 5325, Loss: 4.8597, Time: 3167.3s, Step: 5326, GPU: 4.9GB\n",
      "Epoch 1, Batch 5330, Loss: 4.3075, Time: 3169.8s, Step: 5331, GPU: 4.9GB\n",
      "Epoch 1, Batch 5335, Loss: 4.5739, Time: 3173.2s, Step: 5336, GPU: 4.9GB\n",
      "Epoch 1, Batch 5340, Loss: 4.4403, Time: 3175.7s, Step: 5341, GPU: 4.9GB\n",
      "Epoch 1, Batch 5345, Loss: 4.0558, Time: 3179.1s, Step: 5346, GPU: 4.9GB\n",
      "Epoch 1, Batch 5350, Loss: 5.0187, Time: 3181.6s, Step: 5351, GPU: 4.9GB\n",
      "Epoch 1, Batch 5355, Loss: 5.5837, Time: 3185.0s, Step: 5356, GPU: 4.9GB\n",
      "Epoch 1, Batch 5360, Loss: 4.7301, Time: 3187.6s, Step: 5361, GPU: 4.9GB\n",
      "Epoch 1, Batch 5365, Loss: 5.0650, Time: 3191.0s, Step: 5366, GPU: 4.9GB\n",
      "Epoch 1, Batch 5370, Loss: 4.8324, Time: 3193.5s, Step: 5371, GPU: 4.9GB\n",
      "Epoch 1, Batch 5375, Loss: 4.4586, Time: 3196.9s, Step: 5376, GPU: 4.9GB\n",
      "Epoch 1, Batch 5380, Loss: 5.0159, Time: 3199.4s, Step: 5381, GPU: 4.9GB\n",
      "Epoch 1, Batch 5385, Loss: 3.7833, Time: 3202.9s, Step: 5386, GPU: 4.9GB\n",
      "Epoch 1, Batch 5390, Loss: 4.7118, Time: 3205.4s, Step: 5391, GPU: 4.9GB\n",
      "Epoch 1, Batch 5395, Loss: 4.5431, Time: 3208.8s, Step: 5396, GPU: 4.9GB\n",
      "Epoch 1, Batch 5400, Loss: 3.7221, Time: 3211.4s, Step: 5401, GPU: 4.9GB\n",
      "Epoch 1, Batch 5405, Loss: 5.0594, Time: 3214.8s, Step: 5406, GPU: 4.9GB\n",
      "Epoch 1, Batch 5410, Loss: 5.1266, Time: 3217.3s, Step: 5411, GPU: 4.9GB\n",
      "Epoch 1, Batch 5415, Loss: 4.3763, Time: 3220.7s, Step: 5416, GPU: 4.9GB\n",
      "Epoch 1, Batch 5420, Loss: 4.3954, Time: 3223.3s, Step: 5421, GPU: 4.9GB\n",
      "Epoch 1, Batch 5425, Loss: 4.4230, Time: 3226.7s, Step: 5426, GPU: 4.9GB\n",
      "Epoch 1, Batch 5430, Loss: 4.2281, Time: 3229.2s, Step: 5431, GPU: 4.9GB\n",
      "Epoch 1, Batch 5435, Loss: 5.4324, Time: 3232.6s, Step: 5436, GPU: 4.9GB\n",
      "Epoch 1, Batch 5440, Loss: 5.9661, Time: 3235.1s, Step: 5441, GPU: 4.9GB\n",
      "Epoch 1, Batch 5445, Loss: 4.4958, Time: 3238.6s, Step: 5446, GPU: 4.9GB\n",
      "Epoch 1, Batch 5450, Loss: 4.4730, Time: 3241.1s, Step: 5451, GPU: 4.9GB\n",
      "Epoch 1, Batch 5455, Loss: 4.7537, Time: 3244.5s, Step: 5456, GPU: 4.9GB\n",
      "Epoch 1, Batch 5460, Loss: 4.5826, Time: 3247.1s, Step: 5461, GPU: 4.9GB\n",
      "Epoch 1, Batch 5465, Loss: 4.4730, Time: 3250.6s, Step: 5466, GPU: 4.9GB\n",
      "Epoch 1, Batch 5470, Loss: 4.7338, Time: 3253.1s, Step: 5471, GPU: 4.9GB\n",
      "Epoch 1, Batch 5475, Loss: 4.7106, Time: 3256.5s, Step: 5476, GPU: 4.9GB\n",
      "Epoch 1, Batch 5480, Loss: 4.9940, Time: 3259.0s, Step: 5481, GPU: 4.9GB\n",
      "Epoch 1, Batch 5485, Loss: 4.9695, Time: 3262.5s, Step: 5486, GPU: 4.9GB\n",
      "Epoch 1, Batch 5490, Loss: 4.4628, Time: 3265.0s, Step: 5491, GPU: 4.9GB\n",
      "Epoch 1, Batch 5495, Loss: 5.4837, Time: 3268.4s, Step: 5496, GPU: 4.9GB\n",
      "Epoch 1, Batch 5500, Loss: 4.6403, Time: 3270.9s, Step: 5501, GPU: 4.9GB\n",
      "Epoch 1, Batch 5505, Loss: 4.4478, Time: 3274.4s, Step: 5506, GPU: 4.9GB\n",
      "Epoch 1, Batch 5510, Loss: 4.5251, Time: 3276.9s, Step: 5511, GPU: 4.9GB\n",
      "Epoch 1, Batch 5515, Loss: 4.6247, Time: 3280.4s, Step: 5516, GPU: 4.9GB\n",
      "Epoch 1, Batch 5520, Loss: 4.3156, Time: 3282.9s, Step: 5521, GPU: 4.9GB\n",
      "Epoch 1, Batch 5525, Loss: 4.2226, Time: 3286.3s, Step: 5526, GPU: 4.9GB\n",
      "Epoch 1, Batch 5530, Loss: 5.3654, Time: 3288.9s, Step: 5531, GPU: 4.9GB\n",
      "Epoch 1, Batch 5535, Loss: 4.7481, Time: 3292.3s, Step: 5536, GPU: 4.9GB\n",
      "Epoch 1, Batch 5540, Loss: 4.5836, Time: 3294.8s, Step: 5541, GPU: 4.9GB\n",
      "Epoch 1, Batch 5545, Loss: 5.1444, Time: 3298.2s, Step: 5546, GPU: 4.9GB\n",
      "Epoch 1, Batch 5550, Loss: 3.9977, Time: 3300.7s, Step: 5551, GPU: 4.9GB\n",
      "Epoch 1, Batch 5555, Loss: 4.6182, Time: 3304.1s, Step: 5556, GPU: 4.9GB\n",
      "Epoch 1, Batch 5560, Loss: 4.2602, Time: 3306.7s, Step: 5561, GPU: 4.9GB\n",
      "Epoch 1, Batch 5565, Loss: 4.7209, Time: 3310.1s, Step: 5566, GPU: 4.9GB\n",
      "Epoch 1, Batch 5570, Loss: 4.0504, Time: 3312.6s, Step: 5571, GPU: 4.9GB\n",
      "Epoch 1, Batch 5575, Loss: 4.5726, Time: 3316.0s, Step: 5576, GPU: 4.9GB\n",
      "Epoch 1, Batch 5580, Loss: 5.2213, Time: 3318.5s, Step: 5581, GPU: 4.9GB\n",
      "Epoch 1, Batch 5585, Loss: 4.6966, Time: 3322.0s, Step: 5586, GPU: 4.9GB\n",
      "Epoch 1, Batch 5590, Loss: 4.1102, Time: 3324.5s, Step: 5591, GPU: 4.9GB\n",
      "Epoch 1, Batch 5595, Loss: 4.5862, Time: 3327.9s, Step: 5596, GPU: 4.9GB\n",
      "Epoch 1, Batch 5600, Loss: 5.3636, Time: 3330.5s, Step: 5601, GPU: 4.9GB\n",
      "Epoch 1, Batch 5605, Loss: 6.1675, Time: 3333.9s, Step: 5606, GPU: 4.9GB\n",
      "Epoch 1, Batch 5610, Loss: 5.1991, Time: 3336.4s, Step: 5611, GPU: 4.9GB\n",
      "Epoch 1, Batch 5615, Loss: 4.9735, Time: 3339.9s, Step: 5616, GPU: 4.9GB\n",
      "Epoch 1, Batch 5620, Loss: 4.6772, Time: 3342.4s, Step: 5621, GPU: 4.9GB\n",
      "Epoch 1, Batch 5625, Loss: 5.0201, Time: 3345.8s, Step: 5626, GPU: 4.9GB\n",
      "Epoch 1, Batch 5630, Loss: 4.9168, Time: 3348.4s, Step: 5631, GPU: 4.9GB\n",
      "Epoch 1, Batch 5635, Loss: 5.2639, Time: 3351.8s, Step: 5636, GPU: 4.9GB\n",
      "Epoch 1, Batch 5640, Loss: 4.3379, Time: 3354.3s, Step: 5641, GPU: 4.9GB\n",
      "Epoch 1, Batch 5645, Loss: 4.5567, Time: 3357.7s, Step: 5646, GPU: 4.9GB\n",
      "Epoch 1, Batch 5650, Loss: 4.8980, Time: 3360.2s, Step: 5651, GPU: 4.9GB\n",
      "Epoch 1, Batch 5655, Loss: 5.2428, Time: 3363.7s, Step: 5656, GPU: 4.9GB\n",
      "Epoch 1, Batch 5660, Loss: 4.2791, Time: 3366.2s, Step: 5661, GPU: 4.9GB\n",
      "Epoch 1, Batch 5665, Loss: 4.4144, Time: 3369.6s, Step: 5666, GPU: 4.9GB\n",
      "Epoch 1, Batch 5670, Loss: 4.6474, Time: 3372.1s, Step: 5671, GPU: 4.9GB\n",
      "Epoch 1, Batch 5675, Loss: 4.5934, Time: 3375.6s, Step: 5676, GPU: 4.9GB\n",
      "Epoch 1, Batch 5680, Loss: 5.2552, Time: 3378.1s, Step: 5681, GPU: 4.9GB\n",
      "Epoch 1, Batch 5685, Loss: 4.9928, Time: 3381.5s, Step: 5686, GPU: 4.9GB\n",
      "Epoch 1, Batch 5690, Loss: 4.5674, Time: 3384.0s, Step: 5691, GPU: 4.9GB\n",
      "Epoch 1, Batch 5695, Loss: 4.6626, Time: 3387.4s, Step: 5696, GPU: 4.9GB\n",
      "Epoch 1, Batch 5700, Loss: 4.0722, Time: 3390.0s, Step: 5701, GPU: 4.9GB\n",
      "Epoch 1, Batch 5705, Loss: 5.6136, Time: 3393.4s, Step: 5706, GPU: 4.9GB\n",
      "Epoch 1, Batch 5710, Loss: 5.1784, Time: 3395.9s, Step: 5711, GPU: 4.9GB\n",
      "Epoch 1, Batch 5715, Loss: 5.3068, Time: 3399.3s, Step: 5716, GPU: 4.9GB\n",
      "Epoch 1, Batch 5720, Loss: 4.7904, Time: 3401.8s, Step: 5721, GPU: 4.9GB\n",
      "Epoch 1, Batch 5725, Loss: 5.1069, Time: 3405.3s, Step: 5726, GPU: 4.9GB\n",
      "Epoch 1, Batch 5730, Loss: 4.8760, Time: 3407.8s, Step: 5731, GPU: 4.9GB\n",
      "Epoch 1, Batch 5735, Loss: 3.9612, Time: 3411.2s, Step: 5736, GPU: 4.9GB\n",
      "Epoch 1, Batch 5740, Loss: 4.7042, Time: 3413.7s, Step: 5741, GPU: 4.9GB\n",
      "Epoch 1, Batch 5745, Loss: 4.5855, Time: 3417.2s, Step: 5746, GPU: 4.9GB\n",
      "Epoch 1, Batch 5750, Loss: 4.7689, Time: 3419.7s, Step: 5751, GPU: 4.9GB\n",
      "Epoch 1, Batch 5755, Loss: 4.9094, Time: 3423.1s, Step: 5756, GPU: 4.9GB\n",
      "Epoch 1, Batch 5760, Loss: 4.9297, Time: 3425.6s, Step: 5761, GPU: 4.9GB\n",
      "Epoch 1, Batch 5765, Loss: 4.6181, Time: 3429.0s, Step: 5766, GPU: 4.9GB\n",
      "Epoch 1, Batch 5770, Loss: 4.3149, Time: 3431.6s, Step: 5771, GPU: 4.9GB\n",
      "Epoch 1, Batch 5775, Loss: 5.9225, Time: 3435.0s, Step: 5776, GPU: 4.9GB\n",
      "Epoch 1, Batch 5780, Loss: 5.1000, Time: 3437.5s, Step: 5781, GPU: 4.9GB\n",
      "Epoch 1, Batch 5785, Loss: 4.8873, Time: 3440.9s, Step: 5786, GPU: 4.9GB\n",
      "Epoch 1, Batch 5790, Loss: 4.1551, Time: 3443.5s, Step: 5791, GPU: 4.9GB\n",
      "Epoch 1, Batch 5795, Loss: 4.1451, Time: 3446.9s, Step: 5796, GPU: 4.9GB\n",
      "Epoch 1, Batch 5800, Loss: 4.9745, Time: 3449.6s, Step: 5801, GPU: 4.9GB\n",
      "Epoch 1, Batch 5805, Loss: 6.1436, Time: 3453.0s, Step: 5806, GPU: 4.9GB\n",
      "Epoch 1, Batch 5810, Loss: 4.8856, Time: 3455.6s, Step: 5811, GPU: 4.9GB\n",
      "Epoch 1, Batch 5815, Loss: 5.1256, Time: 3459.0s, Step: 5816, GPU: 4.9GB\n",
      "Epoch 1, Batch 5820, Loss: 4.8345, Time: 3461.5s, Step: 5821, GPU: 4.9GB\n",
      "Epoch 1, Batch 5825, Loss: 4.2090, Time: 3464.9s, Step: 5826, GPU: 4.9GB\n",
      "Epoch 1, Batch 5830, Loss: 4.2516, Time: 3467.4s, Step: 5831, GPU: 4.9GB\n",
      "Epoch 1, Batch 5835, Loss: 3.9825, Time: 3470.8s, Step: 5836, GPU: 4.9GB\n",
      "Epoch 1, Batch 5840, Loss: 5.5203, Time: 3473.4s, Step: 5841, GPU: 4.9GB\n",
      "Epoch 1, Batch 5845, Loss: 3.4280, Time: 3476.8s, Step: 5846, GPU: 4.9GB\n",
      "Epoch 1, Batch 5850, Loss: 4.2154, Time: 3479.3s, Step: 5851, GPU: 4.9GB\n",
      "Epoch 1, Batch 5855, Loss: 4.7388, Time: 3482.7s, Step: 5856, GPU: 4.9GB\n",
      "Epoch 1, Batch 5860, Loss: 4.8678, Time: 3485.2s, Step: 5861, GPU: 4.9GB\n",
      "Epoch 1, Batch 5865, Loss: 4.2756, Time: 3488.7s, Step: 5866, GPU: 4.9GB\n",
      "Epoch 1, Batch 5870, Loss: 4.5593, Time: 3491.2s, Step: 5871, GPU: 4.9GB\n",
      "Epoch 1, Batch 5875, Loss: 3.9794, Time: 3494.7s, Step: 5876, GPU: 4.9GB\n",
      "Epoch 1, Batch 5880, Loss: 5.3677, Time: 3497.2s, Step: 5881, GPU: 4.9GB\n",
      "Epoch 1, Batch 5885, Loss: 5.1256, Time: 3500.6s, Step: 5886, GPU: 4.9GB\n",
      "Epoch 1, Batch 5890, Loss: 4.7293, Time: 3503.1s, Step: 5891, GPU: 4.9GB\n",
      "Epoch 1, Batch 5895, Loss: 4.5295, Time: 3506.5s, Step: 5896, GPU: 4.9GB\n",
      "Epoch 1, Batch 5900, Loss: 3.8128, Time: 3509.0s, Step: 5901, GPU: 4.9GB\n",
      "Epoch 1, Batch 5905, Loss: 4.0151, Time: 3512.5s, Step: 5906, GPU: 4.9GB\n",
      "Epoch 1, Batch 5910, Loss: 3.9335, Time: 3515.0s, Step: 5911, GPU: 4.9GB\n",
      "Epoch 1, Batch 5915, Loss: 5.9702, Time: 3518.4s, Step: 5916, GPU: 4.9GB\n",
      "Epoch 1, Batch 5920, Loss: 5.6386, Time: 3521.0s, Step: 5921, GPU: 4.9GB\n",
      "Epoch 1, Batch 5925, Loss: 5.4809, Time: 3524.4s, Step: 5926, GPU: 4.9GB\n",
      "Epoch 1, Batch 5930, Loss: 4.9130, Time: 3526.9s, Step: 5931, GPU: 4.9GB\n",
      "Epoch 1, Batch 5935, Loss: 4.8612, Time: 3530.3s, Step: 5936, GPU: 4.9GB\n",
      "Epoch 1, Batch 5940, Loss: 4.2292, Time: 3532.9s, Step: 5941, GPU: 4.9GB\n",
      "Epoch 1, Batch 5945, Loss: 4.0545, Time: 3536.3s, Step: 5946, GPU: 4.9GB\n",
      "Epoch 1, Batch 5950, Loss: 4.6448, Time: 3538.8s, Step: 5951, GPU: 4.9GB\n",
      "Epoch 1, Batch 5955, Loss: 4.6099, Time: 3542.2s, Step: 5956, GPU: 4.9GB\n",
      "Epoch 1, Batch 5960, Loss: 4.3920, Time: 3544.7s, Step: 5961, GPU: 4.9GB\n",
      "Epoch 1, Batch 5965, Loss: 4.8261, Time: 3548.1s, Step: 5966, GPU: 4.9GB\n",
      "Epoch 1, Batch 5970, Loss: 4.2015, Time: 3550.6s, Step: 5971, GPU: 4.9GB\n",
      "Epoch 1, Batch 5975, Loss: 4.2670, Time: 3554.0s, Step: 5976, GPU: 4.9GB\n",
      "Epoch 1, Batch 5980, Loss: 5.1917, Time: 3556.5s, Step: 5981, GPU: 4.9GB\n",
      "Epoch 1, Batch 5985, Loss: 4.9065, Time: 3559.9s, Step: 5986, GPU: 4.9GB\n",
      "Epoch 1, Batch 5990, Loss: 4.9310, Time: 3562.4s, Step: 5991, GPU: 4.9GB\n",
      "Epoch 1, Batch 5995, Loss: 5.7199, Time: 3565.9s, Step: 5996, GPU: 4.9GB\n",
      "Epoch 1, Batch 6000, Loss: 5.1366, Time: 3568.5s, Step: 6001, GPU: 4.9GB\n",
      "Epoch 1, Batch 6005, Loss: 4.7093, Time: 3571.9s, Step: 6006, GPU: 4.9GB\n",
      "Epoch 1, Batch 6010, Loss: 4.8269, Time: 3574.4s, Step: 6011, GPU: 4.9GB\n",
      "Epoch 1, Batch 6015, Loss: 4.8740, Time: 3577.8s, Step: 6016, GPU: 4.9GB\n",
      "Epoch 1, Batch 6020, Loss: 3.9530, Time: 3580.3s, Step: 6021, GPU: 4.9GB\n",
      "Epoch 1, Batch 6025, Loss: 5.1813, Time: 3583.7s, Step: 6026, GPU: 4.9GB\n",
      "Epoch 1, Batch 6030, Loss: 4.5260, Time: 3586.3s, Step: 6031, GPU: 4.9GB\n",
      "Epoch 1, Batch 6035, Loss: 4.2590, Time: 3589.7s, Step: 6036, GPU: 4.9GB\n",
      "Epoch 1, Batch 6040, Loss: 4.6951, Time: 3592.2s, Step: 6041, GPU: 4.9GB\n",
      "Epoch 1, Batch 6045, Loss: 4.2467, Time: 3595.6s, Step: 6046, GPU: 4.9GB\n",
      "Epoch 1, Batch 6050, Loss: 5.3683, Time: 3598.2s, Step: 6051, GPU: 4.9GB\n",
      "Epoch 1, Batch 6055, Loss: 4.2928, Time: 3601.6s, Step: 6056, GPU: 4.9GB\n",
      "Epoch 1, Batch 6060, Loss: 5.0804, Time: 3604.1s, Step: 6061, GPU: 4.9GB\n",
      "Epoch 1, Batch 6065, Loss: 6.0898, Time: 3607.5s, Step: 6066, GPU: 4.9GB\n",
      "Epoch 1, Batch 6070, Loss: 4.7731, Time: 3610.1s, Step: 6071, GPU: 4.9GB\n",
      "Epoch 1, Batch 6075, Loss: 4.4378, Time: 3613.5s, Step: 6076, GPU: 4.9GB\n",
      "Epoch 1, Batch 6080, Loss: 4.4943, Time: 3616.0s, Step: 6081, GPU: 4.9GB\n",
      "Epoch 1, Batch 6085, Loss: 4.6931, Time: 3619.4s, Step: 6086, GPU: 4.9GB\n",
      "Epoch 1, Batch 6090, Loss: 3.7140, Time: 3621.9s, Step: 6091, GPU: 4.9GB\n",
      "Epoch 1, Batch 6095, Loss: 4.7492, Time: 3625.3s, Step: 6096, GPU: 4.9GB\n",
      "Epoch 1, Batch 6100, Loss: 4.7550, Time: 3627.8s, Step: 6101, GPU: 4.9GB\n",
      "Epoch 1, Batch 6105, Loss: 4.0307, Time: 3631.2s, Step: 6106, GPU: 4.9GB\n",
      "Epoch 1, Batch 6110, Loss: 4.2303, Time: 3633.7s, Step: 6111, GPU: 4.9GB\n",
      "Epoch 1, Batch 6115, Loss: 5.2846, Time: 3637.2s, Step: 6116, GPU: 4.9GB\n",
      "Epoch 1, Batch 6120, Loss: 4.8704, Time: 3639.7s, Step: 6121, GPU: 4.9GB\n",
      "Epoch 1, Batch 6125, Loss: 4.8035, Time: 3643.1s, Step: 6126, GPU: 4.9GB\n",
      "Epoch 1, Batch 6130, Loss: 5.2672, Time: 3645.6s, Step: 6131, GPU: 4.9GB\n",
      "Epoch 1, Batch 6135, Loss: 4.6249, Time: 3649.0s, Step: 6136, GPU: 4.9GB\n",
      "Epoch 1, Batch 6140, Loss: 5.5840, Time: 3651.5s, Step: 6141, GPU: 4.9GB\n",
      "Epoch 1, Batch 6145, Loss: 3.9434, Time: 3654.9s, Step: 6146, GPU: 4.9GB\n",
      "Epoch 1, Batch 6150, Loss: 4.2005, Time: 3657.4s, Step: 6151, GPU: 4.9GB\n",
      "Epoch 1, Batch 6155, Loss: 5.5446, Time: 3660.8s, Step: 6156, GPU: 4.9GB\n",
      "Epoch 1, Batch 6160, Loss: 4.5165, Time: 3663.3s, Step: 6161, GPU: 4.9GB\n",
      "Epoch 1, Batch 6165, Loss: 4.6516, Time: 3666.7s, Step: 6166, GPU: 4.9GB\n",
      "Epoch 1, Batch 6170, Loss: 4.9110, Time: 3669.3s, Step: 6171, GPU: 4.9GB\n",
      "Epoch 1, Batch 6175, Loss: 4.8198, Time: 3672.7s, Step: 6176, GPU: 4.9GB\n",
      "Epoch 1, Batch 6180, Loss: 4.6343, Time: 3675.2s, Step: 6181, GPU: 4.9GB\n",
      "Epoch 1, Batch 6185, Loss: 4.2012, Time: 3678.6s, Step: 6186, GPU: 4.9GB\n",
      "Epoch 1, Batch 6190, Loss: 4.1855, Time: 3681.1s, Step: 6191, GPU: 4.9GB\n",
      "Epoch 1, Batch 6195, Loss: 4.9185, Time: 3684.6s, Step: 6196, GPU: 4.9GB\n",
      "Epoch 1, Batch 6200, Loss: 5.2987, Time: 3687.1s, Step: 6201, GPU: 4.9GB\n",
      "Epoch 1, Batch 6205, Loss: 4.7672, Time: 3690.5s, Step: 6206, GPU: 4.9GB\n",
      "Epoch 1, Batch 6210, Loss: 4.7719, Time: 3693.1s, Step: 6211, GPU: 4.9GB\n",
      "Epoch 1, Batch 6215, Loss: 3.8518, Time: 3696.9s, Step: 6216, GPU: 4.9GB\n",
      "Epoch 1, Batch 6220, Loss: 4.4834, Time: 3699.4s, Step: 6221, GPU: 4.9GB\n",
      "Epoch 1, Batch 6225, Loss: 5.9155, Time: 3702.9s, Step: 6226, GPU: 4.9GB\n",
      "Epoch 1, Batch 6230, Loss: 5.5365, Time: 3705.4s, Step: 6231, GPU: 4.9GB\n",
      "Epoch 1, Batch 6235, Loss: 3.9633, Time: 3708.8s, Step: 6236, GPU: 4.9GB\n",
      "Epoch 1, Batch 6240, Loss: 4.7991, Time: 3711.3s, Step: 6241, GPU: 4.9GB\n",
      "Epoch 1, Batch 6245, Loss: 5.2200, Time: 3714.8s, Step: 6246, GPU: 4.9GB\n",
      "Epoch 1, Batch 6250, Loss: 5.1251, Time: 3717.3s, Step: 6251, GPU: 4.9GB\n",
      "Epoch 1, Batch 6255, Loss: 5.6633, Time: 3720.8s, Step: 6256, GPU: 4.9GB\n",
      "Epoch 1, Batch 6260, Loss: 4.1511, Time: 3723.3s, Step: 6261, GPU: 4.9GB\n",
      "Epoch 1, Batch 6265, Loss: 5.2130, Time: 3726.7s, Step: 6266, GPU: 4.9GB\n",
      "Epoch 1, Batch 6270, Loss: 4.6058, Time: 3729.2s, Step: 6271, GPU: 4.9GB\n",
      "Epoch 1, Batch 6275, Loss: 5.0274, Time: 3732.7s, Step: 6276, GPU: 4.9GB\n",
      "Epoch 1, Batch 6280, Loss: 4.0784, Time: 3735.2s, Step: 6281, GPU: 4.9GB\n",
      "Epoch 1, Batch 6285, Loss: 4.8157, Time: 3738.6s, Step: 6286, GPU: 4.9GB\n",
      "Epoch 1, Batch 6290, Loss: 4.2917, Time: 3741.1s, Step: 6291, GPU: 4.9GB\n",
      "Epoch 1, Batch 6295, Loss: 4.4612, Time: 3744.6s, Step: 6296, GPU: 4.9GB\n",
      "Epoch 1, Batch 6300, Loss: 4.5780, Time: 3747.1s, Step: 6301, GPU: 4.9GB\n",
      "Epoch 1, Batch 6305, Loss: 4.1446, Time: 3750.5s, Step: 6306, GPU: 4.9GB\n",
      "Epoch 1, Batch 6310, Loss: 5.1550, Time: 3753.0s, Step: 6311, GPU: 4.9GB\n",
      "Epoch 1, Batch 6315, Loss: 4.6195, Time: 3756.4s, Step: 6316, GPU: 4.9GB\n",
      "Epoch 1, Batch 6320, Loss: 5.1837, Time: 3758.9s, Step: 6321, GPU: 4.9GB\n",
      "Epoch 1, Batch 6325, Loss: 4.5844, Time: 3762.3s, Step: 6326, GPU: 4.9GB\n",
      "Epoch 1, Batch 6330, Loss: 4.7827, Time: 3764.9s, Step: 6331, GPU: 4.9GB\n",
      "Epoch 1, Batch 6335, Loss: 5.5632, Time: 3768.3s, Step: 6336, GPU: 4.9GB\n",
      "Epoch 1, Batch 6340, Loss: 4.8881, Time: 3770.8s, Step: 6341, GPU: 4.9GB\n",
      "Epoch 1, Batch 6345, Loss: 4.3917, Time: 3774.2s, Step: 6346, GPU: 4.9GB\n",
      "Epoch 1, Batch 6350, Loss: 4.1088, Time: 3776.7s, Step: 6351, GPU: 4.9GB\n",
      "Epoch 1, Batch 6355, Loss: 4.1299, Time: 3780.1s, Step: 6356, GPU: 4.9GB\n",
      "Epoch 1, Batch 6360, Loss: 4.2332, Time: 3782.7s, Step: 6361, GPU: 4.9GB\n",
      "Epoch 1, Batch 6365, Loss: 4.1740, Time: 3786.1s, Step: 6366, GPU: 4.9GB\n",
      "Epoch 1, Batch 6370, Loss: 4.7582, Time: 3788.6s, Step: 6371, GPU: 4.9GB\n",
      "Epoch 1, Batch 6375, Loss: 4.6128, Time: 3792.1s, Step: 6376, GPU: 4.9GB\n",
      "Epoch 1, Batch 6380, Loss: 3.5386, Time: 3794.6s, Step: 6381, GPU: 4.9GB\n",
      "Epoch 1, Batch 6385, Loss: 4.1875, Time: 3798.0s, Step: 6386, GPU: 4.9GB\n",
      "Epoch 1, Batch 6390, Loss: 4.6104, Time: 3800.5s, Step: 6391, GPU: 4.9GB\n",
      "Epoch 1, Batch 6395, Loss: 4.6215, Time: 3803.9s, Step: 6396, GPU: 4.9GB\n",
      "Epoch 1, Batch 6400, Loss: 4.6907, Time: 3806.5s, Step: 6401, GPU: 4.9GB\n",
      "Epoch 1, Batch 6405, Loss: 4.7905, Time: 3810.0s, Step: 6406, GPU: 4.9GB\n",
      "Epoch 1, Batch 6410, Loss: 4.2580, Time: 3812.5s, Step: 6411, GPU: 4.9GB\n",
      "Epoch 1, Batch 6415, Loss: 3.8437, Time: 3815.9s, Step: 6416, GPU: 4.9GB\n",
      "Epoch 1, Batch 6420, Loss: 5.8241, Time: 3818.4s, Step: 6421, GPU: 4.9GB\n",
      "Epoch 1, Batch 6425, Loss: 4.1889, Time: 3821.8s, Step: 6426, GPU: 4.9GB\n",
      "Epoch 1, Batch 6430, Loss: 4.7280, Time: 3824.3s, Step: 6431, GPU: 4.9GB\n",
      "Epoch 1, Batch 6435, Loss: 4.5200, Time: 3827.8s, Step: 6436, GPU: 4.9GB\n",
      "Epoch 1, Batch 6440, Loss: 4.6979, Time: 3830.3s, Step: 6441, GPU: 4.9GB\n",
      "Epoch 1, Batch 6445, Loss: 4.8128, Time: 3833.7s, Step: 6446, GPU: 4.9GB\n",
      "Epoch 1, Batch 6450, Loss: 4.2849, Time: 3836.2s, Step: 6451, GPU: 4.9GB\n",
      "Epoch 1, Batch 6455, Loss: 5.0104, Time: 3839.6s, Step: 6456, GPU: 4.9GB\n",
      "Epoch 1, Batch 6460, Loss: 4.1823, Time: 3842.1s, Step: 6461, GPU: 4.9GB\n",
      "Epoch 1, Batch 6465, Loss: 4.6951, Time: 3845.6s, Step: 6466, GPU: 4.9GB\n",
      "Epoch 1, Batch 6470, Loss: 4.3399, Time: 3848.1s, Step: 6471, GPU: 4.9GB\n",
      "Epoch 1, Batch 6475, Loss: 4.4557, Time: 3851.5s, Step: 6476, GPU: 4.9GB\n",
      "Epoch 1, Batch 6480, Loss: 5.6078, Time: 3854.0s, Step: 6481, GPU: 4.9GB\n",
      "Epoch 1, Batch 6485, Loss: 4.4703, Time: 3857.4s, Step: 6486, GPU: 4.9GB\n",
      "Epoch 1, Batch 6490, Loss: 4.0599, Time: 3859.9s, Step: 6491, GPU: 4.9GB\n",
      "Epoch 1, Batch 6495, Loss: 4.4000, Time: 3863.3s, Step: 6496, GPU: 4.9GB\n",
      "Epoch 1, Batch 6500, Loss: 4.8778, Time: 3865.9s, Step: 6501, GPU: 4.9GB\n",
      "Epoch 1, Batch 6505, Loss: 4.1075, Time: 3869.3s, Step: 6506, GPU: 4.9GB\n",
      "Epoch 1, Batch 6510, Loss: 4.3120, Time: 3871.8s, Step: 6511, GPU: 4.9GB\n",
      "Epoch 1, Batch 6515, Loss: 4.5825, Time: 3875.2s, Step: 6516, GPU: 4.9GB\n",
      "Epoch 1, Batch 6520, Loss: 5.2850, Time: 3877.7s, Step: 6521, GPU: 4.9GB\n",
      "Epoch 1, Batch 6525, Loss: 4.7636, Time: 3881.1s, Step: 6526, GPU: 4.9GB\n",
      "Epoch 1, Batch 6530, Loss: 3.7184, Time: 3883.7s, Step: 6531, GPU: 4.9GB\n",
      "Epoch 1, Batch 6535, Loss: 5.3080, Time: 3887.1s, Step: 6536, GPU: 4.9GB\n",
      "Epoch 1, Batch 6540, Loss: 4.4703, Time: 3889.6s, Step: 6541, GPU: 4.9GB\n",
      "Epoch 1, Batch 6545, Loss: 4.9539, Time: 3893.0s, Step: 6546, GPU: 4.9GB\n",
      "Epoch 1, Batch 6550, Loss: 4.8369, Time: 3895.5s, Step: 6551, GPU: 4.9GB\n",
      "Epoch 1, Batch 6555, Loss: 4.6521, Time: 3898.9s, Step: 6556, GPU: 4.9GB\n",
      "Epoch 1, Batch 6560, Loss: 5.2841, Time: 3901.4s, Step: 6561, GPU: 4.9GB\n",
      "Epoch 1, Batch 6565, Loss: 4.7052, Time: 3904.8s, Step: 6566, GPU: 4.9GB\n",
      "Epoch 1, Batch 6570, Loss: 4.7167, Time: 3907.3s, Step: 6571, GPU: 4.9GB\n",
      "Epoch 1, Batch 6575, Loss: 5.6445, Time: 3910.7s, Step: 6576, GPU: 4.9GB\n",
      "Epoch 1, Batch 6580, Loss: 4.9789, Time: 3913.3s, Step: 6581, GPU: 4.9GB\n",
      "Epoch 1, Batch 6585, Loss: 4.6274, Time: 3916.7s, Step: 6586, GPU: 4.9GB\n",
      "Epoch 1, Batch 6590, Loss: 4.1859, Time: 3919.2s, Step: 6591, GPU: 4.9GB\n",
      "Epoch 1, Batch 6595, Loss: 4.4117, Time: 3922.5s, Step: 6596, GPU: 4.9GB\n",
      "Epoch 1, Batch 6600, Loss: 5.1335, Time: 3925.1s, Step: 6601, GPU: 4.9GB\n",
      "Epoch 1, Batch 6605, Loss: 4.9031, Time: 3928.5s, Step: 6606, GPU: 4.9GB\n",
      "Epoch 1, Batch 6610, Loss: 4.5633, Time: 3931.0s, Step: 6611, GPU: 4.9GB\n",
      "Epoch 1, Batch 6615, Loss: 4.2041, Time: 3934.4s, Step: 6616, GPU: 4.9GB\n",
      "Epoch 1, Batch 6620, Loss: 4.4696, Time: 3936.9s, Step: 6621, GPU: 4.9GB\n",
      "Epoch 1, Batch 6625, Loss: 4.3567, Time: 3940.4s, Step: 6626, GPU: 4.9GB\n",
      "Epoch 1, Batch 6630, Loss: 5.2127, Time: 3942.9s, Step: 6631, GPU: 4.9GB\n",
      "Epoch 1, Batch 6635, Loss: 4.2900, Time: 3946.3s, Step: 6636, GPU: 4.9GB\n",
      "Epoch 1, Batch 6640, Loss: 4.9293, Time: 3948.8s, Step: 6641, GPU: 4.9GB\n",
      "Epoch 1, Batch 6645, Loss: 4.0593, Time: 3952.2s, Step: 6646, GPU: 4.9GB\n",
      "Epoch 1, Batch 6650, Loss: 4.4234, Time: 3954.7s, Step: 6651, GPU: 4.9GB\n",
      "Epoch 1, Batch 6655, Loss: 5.3400, Time: 3958.1s, Step: 6656, GPU: 4.9GB\n",
      "Epoch 1, Batch 6660, Loss: 4.1466, Time: 3960.6s, Step: 6661, GPU: 4.9GB\n",
      "Epoch 1, Batch 6665, Loss: 4.9862, Time: 3964.1s, Step: 6666, GPU: 4.9GB\n",
      "Epoch 1, Batch 6670, Loss: 4.7468, Time: 3966.6s, Step: 6671, GPU: 4.9GB\n",
      "Epoch 1, Batch 6675, Loss: 4.1807, Time: 3970.0s, Step: 6676, GPU: 4.9GB\n",
      "Epoch 1, Batch 6680, Loss: 4.7009, Time: 3972.6s, Step: 6681, GPU: 4.9GB\n",
      "Epoch 1, Batch 6685, Loss: 4.8385, Time: 3976.0s, Step: 6686, GPU: 4.9GB\n",
      "Epoch 1, Batch 6690, Loss: 3.7391, Time: 3978.5s, Step: 6691, GPU: 4.9GB\n",
      "Epoch 1, Batch 6695, Loss: 4.1700, Time: 3981.9s, Step: 6696, GPU: 4.9GB\n",
      "Epoch 1, Batch 6700, Loss: 5.4672, Time: 3984.5s, Step: 6701, GPU: 4.9GB\n",
      "Epoch 1, Batch 6705, Loss: 4.6284, Time: 3987.9s, Step: 6706, GPU: 4.9GB\n",
      "Epoch 1, Batch 6710, Loss: 4.4774, Time: 3990.4s, Step: 6711, GPU: 4.9GB\n",
      "Epoch 1, Batch 6715, Loss: 4.7514, Time: 3993.9s, Step: 6716, GPU: 4.9GB\n",
      "Epoch 1, Batch 6720, Loss: 5.9441, Time: 3996.4s, Step: 6721, GPU: 4.9GB\n",
      "Epoch 1, Batch 6725, Loss: 4.3158, Time: 3999.8s, Step: 6726, GPU: 4.9GB\n",
      "Epoch 1, Batch 6730, Loss: 5.1385, Time: 4002.3s, Step: 6731, GPU: 4.9GB\n",
      "Epoch 1, Batch 6735, Loss: 4.9933, Time: 4005.8s, Step: 6736, GPU: 4.9GB\n",
      "Epoch 1, Batch 6740, Loss: 4.5117, Time: 4008.3s, Step: 6741, GPU: 4.9GB\n",
      "Epoch 1, Batch 6745, Loss: 4.7502, Time: 4011.7s, Step: 6746, GPU: 4.9GB\n",
      "Epoch 1, Batch 6750, Loss: 5.2887, Time: 4014.2s, Step: 6751, GPU: 4.9GB\n",
      "Epoch 1, Batch 6755, Loss: 3.8207, Time: 4017.6s, Step: 6756, GPU: 4.9GB\n",
      "Epoch 1, Batch 6760, Loss: 4.0842, Time: 4020.2s, Step: 6761, GPU: 4.9GB\n",
      "Epoch 1, Batch 6765, Loss: 4.5440, Time: 4023.6s, Step: 6766, GPU: 4.9GB\n",
      "Epoch 1, Batch 6770, Loss: 4.5008, Time: 4026.1s, Step: 6771, GPU: 4.9GB\n",
      "Epoch 1, Batch 6775, Loss: 4.5838, Time: 4029.5s, Step: 6776, GPU: 4.9GB\n",
      "Epoch 1, Batch 6780, Loss: 4.3732, Time: 4032.1s, Step: 6781, GPU: 4.9GB\n",
      "Epoch 1, Batch 6785, Loss: 4.6737, Time: 4035.5s, Step: 6786, GPU: 4.9GB\n",
      "Epoch 1, Batch 6790, Loss: 4.0190, Time: 4038.0s, Step: 6791, GPU: 4.9GB\n",
      "Epoch 1, Batch 6795, Loss: 4.3762, Time: 4041.4s, Step: 6796, GPU: 4.9GB\n",
      "Epoch 1, Batch 6800, Loss: 4.3281, Time: 4044.0s, Step: 6801, GPU: 4.9GB\n",
      "Epoch 1, Batch 6805, Loss: 5.6046, Time: 4047.5s, Step: 6806, GPU: 4.9GB\n",
      "Epoch 1, Batch 6810, Loss: 5.0690, Time: 4050.0s, Step: 6811, GPU: 4.9GB\n",
      "Epoch 1, Batch 6815, Loss: 4.4314, Time: 4053.4s, Step: 6816, GPU: 4.9GB\n",
      "Epoch 1, Batch 6820, Loss: 4.6492, Time: 4055.9s, Step: 6821, GPU: 4.9GB\n",
      "Epoch 1, Batch 6825, Loss: 3.7160, Time: 4059.3s, Step: 6826, GPU: 4.9GB\n",
      "Epoch 1, Batch 6830, Loss: 4.4262, Time: 4061.8s, Step: 6831, GPU: 4.9GB\n",
      "Epoch 1, Batch 6835, Loss: 4.6239, Time: 4065.3s, Step: 6836, GPU: 4.9GB\n",
      "Epoch 1, Batch 6840, Loss: 5.1655, Time: 4067.8s, Step: 6841, GPU: 4.9GB\n",
      "Epoch 1, Batch 6845, Loss: 4.9709, Time: 4071.2s, Step: 6846, GPU: 4.9GB\n",
      "Epoch 1, Batch 6850, Loss: 4.3630, Time: 4073.7s, Step: 6851, GPU: 4.9GB\n",
      "Epoch 1, Batch 6855, Loss: 4.5071, Time: 4077.1s, Step: 6856, GPU: 4.9GB\n",
      "Epoch 1, Batch 6860, Loss: 4.2609, Time: 4079.6s, Step: 6861, GPU: 4.9GB\n",
      "Epoch 1, Batch 6865, Loss: 4.2691, Time: 4083.0s, Step: 6866, GPU: 4.9GB\n",
      "Epoch 1, Batch 6870, Loss: 4.1379, Time: 4085.5s, Step: 6871, GPU: 4.9GB\n",
      "Epoch 1, Batch 6875, Loss: 4.6735, Time: 4088.9s, Step: 6876, GPU: 4.9GB\n",
      "Epoch 1, Batch 6880, Loss: 4.9697, Time: 4091.4s, Step: 6881, GPU: 4.9GB\n",
      "Epoch 1, Batch 6885, Loss: 4.5177, Time: 4094.9s, Step: 6886, GPU: 4.9GB\n",
      "Epoch 1, Batch 6890, Loss: 4.3976, Time: 4097.4s, Step: 6891, GPU: 4.9GB\n",
      "Epoch 1, Batch 6895, Loss: 4.5152, Time: 4100.8s, Step: 6896, GPU: 4.9GB\n",
      "Epoch 1, Batch 6900, Loss: 4.8230, Time: 4103.3s, Step: 6901, GPU: 4.9GB\n",
      "Epoch 1, Batch 6905, Loss: 4.3199, Time: 4106.8s, Step: 6906, GPU: 4.9GB\n",
      "Epoch 1, Batch 6910, Loss: 5.1683, Time: 4109.3s, Step: 6911, GPU: 4.9GB\n",
      "Epoch 1, Batch 6915, Loss: 5.2539, Time: 4112.8s, Step: 6916, GPU: 4.9GB\n",
      "Epoch 1, Batch 6920, Loss: 4.3471, Time: 4115.3s, Step: 6921, GPU: 4.9GB\n",
      "Epoch 1, Batch 6925, Loss: 4.5873, Time: 4118.7s, Step: 6926, GPU: 4.9GB\n",
      "Epoch 1, Batch 6930, Loss: 3.7243, Time: 4121.2s, Step: 6931, GPU: 4.9GB\n",
      "Epoch 1, Batch 6935, Loss: 5.2222, Time: 4124.6s, Step: 6936, GPU: 4.9GB\n",
      "Epoch 1, Batch 6940, Loss: 4.2758, Time: 4127.2s, Step: 6941, GPU: 4.9GB\n",
      "Epoch 1, Batch 6945, Loss: 3.9507, Time: 4130.6s, Step: 6946, GPU: 4.9GB\n",
      "Epoch 1, Batch 6950, Loss: 3.8217, Time: 4133.2s, Step: 6951, GPU: 4.9GB\n",
      "Epoch 1, Batch 6955, Loss: 4.8782, Time: 4136.6s, Step: 6956, GPU: 4.9GB\n",
      "Epoch 1, Batch 6960, Loss: 4.5447, Time: 4139.1s, Step: 6961, GPU: 4.9GB\n",
      "Epoch 1, Batch 6965, Loss: 4.1765, Time: 4142.6s, Step: 6966, GPU: 4.9GB\n",
      "Epoch 1, Batch 6970, Loss: 3.6447, Time: 4145.1s, Step: 6971, GPU: 4.9GB\n",
      "Epoch 1, Batch 6975, Loss: 5.1766, Time: 4148.5s, Step: 6976, GPU: 4.9GB\n",
      "Epoch 1, Batch 6980, Loss: 4.2516, Time: 4151.0s, Step: 6981, GPU: 4.9GB\n",
      "Epoch 1, Batch 6985, Loss: 4.3883, Time: 4154.4s, Step: 6986, GPU: 4.9GB\n",
      "Epoch 1, Batch 6990, Loss: 4.5525, Time: 4157.0s, Step: 6991, GPU: 4.9GB\n",
      "Epoch 1, Batch 6995, Loss: 4.6961, Time: 4160.4s, Step: 6996, GPU: 4.9GB\n",
      "Epoch 1, Batch 7000, Loss: 4.6957, Time: 4163.0s, Step: 7001, GPU: 4.9GB\n",
      "Epoch 1, Batch 7005, Loss: 4.7956, Time: 4166.4s, Step: 7006, GPU: 4.9GB\n",
      "Epoch 1, Batch 7010, Loss: 4.6723, Time: 4168.9s, Step: 7011, GPU: 4.9GB\n",
      "Epoch 1, Batch 7015, Loss: 4.1493, Time: 4172.4s, Step: 7016, GPU: 4.9GB\n",
      "Epoch 1, Batch 7020, Loss: 5.0018, Time: 4174.9s, Step: 7021, GPU: 4.9GB\n",
      "Epoch 1, Batch 7025, Loss: 3.7885, Time: 4178.3s, Step: 7026, GPU: 4.9GB\n",
      "Epoch 1, Batch 7030, Loss: 4.6807, Time: 4180.9s, Step: 7031, GPU: 4.9GB\n",
      "Epoch 1, Batch 7035, Loss: 5.4173, Time: 4184.3s, Step: 7036, GPU: 4.9GB\n",
      "Epoch 1, Batch 7040, Loss: 4.4806, Time: 4186.9s, Step: 7041, GPU: 4.9GB\n",
      "Epoch 1, Batch 7045, Loss: 4.5037, Time: 4190.4s, Step: 7046, GPU: 4.9GB\n",
      "Epoch 1, Batch 7050, Loss: 5.0484, Time: 4192.9s, Step: 7051, GPU: 4.9GB\n",
      "Epoch 1, Batch 7055, Loss: 4.8968, Time: 4196.4s, Step: 7056, GPU: 4.9GB\n",
      "Epoch 1, Batch 7060, Loss: 4.5710, Time: 4198.9s, Step: 7061, GPU: 4.9GB\n",
      "Epoch 1, Batch 7065, Loss: 4.3767, Time: 4202.3s, Step: 7066, GPU: 4.9GB\n",
      "Epoch 1, Batch 7070, Loss: 4.3043, Time: 4204.9s, Step: 7071, GPU: 4.9GB\n",
      "Epoch 1, Batch 7075, Loss: 4.4470, Time: 4208.3s, Step: 7076, GPU: 4.9GB\n",
      "Epoch 1, Batch 7080, Loss: 4.4590, Time: 4210.9s, Step: 7081, GPU: 4.9GB\n",
      "Epoch 1, Batch 7085, Loss: 4.5993, Time: 4214.3s, Step: 7086, GPU: 4.9GB\n",
      "Epoch 1, Batch 7090, Loss: 5.1275, Time: 4216.8s, Step: 7091, GPU: 4.9GB\n",
      "Epoch 1, Batch 7095, Loss: 4.1678, Time: 4220.2s, Step: 7096, GPU: 4.9GB\n",
      "Epoch 1, Batch 7100, Loss: 4.2901, Time: 4222.7s, Step: 7101, GPU: 4.9GB\n",
      "Epoch 1, Batch 7105, Loss: 4.9880, Time: 4226.1s, Step: 7106, GPU: 4.9GB\n",
      "Epoch 1, Batch 7110, Loss: 4.5136, Time: 4228.7s, Step: 7111, GPU: 4.9GB\n",
      "Epoch 1, Batch 7115, Loss: 4.4853, Time: 4232.1s, Step: 7116, GPU: 4.9GB\n",
      "Epoch 1, Batch 7120, Loss: 4.3529, Time: 4234.6s, Step: 7121, GPU: 4.9GB\n",
      "Epoch 1, Batch 7125, Loss: 4.9165, Time: 4238.1s, Step: 7126, GPU: 4.9GB\n",
      "Epoch 1, Batch 7130, Loss: 5.0090, Time: 4240.6s, Step: 7131, GPU: 4.9GB\n",
      "Epoch 1, Batch 7135, Loss: 4.4052, Time: 4244.1s, Step: 7136, GPU: 4.9GB\n",
      "Epoch 1, Batch 7140, Loss: 3.7850, Time: 4246.6s, Step: 7141, GPU: 4.9GB\n",
      "Epoch 1, Batch 7145, Loss: 4.0991, Time: 4250.0s, Step: 7146, GPU: 4.9GB\n",
      "Epoch 1, Batch 7150, Loss: 4.4098, Time: 4252.5s, Step: 7151, GPU: 4.9GB\n",
      "Epoch 1, Batch 7155, Loss: 4.4515, Time: 4255.9s, Step: 7156, GPU: 4.9GB\n",
      "Epoch 1, Batch 7160, Loss: 4.4172, Time: 4258.5s, Step: 7161, GPU: 4.9GB\n",
      "Epoch 1, Batch 7165, Loss: 4.8081, Time: 4261.9s, Step: 7166, GPU: 4.9GB\n",
      "Epoch 1, Batch 7170, Loss: 3.8744, Time: 4264.4s, Step: 7171, GPU: 4.9GB\n",
      "Epoch 1, Batch 7175, Loss: 4.6687, Time: 4267.8s, Step: 7176, GPU: 4.9GB\n",
      "Epoch 1, Batch 7180, Loss: 5.4420, Time: 4270.3s, Step: 7181, GPU: 4.9GB\n",
      "Epoch 1, Batch 7185, Loss: 4.8557, Time: 4273.8s, Step: 7186, GPU: 4.9GB\n",
      "Epoch 1, Batch 7190, Loss: 5.1870, Time: 4276.3s, Step: 7191, GPU: 4.9GB\n",
      "Epoch 1, Batch 7195, Loss: 4.8351, Time: 4279.7s, Step: 7196, GPU: 4.9GB\n",
      "Epoch 1, Batch 7200, Loss: 4.9739, Time: 4282.3s, Step: 7201, GPU: 4.9GB\n",
      "Epoch 1, Batch 7205, Loss: 4.8104, Time: 4285.7s, Step: 7206, GPU: 4.9GB\n",
      "Epoch 1, Batch 7210, Loss: 5.8251, Time: 4288.2s, Step: 7211, GPU: 4.9GB\n",
      "Epoch 1, Batch 7215, Loss: 4.2564, Time: 4291.7s, Step: 7216, GPU: 4.9GB\n",
      "Epoch 1, Batch 7220, Loss: 4.5626, Time: 4294.2s, Step: 7221, GPU: 4.9GB\n",
      "Epoch 1, Batch 7225, Loss: 4.8829, Time: 4297.6s, Step: 7226, GPU: 4.9GB\n",
      "Epoch 1, Batch 7230, Loss: 4.3486, Time: 4300.2s, Step: 7231, GPU: 4.9GB\n",
      "Epoch 1, Batch 7235, Loss: 3.7113, Time: 4303.6s, Step: 7236, GPU: 4.9GB\n",
      "Epoch 1, Batch 7240, Loss: 5.6495, Time: 4306.1s, Step: 7241, GPU: 4.9GB\n",
      "Epoch 1, Batch 7245, Loss: 3.7887, Time: 4309.5s, Step: 7246, GPU: 4.9GB\n",
      "Epoch 1, Batch 7250, Loss: 4.5676, Time: 4312.1s, Step: 7251, GPU: 4.9GB\n",
      "Epoch 1, Batch 7255, Loss: 4.5823, Time: 4315.5s, Step: 7256, GPU: 4.9GB\n",
      "Epoch 1, Batch 7260, Loss: 4.4974, Time: 4318.0s, Step: 7261, GPU: 4.9GB\n",
      "Epoch 1, Batch 7265, Loss: 4.3457, Time: 4321.4s, Step: 7266, GPU: 4.9GB\n",
      "Epoch 1, Batch 7270, Loss: 4.1614, Time: 4323.9s, Step: 7271, GPU: 4.9GB\n",
      "Epoch 1, Batch 7275, Loss: 4.5820, Time: 4327.4s, Step: 7276, GPU: 4.9GB\n",
      "Epoch 1, Batch 7280, Loss: 4.3515, Time: 4329.9s, Step: 7281, GPU: 4.9GB\n",
      "Epoch 1, Batch 7285, Loss: 4.5815, Time: 4333.3s, Step: 7286, GPU: 4.9GB\n",
      "Epoch 1, Batch 7290, Loss: 5.1798, Time: 4335.8s, Step: 7291, GPU: 4.9GB\n",
      "Epoch 1, Batch 7295, Loss: 4.6466, Time: 4339.2s, Step: 7296, GPU: 4.9GB\n",
      "Epoch 1, Batch 7300, Loss: 3.7928, Time: 4341.7s, Step: 7301, GPU: 4.9GB\n",
      "Epoch 1, Batch 7305, Loss: 4.3522, Time: 4345.2s, Step: 7306, GPU: 4.9GB\n",
      "Epoch 1, Batch 7310, Loss: 5.4211, Time: 4347.7s, Step: 7311, GPU: 4.9GB\n",
      "Epoch 1, Batch 7315, Loss: 4.9476, Time: 4351.1s, Step: 7316, GPU: 4.9GB\n",
      "Epoch 1, Batch 7320, Loss: 4.1967, Time: 4353.6s, Step: 7321, GPU: 4.9GB\n",
      "Epoch 1, Batch 7325, Loss: 4.4805, Time: 4357.1s, Step: 7326, GPU: 4.9GB\n",
      "Epoch 1, Batch 7330, Loss: 3.8747, Time: 4359.6s, Step: 7331, GPU: 4.9GB\n",
      "Epoch 1, Batch 7335, Loss: 5.7713, Time: 4363.0s, Step: 7336, GPU: 4.9GB\n",
      "Epoch 1, Batch 7340, Loss: 3.3898, Time: 4365.5s, Step: 7341, GPU: 4.9GB\n",
      "Epoch 1, Batch 7345, Loss: 4.5027, Time: 4369.0s, Step: 7346, GPU: 4.9GB\n",
      "Epoch 1, Batch 7350, Loss: 4.4636, Time: 4371.5s, Step: 7351, GPU: 4.9GB\n",
      "Epoch 1, Batch 7355, Loss: 4.1089, Time: 4374.9s, Step: 7356, GPU: 4.9GB\n",
      "Epoch 1, Batch 7360, Loss: 4.8660, Time: 4377.4s, Step: 7361, GPU: 4.9GB\n",
      "Epoch 1, Batch 7365, Loss: 4.2062, Time: 4380.8s, Step: 7366, GPU: 4.9GB\n",
      "Epoch 1, Batch 7370, Loss: 4.5480, Time: 4383.3s, Step: 7371, GPU: 4.9GB\n",
      "Epoch 1, Batch 7375, Loss: 3.8557, Time: 4386.7s, Step: 7376, GPU: 4.9GB\n",
      "Epoch 1, Batch 7380, Loss: 4.8739, Time: 4389.2s, Step: 7381, GPU: 4.9GB\n",
      "Epoch 1, Batch 7385, Loss: 4.4193, Time: 4392.6s, Step: 7386, GPU: 4.9GB\n",
      "Epoch 1, Batch 7390, Loss: 3.9460, Time: 4395.1s, Step: 7391, GPU: 4.9GB\n",
      "Epoch 1, Batch 7395, Loss: 5.2638, Time: 4398.5s, Step: 7396, GPU: 4.9GB\n",
      "Epoch 1, Batch 7400, Loss: 4.3853, Time: 4401.1s, Step: 7401, GPU: 4.9GB\n",
      "Epoch 1, Batch 7405, Loss: 4.7646, Time: 4404.5s, Step: 7406, GPU: 4.9GB\n",
      "Epoch 1, Batch 7410, Loss: 5.5289, Time: 4407.0s, Step: 7411, GPU: 4.9GB\n",
      "Epoch 1, Batch 7415, Loss: 3.9387, Time: 4410.4s, Step: 7416, GPU: 4.9GB\n",
      "Epoch 1, Batch 7420, Loss: 4.6445, Time: 4413.0s, Step: 7421, GPU: 4.9GB\n",
      "Epoch 1, Batch 7425, Loss: 5.4272, Time: 4416.4s, Step: 7426, GPU: 4.9GB\n",
      "Epoch 1, Batch 7430, Loss: 4.7590, Time: 4418.9s, Step: 7431, GPU: 4.9GB\n",
      "Epoch 1, Batch 7435, Loss: 4.4400, Time: 4422.3s, Step: 7436, GPU: 4.9GB\n",
      "Epoch 1, Batch 7440, Loss: 4.5810, Time: 4424.8s, Step: 7441, GPU: 4.9GB\n",
      "Epoch 1, Batch 7445, Loss: 5.1971, Time: 4428.2s, Step: 7446, GPU: 4.9GB\n",
      "Epoch 1, Batch 7450, Loss: 4.2630, Time: 4430.7s, Step: 7451, GPU: 4.9GB\n",
      "Epoch 1, Batch 7455, Loss: 4.1866, Time: 4434.1s, Step: 7456, GPU: 4.9GB\n",
      "Epoch 1, Batch 7460, Loss: 4.8426, Time: 4436.6s, Step: 7461, GPU: 4.9GB\n",
      "Epoch 1, Batch 7465, Loss: 3.5342, Time: 4440.0s, Step: 7466, GPU: 4.9GB\n",
      "Epoch 1, Batch 7470, Loss: 4.8610, Time: 4442.6s, Step: 7471, GPU: 4.9GB\n",
      "Epoch 1, Batch 7475, Loss: 5.1739, Time: 4446.0s, Step: 7476, GPU: 4.9GB\n",
      "Epoch 1, Batch 7480, Loss: 5.0304, Time: 4448.5s, Step: 7481, GPU: 4.9GB\n",
      "Epoch 1, Batch 7485, Loss: 5.0413, Time: 4451.9s, Step: 7486, GPU: 4.9GB\n",
      "Epoch 1, Batch 7490, Loss: 3.9341, Time: 4454.5s, Step: 7491, GPU: 4.9GB\n",
      "Epoch 1, Batch 7495, Loss: 4.7486, Time: 4457.9s, Step: 7496, GPU: 4.9GB\n",
      "Epoch 1, Batch 7500, Loss: 4.4650, Time: 4460.4s, Step: 7501, GPU: 4.9GB\n",
      "Epoch 1, Batch 7505, Loss: 4.1302, Time: 4463.8s, Step: 7506, GPU: 4.9GB\n",
      "Epoch 1, Batch 7510, Loss: 3.8495, Time: 4466.3s, Step: 7511, GPU: 4.9GB\n",
      "Epoch 1, Batch 7515, Loss: 5.1799, Time: 4469.7s, Step: 7516, GPU: 4.9GB\n",
      "Epoch 1, Batch 7520, Loss: 4.9215, Time: 4472.2s, Step: 7521, GPU: 4.9GB\n",
      "Epoch 1, Batch 7525, Loss: 4.1521, Time: 4475.7s, Step: 7526, GPU: 4.9GB\n",
      "Epoch 1, Batch 7530, Loss: 4.4439, Time: 4478.2s, Step: 7531, GPU: 4.9GB\n",
      "Epoch 1, Batch 7535, Loss: 4.2108, Time: 4481.6s, Step: 7536, GPU: 4.9GB\n",
      "Epoch 1, Batch 7540, Loss: 4.2005, Time: 4484.1s, Step: 7541, GPU: 4.9GB\n",
      "Epoch 1, Batch 7545, Loss: 3.9425, Time: 4487.6s, Step: 7546, GPU: 4.9GB\n",
      "Epoch 1, Batch 7550, Loss: 4.4436, Time: 4490.1s, Step: 7551, GPU: 4.9GB\n",
      "Epoch 1, Batch 7555, Loss: 4.9739, Time: 4493.5s, Step: 7556, GPU: 4.9GB\n",
      "Epoch 1, Batch 7560, Loss: 4.5115, Time: 4496.1s, Step: 7561, GPU: 4.9GB\n",
      "Epoch 1, Batch 7565, Loss: 4.4964, Time: 4499.5s, Step: 7566, GPU: 4.9GB\n",
      "Epoch 1, Batch 7570, Loss: 4.3128, Time: 4502.1s, Step: 7571, GPU: 4.9GB\n",
      "Epoch 1, Batch 7575, Loss: 4.8144, Time: 4505.5s, Step: 7576, GPU: 4.9GB\n",
      "Epoch 1, Batch 7580, Loss: 4.4200, Time: 4508.0s, Step: 7581, GPU: 4.9GB\n",
      "Epoch 1, Batch 7585, Loss: 4.2791, Time: 4511.4s, Step: 7586, GPU: 4.9GB\n",
      "Epoch 1, Batch 7590, Loss: 4.2503, Time: 4514.0s, Step: 7591, GPU: 4.9GB\n",
      "Epoch 1, Batch 7595, Loss: 3.8239, Time: 4517.4s, Step: 7596, GPU: 4.9GB\n",
      "Epoch 1, Batch 7600, Loss: 4.8917, Time: 4520.0s, Step: 7601, GPU: 4.9GB\n",
      "Epoch 1, Batch 7605, Loss: 3.7654, Time: 4523.4s, Step: 7606, GPU: 4.9GB\n",
      "Epoch 1, Batch 7610, Loss: 4.5641, Time: 4525.9s, Step: 7611, GPU: 4.9GB\n",
      "Epoch 1, Batch 7615, Loss: 4.2903, Time: 4529.3s, Step: 7616, GPU: 4.9GB\n",
      "Epoch 1, Batch 7620, Loss: 3.8283, Time: 4531.8s, Step: 7621, GPU: 4.9GB\n",
      "Epoch 1, Batch 7625, Loss: 4.6927, Time: 4535.3s, Step: 7626, GPU: 4.9GB\n",
      "Epoch 1, Batch 7630, Loss: 4.5756, Time: 4537.8s, Step: 7631, GPU: 4.9GB\n",
      "Epoch 1, Batch 7635, Loss: 5.0507, Time: 4541.2s, Step: 7636, GPU: 4.9GB\n",
      "Epoch 1, Batch 7640, Loss: 4.5173, Time: 4543.7s, Step: 7641, GPU: 4.9GB\n",
      "Epoch 1, Batch 7645, Loss: 4.9134, Time: 4547.1s, Step: 7646, GPU: 4.9GB\n",
      "Epoch 1, Batch 7650, Loss: 5.2020, Time: 4549.6s, Step: 7651, GPU: 4.9GB\n",
      "Epoch 1, Batch 7655, Loss: 4.9058, Time: 4553.1s, Step: 7656, GPU: 4.9GB\n",
      "Epoch 1, Batch 7660, Loss: 4.8357, Time: 4555.6s, Step: 7661, GPU: 4.9GB\n",
      "Epoch 1, Batch 7665, Loss: 4.0269, Time: 4559.0s, Step: 7666, GPU: 4.9GB\n",
      "Epoch 1, Batch 7670, Loss: 4.0333, Time: 4561.5s, Step: 7671, GPU: 4.9GB\n",
      "Epoch 1, Batch 7675, Loss: 4.4861, Time: 4564.9s, Step: 7676, GPU: 4.9GB\n",
      "Epoch 1, Batch 7680, Loss: 4.8711, Time: 4567.4s, Step: 7681, GPU: 4.9GB\n",
      "Epoch 1, Batch 7685, Loss: 4.7057, Time: 4570.8s, Step: 7686, GPU: 4.9GB\n",
      "Epoch 1, Batch 7690, Loss: 4.5334, Time: 4573.3s, Step: 7691, GPU: 4.9GB\n",
      "Epoch 1, Batch 7695, Loss: 5.6827, Time: 4576.7s, Step: 7696, GPU: 4.9GB\n",
      "Epoch 1, Batch 7700, Loss: 4.0380, Time: 4579.2s, Step: 7701, GPU: 4.9GB\n",
      "Epoch 1, Batch 7705, Loss: 4.2098, Time: 4582.6s, Step: 7706, GPU: 4.9GB\n",
      "Epoch 1, Batch 7710, Loss: 5.1181, Time: 4585.2s, Step: 7711, GPU: 4.9GB\n",
      "Epoch 1, Batch 7715, Loss: 4.6291, Time: 4588.6s, Step: 7716, GPU: 4.9GB\n",
      "Epoch 1, Batch 7720, Loss: 4.3863, Time: 4591.1s, Step: 7721, GPU: 4.9GB\n",
      "Epoch 1, Batch 7725, Loss: 4.4983, Time: 4594.5s, Step: 7726, GPU: 4.9GB\n",
      "Epoch 1, Batch 7730, Loss: 3.6235, Time: 4597.1s, Step: 7731, GPU: 4.9GB\n",
      "Epoch 1, Batch 7735, Loss: 5.2878, Time: 4600.5s, Step: 7736, GPU: 4.9GB\n",
      "Epoch 1, Batch 7740, Loss: 3.9803, Time: 4603.0s, Step: 7741, GPU: 4.9GB\n",
      "Epoch 1, Batch 7745, Loss: 4.3112, Time: 4606.5s, Step: 7746, GPU: 4.9GB\n",
      "Epoch 1, Batch 7750, Loss: 3.8420, Time: 4609.0s, Step: 7751, GPU: 4.9GB\n",
      "Epoch 1, Batch 7755, Loss: 3.7739, Time: 4612.4s, Step: 7756, GPU: 4.9GB\n",
      "Epoch 1, Batch 7760, Loss: 4.4060, Time: 4614.9s, Step: 7761, GPU: 4.9GB\n",
      "Epoch 1, Batch 7765, Loss: 5.1852, Time: 4618.3s, Step: 7766, GPU: 4.9GB\n",
      "Epoch 1, Batch 7770, Loss: 4.8720, Time: 4620.9s, Step: 7771, GPU: 4.9GB\n",
      "Epoch 1, Batch 7775, Loss: 5.3699, Time: 4624.3s, Step: 7776, GPU: 4.9GB\n",
      "Epoch 1, Batch 7780, Loss: 4.3245, Time: 4626.8s, Step: 7781, GPU: 4.9GB\n",
      "Epoch 1, Batch 7785, Loss: 4.5530, Time: 4630.2s, Step: 7786, GPU: 4.9GB\n",
      "Epoch 1, Batch 7790, Loss: 4.1874, Time: 4632.8s, Step: 7791, GPU: 4.9GB\n",
      "Epoch 1, Batch 7795, Loss: 4.1715, Time: 4636.2s, Step: 7796, GPU: 4.9GB\n",
      "Epoch 1, Batch 7800, Loss: 4.4908, Time: 4638.8s, Step: 7801, GPU: 4.9GB\n",
      "Epoch 1, Batch 7805, Loss: 4.8304, Time: 4642.2s, Step: 7806, GPU: 4.9GB\n",
      "Epoch 1, Batch 7810, Loss: 5.1974, Time: 4644.7s, Step: 7811, GPU: 4.9GB\n",
      "Epoch 1, Batch 7815, Loss: 4.3744, Time: 4648.2s, Step: 7816, GPU: 4.9GB\n",
      "Epoch 1, Batch 7820, Loss: 4.7865, Time: 4650.7s, Step: 7821, GPU: 4.9GB\n",
      "Epoch 1, Batch 7825, Loss: 4.8759, Time: 4654.1s, Step: 7826, GPU: 4.9GB\n",
      "Epoch 1, Batch 7830, Loss: 4.3091, Time: 4656.7s, Step: 7831, GPU: 4.9GB\n",
      "Epoch 1, Batch 7835, Loss: 4.0835, Time: 4660.1s, Step: 7836, GPU: 4.9GB\n",
      "Epoch 1, Batch 7840, Loss: 4.3175, Time: 4662.6s, Step: 7841, GPU: 4.9GB\n",
      "Epoch 1, Batch 7845, Loss: 5.0316, Time: 4666.0s, Step: 7846, GPU: 4.9GB\n",
      "Epoch 1, Batch 7850, Loss: 4.4331, Time: 4668.5s, Step: 7851, GPU: 4.9GB\n",
      "Epoch 1, Batch 7855, Loss: 4.8745, Time: 4672.0s, Step: 7856, GPU: 4.9GB\n",
      "Epoch 1, Batch 7860, Loss: 4.8237, Time: 4674.5s, Step: 7861, GPU: 4.9GB\n",
      "Epoch 1, Batch 7865, Loss: 4.6384, Time: 4677.9s, Step: 7866, GPU: 4.9GB\n",
      "Epoch 1, Batch 7870, Loss: 4.6641, Time: 4680.4s, Step: 7871, GPU: 4.9GB\n",
      "Epoch 1, Batch 7875, Loss: 4.5485, Time: 4683.8s, Step: 7876, GPU: 4.9GB\n",
      "Epoch 1, Batch 7880, Loss: 3.5875, Time: 4686.4s, Step: 7881, GPU: 4.9GB\n",
      "Epoch 1, Batch 7885, Loss: 4.3072, Time: 4689.8s, Step: 7886, GPU: 4.9GB\n",
      "Epoch 1, Batch 7890, Loss: 4.5933, Time: 4692.3s, Step: 7891, GPU: 4.9GB\n",
      "Epoch 1, Batch 7895, Loss: 4.4016, Time: 4695.7s, Step: 7896, GPU: 4.9GB\n",
      "Epoch 1, Batch 7900, Loss: 4.6910, Time: 4698.2s, Step: 7901, GPU: 4.9GB\n",
      "Epoch 1, Batch 7905, Loss: 5.2075, Time: 4701.7s, Step: 7906, GPU: 4.9GB\n",
      "Epoch 1, Batch 7910, Loss: 4.6890, Time: 4704.2s, Step: 7911, GPU: 4.9GB\n",
      "Epoch 1, Batch 7915, Loss: 5.6551, Time: 4707.6s, Step: 7916, GPU: 4.9GB\n",
      "Epoch 1, Batch 7920, Loss: 4.2041, Time: 4710.1s, Step: 7921, GPU: 4.9GB\n",
      "Epoch 1, Batch 7925, Loss: 4.9212, Time: 4713.5s, Step: 7926, GPU: 4.9GB\n",
      "Epoch 1, Batch 7930, Loss: 4.4288, Time: 4716.0s, Step: 7931, GPU: 4.9GB\n",
      "Epoch 1, Batch 7935, Loss: 4.9803, Time: 4719.5s, Step: 7936, GPU: 4.9GB\n",
      "Epoch 1, Batch 7940, Loss: 4.5762, Time: 4722.0s, Step: 7941, GPU: 4.9GB\n",
      "Epoch 1, Batch 7945, Loss: 3.8509, Time: 4725.4s, Step: 7946, GPU: 4.9GB\n",
      "Epoch 1, Batch 7950, Loss: 4.1554, Time: 4727.9s, Step: 7951, GPU: 4.9GB\n",
      "Epoch 1, Batch 7955, Loss: 4.3527, Time: 4731.3s, Step: 7956, GPU: 4.9GB\n",
      "Epoch 1, Batch 7960, Loss: 4.1887, Time: 4733.9s, Step: 7961, GPU: 4.9GB\n",
      "Epoch 1, Batch 7965, Loss: 4.3620, Time: 4737.4s, Step: 7966, GPU: 4.9GB\n",
      "Epoch 1, Batch 7970, Loss: 4.7901, Time: 4739.9s, Step: 7971, GPU: 4.9GB\n",
      "Epoch 1, Batch 7975, Loss: 4.2542, Time: 4743.3s, Step: 7976, GPU: 4.9GB\n",
      "Epoch 1, Batch 7980, Loss: 4.1990, Time: 4745.8s, Step: 7981, GPU: 4.9GB\n",
      "Epoch 1, Batch 7985, Loss: 4.9433, Time: 4749.2s, Step: 7986, GPU: 4.9GB\n",
      "Epoch 1, Batch 7990, Loss: 4.7171, Time: 4751.7s, Step: 7991, GPU: 4.9GB\n",
      "Epoch 1, Batch 7995, Loss: 4.8700, Time: 4755.1s, Step: 7996, GPU: 4.9GB\n",
      "Epoch 1, Batch 8000, Loss: 4.2486, Time: 4757.7s, Step: 8001, GPU: 4.9GB\n",
      "Epoch 1, Batch 8005, Loss: 4.3397, Time: 4761.1s, Step: 8006, GPU: 4.9GB\n",
      "Epoch 1, Batch 8010, Loss: 4.3414, Time: 4763.6s, Step: 8011, GPU: 4.9GB\n",
      "Epoch 1, Batch 8015, Loss: 4.2602, Time: 4767.0s, Step: 8016, GPU: 4.9GB\n",
      "Epoch 1, Batch 8020, Loss: 4.4889, Time: 4769.5s, Step: 8021, GPU: 4.9GB\n",
      "Epoch 1, Batch 8025, Loss: 4.2549, Time: 4772.9s, Step: 8026, GPU: 4.9GB\n",
      "Epoch 1, Batch 8030, Loss: 5.1366, Time: 4775.4s, Step: 8031, GPU: 4.9GB\n",
      "Epoch 1, Batch 8035, Loss: 5.3476, Time: 4778.8s, Step: 8036, GPU: 4.9GB\n",
      "Epoch 1, Batch 8040, Loss: 3.9763, Time: 4781.3s, Step: 8041, GPU: 4.9GB\n",
      "Epoch 1, Batch 8045, Loss: 3.3573, Time: 4784.7s, Step: 8046, GPU: 4.9GB\n",
      "Epoch 1, Batch 8050, Loss: 4.7299, Time: 4787.2s, Step: 8051, GPU: 4.9GB\n",
      "Epoch 1, Batch 8055, Loss: 4.1953, Time: 4790.7s, Step: 8056, GPU: 4.9GB\n",
      "Epoch 1, Batch 8060, Loss: 4.9793, Time: 4793.2s, Step: 8061, GPU: 4.9GB\n",
      "Epoch 1, Batch 8065, Loss: 4.9029, Time: 4796.7s, Step: 8066, GPU: 4.9GB\n",
      "Epoch 1, Batch 8070, Loss: 4.4550, Time: 4799.2s, Step: 8071, GPU: 4.9GB\n",
      "Epoch 1, Batch 8075, Loss: 4.4823, Time: 4802.6s, Step: 8076, GPU: 4.9GB\n",
      "Epoch 1, Batch 8080, Loss: 5.7041, Time: 4805.2s, Step: 8081, GPU: 4.9GB\n",
      "Epoch 1, Batch 8085, Loss: 5.2602, Time: 4808.6s, Step: 8086, GPU: 4.9GB\n",
      "Epoch 1, Batch 8090, Loss: 4.3008, Time: 4811.1s, Step: 8091, GPU: 4.9GB\n",
      "Epoch 1, Batch 8095, Loss: 4.7923, Time: 4814.5s, Step: 8096, GPU: 4.9GB\n",
      "Epoch 1, Batch 8100, Loss: 3.9624, Time: 4817.1s, Step: 8101, GPU: 4.9GB\n",
      "Epoch 1, Batch 8105, Loss: 4.6137, Time: 4820.5s, Step: 8106, GPU: 4.9GB\n",
      "Epoch 1, Batch 8110, Loss: 4.6916, Time: 4823.0s, Step: 8111, GPU: 4.9GB\n",
      "Epoch 1, Batch 8115, Loss: 4.3862, Time: 4826.5s, Step: 8116, GPU: 4.9GB\n",
      "Epoch 1, Batch 8120, Loss: 3.8065, Time: 4829.0s, Step: 8121, GPU: 4.9GB\n",
      "Epoch 1, Batch 8125, Loss: 4.8305, Time: 4832.4s, Step: 8126, GPU: 4.9GB\n",
      "Epoch 1, Batch 8130, Loss: 5.7724, Time: 4834.9s, Step: 8131, GPU: 4.9GB\n",
      "Epoch 1, Batch 8135, Loss: 4.3003, Time: 4838.3s, Step: 8136, GPU: 4.9GB\n",
      "Epoch 1, Batch 8140, Loss: 4.7924, Time: 4840.8s, Step: 8141, GPU: 4.9GB\n",
      "Epoch 1, Batch 8145, Loss: 4.7076, Time: 4844.2s, Step: 8146, GPU: 4.9GB\n",
      "Epoch 1, Batch 8150, Loss: 3.7722, Time: 4846.8s, Step: 8151, GPU: 4.9GB\n",
      "Epoch 1, Batch 8155, Loss: 4.0952, Time: 4850.2s, Step: 8156, GPU: 4.9GB\n",
      "Epoch 1, Batch 8160, Loss: 5.1383, Time: 4852.7s, Step: 8161, GPU: 4.9GB\n",
      "Epoch 1, Batch 8165, Loss: 4.3549, Time: 4856.2s, Step: 8166, GPU: 4.9GB\n",
      "Epoch 1, Batch 8170, Loss: 4.7271, Time: 4858.7s, Step: 8171, GPU: 4.9GB\n",
      "Epoch 1, Batch 8175, Loss: 3.9879, Time: 4862.1s, Step: 8176, GPU: 4.9GB\n",
      "Epoch 1, Batch 8180, Loss: 5.0489, Time: 4864.6s, Step: 8181, GPU: 4.9GB\n",
      "Epoch 1, Batch 8185, Loss: 4.2713, Time: 4868.1s, Step: 8186, GPU: 4.9GB\n",
      "Epoch 1, Batch 8190, Loss: 4.5331, Time: 4870.6s, Step: 8191, GPU: 4.9GB\n",
      "Epoch 1, Batch 8195, Loss: 4.2347, Time: 4874.0s, Step: 8196, GPU: 4.9GB\n",
      "Epoch 1, Batch 8200, Loss: 4.0810, Time: 4876.6s, Step: 8201, GPU: 4.9GB\n",
      "Epoch 1, Batch 8205, Loss: 4.4198, Time: 4880.1s, Step: 8206, GPU: 4.9GB\n",
      "Epoch 1, Batch 8210, Loss: 4.6531, Time: 4882.6s, Step: 8211, GPU: 4.9GB\n",
      "Epoch 1, Batch 8215, Loss: 4.6178, Time: 4886.0s, Step: 8216, GPU: 4.9GB\n",
      "Epoch 1, Batch 8220, Loss: 4.0994, Time: 4888.5s, Step: 8221, GPU: 4.9GB\n",
      "Epoch 1, Batch 8225, Loss: 4.8807, Time: 4892.0s, Step: 8226, GPU: 4.9GB\n",
      "Epoch 1, Batch 8230, Loss: 5.5291, Time: 4894.5s, Step: 8231, GPU: 4.9GB\n",
      "Epoch 1, Batch 8235, Loss: 3.9841, Time: 4897.9s, Step: 8236, GPU: 4.9GB\n",
      "Epoch 1, Batch 8240, Loss: 3.5828, Time: 4900.4s, Step: 8241, GPU: 4.9GB\n",
      "Epoch 1, Batch 8245, Loss: 4.3804, Time: 4903.8s, Step: 8246, GPU: 4.9GB\n",
      "Epoch 1, Batch 8250, Loss: 3.8949, Time: 4906.3s, Step: 8251, GPU: 4.9GB\n",
      "Epoch 1, Batch 8255, Loss: 4.0821, Time: 4909.8s, Step: 8256, GPU: 4.9GB\n",
      "Epoch 1, Batch 8260, Loss: 5.6752, Time: 4912.3s, Step: 8261, GPU: 4.9GB\n",
      "Epoch 1, Batch 8265, Loss: 4.7786, Time: 4915.7s, Step: 8266, GPU: 4.9GB\n",
      "Epoch 1, Batch 8270, Loss: 4.5631, Time: 4918.2s, Step: 8271, GPU: 4.9GB\n",
      "Epoch 1, Batch 8275, Loss: 4.0846, Time: 4921.7s, Step: 8276, GPU: 4.9GB\n",
      "Epoch 1, Batch 8280, Loss: 3.8672, Time: 4924.2s, Step: 8281, GPU: 4.9GB\n",
      "Epoch 1, Batch 8285, Loss: 5.1584, Time: 4927.6s, Step: 8286, GPU: 4.9GB\n",
      "Epoch 1, Batch 8290, Loss: 3.7028, Time: 4930.1s, Step: 8291, GPU: 4.9GB\n",
      "Epoch 1, Batch 8295, Loss: 3.4203, Time: 4933.5s, Step: 8296, GPU: 4.9GB\n",
      "Epoch 1, Batch 8300, Loss: 4.6644, Time: 4936.0s, Step: 8301, GPU: 4.9GB\n",
      "Epoch 1, Batch 8305, Loss: 4.2402, Time: 4939.4s, Step: 8306, GPU: 4.9GB\n",
      "Epoch 1, Batch 8310, Loss: 4.0996, Time: 4941.9s, Step: 8311, GPU: 4.9GB\n",
      "Epoch 1, Batch 8315, Loss: 6.0726, Time: 4945.3s, Step: 8316, GPU: 4.9GB\n",
      "Epoch 1, Batch 8320, Loss: 3.0923, Time: 4947.8s, Step: 8321, GPU: 4.9GB\n",
      "Epoch 1, Batch 8325, Loss: 3.4786, Time: 4951.2s, Step: 8326, GPU: 4.9GB\n",
      "Epoch 1, Batch 8330, Loss: 4.0044, Time: 4953.7s, Step: 8331, GPU: 4.9GB\n",
      "Epoch 1, Batch 8335, Loss: 5.1388, Time: 4957.1s, Step: 8336, GPU: 4.9GB\n",
      "Epoch 1, Batch 8340, Loss: 4.1745, Time: 4959.6s, Step: 8341, GPU: 4.9GB\n",
      "Epoch 1, Batch 8345, Loss: 4.3658, Time: 4963.1s, Step: 8346, GPU: 4.9GB\n",
      "Epoch 1, Batch 8350, Loss: 4.3714, Time: 4965.6s, Step: 8351, GPU: 4.9GB\n",
      "Epoch 1, Batch 8355, Loss: 4.5572, Time: 4969.0s, Step: 8356, GPU: 4.9GB\n",
      "Epoch 1, Batch 8360, Loss: 4.3274, Time: 4971.5s, Step: 8361, GPU: 4.9GB\n",
      "Epoch 1, Batch 8365, Loss: 4.3268, Time: 4974.9s, Step: 8366, GPU: 4.9GB\n",
      "Epoch 1, Batch 8370, Loss: 4.2877, Time: 4977.4s, Step: 8371, GPU: 4.9GB\n",
      "Epoch 1, Batch 8375, Loss: 5.5875, Time: 4980.9s, Step: 8376, GPU: 4.9GB\n",
      "Epoch 1, Batch 8380, Loss: 5.3299, Time: 4983.4s, Step: 8381, GPU: 4.9GB\n",
      "Epoch 1, Batch 8385, Loss: 4.9721, Time: 4986.8s, Step: 8386, GPU: 4.9GB\n",
      "Epoch 1, Batch 8390, Loss: 4.3845, Time: 4989.3s, Step: 8391, GPU: 4.9GB\n",
      "Epoch 1, Batch 8395, Loss: 5.4692, Time: 4992.7s, Step: 8396, GPU: 4.9GB\n",
      "Epoch 1, Batch 8400, Loss: 4.3340, Time: 4995.4s, Step: 8401, GPU: 4.9GB\n",
      "Epoch 1, Batch 8405, Loss: 4.5681, Time: 4998.8s, Step: 8406, GPU: 4.9GB\n",
      "Epoch 1, Batch 8410, Loss: 4.8122, Time: 5001.3s, Step: 8411, GPU: 4.9GB\n",
      "Epoch 1, Batch 8415, Loss: 4.1922, Time: 5004.8s, Step: 8416, GPU: 4.9GB\n",
      "Epoch 1, Batch 8420, Loss: 4.7153, Time: 5007.3s, Step: 8421, GPU: 4.9GB\n",
      "Epoch 1, Batch 8425, Loss: 4.0706, Time: 5010.7s, Step: 8426, GPU: 4.9GB\n",
      "Epoch 1, Batch 8430, Loss: 4.5208, Time: 5013.2s, Step: 8431, GPU: 4.9GB\n",
      "Epoch 1, Batch 8435, Loss: 4.7988, Time: 5016.6s, Step: 8436, GPU: 4.9GB\n",
      "Epoch 1, Batch 8440, Loss: 4.6152, Time: 5019.1s, Step: 8441, GPU: 4.9GB\n",
      "Epoch 1, Batch 8445, Loss: 4.7258, Time: 5022.5s, Step: 8446, GPU: 4.9GB\n",
      "Epoch 1, Batch 8450, Loss: 4.9126, Time: 5025.0s, Step: 8451, GPU: 4.9GB\n",
      "Epoch 1, Batch 8455, Loss: 4.6509, Time: 5028.5s, Step: 8456, GPU: 4.9GB\n",
      "Epoch 1, Batch 8460, Loss: 4.0303, Time: 5031.0s, Step: 8461, GPU: 4.9GB\n",
      "Epoch 1, Batch 8465, Loss: 4.4458, Time: 5034.4s, Step: 8466, GPU: 4.9GB\n",
      "Epoch 1, Batch 8470, Loss: 3.8229, Time: 5036.9s, Step: 8471, GPU: 4.9GB\n",
      "Epoch 1, Batch 8475, Loss: 4.1205, Time: 5040.4s, Step: 8476, GPU: 4.9GB\n",
      "Epoch 1, Batch 8480, Loss: 3.7957, Time: 5042.9s, Step: 8481, GPU: 4.9GB\n",
      "Epoch 1, Batch 8485, Loss: 5.1255, Time: 5046.3s, Step: 8486, GPU: 4.9GB\n",
      "Epoch 1, Batch 8490, Loss: 5.1491, Time: 5048.8s, Step: 8491, GPU: 4.9GB\n",
      "Epoch 1, Batch 8495, Loss: 4.7276, Time: 5052.2s, Step: 8496, GPU: 4.9GB\n",
      "Epoch 1, Batch 8500, Loss: 5.3081, Time: 5054.7s, Step: 8501, GPU: 4.9GB\n",
      "Epoch 1, Batch 8505, Loss: 4.5450, Time: 5058.1s, Step: 8506, GPU: 4.9GB\n",
      "Epoch 1, Batch 8510, Loss: 4.5813, Time: 5060.6s, Step: 8511, GPU: 4.9GB\n",
      "Epoch 1, Batch 8515, Loss: 5.1134, Time: 5064.0s, Step: 8516, GPU: 4.9GB\n",
      "Epoch 1, Batch 8520, Loss: 4.1384, Time: 5066.6s, Step: 8521, GPU: 4.9GB\n",
      "Epoch 1, Batch 8525, Loss: 4.4922, Time: 5070.0s, Step: 8526, GPU: 4.9GB\n",
      "Epoch 1, Batch 8530, Loss: 5.1943, Time: 5072.5s, Step: 8531, GPU: 4.9GB\n",
      "Epoch 1, Batch 8535, Loss: 4.6930, Time: 5075.9s, Step: 8536, GPU: 4.9GB\n",
      "Epoch 1, Batch 8540, Loss: 4.7502, Time: 5078.4s, Step: 8541, GPU: 4.9GB\n",
      "Epoch 1, Batch 8545, Loss: 4.5056, Time: 5081.8s, Step: 8546, GPU: 4.9GB\n",
      "Epoch 1, Batch 8550, Loss: 5.4558, Time: 5084.3s, Step: 8551, GPU: 4.9GB\n",
      "Epoch 1, Batch 8555, Loss: 4.3459, Time: 5087.7s, Step: 8556, GPU: 4.9GB\n",
      "Epoch 1, Batch 8560, Loss: 4.0518, Time: 5090.3s, Step: 8561, GPU: 4.9GB\n",
      "Epoch 1, Batch 8565, Loss: 5.0654, Time: 5093.7s, Step: 8566, GPU: 4.9GB\n",
      "Epoch 1, Batch 8570, Loss: 4.0179, Time: 5096.2s, Step: 8571, GPU: 4.9GB\n",
      "Epoch 1, Batch 8575, Loss: 4.6420, Time: 5099.6s, Step: 8576, GPU: 4.9GB\n",
      "Epoch 1, Batch 8580, Loss: 4.5508, Time: 5102.1s, Step: 8581, GPU: 4.9GB\n",
      "Epoch 1, Batch 8585, Loss: 4.4856, Time: 5105.5s, Step: 8586, GPU: 4.9GB\n",
      "Epoch 1, Batch 8590, Loss: 4.1391, Time: 5108.0s, Step: 8591, GPU: 4.9GB\n",
      "Epoch 1, Batch 8595, Loss: 4.4792, Time: 5111.4s, Step: 8596, GPU: 4.9GB\n",
      "Epoch 1, Batch 8600, Loss: 4.5326, Time: 5114.0s, Step: 8601, GPU: 4.9GB\n",
      "Epoch 1, Batch 8605, Loss: 3.8683, Time: 5117.4s, Step: 8606, GPU: 4.9GB\n",
      "Epoch 1, Batch 8610, Loss: 4.0498, Time: 5120.0s, Step: 8611, GPU: 4.9GB\n",
      "Epoch 1, Batch 8615, Loss: 4.2081, Time: 5123.4s, Step: 8616, GPU: 4.9GB\n",
      "Epoch 1, Batch 8620, Loss: 4.5773, Time: 5125.9s, Step: 8621, GPU: 4.9GB\n",
      "Epoch 1, Batch 8625, Loss: 4.9752, Time: 5129.3s, Step: 8626, GPU: 4.9GB\n",
      "Epoch 1, Batch 8630, Loss: 3.5627, Time: 5131.8s, Step: 8631, GPU: 4.9GB\n",
      "Epoch 1, Batch 8635, Loss: 5.7880, Time: 5135.3s, Step: 8636, GPU: 4.9GB\n",
      "Epoch 1, Batch 8640, Loss: 4.0022, Time: 5137.8s, Step: 8641, GPU: 4.9GB\n",
      "Epoch 1, Batch 8645, Loss: 4.0683, Time: 5141.2s, Step: 8646, GPU: 4.9GB\n",
      "Epoch 1, Batch 8650, Loss: 4.0041, Time: 5143.7s, Step: 8651, GPU: 4.9GB\n",
      "Epoch 1, Batch 8655, Loss: 4.1763, Time: 5147.1s, Step: 8656, GPU: 4.9GB\n",
      "Epoch 1, Batch 8660, Loss: 4.3339, Time: 5149.7s, Step: 8661, GPU: 4.9GB\n",
      "Epoch 1, Batch 8665, Loss: 4.4147, Time: 5153.1s, Step: 8666, GPU: 4.9GB\n",
      "Epoch 1, Batch 8670, Loss: 5.4776, Time: 5155.6s, Step: 8671, GPU: 4.9GB\n",
      "Epoch 1, Batch 8675, Loss: 3.9501, Time: 5159.0s, Step: 8676, GPU: 4.9GB\n",
      "Epoch 1, Batch 8680, Loss: 4.8336, Time: 5161.5s, Step: 8681, GPU: 4.9GB\n",
      "Epoch 1, Batch 8685, Loss: 4.8933, Time: 5165.0s, Step: 8686, GPU: 4.9GB\n",
      "Epoch 1, Batch 8690, Loss: 5.0716, Time: 5167.5s, Step: 8691, GPU: 4.9GB\n",
      "Epoch 1, Batch 8695, Loss: 4.6492, Time: 5170.9s, Step: 8696, GPU: 4.9GB\n",
      "Epoch 1, Batch 8700, Loss: 4.6791, Time: 5173.4s, Step: 8701, GPU: 4.9GB\n",
      "Epoch 1, Batch 8705, Loss: 3.6489, Time: 5176.8s, Step: 8706, GPU: 4.9GB\n",
      "Epoch 1, Batch 8710, Loss: 4.1082, Time: 5179.4s, Step: 8711, GPU: 4.9GB\n",
      "Epoch 1, Batch 8715, Loss: 3.8051, Time: 5182.8s, Step: 8716, GPU: 4.9GB\n",
      "Epoch 1, Batch 8720, Loss: 5.0830, Time: 5185.3s, Step: 8721, GPU: 4.9GB\n",
      "Epoch 1, Batch 8725, Loss: 5.0965, Time: 5188.7s, Step: 8726, GPU: 4.9GB\n",
      "Epoch 1, Batch 8730, Loss: 4.8897, Time: 5191.2s, Step: 8731, GPU: 4.9GB\n",
      "Epoch 1, Batch 8735, Loss: 4.9268, Time: 5194.6s, Step: 8736, GPU: 4.9GB\n",
      "Epoch 1, Batch 8740, Loss: 4.6756, Time: 5197.2s, Step: 8741, GPU: 4.9GB\n",
      "Epoch 1, Batch 8745, Loss: 3.8408, Time: 5200.6s, Step: 8746, GPU: 4.9GB\n",
      "Epoch 1, Batch 8750, Loss: 4.0114, Time: 5203.1s, Step: 8751, GPU: 4.9GB\n",
      "Epoch 1, Batch 8755, Loss: 4.8496, Time: 5206.5s, Step: 8756, GPU: 4.9GB\n",
      "Epoch 1, Batch 8760, Loss: 4.6813, Time: 5209.0s, Step: 8761, GPU: 4.9GB\n",
      "Epoch 1, Batch 8765, Loss: 4.3926, Time: 5212.5s, Step: 8766, GPU: 4.9GB\n",
      "Epoch 1, Batch 8770, Loss: 4.2364, Time: 5215.0s, Step: 8771, GPU: 4.9GB\n",
      "Epoch 1, Batch 8775, Loss: 3.8002, Time: 5218.4s, Step: 8776, GPU: 4.9GB\n",
      "Epoch 1, Batch 8780, Loss: 3.9990, Time: 5220.9s, Step: 8781, GPU: 4.9GB\n",
      "Epoch 1, Batch 8785, Loss: 3.9712, Time: 5224.3s, Step: 8786, GPU: 4.9GB\n",
      "Epoch 1, Batch 8790, Loss: 4.5576, Time: 5226.8s, Step: 8791, GPU: 4.9GB\n",
      "Epoch 1, Batch 8795, Loss: 4.7596, Time: 5230.2s, Step: 8796, GPU: 4.9GB\n",
      "Epoch 1, Batch 8800, Loss: 4.4887, Time: 5232.8s, Step: 8801, GPU: 4.9GB\n",
      "Epoch 1, Batch 8805, Loss: 4.2208, Time: 5236.2s, Step: 8806, GPU: 4.9GB\n",
      "Epoch 1, Batch 8810, Loss: 4.1118, Time: 5238.8s, Step: 8811, GPU: 4.9GB\n",
      "Epoch 1, Batch 8815, Loss: 5.0099, Time: 5242.2s, Step: 8816, GPU: 4.9GB\n",
      "Epoch 1, Batch 8820, Loss: 4.3468, Time: 5244.7s, Step: 8821, GPU: 4.9GB\n",
      "Epoch 1, Batch 8825, Loss: 4.2639, Time: 5248.1s, Step: 8826, GPU: 4.9GB\n",
      "Epoch 1, Batch 8830, Loss: 3.7091, Time: 5250.6s, Step: 8831, GPU: 4.9GB\n",
      "Epoch 1, Batch 8835, Loss: 4.5050, Time: 5254.0s, Step: 8836, GPU: 4.9GB\n",
      "Epoch 1, Batch 8840, Loss: 4.8813, Time: 5256.5s, Step: 8841, GPU: 4.9GB\n",
      "Epoch 1, Batch 8845, Loss: 4.5394, Time: 5260.0s, Step: 8846, GPU: 4.9GB\n",
      "Epoch 1, Batch 8850, Loss: 5.2999, Time: 5262.5s, Step: 8851, GPU: 4.9GB\n",
      "Epoch 1, Batch 8855, Loss: 5.0923, Time: 5265.9s, Step: 8856, GPU: 4.9GB\n",
      "Epoch 1, Batch 8860, Loss: 3.8813, Time: 5268.4s, Step: 8861, GPU: 4.9GB\n",
      "Epoch 1, Batch 8865, Loss: 4.9810, Time: 5271.8s, Step: 8866, GPU: 4.9GB\n",
      "Epoch 1, Batch 8870, Loss: 3.7445, Time: 5274.3s, Step: 8871, GPU: 4.9GB\n",
      "Epoch 1, Batch 8875, Loss: 4.1965, Time: 5277.7s, Step: 8876, GPU: 4.9GB\n",
      "Epoch 1, Batch 8880, Loss: 4.3359, Time: 5280.2s, Step: 8881, GPU: 4.9GB\n",
      "Epoch 1, Batch 8885, Loss: 4.9717, Time: 5283.7s, Step: 8886, GPU: 4.9GB\n",
      "Epoch 1, Batch 8890, Loss: 5.1053, Time: 5286.2s, Step: 8891, GPU: 4.9GB\n",
      "Epoch 1, Batch 8895, Loss: 4.3912, Time: 5289.6s, Step: 8896, GPU: 4.9GB\n",
      "Epoch 1, Batch 8900, Loss: 4.7259, Time: 5292.1s, Step: 8901, GPU: 4.9GB\n",
      "Epoch 1, Batch 8905, Loss: 4.3140, Time: 5295.5s, Step: 8906, GPU: 4.9GB\n",
      "Epoch 1, Batch 8910, Loss: 3.4664, Time: 5298.0s, Step: 8911, GPU: 4.9GB\n",
      "Epoch 1, Batch 8915, Loss: 4.2239, Time: 5301.5s, Step: 8916, GPU: 4.9GB\n",
      "Epoch 1, Batch 8920, Loss: 4.4989, Time: 5304.0s, Step: 8921, GPU: 4.9GB\n",
      "Epoch 1, Batch 8925, Loss: 4.4186, Time: 5307.4s, Step: 8926, GPU: 4.9GB\n",
      "Epoch 1, Batch 8930, Loss: 3.7458, Time: 5309.9s, Step: 8931, GPU: 4.9GB\n",
      "Epoch 1, Batch 8935, Loss: 4.7836, Time: 5313.4s, Step: 8936, GPU: 4.9GB\n",
      "Epoch 1, Batch 8940, Loss: 4.2745, Time: 5315.9s, Step: 8941, GPU: 4.9GB\n",
      "Epoch 1, Batch 8945, Loss: 3.8966, Time: 5319.3s, Step: 8946, GPU: 4.9GB\n",
      "Epoch 1, Batch 8950, Loss: 4.3024, Time: 5321.8s, Step: 8951, GPU: 4.9GB\n",
      "Epoch 1, Batch 8955, Loss: 4.6843, Time: 5325.2s, Step: 8956, GPU: 4.9GB\n",
      "Epoch 1, Batch 8960, Loss: 4.5425, Time: 5327.7s, Step: 8961, GPU: 4.9GB\n",
      "Epoch 1, Batch 8965, Loss: 3.8751, Time: 5331.2s, Step: 8966, GPU: 4.9GB\n",
      "Epoch 1, Batch 8970, Loss: 4.1079, Time: 5333.7s, Step: 8971, GPU: 4.9GB\n",
      "Epoch 1, Batch 8975, Loss: 4.9266, Time: 5337.1s, Step: 8976, GPU: 4.9GB\n",
      "Epoch 1, Batch 8980, Loss: 3.8800, Time: 5339.6s, Step: 8981, GPU: 4.9GB\n",
      "Epoch 1, Batch 8985, Loss: 4.1824, Time: 5343.0s, Step: 8986, GPU: 4.9GB\n",
      "Epoch 1, Batch 8990, Loss: 4.5793, Time: 5345.5s, Step: 8991, GPU: 4.9GB\n",
      "Epoch 1, Batch 8995, Loss: 4.3017, Time: 5348.9s, Step: 8996, GPU: 4.9GB\n",
      "Epoch 1, Batch 9000, Loss: 4.3253, Time: 5351.5s, Step: 9001, GPU: 4.9GB\n",
      "Epoch 1, Batch 9005, Loss: 4.5615, Time: 5354.9s, Step: 9006, GPU: 4.9GB\n",
      "Epoch 1, Batch 9010, Loss: 4.7911, Time: 5357.4s, Step: 9011, GPU: 4.9GB\n",
      "Epoch 1, Batch 9015, Loss: 4.2519, Time: 5360.8s, Step: 9016, GPU: 4.9GB\n",
      "Epoch 1, Batch 9020, Loss: 4.0874, Time: 5363.4s, Step: 9021, GPU: 4.9GB\n",
      "Epoch 1, Batch 9025, Loss: 3.4995, Time: 5366.8s, Step: 9026, GPU: 4.9GB\n",
      "Epoch 1, Batch 9030, Loss: 4.5557, Time: 5369.3s, Step: 9031, GPU: 4.9GB\n",
      "Epoch 1, Batch 9035, Loss: 4.5538, Time: 5372.7s, Step: 9036, GPU: 4.9GB\n",
      "Epoch 1, Batch 9040, Loss: 4.5216, Time: 5375.2s, Step: 9041, GPU: 4.9GB\n",
      "Epoch 1, Batch 9045, Loss: 4.4375, Time: 5378.6s, Step: 9046, GPU: 4.9GB\n",
      "Epoch 1, Batch 9050, Loss: 4.6342, Time: 5381.1s, Step: 9051, GPU: 4.9GB\n",
      "Epoch 1, Batch 9055, Loss: 4.1559, Time: 5384.6s, Step: 9056, GPU: 4.9GB\n",
      "Epoch 1, Batch 9060, Loss: 5.4682, Time: 5387.1s, Step: 9061, GPU: 4.9GB\n",
      "Epoch 1, Batch 9065, Loss: 5.2362, Time: 5390.6s, Step: 9066, GPU: 4.9GB\n",
      "Epoch 1, Batch 9070, Loss: 3.8687, Time: 5393.1s, Step: 9071, GPU: 4.9GB\n",
      "Epoch 1, Batch 9075, Loss: 5.3251, Time: 5396.5s, Step: 9076, GPU: 4.9GB\n",
      "Epoch 1, Batch 9080, Loss: 4.2363, Time: 5399.0s, Step: 9081, GPU: 4.9GB\n",
      "Epoch 1, Batch 9085, Loss: 4.7356, Time: 5402.4s, Step: 9086, GPU: 4.9GB\n",
      "Epoch 1, Batch 9090, Loss: 4.3490, Time: 5405.0s, Step: 9091, GPU: 4.9GB\n",
      "Epoch 1, Batch 9095, Loss: 4.6551, Time: 5408.4s, Step: 9096, GPU: 4.9GB\n",
      "Epoch 1, Batch 9100, Loss: 4.0503, Time: 5410.9s, Step: 9101, GPU: 4.9GB\n",
      "Epoch 1, Batch 9105, Loss: 4.7062, Time: 5414.3s, Step: 9106, GPU: 4.9GB\n",
      "Epoch 1, Batch 9110, Loss: 4.4380, Time: 5416.8s, Step: 9111, GPU: 4.9GB\n",
      "Epoch 1, Batch 9115, Loss: 4.9318, Time: 5420.2s, Step: 9116, GPU: 4.9GB\n",
      "Epoch 1, Batch 9120, Loss: 4.2241, Time: 5422.8s, Step: 9121, GPU: 4.9GB\n",
      "Epoch 1, Batch 9125, Loss: 3.9509, Time: 5426.2s, Step: 9126, GPU: 4.9GB\n",
      "Epoch 1, Batch 9130, Loss: 3.5532, Time: 5428.7s, Step: 9131, GPU: 4.9GB\n",
      "Epoch 1, Batch 9135, Loss: 4.2466, Time: 5432.1s, Step: 9136, GPU: 4.9GB\n",
      "Epoch 1, Batch 9140, Loss: 3.2629, Time: 5434.6s, Step: 9141, GPU: 4.9GB\n",
      "Epoch 1, Batch 9145, Loss: 4.3508, Time: 5438.0s, Step: 9146, GPU: 4.9GB\n",
      "Epoch 1, Batch 9150, Loss: 5.6838, Time: 5440.5s, Step: 9151, GPU: 4.9GB\n",
      "Epoch 1, Batch 9155, Loss: 4.9658, Time: 5443.9s, Step: 9156, GPU: 4.9GB\n",
      "Epoch 1, Batch 9160, Loss: 4.0764, Time: 5446.4s, Step: 9161, GPU: 4.9GB\n",
      "Epoch 1, Batch 9165, Loss: 3.7353, Time: 5449.8s, Step: 9166, GPU: 4.9GB\n",
      "Epoch 1, Batch 9170, Loss: 4.4476, Time: 5452.4s, Step: 9171, GPU: 4.9GB\n",
      "Epoch 1, Batch 9175, Loss: 4.4107, Time: 5455.8s, Step: 9176, GPU: 4.9GB\n",
      "Epoch 1, Batch 9180, Loss: 5.0554, Time: 5458.3s, Step: 9181, GPU: 4.9GB\n",
      "Epoch 1, Batch 9185, Loss: 4.4337, Time: 5461.7s, Step: 9186, GPU: 4.9GB\n",
      "Epoch 1, Batch 9190, Loss: 3.3257, Time: 5464.3s, Step: 9191, GPU: 4.9GB\n",
      "Epoch 1, Batch 9195, Loss: 4.1335, Time: 5467.6s, Step: 9196, GPU: 4.9GB\n",
      "Epoch 1, Batch 9200, Loss: 3.8407, Time: 5470.3s, Step: 9201, GPU: 4.9GB\n",
      "Epoch 1, Batch 9205, Loss: 4.9089, Time: 5473.7s, Step: 9206, GPU: 4.9GB\n",
      "Epoch 1, Batch 9210, Loss: 4.8248, Time: 5476.2s, Step: 9211, GPU: 4.9GB\n",
      "Epoch 1, Batch 9215, Loss: 4.3766, Time: 5479.6s, Step: 9216, GPU: 4.9GB\n",
      "Epoch 1, Batch 9220, Loss: 4.8015, Time: 5482.1s, Step: 9221, GPU: 4.9GB\n",
      "Epoch 1, Batch 9225, Loss: 4.9591, Time: 5485.6s, Step: 9226, GPU: 4.9GB\n",
      "Epoch 1, Batch 9230, Loss: 4.8083, Time: 5488.1s, Step: 9231, GPU: 4.9GB\n",
      "Epoch 1, Batch 9235, Loss: 4.3762, Time: 5491.5s, Step: 9236, GPU: 4.9GB\n",
      "Epoch 1, Batch 9240, Loss: 4.4756, Time: 5494.0s, Step: 9241, GPU: 4.9GB\n",
      "Epoch 1, Batch 9245, Loss: 4.0756, Time: 5497.4s, Step: 9246, GPU: 4.9GB\n",
      "Epoch 1, Batch 9250, Loss: 4.3696, Time: 5500.0s, Step: 9251, GPU: 4.9GB\n",
      "Epoch 1, Batch 9255, Loss: 4.6613, Time: 5503.4s, Step: 9256, GPU: 4.9GB\n",
      "Epoch 1, Batch 9260, Loss: 4.4369, Time: 5505.9s, Step: 9261, GPU: 4.9GB\n",
      "Epoch 1, Batch 9265, Loss: 4.9431, Time: 5509.3s, Step: 9266, GPU: 4.9GB\n",
      "Epoch 1, Batch 9270, Loss: 5.2010, Time: 5511.9s, Step: 9271, GPU: 4.9GB\n",
      "Epoch 1, Batch 9275, Loss: 4.3289, Time: 5515.3s, Step: 9276, GPU: 4.9GB\n",
      "Epoch 1, Batch 9280, Loss: 5.7357, Time: 5517.8s, Step: 9281, GPU: 4.9GB\n",
      "Epoch 1, Batch 9285, Loss: 4.0296, Time: 5521.2s, Step: 9286, GPU: 4.9GB\n",
      "Epoch 1, Batch 9290, Loss: 5.0567, Time: 5523.7s, Step: 9291, GPU: 4.9GB\n",
      "Epoch 1, Batch 9295, Loss: 4.3111, Time: 5527.1s, Step: 9296, GPU: 4.9GB\n",
      "Epoch 1, Batch 9300, Loss: 3.8100, Time: 5529.7s, Step: 9301, GPU: 4.9GB\n",
      "Epoch 1, Batch 9305, Loss: 4.0093, Time: 5533.1s, Step: 9306, GPU: 4.9GB\n",
      "Epoch 1, Batch 9310, Loss: 4.5775, Time: 5535.6s, Step: 9311, GPU: 4.9GB\n",
      "Epoch 1, Batch 9315, Loss: 4.5219, Time: 5539.0s, Step: 9316, GPU: 4.9GB\n",
      "Epoch 1, Batch 9320, Loss: 5.3833, Time: 5541.6s, Step: 9321, GPU: 4.9GB\n",
      "Epoch 1, Batch 9325, Loss: 4.5390, Time: 5545.0s, Step: 9326, GPU: 4.9GB\n",
      "Epoch 1, Batch 9330, Loss: 4.9553, Time: 5547.5s, Step: 9331, GPU: 4.9GB\n",
      "Epoch 1, Batch 9335, Loss: 4.2759, Time: 5550.9s, Step: 9336, GPU: 4.9GB\n",
      "Epoch 1, Batch 9340, Loss: 3.7067, Time: 5553.4s, Step: 9341, GPU: 4.9GB\n",
      "Epoch 1, Batch 9345, Loss: 4.2908, Time: 5556.9s, Step: 9346, GPU: 4.9GB\n",
      "Epoch 1, Batch 9350, Loss: 4.5154, Time: 5559.4s, Step: 9351, GPU: 4.9GB\n",
      "Epoch 1, Batch 9355, Loss: 4.1175, Time: 5562.8s, Step: 9356, GPU: 4.9GB\n",
      "Epoch 1, Batch 9360, Loss: 3.8353, Time: 5565.3s, Step: 9361, GPU: 4.9GB\n",
      "Epoch 1, Batch 9365, Loss: 4.7263, Time: 5568.8s, Step: 9366, GPU: 4.9GB\n",
      "Epoch 1, Batch 9370, Loss: 4.3752, Time: 5571.3s, Step: 9371, GPU: 4.9GB\n",
      "Epoch 1, Batch 9375, Loss: 4.2915, Time: 5574.7s, Step: 9376, GPU: 4.9GB\n",
      "Epoch 1, Batch 9380, Loss: 4.4238, Time: 5577.3s, Step: 9381, GPU: 4.9GB\n",
      "Epoch 1, Batch 9385, Loss: 4.1858, Time: 5580.7s, Step: 9386, GPU: 4.9GB\n",
      "Epoch 1, Batch 9390, Loss: 4.0013, Time: 5583.2s, Step: 9391, GPU: 4.9GB\n",
      "Epoch 1, Batch 9395, Loss: 4.5433, Time: 5586.6s, Step: 9396, GPU: 4.9GB\n",
      "Epoch 1, Batch 9400, Loss: 4.4589, Time: 5589.2s, Step: 9401, GPU: 4.9GB\n",
      "Epoch 1, Batch 9405, Loss: 5.0518, Time: 5592.6s, Step: 9406, GPU: 4.9GB\n",
      "Epoch 1, Batch 9410, Loss: 4.2539, Time: 5595.1s, Step: 9411, GPU: 4.9GB\n",
      "Epoch 1, Batch 9415, Loss: 4.7022, Time: 5598.6s, Step: 9416, GPU: 4.9GB\n",
      "Epoch 1, Batch 9420, Loss: 4.9063, Time: 5601.1s, Step: 9421, GPU: 4.9GB\n",
      "Epoch 1, Batch 9425, Loss: 3.4542, Time: 5604.5s, Step: 9426, GPU: 4.9GB\n",
      "Epoch 1, Batch 9430, Loss: 4.9622, Time: 5607.0s, Step: 9431, GPU: 4.9GB\n",
      "Epoch 1, Batch 9435, Loss: 4.8875, Time: 5610.5s, Step: 9436, GPU: 4.9GB\n",
      "Epoch 1, Batch 9440, Loss: 5.2907, Time: 5613.0s, Step: 9441, GPU: 4.9GB\n",
      "Epoch 1, Batch 9445, Loss: 4.2679, Time: 5616.4s, Step: 9446, GPU: 4.9GB\n",
      "Epoch 1, Batch 9450, Loss: 3.9725, Time: 5618.9s, Step: 9451, GPU: 4.9GB\n",
      "Epoch 1, Batch 9455, Loss: 4.6292, Time: 5622.4s, Step: 9456, GPU: 4.9GB\n",
      "Epoch 1, Batch 9460, Loss: 4.2226, Time: 5624.9s, Step: 9461, GPU: 4.9GB\n",
      "Epoch 1, Batch 9465, Loss: 3.6592, Time: 5628.3s, Step: 9466, GPU: 4.9GB\n",
      "Epoch 1, Batch 9470, Loss: 4.1817, Time: 5630.8s, Step: 9471, GPU: 4.9GB\n",
      "Epoch 1, Batch 9475, Loss: 4.6285, Time: 5634.2s, Step: 9476, GPU: 4.9GB\n",
      "Epoch 1, Batch 9480, Loss: 4.7781, Time: 5636.8s, Step: 9481, GPU: 4.9GB\n",
      "Epoch 1, Batch 9485, Loss: 3.8321, Time: 5640.2s, Step: 9486, GPU: 4.9GB\n",
      "Epoch 1, Batch 9490, Loss: 4.0967, Time: 5642.7s, Step: 9491, GPU: 4.9GB\n",
      "Epoch 1, Batch 9495, Loss: 4.6396, Time: 5646.3s, Step: 9496, GPU: 4.9GB\n",
      "Epoch 1, Batch 9500, Loss: 4.4772, Time: 5648.8s, Step: 9501, GPU: 4.9GB\n",
      "Epoch 1, Batch 9505, Loss: 4.2511, Time: 5652.2s, Step: 9506, GPU: 4.9GB\n",
      "Epoch 1, Batch 9510, Loss: 4.1673, Time: 5654.8s, Step: 9511, GPU: 4.9GB\n",
      "Epoch 1, Batch 9515, Loss: 4.5694, Time: 5658.2s, Step: 9516, GPU: 4.9GB\n",
      "Epoch 1, Batch 9520, Loss: 4.8207, Time: 5660.7s, Step: 9521, GPU: 4.9GB\n",
      "Epoch 1, Batch 9525, Loss: 4.9057, Time: 5664.1s, Step: 9526, GPU: 4.9GB\n",
      "Epoch 1, Batch 9530, Loss: 4.2168, Time: 5666.7s, Step: 9531, GPU: 4.9GB\n",
      "Epoch 1, Batch 9535, Loss: 3.9774, Time: 5670.1s, Step: 9536, GPU: 4.9GB\n",
      "Epoch 1, Batch 9540, Loss: 4.4007, Time: 5672.6s, Step: 9541, GPU: 4.9GB\n",
      "Epoch 1, Batch 9545, Loss: 4.9426, Time: 5676.0s, Step: 9546, GPU: 4.9GB\n",
      "Epoch 1, Batch 9550, Loss: 3.3524, Time: 5678.5s, Step: 9551, GPU: 4.9GB\n",
      "Epoch 1, Batch 9555, Loss: 3.5211, Time: 5682.0s, Step: 9556, GPU: 4.9GB\n",
      "Epoch 1, Batch 9560, Loss: 3.7505, Time: 5684.5s, Step: 9561, GPU: 4.9GB\n",
      "Epoch 1, Batch 9565, Loss: 4.2484, Time: 5687.9s, Step: 9566, GPU: 4.9GB\n",
      "Epoch 1, Batch 9570, Loss: 4.6023, Time: 5690.4s, Step: 9571, GPU: 4.9GB\n",
      "Epoch 1, Batch 9575, Loss: 5.3393, Time: 5693.8s, Step: 9576, GPU: 4.9GB\n",
      "Epoch 1, Batch 9580, Loss: 3.6536, Time: 5696.3s, Step: 9581, GPU: 4.9GB\n",
      "Epoch 1, Batch 9585, Loss: 4.1502, Time: 5699.8s, Step: 9586, GPU: 4.9GB\n",
      "Epoch 1, Batch 9590, Loss: 3.7978, Time: 5702.3s, Step: 9591, GPU: 4.9GB\n",
      "Epoch 1, Batch 9595, Loss: 3.5328, Time: 5705.8s, Step: 9596, GPU: 4.9GB\n",
      "Epoch 1, Batch 9600, Loss: 4.0622, Time: 5708.4s, Step: 9601, GPU: 4.9GB\n",
      "Epoch 1, Batch 9605, Loss: 4.6006, Time: 5711.8s, Step: 9606, GPU: 4.9GB\n",
      "Epoch 1, Batch 9610, Loss: 4.6461, Time: 5714.3s, Step: 9611, GPU: 4.9GB\n",
      "Epoch 1, Batch 9615, Loss: 4.7832, Time: 5717.7s, Step: 9616, GPU: 4.9GB\n",
      "Epoch 1, Batch 9620, Loss: 4.6025, Time: 5720.2s, Step: 9621, GPU: 4.9GB\n",
      "Epoch 1, Batch 9625, Loss: 4.4759, Time: 5723.6s, Step: 9626, GPU: 4.9GB\n",
      "Epoch 1, Batch 9630, Loss: 3.9722, Time: 5726.2s, Step: 9631, GPU: 4.9GB\n",
      "Epoch 1, Batch 9635, Loss: 4.3558, Time: 5729.6s, Step: 9636, GPU: 4.9GB\n",
      "Epoch 1, Batch 9640, Loss: 4.0443, Time: 5732.1s, Step: 9641, GPU: 4.9GB\n",
      "Epoch 1, Batch 9645, Loss: 3.5979, Time: 5735.5s, Step: 9646, GPU: 4.9GB\n",
      "Epoch 1, Batch 9650, Loss: 4.5934, Time: 5738.0s, Step: 9651, GPU: 4.9GB\n",
      "Epoch 1, Batch 9655, Loss: 3.8636, Time: 5741.5s, Step: 9656, GPU: 4.9GB\n",
      "Epoch 1, Batch 9660, Loss: 4.6757, Time: 5744.0s, Step: 9661, GPU: 4.9GB\n",
      "Epoch 1, Batch 9665, Loss: 3.8581, Time: 5747.4s, Step: 9666, GPU: 4.9GB\n",
      "Epoch 1, Batch 9670, Loss: 4.1600, Time: 5749.9s, Step: 9671, GPU: 4.9GB\n",
      "Epoch 1, Batch 9675, Loss: 4.1085, Time: 5753.3s, Step: 9676, GPU: 4.9GB\n",
      "Epoch 1, Batch 9680, Loss: 4.3252, Time: 5755.8s, Step: 9681, GPU: 4.9GB\n",
      "Epoch 1, Batch 9685, Loss: 4.2892, Time: 5759.2s, Step: 9686, GPU: 4.9GB\n",
      "Epoch 1, Batch 9690, Loss: 4.2752, Time: 5761.7s, Step: 9691, GPU: 4.9GB\n",
      "Epoch 1, Batch 9695, Loss: 3.8404, Time: 5765.1s, Step: 9696, GPU: 4.9GB\n",
      "Epoch 1, Batch 9700, Loss: 4.4291, Time: 5767.6s, Step: 9701, GPU: 4.9GB\n",
      "Epoch 1, Batch 9705, Loss: 3.5174, Time: 5771.0s, Step: 9706, GPU: 4.9GB\n",
      "Epoch 1, Batch 9710, Loss: 4.7497, Time: 5773.5s, Step: 9711, GPU: 4.9GB\n",
      "Epoch 1, Batch 9715, Loss: 5.5071, Time: 5777.5s, Step: 9716, GPU: 4.9GB\n",
      "Epoch 1, Batch 9720, Loss: 4.6307, Time: 5780.1s, Step: 9721, GPU: 4.9GB\n",
      "Epoch 1, Batch 9725, Loss: 4.0726, Time: 5783.5s, Step: 9726, GPU: 4.9GB\n",
      "Epoch 1, Batch 9730, Loss: 4.3201, Time: 5786.0s, Step: 9731, GPU: 4.9GB\n",
      "Epoch 1, Batch 9735, Loss: 4.2418, Time: 5789.4s, Step: 9736, GPU: 4.9GB\n",
      "Epoch 1, Batch 9740, Loss: 4.7883, Time: 5791.9s, Step: 9741, GPU: 4.9GB\n",
      "Epoch 1, Batch 9745, Loss: 4.0575, Time: 5795.4s, Step: 9746, GPU: 4.9GB\n",
      "Epoch 1, Batch 9750, Loss: 3.7575, Time: 5797.9s, Step: 9751, GPU: 4.9GB\n",
      "Epoch 1, Batch 9755, Loss: 3.7741, Time: 5801.3s, Step: 9756, GPU: 4.9GB\n",
      "Epoch 1, Batch 9760, Loss: 4.7373, Time: 5803.9s, Step: 9761, GPU: 4.9GB\n",
      "Epoch 1, Batch 9765, Loss: 5.0993, Time: 5807.3s, Step: 9766, GPU: 4.9GB\n",
      "Epoch 1, Batch 9770, Loss: 5.0409, Time: 5809.8s, Step: 9771, GPU: 4.9GB\n",
      "Epoch 1, Batch 9775, Loss: 4.2935, Time: 5813.2s, Step: 9776, GPU: 4.9GB\n",
      "Epoch 1, Batch 9780, Loss: 4.8914, Time: 5815.7s, Step: 9781, GPU: 4.9GB\n",
      "Epoch 1, Batch 9785, Loss: 4.0770, Time: 5819.2s, Step: 9786, GPU: 4.9GB\n",
      "Epoch 1, Batch 9790, Loss: 4.3262, Time: 5821.7s, Step: 9791, GPU: 4.9GB\n",
      "Epoch 1, Batch 9795, Loss: 4.1947, Time: 5825.2s, Step: 9796, GPU: 4.9GB\n",
      "Epoch 1, Batch 9800, Loss: 3.5166, Time: 5827.8s, Step: 9801, GPU: 4.9GB\n",
      "Epoch 1, Batch 9805, Loss: 5.8410, Time: 5831.2s, Step: 9806, GPU: 4.9GB\n",
      "Epoch 1, Batch 9810, Loss: 4.5496, Time: 5833.7s, Step: 9811, GPU: 4.9GB\n",
      "Epoch 1, Batch 9815, Loss: 3.8196, Time: 5837.2s, Step: 9816, GPU: 4.9GB\n",
      "Epoch 1, Batch 9820, Loss: 4.8867, Time: 5839.7s, Step: 9821, GPU: 4.9GB\n",
      "Epoch 1, Batch 9825, Loss: 4.6126, Time: 5843.1s, Step: 9826, GPU: 4.9GB\n",
      "Epoch 1, Batch 9830, Loss: 4.6737, Time: 5845.6s, Step: 9831, GPU: 4.9GB\n",
      "Epoch 1, Batch 9835, Loss: 3.9291, Time: 5849.0s, Step: 9836, GPU: 4.9GB\n",
      "Epoch 1, Batch 9840, Loss: 4.9949, Time: 5851.5s, Step: 9841, GPU: 4.9GB\n",
      "Epoch 1, Batch 9845, Loss: 4.2170, Time: 5855.0s, Step: 9846, GPU: 4.9GB\n",
      "Epoch 1, Batch 9850, Loss: 3.5945, Time: 5857.5s, Step: 9851, GPU: 4.9GB\n",
      "Epoch 1, Batch 9855, Loss: 5.4024, Time: 5860.9s, Step: 9856, GPU: 4.9GB\n",
      "Epoch 1, Batch 9860, Loss: 4.2247, Time: 5863.4s, Step: 9861, GPU: 4.9GB\n",
      "Epoch 1, Batch 9865, Loss: 4.5018, Time: 5866.8s, Step: 9866, GPU: 4.9GB\n",
      "Epoch 1, Batch 9870, Loss: 3.8490, Time: 5869.4s, Step: 9871, GPU: 4.9GB\n",
      "Epoch 1, Batch 9875, Loss: 4.1543, Time: 5872.8s, Step: 9876, GPU: 4.9GB\n",
      "Epoch 1, Batch 9880, Loss: 4.7127, Time: 5875.3s, Step: 9881, GPU: 4.9GB\n",
      "Epoch 1, Batch 9885, Loss: 4.7551, Time: 5878.7s, Step: 9886, GPU: 4.9GB\n",
      "Epoch 1, Batch 9890, Loss: 4.4217, Time: 5881.2s, Step: 9891, GPU: 4.9GB\n",
      "Epoch 1, Batch 9895, Loss: 4.7932, Time: 5884.7s, Step: 9896, GPU: 4.9GB\n",
      "Epoch 1, Batch 9900, Loss: 4.4268, Time: 5887.2s, Step: 9901, GPU: 4.9GB\n",
      "Epoch 1, Batch 9905, Loss: 4.3374, Time: 5890.6s, Step: 9906, GPU: 4.9GB\n",
      "Epoch 1, Batch 9910, Loss: 4.0094, Time: 5893.1s, Step: 9911, GPU: 4.9GB\n",
      "Epoch 1, Batch 9915, Loss: 4.0394, Time: 5896.6s, Step: 9916, GPU: 4.9GB\n",
      "Epoch 1, Batch 9920, Loss: 4.1310, Time: 5899.1s, Step: 9921, GPU: 4.9GB\n",
      "Epoch 1, Batch 9925, Loss: 4.8959, Time: 5902.5s, Step: 9926, GPU: 4.9GB\n",
      "Epoch 1, Batch 9930, Loss: 4.3919, Time: 5905.0s, Step: 9931, GPU: 4.9GB\n",
      "Epoch 1, Batch 9935, Loss: 4.9997, Time: 5908.5s, Step: 9936, GPU: 4.9GB\n",
      "Epoch 1, Batch 9940, Loss: 4.3296, Time: 5911.0s, Step: 9941, GPU: 4.9GB\n",
      "Epoch 1, Batch 9945, Loss: 3.8737, Time: 5914.4s, Step: 9946, GPU: 4.9GB\n",
      "Epoch 1, Batch 9950, Loss: 4.5059, Time: 5917.0s, Step: 9951, GPU: 4.9GB\n",
      "Epoch 1, Batch 9955, Loss: 3.6114, Time: 5920.4s, Step: 9956, GPU: 4.9GB\n",
      "Epoch 1, Batch 9960, Loss: 4.7873, Time: 5922.9s, Step: 9961, GPU: 4.9GB\n",
      "Epoch 1, Batch 9965, Loss: 4.3692, Time: 5926.3s, Step: 9966, GPU: 4.9GB\n",
      "Epoch 1, Batch 9970, Loss: 4.3076, Time: 5928.8s, Step: 9971, GPU: 4.9GB\n",
      "Epoch 1, Batch 9975, Loss: 4.8157, Time: 5932.3s, Step: 9976, GPU: 4.9GB\n",
      "Epoch 1, Batch 9980, Loss: 4.6179, Time: 5934.8s, Step: 9981, GPU: 4.9GB\n",
      "Epoch 1, Batch 9985, Loss: 4.1693, Time: 5938.2s, Step: 9986, GPU: 4.9GB\n",
      "Epoch 1, Batch 9990, Loss: 4.1632, Time: 5940.7s, Step: 9991, GPU: 4.9GB\n",
      "Epoch 1, Batch 9995, Loss: 4.3634, Time: 5944.2s, Step: 9996, GPU: 4.9GB\n",
      "Epoch 1, Batch 10000, Loss: 4.5775, Time: 5946.8s, Step: 10001, GPU: 4.9GB\n",
      "Epoch 1, Batch 10005, Loss: 4.1909, Time: 5950.1s, Step: 10006, GPU: 4.9GB\n",
      "Epoch 1, Batch 10010, Loss: 4.8900, Time: 5952.7s, Step: 10011, GPU: 4.9GB\n",
      "Epoch 1, Batch 10015, Loss: 5.0164, Time: 5956.1s, Step: 10016, GPU: 4.9GB\n",
      "Epoch 1, Batch 10020, Loss: 4.1937, Time: 5958.5s, Step: 10021, GPU: 4.9GB\n",
      "Epoch 1, Batch 10025, Loss: 4.1438, Time: 5962.0s, Step: 10026, GPU: 4.9GB\n",
      "Epoch 1, Batch 10030, Loss: 5.8683, Time: 5964.5s, Step: 10031, GPU: 4.9GB\n",
      "Epoch 1, Batch 10035, Loss: 4.9001, Time: 5967.9s, Step: 10036, GPU: 4.9GB\n",
      "Epoch 1, Batch 10040, Loss: 4.2923, Time: 5970.5s, Step: 10041, GPU: 4.9GB\n",
      "Epoch 1, Batch 10045, Loss: 5.1320, Time: 5973.9s, Step: 10046, GPU: 4.9GB\n",
      "Epoch 1, Batch 10050, Loss: 4.3999, Time: 5976.5s, Step: 10051, GPU: 4.9GB\n",
      "Epoch 1, Batch 10055, Loss: 5.0337, Time: 5979.9s, Step: 10056, GPU: 4.9GB\n",
      "Epoch 1, Batch 10060, Loss: 4.4102, Time: 5982.4s, Step: 10061, GPU: 4.9GB\n",
      "Epoch 1, Batch 10065, Loss: 3.9269, Time: 5985.9s, Step: 10066, GPU: 4.9GB\n",
      "Epoch 1, Batch 10070, Loss: 4.2907, Time: 5988.4s, Step: 10071, GPU: 4.9GB\n",
      "Epoch 1, Batch 10075, Loss: 4.4944, Time: 5991.8s, Step: 10076, GPU: 4.9GB\n",
      "Epoch 1, Batch 10080, Loss: 4.2799, Time: 5994.3s, Step: 10081, GPU: 4.9GB\n",
      "Epoch 1, Batch 10085, Loss: 3.3916, Time: 5997.7s, Step: 10086, GPU: 4.9GB\n",
      "Epoch 1, Batch 10090, Loss: 3.6168, Time: 6000.2s, Step: 10091, GPU: 4.9GB\n",
      "Epoch 1, Batch 10095, Loss: 4.6202, Time: 6003.7s, Step: 10096, GPU: 4.9GB\n",
      "Epoch 1, Batch 10100, Loss: 3.9618, Time: 6006.2s, Step: 10101, GPU: 4.9GB\n",
      "Epoch 1, Batch 10105, Loss: 4.1652, Time: 6009.6s, Step: 10106, GPU: 4.9GB\n",
      "Epoch 1, Batch 10110, Loss: 3.7597, Time: 6012.1s, Step: 10111, GPU: 4.9GB\n",
      "Epoch 1, Batch 10115, Loss: 5.4821, Time: 6015.5s, Step: 10116, GPU: 4.9GB\n",
      "Epoch 1, Batch 10120, Loss: 4.4444, Time: 6018.0s, Step: 10121, GPU: 4.9GB\n",
      "Epoch 1, Batch 10125, Loss: 4.5189, Time: 6021.5s, Step: 10126, GPU: 4.9GB\n",
      "Epoch 1, Batch 10130, Loss: 4.2686, Time: 6024.0s, Step: 10131, GPU: 4.9GB\n",
      "Epoch 1, Batch 10135, Loss: 3.8659, Time: 6027.4s, Step: 10136, GPU: 4.9GB\n",
      "Epoch 1, Batch 10140, Loss: 4.4020, Time: 6029.9s, Step: 10141, GPU: 4.9GB\n",
      "Epoch 1, Batch 10145, Loss: 4.2949, Time: 6033.4s, Step: 10146, GPU: 4.9GB\n",
      "Epoch 1, Batch 10150, Loss: 3.4159, Time: 6035.9s, Step: 10151, GPU: 4.9GB\n",
      "Epoch 1, Batch 10155, Loss: 4.6258, Time: 6039.3s, Step: 10156, GPU: 4.9GB\n",
      "Epoch 1, Batch 10160, Loss: 4.1853, Time: 6041.8s, Step: 10161, GPU: 4.9GB\n",
      "Epoch 1, Batch 10165, Loss: 5.1287, Time: 6045.2s, Step: 10166, GPU: 4.9GB\n",
      "Epoch 1, Batch 10170, Loss: 3.8838, Time: 6047.8s, Step: 10171, GPU: 4.9GB\n",
      "Epoch 1, Batch 10175, Loss: 4.2391, Time: 6051.2s, Step: 10176, GPU: 4.9GB\n",
      "Epoch 1, Batch 10180, Loss: 3.9270, Time: 6053.7s, Step: 10181, GPU: 4.9GB\n",
      "Epoch 1, Batch 10185, Loss: 4.9064, Time: 6057.1s, Step: 10186, GPU: 4.9GB\n",
      "Epoch 1, Batch 10190, Loss: 4.3894, Time: 6059.6s, Step: 10191, GPU: 4.9GB\n",
      "Epoch 1, Batch 10195, Loss: 4.2711, Time: 6063.0s, Step: 10196, GPU: 4.9GB\n",
      "Epoch 1, Batch 10200, Loss: 4.1505, Time: 6065.6s, Step: 10201, GPU: 4.9GB\n",
      "Epoch 1, Batch 10205, Loss: 5.0836, Time: 6069.0s, Step: 10206, GPU: 4.9GB\n",
      "Epoch 1, Batch 10210, Loss: 4.8027, Time: 6071.6s, Step: 10211, GPU: 4.9GB\n",
      "Epoch 1, Batch 10215, Loss: 4.1133, Time: 6075.0s, Step: 10216, GPU: 4.9GB\n",
      "Epoch 1, Batch 10220, Loss: 5.0297, Time: 6077.5s, Step: 10221, GPU: 4.9GB\n",
      "Epoch 1, Batch 10225, Loss: 3.7914, Time: 6080.9s, Step: 10226, GPU: 4.9GB\n",
      "Epoch 1, Batch 10230, Loss: 5.2285, Time: 6083.5s, Step: 10231, GPU: 4.9GB\n",
      "Epoch 1, Batch 10235, Loss: 5.1778, Time: 6086.9s, Step: 10236, GPU: 4.9GB\n",
      "Epoch 1, Batch 10240, Loss: 4.3571, Time: 6089.4s, Step: 10241, GPU: 4.9GB\n",
      "Epoch 1, Batch 10245, Loss: 4.1907, Time: 6092.8s, Step: 10246, GPU: 4.9GB\n",
      "Epoch 1, Batch 10250, Loss: 4.7523, Time: 6095.3s, Step: 10251, GPU: 4.9GB\n",
      "Epoch 1, Batch 10255, Loss: 4.2343, Time: 6098.7s, Step: 10256, GPU: 4.9GB\n",
      "Epoch 1, Batch 10260, Loss: 5.3068, Time: 6101.2s, Step: 10261, GPU: 4.9GB\n",
      "Epoch 1, Batch 10265, Loss: 3.5987, Time: 6104.7s, Step: 10266, GPU: 4.9GB\n",
      "Epoch 1, Batch 10270, Loss: 5.1804, Time: 6107.2s, Step: 10271, GPU: 4.9GB\n",
      "Epoch 1, Batch 10275, Loss: 3.2117, Time: 6110.6s, Step: 10276, GPU: 4.9GB\n",
      "Epoch 1, Batch 10280, Loss: 5.2955, Time: 6113.1s, Step: 10281, GPU: 4.9GB\n",
      "Epoch 1, Batch 10285, Loss: 4.5161, Time: 6116.5s, Step: 10286, GPU: 4.9GB\n",
      "Epoch 1, Batch 10290, Loss: 3.9209, Time: 6119.0s, Step: 10291, GPU: 4.9GB\n",
      "Epoch 1, Batch 10295, Loss: 3.7677, Time: 6122.4s, Step: 10296, GPU: 4.9GB\n",
      "Epoch 1, Batch 10300, Loss: 4.9812, Time: 6124.9s, Step: 10301, GPU: 4.9GB\n",
      "Epoch 1, Batch 10305, Loss: 4.2626, Time: 6128.3s, Step: 10306, GPU: 4.9GB\n",
      "Epoch 1, Batch 10310, Loss: 4.3803, Time: 6130.9s, Step: 10311, GPU: 4.9GB\n",
      "Epoch 1, Batch 10315, Loss: 4.7254, Time: 6134.3s, Step: 10316, GPU: 4.9GB\n",
      "Epoch 1, Batch 10320, Loss: 3.7726, Time: 6136.8s, Step: 10321, GPU: 4.9GB\n",
      "Epoch 1, Batch 10325, Loss: 4.6475, Time: 6140.2s, Step: 10326, GPU: 4.9GB\n",
      "Epoch 1, Batch 10330, Loss: 4.6519, Time: 6142.7s, Step: 10331, GPU: 4.9GB\n",
      "Epoch 1, Batch 10335, Loss: 4.7088, Time: 6146.1s, Step: 10336, GPU: 4.9GB\n",
      "Epoch 1, Batch 10340, Loss: 4.2269, Time: 6148.7s, Step: 10341, GPU: 4.9GB\n",
      "Epoch 1, Batch 10345, Loss: 5.9175, Time: 6152.1s, Step: 10346, GPU: 4.9GB\n",
      "Epoch 1, Batch 10350, Loss: 4.4200, Time: 6154.6s, Step: 10351, GPU: 4.9GB\n",
      "Epoch 1, Batch 10355, Loss: 4.5960, Time: 6158.0s, Step: 10356, GPU: 4.9GB\n",
      "Epoch 1, Batch 10360, Loss: 4.0084, Time: 6160.5s, Step: 10361, GPU: 4.9GB\n",
      "Epoch 1, Batch 10365, Loss: 3.8648, Time: 6164.0s, Step: 10366, GPU: 4.9GB\n",
      "Epoch 1, Batch 10370, Loss: 3.7809, Time: 6166.5s, Step: 10371, GPU: 4.9GB\n",
      "Epoch 1, Batch 10375, Loss: 4.0844, Time: 6170.1s, Step: 10376, GPU: 4.9GB\n",
      "Epoch 1, Batch 10380, Loss: 3.7824, Time: 6172.6s, Step: 10381, GPU: 4.9GB\n",
      "Epoch 1, Batch 10385, Loss: 4.0041, Time: 6176.0s, Step: 10386, GPU: 4.9GB\n",
      "Epoch 1, Batch 10390, Loss: 4.8066, Time: 6178.6s, Step: 10391, GPU: 4.9GB\n",
      "Epoch 1, Batch 10395, Loss: 3.5049, Time: 6182.0s, Step: 10396, GPU: 4.9GB\n",
      "Epoch 1, Batch 10400, Loss: 3.8424, Time: 6184.6s, Step: 10401, GPU: 4.9GB\n",
      "Epoch 1, Batch 10405, Loss: 3.7785, Time: 6188.1s, Step: 10406, GPU: 4.9GB\n",
      "Epoch 1, Batch 10410, Loss: 4.1899, Time: 6190.6s, Step: 10411, GPU: 4.9GB\n",
      "Epoch 1, Batch 10415, Loss: 4.0454, Time: 6194.0s, Step: 10416, GPU: 4.9GB\n",
      "Epoch 1, Batch 10420, Loss: 4.9393, Time: 6196.5s, Step: 10421, GPU: 4.9GB\n",
      "Epoch 1, Batch 10425, Loss: 4.4808, Time: 6200.0s, Step: 10426, GPU: 4.9GB\n",
      "Epoch 1, Batch 10430, Loss: 4.3935, Time: 6202.5s, Step: 10431, GPU: 4.9GB\n",
      "Epoch 1, Batch 10435, Loss: 4.7124, Time: 6205.9s, Step: 10436, GPU: 4.9GB\n",
      "Epoch 1, Batch 10440, Loss: 3.8715, Time: 6208.5s, Step: 10441, GPU: 4.9GB\n",
      "Epoch 1, Batch 10445, Loss: 4.7378, Time: 6211.9s, Step: 10446, GPU: 4.9GB\n",
      "Epoch 1, Batch 10450, Loss: 4.8540, Time: 6214.4s, Step: 10451, GPU: 4.9GB\n",
      "Epoch 1, Batch 10455, Loss: 5.7177, Time: 6217.8s, Step: 10456, GPU: 4.9GB\n",
      "Epoch 1, Batch 10460, Loss: 4.5686, Time: 6220.3s, Step: 10461, GPU: 4.9GB\n",
      "Epoch 1, Batch 10465, Loss: 4.8598, Time: 6223.8s, Step: 10466, GPU: 4.9GB\n",
      "Epoch 1, Batch 10470, Loss: 3.8231, Time: 6226.3s, Step: 10471, GPU: 4.9GB\n",
      "Epoch 1, Batch 10475, Loss: 5.2915, Time: 6229.7s, Step: 10476, GPU: 4.9GB\n",
      "Epoch 1, Batch 10480, Loss: 4.5018, Time: 6232.2s, Step: 10481, GPU: 4.9GB\n",
      "Epoch 1, Batch 10485, Loss: 4.0254, Time: 6235.6s, Step: 10486, GPU: 4.9GB\n",
      "Epoch 1, Batch 10490, Loss: 5.8924, Time: 6238.2s, Step: 10491, GPU: 4.9GB\n",
      "Epoch 1, Batch 10495, Loss: 4.0251, Time: 6241.6s, Step: 10496, GPU: 4.9GB\n",
      "Epoch 1, Batch 10500, Loss: 4.2337, Time: 6244.1s, Step: 10501, GPU: 4.9GB\n",
      "Epoch 1, Batch 10505, Loss: 4.9024, Time: 6247.5s, Step: 10506, GPU: 4.9GB\n",
      "Epoch 1, Batch 10510, Loss: 4.8087, Time: 6250.0s, Step: 10511, GPU: 4.9GB\n",
      "Epoch 1, Batch 10515, Loss: 5.6410, Time: 6253.4s, Step: 10516, GPU: 4.9GB\n",
      "Epoch 1, Batch 10520, Loss: 4.1764, Time: 6256.0s, Step: 10521, GPU: 4.9GB\n",
      "Epoch 1, Batch 10525, Loss: 4.4061, Time: 6259.4s, Step: 10526, GPU: 4.9GB\n",
      "Epoch 1, Batch 10530, Loss: 4.5867, Time: 6261.9s, Step: 10531, GPU: 4.9GB\n",
      "Epoch 1, Batch 10535, Loss: 4.3093, Time: 6265.3s, Step: 10536, GPU: 4.9GB\n",
      "Epoch 1, Batch 10540, Loss: 4.7512, Time: 6267.9s, Step: 10541, GPU: 4.9GB\n",
      "Epoch 1, Batch 10545, Loss: 3.2374, Time: 6271.3s, Step: 10546, GPU: 4.9GB\n",
      "Epoch 1, Batch 10550, Loss: 4.8700, Time: 6273.8s, Step: 10551, GPU: 4.9GB\n",
      "Epoch 1, Batch 10555, Loss: 4.9553, Time: 6277.3s, Step: 10556, GPU: 4.9GB\n",
      "Epoch 1, Batch 10560, Loss: 4.5776, Time: 6279.8s, Step: 10561, GPU: 4.9GB\n",
      "Epoch 1, Batch 10565, Loss: 4.5477, Time: 6283.2s, Step: 10566, GPU: 4.9GB\n",
      "Epoch 1, Batch 10570, Loss: 4.9218, Time: 6285.7s, Step: 10571, GPU: 4.9GB\n",
      "Epoch 1, Batch 10575, Loss: 4.9871, Time: 6289.1s, Step: 10576, GPU: 4.9GB\n",
      "Epoch 1, Batch 10580, Loss: 3.9926, Time: 6291.6s, Step: 10581, GPU: 4.9GB\n",
      "Epoch 1, Batch 10585, Loss: 3.6096, Time: 6295.1s, Step: 10586, GPU: 4.9GB\n",
      "Epoch 1, Batch 10590, Loss: 4.0152, Time: 6297.6s, Step: 10591, GPU: 4.9GB\n",
      "Epoch 1, Batch 10595, Loss: 4.2463, Time: 6301.0s, Step: 10596, GPU: 4.9GB\n",
      "Epoch 1, Batch 10600, Loss: 3.5588, Time: 6303.6s, Step: 10601, GPU: 4.9GB\n",
      "Epoch 1, Batch 10605, Loss: 4.1218, Time: 6307.0s, Step: 10606, GPU: 4.9GB\n",
      "Epoch 1, Batch 10610, Loss: 3.5478, Time: 6309.6s, Step: 10611, GPU: 4.9GB\n",
      "Epoch 1, Batch 10615, Loss: 5.2036, Time: 6313.0s, Step: 10616, GPU: 4.9GB\n",
      "Epoch 1, Batch 10620, Loss: 4.5495, Time: 6315.5s, Step: 10621, GPU: 4.9GB\n",
      "Epoch 1, Batch 10625, Loss: 4.4583, Time: 6318.9s, Step: 10626, GPU: 4.9GB\n",
      "Epoch 1, Batch 10630, Loss: 4.3885, Time: 6321.4s, Step: 10631, GPU: 4.9GB\n",
      "Epoch 1, Batch 10635, Loss: 4.8178, Time: 6324.9s, Step: 10636, GPU: 4.9GB\n",
      "Epoch 1, Batch 10640, Loss: 4.0585, Time: 6327.4s, Step: 10641, GPU: 4.9GB\n",
      "Epoch 1, Batch 10645, Loss: 4.2097, Time: 6330.8s, Step: 10646, GPU: 4.9GB\n",
      "Epoch 1, Batch 10650, Loss: 5.0091, Time: 6333.3s, Step: 10651, GPU: 4.9GB\n",
      "Epoch 1, Batch 10655, Loss: 5.4092, Time: 6336.7s, Step: 10656, GPU: 4.9GB\n",
      "Epoch 1, Batch 10660, Loss: 3.3218, Time: 6339.2s, Step: 10661, GPU: 4.9GB\n",
      "Epoch 1, Batch 10665, Loss: 4.6657, Time: 6342.7s, Step: 10666, GPU: 4.9GB\n",
      "Epoch 1, Batch 10670, Loss: 4.3273, Time: 6345.2s, Step: 10671, GPU: 4.9GB\n",
      "Epoch 1, Batch 10675, Loss: 4.8068, Time: 6348.6s, Step: 10676, GPU: 4.9GB\n",
      "Epoch 1, Batch 10680, Loss: 4.0860, Time: 6351.1s, Step: 10681, GPU: 4.9GB\n",
      "Epoch 1, Batch 10685, Loss: 3.7550, Time: 6354.5s, Step: 10686, GPU: 4.9GB\n",
      "Epoch 1, Batch 10690, Loss: 4.2267, Time: 6357.0s, Step: 10691, GPU: 4.9GB\n",
      "Epoch 1, Batch 10695, Loss: 4.6837, Time: 6360.4s, Step: 10696, GPU: 4.9GB\n",
      "Epoch 1, Batch 10700, Loss: 4.2980, Time: 6363.0s, Step: 10701, GPU: 4.9GB\n",
      "Epoch 1, Batch 10705, Loss: 4.3861, Time: 6366.4s, Step: 10706, GPU: 4.9GB\n",
      "Epoch 1, Batch 10710, Loss: 4.8290, Time: 6368.9s, Step: 10711, GPU: 4.9GB\n",
      "Epoch 1, Batch 10715, Loss: 5.4970, Time: 6372.3s, Step: 10716, GPU: 4.9GB\n",
      "Epoch 1, Batch 10720, Loss: 4.8216, Time: 6374.8s, Step: 10721, GPU: 4.9GB\n",
      "Epoch 1, Batch 10725, Loss: 4.1248, Time: 6378.2s, Step: 10726, GPU: 4.9GB\n",
      "Epoch 1, Batch 10730, Loss: 4.0361, Time: 6380.8s, Step: 10731, GPU: 4.9GB\n",
      "Epoch 1, Batch 10735, Loss: 4.0982, Time: 6384.2s, Step: 10736, GPU: 4.9GB\n",
      "Epoch 1, Batch 10740, Loss: 4.2596, Time: 6386.7s, Step: 10741, GPU: 4.9GB\n",
      "Epoch 1, Batch 10745, Loss: 3.3727, Time: 6390.2s, Step: 10746, GPU: 4.9GB\n",
      "Epoch 1, Batch 10750, Loss: 4.4356, Time: 6392.7s, Step: 10751, GPU: 4.9GB\n",
      "Epoch 1, Batch 10755, Loss: 5.1301, Time: 6396.1s, Step: 10756, GPU: 4.9GB\n",
      "Epoch 1, Batch 10760, Loss: 4.8601, Time: 6398.6s, Step: 10761, GPU: 4.9GB\n",
      "Epoch 1, Batch 10765, Loss: 3.9343, Time: 6402.0s, Step: 10766, GPU: 4.9GB\n",
      "Epoch 1, Batch 10770, Loss: 4.4061, Time: 6404.5s, Step: 10771, GPU: 4.9GB\n",
      "Epoch 1, Batch 10775, Loss: 4.9117, Time: 6408.0s, Step: 10776, GPU: 4.9GB\n",
      "Epoch 1, Batch 10780, Loss: 5.7523, Time: 6410.5s, Step: 10781, GPU: 4.9GB\n",
      "Epoch 1, Batch 10785, Loss: 4.3753, Time: 6413.9s, Step: 10786, GPU: 4.9GB\n",
      "Epoch 1, Batch 10790, Loss: 3.3370, Time: 6416.4s, Step: 10791, GPU: 4.9GB\n",
      "Epoch 1, Batch 10795, Loss: 4.1923, Time: 6419.8s, Step: 10796, GPU: 4.9GB\n",
      "Epoch 1, Batch 10800, Loss: 4.1506, Time: 6422.4s, Step: 10801, GPU: 4.9GB\n",
      "Epoch 1, Batch 10805, Loss: 3.8782, Time: 6425.9s, Step: 10806, GPU: 4.9GB\n",
      "Epoch 1, Batch 10810, Loss: 4.3978, Time: 6428.4s, Step: 10811, GPU: 4.9GB\n",
      "Epoch 1, Batch 10815, Loss: 3.2975, Time: 6431.8s, Step: 10816, GPU: 4.9GB\n",
      "Epoch 1, Batch 10820, Loss: 3.9319, Time: 6434.3s, Step: 10821, GPU: 4.9GB\n",
      "Epoch 1, Batch 10825, Loss: 4.1994, Time: 6437.7s, Step: 10826, GPU: 4.9GB\n",
      "Epoch 1, Batch 10830, Loss: 4.3093, Time: 6440.2s, Step: 10831, GPU: 4.9GB\n",
      "Epoch 1, Batch 10835, Loss: 4.3360, Time: 6443.6s, Step: 10836, GPU: 4.9GB\n",
      "Epoch 1, Batch 10840, Loss: 4.5329, Time: 6446.1s, Step: 10841, GPU: 4.9GB\n",
      "Epoch 1, Batch 10845, Loss: 3.2330, Time: 6449.6s, Step: 10846, GPU: 4.9GB\n",
      "Epoch 1, Batch 10850, Loss: 4.1262, Time: 6452.1s, Step: 10851, GPU: 4.9GB\n",
      "Epoch 1, Batch 10855, Loss: 5.3835, Time: 6455.5s, Step: 10856, GPU: 4.9GB\n",
      "Epoch 1, Batch 10860, Loss: 4.1426, Time: 6458.0s, Step: 10861, GPU: 4.9GB\n",
      "Epoch 1, Batch 10865, Loss: 3.6731, Time: 6461.4s, Step: 10866, GPU: 4.9GB\n",
      "Epoch 1, Batch 10870, Loss: 4.3060, Time: 6463.9s, Step: 10871, GPU: 4.9GB\n",
      "Epoch 1, Batch 10875, Loss: 4.6428, Time: 6467.3s, Step: 10876, GPU: 4.9GB\n",
      "Epoch 1, Batch 10880, Loss: 4.0488, Time: 6469.8s, Step: 10881, GPU: 4.9GB\n",
      "Epoch 1, Batch 10885, Loss: 3.9372, Time: 6473.2s, Step: 10886, GPU: 4.9GB\n",
      "Epoch 1, Batch 10890, Loss: 4.8318, Time: 6475.8s, Step: 10891, GPU: 4.9GB\n",
      "Epoch 1, Batch 10895, Loss: 5.6951, Time: 6479.2s, Step: 10896, GPU: 4.9GB\n",
      "Epoch 1, Batch 10900, Loss: 4.7281, Time: 6481.7s, Step: 10901, GPU: 4.9GB\n",
      "Epoch 1, Batch 10905, Loss: 4.3305, Time: 6485.1s, Step: 10906, GPU: 4.9GB\n",
      "Epoch 1, Batch 10910, Loss: 4.3683, Time: 6487.6s, Step: 10911, GPU: 4.9GB\n",
      "Epoch 1, Batch 10915, Loss: 4.1818, Time: 6491.0s, Step: 10916, GPU: 4.9GB\n",
      "Epoch 1, Batch 10920, Loss: 4.8254, Time: 6493.5s, Step: 10921, GPU: 4.9GB\n",
      "Epoch 1, Batch 10925, Loss: 3.8718, Time: 6496.9s, Step: 10926, GPU: 4.9GB\n",
      "Epoch 1, Batch 10930, Loss: 4.0436, Time: 6499.4s, Step: 10931, GPU: 4.9GB\n",
      "Epoch 1, Batch 10935, Loss: 4.7279, Time: 6502.8s, Step: 10936, GPU: 4.9GB\n",
      "Epoch 1, Batch 10940, Loss: 3.8479, Time: 6505.3s, Step: 10941, GPU: 4.9GB\n",
      "Epoch 1, Batch 10945, Loss: 5.2041, Time: 6508.7s, Step: 10946, GPU: 4.9GB\n",
      "Epoch 1, Batch 10950, Loss: 3.5738, Time: 6511.2s, Step: 10951, GPU: 4.9GB\n",
      "Epoch 1, Batch 10955, Loss: 4.3432, Time: 6514.6s, Step: 10956, GPU: 4.9GB\n",
      "Epoch 1, Batch 10960, Loss: 3.9230, Time: 6517.2s, Step: 10961, GPU: 4.9GB\n",
      "Epoch 1, Batch 10965, Loss: 4.1947, Time: 6520.5s, Step: 10966, GPU: 4.9GB\n",
      "Epoch 1, Batch 10970, Loss: 4.1668, Time: 6523.1s, Step: 10971, GPU: 4.9GB\n",
      "Epoch 1, Batch 10975, Loss: 4.9942, Time: 6526.4s, Step: 10976, GPU: 4.9GB\n",
      "Epoch 1, Batch 10980, Loss: 3.3990, Time: 6529.0s, Step: 10981, GPU: 4.9GB\n",
      "Epoch 1, Batch 10985, Loss: 4.4113, Time: 6532.4s, Step: 10986, GPU: 4.9GB\n",
      "Epoch 1, Batch 10990, Loss: 4.0970, Time: 6534.9s, Step: 10991, GPU: 4.9GB\n",
      "Epoch 1, Batch 10995, Loss: 3.9210, Time: 6538.3s, Step: 10996, GPU: 4.9GB\n",
      "Epoch 1, Batch 11000, Loss: 4.6461, Time: 6540.9s, Step: 11001, GPU: 4.9GB\n",
      "Epoch 1, Batch 11005, Loss: 4.4066, Time: 6544.3s, Step: 11006, GPU: 4.9GB\n",
      "Epoch 1, Batch 11010, Loss: 4.1075, Time: 6546.8s, Step: 11011, GPU: 4.9GB\n",
      "Epoch 1, Batch 11015, Loss: 3.8167, Time: 6550.2s, Step: 11016, GPU: 4.9GB\n",
      "Epoch 1, Batch 11020, Loss: 4.2563, Time: 6552.8s, Step: 11021, GPU: 4.9GB\n",
      "Epoch 1, Batch 11025, Loss: 3.7110, Time: 6556.3s, Step: 11026, GPU: 4.9GB\n",
      "Epoch 1, Batch 11030, Loss: 4.5425, Time: 6558.8s, Step: 11031, GPU: 4.9GB\n",
      "Epoch 1, Batch 11035, Loss: 4.2231, Time: 6562.3s, Step: 11036, GPU: 4.9GB\n",
      "Epoch 1, Batch 11040, Loss: 4.5177, Time: 6564.8s, Step: 11041, GPU: 4.9GB\n",
      "Epoch 1, Batch 11045, Loss: 4.7021, Time: 6568.2s, Step: 11046, GPU: 4.9GB\n",
      "Epoch 1, Batch 11050, Loss: 4.7298, Time: 6570.8s, Step: 11051, GPU: 4.9GB\n",
      "Epoch 1, Batch 11055, Loss: 4.8243, Time: 6574.2s, Step: 11056, GPU: 4.9GB\n",
      "Epoch 1, Batch 11060, Loss: 3.8879, Time: 6576.7s, Step: 11061, GPU: 4.9GB\n",
      "Epoch 1, Batch 11065, Loss: 3.7691, Time: 6580.1s, Step: 11066, GPU: 4.9GB\n",
      "Epoch 1, Batch 11070, Loss: 4.0489, Time: 6582.6s, Step: 11071, GPU: 4.9GB\n",
      "Epoch 1, Batch 11075, Loss: 4.6360, Time: 6586.0s, Step: 11076, GPU: 4.9GB\n",
      "Epoch 1, Batch 11080, Loss: 5.1153, Time: 6588.6s, Step: 11081, GPU: 4.9GB\n",
      "Epoch 1, Batch 11085, Loss: 3.4086, Time: 6592.0s, Step: 11086, GPU: 4.9GB\n",
      "Epoch 1, Batch 11090, Loss: 4.1955, Time: 6594.6s, Step: 11091, GPU: 4.9GB\n",
      "Epoch 1, Batch 11095, Loss: 5.0244, Time: 6598.0s, Step: 11096, GPU: 4.9GB\n",
      "Epoch 1, Batch 11100, Loss: 4.4012, Time: 6600.5s, Step: 11101, GPU: 4.9GB\n",
      "Epoch 1, Batch 11105, Loss: 5.3728, Time: 6603.9s, Step: 11106, GPU: 4.9GB\n",
      "Epoch 1, Batch 11110, Loss: 4.1710, Time: 6606.5s, Step: 11111, GPU: 4.9GB\n",
      "Epoch 1, Batch 11115, Loss: 3.9180, Time: 6609.9s, Step: 11116, GPU: 4.9GB\n",
      "Epoch 1, Batch 11120, Loss: 5.0443, Time: 6612.4s, Step: 11121, GPU: 4.9GB\n",
      "Epoch 1, Batch 11125, Loss: 4.7072, Time: 6615.8s, Step: 11126, GPU: 4.9GB\n",
      "Epoch 1, Batch 11130, Loss: 4.5541, Time: 6618.3s, Step: 11131, GPU: 4.9GB\n",
      "Epoch 1, Batch 11135, Loss: 4.3723, Time: 6621.8s, Step: 11136, GPU: 4.9GB\n",
      "Epoch 1, Batch 11140, Loss: 4.0770, Time: 6624.3s, Step: 11141, GPU: 4.9GB\n",
      "Epoch 1, Batch 11145, Loss: 4.0639, Time: 6627.7s, Step: 11146, GPU: 4.9GB\n",
      "Epoch 1, Batch 11150, Loss: 4.2352, Time: 6630.3s, Step: 11151, GPU: 4.9GB\n",
      "Epoch 1, Batch 11155, Loss: 4.1114, Time: 6633.7s, Step: 11156, GPU: 4.9GB\n",
      "Epoch 1, Batch 11160, Loss: 4.2319, Time: 6636.3s, Step: 11161, GPU: 4.9GB\n",
      "Epoch 1, Batch 11165, Loss: 4.9077, Time: 6639.7s, Step: 11166, GPU: 4.9GB\n",
      "Epoch 1, Batch 11170, Loss: 4.6506, Time: 6642.2s, Step: 11171, GPU: 4.9GB\n",
      "Epoch 1, Batch 11175, Loss: 3.6292, Time: 6645.6s, Step: 11176, GPU: 4.9GB\n",
      "Epoch 1, Batch 11180, Loss: 4.5448, Time: 6648.1s, Step: 11181, GPU: 4.9GB\n",
      "Epoch 1, Batch 11185, Loss: 4.5204, Time: 6651.6s, Step: 11186, GPU: 4.9GB\n",
      "Epoch 1, Batch 11190, Loss: 4.6569, Time: 6654.1s, Step: 11191, GPU: 4.9GB\n",
      "Epoch 1, Batch 11195, Loss: 5.0417, Time: 6657.5s, Step: 11196, GPU: 4.9GB\n",
      "Epoch 1, Batch 11200, Loss: 4.4061, Time: 6660.1s, Step: 11201, GPU: 4.9GB\n",
      "Epoch 1, Batch 11205, Loss: 4.3068, Time: 6663.5s, Step: 11206, GPU: 4.9GB\n",
      "Epoch 1, Batch 11210, Loss: 4.0135, Time: 6666.0s, Step: 11211, GPU: 4.9GB\n",
      "Epoch 1, Batch 11215, Loss: 4.2920, Time: 6669.4s, Step: 11216, GPU: 4.9GB\n",
      "Epoch 1, Batch 11220, Loss: 4.4495, Time: 6671.9s, Step: 11221, GPU: 4.9GB\n",
      "Epoch 1, Batch 11225, Loss: 3.6976, Time: 6675.3s, Step: 11226, GPU: 4.9GB\n",
      "Epoch 1, Batch 11230, Loss: 3.9284, Time: 6677.9s, Step: 11231, GPU: 4.9GB\n",
      "Epoch 1, Batch 11235, Loss: 4.4324, Time: 6681.3s, Step: 11236, GPU: 4.9GB\n",
      "Epoch 1, Batch 11240, Loss: 4.2783, Time: 6683.8s, Step: 11241, GPU: 4.9GB\n",
      "Epoch 1, Batch 11245, Loss: 4.9920, Time: 6687.2s, Step: 11246, GPU: 4.9GB\n",
      "Epoch 1, Batch 11250, Loss: 4.5173, Time: 6689.7s, Step: 11251, GPU: 4.9GB\n",
      "Epoch 1, Batch 11255, Loss: 3.9506, Time: 6693.1s, Step: 11256, GPU: 4.9GB\n",
      "Epoch 1, Batch 11260, Loss: 4.3313, Time: 6695.6s, Step: 11261, GPU: 4.9GB\n",
      "Epoch 1, Batch 11265, Loss: 4.7565, Time: 6699.0s, Step: 11266, GPU: 4.9GB\n",
      "Epoch 1, Batch 11270, Loss: 4.3585, Time: 6701.5s, Step: 11271, GPU: 4.9GB\n",
      "Epoch 1, Batch 11275, Loss: 3.7582, Time: 6705.0s, Step: 11276, GPU: 4.9GB\n",
      "Epoch 1, Batch 11280, Loss: 3.8428, Time: 6707.5s, Step: 11281, GPU: 4.9GB\n",
      "Epoch 1, Batch 11285, Loss: 3.4377, Time: 6710.9s, Step: 11286, GPU: 4.9GB\n",
      "Epoch 1, Batch 11290, Loss: 4.6167, Time: 6713.4s, Step: 11291, GPU: 4.9GB\n",
      "Epoch 1, Batch 11295, Loss: 4.2129, Time: 6716.8s, Step: 11296, GPU: 4.9GB\n",
      "Epoch 1, Batch 11300, Loss: 4.1878, Time: 6719.3s, Step: 11301, GPU: 4.9GB\n",
      "Epoch 1, Batch 11305, Loss: 4.6016, Time: 6722.7s, Step: 11306, GPU: 4.9GB\n",
      "Epoch 1, Batch 11310, Loss: 4.5165, Time: 6725.2s, Step: 11311, GPU: 4.9GB\n",
      "Epoch 1, Batch 11315, Loss: 4.2668, Time: 6728.6s, Step: 11316, GPU: 4.9GB\n",
      "Epoch 1, Batch 11320, Loss: 4.3187, Time: 6731.1s, Step: 11321, GPU: 4.9GB\n",
      "Epoch 1, Batch 11325, Loss: 4.2908, Time: 6734.5s, Step: 11326, GPU: 4.9GB\n",
      "Epoch 1, Batch 11330, Loss: 4.3939, Time: 6737.1s, Step: 11331, GPU: 4.9GB\n",
      "Epoch 1, Batch 11335, Loss: 4.1616, Time: 6740.5s, Step: 11336, GPU: 4.9GB\n",
      "Epoch 1, Batch 11340, Loss: 4.2999, Time: 6743.0s, Step: 11341, GPU: 4.9GB\n",
      "Epoch 1, Batch 11345, Loss: 5.0817, Time: 6746.4s, Step: 11346, GPU: 4.9GB\n",
      "Epoch 1, Batch 11350, Loss: 4.1300, Time: 6748.9s, Step: 11351, GPU: 4.9GB\n",
      "Epoch 1, Batch 11355, Loss: 4.0671, Time: 6752.3s, Step: 11356, GPU: 4.9GB\n",
      "Epoch 1, Batch 11360, Loss: 4.7191, Time: 6754.9s, Step: 11361, GPU: 4.9GB\n",
      "Epoch 1, Batch 11365, Loss: 4.0932, Time: 6758.3s, Step: 11366, GPU: 4.9GB\n",
      "Epoch 1, Batch 11370, Loss: 4.5473, Time: 6760.8s, Step: 11371, GPU: 4.9GB\n",
      "Epoch 1, Batch 11375, Loss: 4.9847, Time: 6764.2s, Step: 11376, GPU: 4.9GB\n",
      "Epoch 1, Batch 11380, Loss: 4.4803, Time: 6766.7s, Step: 11381, GPU: 4.9GB\n",
      "Epoch 1, Batch 11385, Loss: 4.4880, Time: 6770.2s, Step: 11386, GPU: 4.9GB\n",
      "Epoch 1, Batch 11390, Loss: 3.6871, Time: 6772.7s, Step: 11391, GPU: 4.9GB\n",
      "Epoch 1, Batch 11395, Loss: 3.4961, Time: 6776.2s, Step: 11396, GPU: 4.9GB\n",
      "Epoch 1, Batch 11400, Loss: 3.9656, Time: 6778.8s, Step: 11401, GPU: 4.9GB\n",
      "Epoch 1, Batch 11405, Loss: 5.0007, Time: 6782.2s, Step: 11406, GPU: 4.9GB\n",
      "Epoch 1, Batch 11410, Loss: 3.9650, Time: 6784.7s, Step: 11411, GPU: 4.9GB\n",
      "Epoch 1, Batch 11415, Loss: 4.9497, Time: 6788.2s, Step: 11416, GPU: 4.9GB\n",
      "Epoch 1, Batch 11420, Loss: 4.7397, Time: 6790.7s, Step: 11421, GPU: 4.9GB\n",
      "Epoch 1, Batch 11425, Loss: 4.4896, Time: 6794.1s, Step: 11426, GPU: 4.9GB\n",
      "Epoch 1, Batch 11430, Loss: 4.5928, Time: 6796.7s, Step: 11431, GPU: 4.9GB\n",
      "Epoch 1, Batch 11435, Loss: 4.2944, Time: 6800.1s, Step: 11436, GPU: 4.9GB\n",
      "Epoch 1, Batch 11440, Loss: 4.2711, Time: 6802.6s, Step: 11441, GPU: 4.9GB\n",
      "Epoch 1, Batch 11445, Loss: 5.5324, Time: 6806.0s, Step: 11446, GPU: 4.9GB\n",
      "Epoch 1, Batch 11450, Loss: 4.1983, Time: 6808.6s, Step: 11451, GPU: 4.9GB\n",
      "Epoch 1, Batch 11455, Loss: 4.6025, Time: 6812.0s, Step: 11456, GPU: 4.9GB\n",
      "Epoch 1, Batch 11460, Loss: 3.7937, Time: 6814.6s, Step: 11461, GPU: 4.9GB\n",
      "Epoch 1, Batch 11465, Loss: 4.1166, Time: 6818.0s, Step: 11466, GPU: 4.9GB\n",
      "Epoch 1, Batch 11470, Loss: 4.2658, Time: 6820.5s, Step: 11471, GPU: 4.9GB\n",
      "Epoch 1, Batch 11475, Loss: 5.0669, Time: 6824.0s, Step: 11476, GPU: 4.9GB\n",
      "Epoch 1, Batch 11480, Loss: 4.5017, Time: 6826.5s, Step: 11481, GPU: 4.9GB\n",
      "Epoch 1, Batch 11485, Loss: 4.4262, Time: 6829.9s, Step: 11486, GPU: 4.9GB\n",
      "Epoch 1, Batch 11490, Loss: 3.7844, Time: 6832.5s, Step: 11491, GPU: 4.9GB\n",
      "Epoch 1, Batch 11495, Loss: 4.5124, Time: 6835.9s, Step: 11496, GPU: 4.9GB\n",
      "Epoch 1, Batch 11500, Loss: 4.0522, Time: 6838.4s, Step: 11501, GPU: 4.9GB\n",
      "Epoch 1, Batch 11505, Loss: 4.5538, Time: 6841.8s, Step: 11506, GPU: 4.9GB\n",
      "Epoch 1, Batch 11510, Loss: 5.1725, Time: 6844.4s, Step: 11511, GPU: 4.9GB\n",
      "Epoch 1, Batch 11515, Loss: 4.4139, Time: 6847.8s, Step: 11516, GPU: 4.9GB\n",
      "Epoch 1, Batch 11520, Loss: 4.4614, Time: 6850.3s, Step: 11521, GPU: 4.9GB\n",
      "Epoch 1, Batch 11525, Loss: 4.2414, Time: 6853.8s, Step: 11526, GPU: 4.9GB\n",
      "Epoch 1, Batch 11530, Loss: 4.2670, Time: 6856.3s, Step: 11531, GPU: 4.9GB\n",
      "Epoch 1, Batch 11535, Loss: 4.3258, Time: 6859.7s, Step: 11536, GPU: 4.9GB\n",
      "Epoch 1, Batch 11540, Loss: 3.9023, Time: 6862.2s, Step: 11541, GPU: 4.9GB\n",
      "Epoch 1, Batch 11545, Loss: 5.5071, Time: 6865.7s, Step: 11546, GPU: 4.9GB\n",
      "Epoch 1, Batch 11550, Loss: 4.6077, Time: 6868.2s, Step: 11551, GPU: 4.9GB\n",
      "Epoch 1, Batch 11555, Loss: 4.9861, Time: 6871.7s, Step: 11556, GPU: 4.9GB\n",
      "Epoch 1, Batch 11560, Loss: 4.0918, Time: 6874.2s, Step: 11561, GPU: 4.9GB\n",
      "Epoch 1, Batch 11565, Loss: 5.1199, Time: 6877.6s, Step: 11566, GPU: 4.9GB\n",
      "Epoch 1, Batch 11570, Loss: 4.1216, Time: 6880.2s, Step: 11571, GPU: 4.9GB\n",
      "Epoch 1, Batch 11575, Loss: 3.8851, Time: 6883.6s, Step: 11576, GPU: 4.9GB\n",
      "Epoch 1, Batch 11580, Loss: 4.8055, Time: 6886.1s, Step: 11581, GPU: 4.9GB\n",
      "Epoch 1, Batch 11585, Loss: 4.5798, Time: 6889.5s, Step: 11586, GPU: 4.9GB\n",
      "Epoch 1, Batch 11590, Loss: 4.1647, Time: 6892.0s, Step: 11591, GPU: 4.9GB\n",
      "Epoch 1, Batch 11595, Loss: 4.4305, Time: 6895.5s, Step: 11596, GPU: 4.9GB\n",
      "Epoch 1, Batch 11600, Loss: 4.5670, Time: 6898.1s, Step: 11601, GPU: 4.9GB\n",
      "Epoch 1, Batch 11605, Loss: 4.6642, Time: 6901.5s, Step: 11606, GPU: 4.9GB\n",
      "Epoch 1, Batch 11610, Loss: 4.2947, Time: 6904.0s, Step: 11611, GPU: 4.9GB\n",
      "Epoch 1, Batch 11615, Loss: 5.1753, Time: 6907.4s, Step: 11616, GPU: 4.9GB\n",
      "Epoch 1, Batch 11620, Loss: 3.5221, Time: 6910.0s, Step: 11621, GPU: 4.9GB\n",
      "Epoch 1, Batch 11625, Loss: 4.1186, Time: 6913.4s, Step: 11626, GPU: 4.9GB\n",
      "Epoch 1, Batch 11630, Loss: 4.7917, Time: 6915.9s, Step: 11631, GPU: 4.9GB\n",
      "Epoch 1, Batch 11635, Loss: 3.5328, Time: 6919.3s, Step: 11636, GPU: 4.9GB\n",
      "Epoch 1, Batch 11640, Loss: 4.2865, Time: 6921.8s, Step: 11641, GPU: 4.9GB\n",
      "Epoch 1, Batch 11645, Loss: 4.7473, Time: 6925.2s, Step: 11646, GPU: 4.9GB\n",
      "Epoch 1, Batch 11650, Loss: 5.2401, Time: 6927.7s, Step: 11651, GPU: 4.9GB\n",
      "Epoch 1, Batch 11655, Loss: 4.3865, Time: 6931.2s, Step: 11656, GPU: 4.9GB\n",
      "Epoch 1, Batch 11660, Loss: 4.8243, Time: 6933.7s, Step: 11661, GPU: 4.9GB\n",
      "Epoch 1, Batch 11665, Loss: 4.0627, Time: 6937.1s, Step: 11666, GPU: 4.9GB\n",
      "Epoch 1, Batch 11670, Loss: 5.2979, Time: 6939.6s, Step: 11671, GPU: 4.9GB\n",
      "Epoch 1, Batch 11675, Loss: 4.1429, Time: 6943.1s, Step: 11676, GPU: 4.9GB\n",
      "Epoch 1, Batch 11680, Loss: 4.7169, Time: 6945.6s, Step: 11681, GPU: 4.9GB\n",
      "Epoch 1, Batch 11685, Loss: 3.5714, Time: 6949.1s, Step: 11686, GPU: 4.9GB\n",
      "Epoch 1, Batch 11690, Loss: 4.1512, Time: 6951.7s, Step: 11691, GPU: 4.9GB\n",
      "Epoch 1, Batch 11695, Loss: 5.0324, Time: 6955.1s, Step: 11696, GPU: 4.9GB\n",
      "Epoch 1, Batch 11700, Loss: 4.6627, Time: 6957.6s, Step: 11701, GPU: 4.9GB\n",
      "Epoch 1, Batch 11705, Loss: 3.7136, Time: 6961.0s, Step: 11706, GPU: 4.9GB\n",
      "Epoch 1, Batch 11710, Loss: 4.6366, Time: 6963.5s, Step: 11711, GPU: 4.9GB\n",
      "Epoch 1, Batch 11715, Loss: 3.8495, Time: 6966.9s, Step: 11716, GPU: 4.9GB\n",
      "Epoch 1, Batch 11720, Loss: 4.2777, Time: 6969.4s, Step: 11721, GPU: 4.9GB\n",
      "Epoch 1, Batch 11725, Loss: 4.2142, Time: 6972.9s, Step: 11726, GPU: 4.9GB\n",
      "Epoch 1, Batch 11730, Loss: 3.2182, Time: 6975.4s, Step: 11731, GPU: 4.9GB\n",
      "Epoch 1, Batch 11735, Loss: 3.5764, Time: 6978.8s, Step: 11736, GPU: 4.9GB\n",
      "Epoch 1, Batch 11740, Loss: 4.6468, Time: 6981.3s, Step: 11741, GPU: 4.9GB\n",
      "Epoch 1, Batch 11745, Loss: 4.4995, Time: 6984.7s, Step: 11746, GPU: 4.9GB\n",
      "Epoch 1, Batch 11750, Loss: 4.2550, Time: 6987.2s, Step: 11751, GPU: 4.9GB\n",
      "Epoch 1, Batch 11755, Loss: 4.2263, Time: 6990.7s, Step: 11756, GPU: 4.9GB\n",
      "Epoch 1, Batch 11760, Loss: 3.6772, Time: 6993.2s, Step: 11761, GPU: 4.9GB\n",
      "Epoch 1, Batch 11765, Loss: 4.1597, Time: 6996.6s, Step: 11766, GPU: 4.9GB\n",
      "Epoch 1, Batch 11770, Loss: 3.9188, Time: 6999.2s, Step: 11771, GPU: 4.9GB\n",
      "Epoch 1, Batch 11775, Loss: 4.3196, Time: 7002.6s, Step: 11776, GPU: 4.9GB\n",
      "Epoch 1, Batch 11780, Loss: 4.4734, Time: 7005.1s, Step: 11781, GPU: 4.9GB\n",
      "Epoch 1, Batch 11785, Loss: 3.6709, Time: 7008.5s, Step: 11786, GPU: 4.9GB\n",
      "Epoch 1, Batch 11790, Loss: 5.0191, Time: 7011.0s, Step: 11791, GPU: 4.9GB\n",
      "Epoch 1, Batch 11795, Loss: 4.1364, Time: 7014.4s, Step: 11796, GPU: 4.9GB\n",
      "Epoch 1, Batch 11800, Loss: 4.5785, Time: 7017.0s, Step: 11801, GPU: 4.9GB\n",
      "Epoch 1, Batch 11805, Loss: 3.7841, Time: 7020.5s, Step: 11806, GPU: 4.9GB\n",
      "Epoch 1, Batch 11810, Loss: 4.4471, Time: 7023.0s, Step: 11811, GPU: 4.9GB\n",
      "Epoch 1, Batch 11815, Loss: 4.8542, Time: 7026.4s, Step: 11816, GPU: 4.9GB\n",
      "Epoch 1, Batch 11820, Loss: 4.1768, Time: 7028.9s, Step: 11821, GPU: 4.9GB\n",
      "Epoch 1, Batch 11825, Loss: 4.6563, Time: 7032.4s, Step: 11826, GPU: 4.9GB\n",
      "Epoch 1, Batch 11830, Loss: 3.1412, Time: 7034.9s, Step: 11831, GPU: 4.9GB\n",
      "Epoch 1, Batch 11835, Loss: 2.9359, Time: 7038.2s, Step: 11836, GPU: 4.9GB\n",
      "Epoch 1, Batch 11840, Loss: 4.7804, Time: 7040.8s, Step: 11841, GPU: 4.9GB\n",
      "Epoch 1, Batch 11845, Loss: 4.3589, Time: 7044.2s, Step: 11846, GPU: 4.9GB\n",
      "Epoch 1, Batch 11850, Loss: 3.6619, Time: 7046.7s, Step: 11851, GPU: 4.9GB\n",
      "Epoch 1, Batch 11855, Loss: 4.0227, Time: 7050.1s, Step: 11856, GPU: 4.9GB\n",
      "Epoch 1, Batch 11860, Loss: 3.7201, Time: 7052.6s, Step: 11861, GPU: 4.9GB\n",
      "Epoch 1, Batch 11865, Loss: 5.1617, Time: 7056.0s, Step: 11866, GPU: 4.9GB\n",
      "Epoch 1, Batch 11870, Loss: 3.0266, Time: 7058.5s, Step: 11871, GPU: 4.9GB\n",
      "Epoch 1, Batch 11875, Loss: 4.6608, Time: 7062.0s, Step: 11876, GPU: 4.9GB\n",
      "Epoch 1, Batch 11880, Loss: 3.8067, Time: 7064.5s, Step: 11881, GPU: 4.9GB\n",
      "Epoch 1, Batch 11885, Loss: 4.0664, Time: 7067.9s, Step: 11886, GPU: 4.9GB\n",
      "Epoch 1, Batch 11890, Loss: 4.0800, Time: 7070.4s, Step: 11891, GPU: 4.9GB\n",
      "Epoch 1, Batch 11895, Loss: 4.9906, Time: 7073.8s, Step: 11896, GPU: 4.9GB\n",
      "Epoch 1, Batch 11900, Loss: 4.2668, Time: 7076.3s, Step: 11901, GPU: 4.9GB\n",
      "Epoch 1, Batch 11905, Loss: 3.8609, Time: 7079.8s, Step: 11906, GPU: 4.9GB\n",
      "Epoch 1, Batch 11910, Loss: 3.9738, Time: 7082.3s, Step: 11911, GPU: 4.9GB\n",
      "Epoch 1, Batch 11915, Loss: 4.6915, Time: 7085.7s, Step: 11916, GPU: 4.9GB\n",
      "Epoch 1, Batch 11920, Loss: 5.0081, Time: 7088.2s, Step: 11921, GPU: 4.9GB\n",
      "Epoch 1, Batch 11925, Loss: 3.9304, Time: 7091.7s, Step: 11926, GPU: 4.9GB\n",
      "Epoch 1, Batch 11930, Loss: 4.3215, Time: 7094.2s, Step: 11931, GPU: 4.9GB\n",
      "Epoch 1, Batch 11935, Loss: 4.5074, Time: 7097.6s, Step: 11936, GPU: 4.9GB\n",
      "Epoch 1, Batch 11940, Loss: 4.0308, Time: 7100.1s, Step: 11941, GPU: 4.9GB\n",
      "Epoch 1, Batch 11945, Loss: 3.9901, Time: 7103.6s, Step: 11946, GPU: 4.9GB\n",
      "Epoch 1, Batch 11950, Loss: 4.4130, Time: 7106.1s, Step: 11951, GPU: 4.9GB\n",
      "Epoch 1, Batch 11955, Loss: 4.5688, Time: 7109.5s, Step: 11956, GPU: 4.9GB\n",
      "Epoch 1, Batch 11960, Loss: 3.3671, Time: 7112.1s, Step: 11961, GPU: 4.9GB\n",
      "Epoch 1, Batch 11965, Loss: 4.3246, Time: 7115.5s, Step: 11966, GPU: 4.9GB\n",
      "Epoch 1, Batch 11970, Loss: 4.3208, Time: 7118.0s, Step: 11971, GPU: 4.9GB\n",
      "Epoch 1, Batch 11975, Loss: 4.2694, Time: 7121.4s, Step: 11976, GPU: 4.9GB\n",
      "Epoch 1, Batch 11980, Loss: 4.8014, Time: 7124.0s, Step: 11981, GPU: 4.9GB\n",
      "Epoch 1, Batch 11985, Loss: 4.2340, Time: 7127.4s, Step: 11986, GPU: 4.9GB\n",
      "Epoch 1, Batch 11990, Loss: 4.2374, Time: 7129.9s, Step: 11991, GPU: 4.9GB\n",
      "Epoch 1, Batch 11995, Loss: 4.0483, Time: 7133.3s, Step: 11996, GPU: 4.9GB\n",
      "Epoch 1, Batch 12000, Loss: 4.0900, Time: 7136.0s, Step: 12001, GPU: 4.9GB\n",
      "Epoch 1, Batch 12005, Loss: 4.2419, Time: 7139.4s, Step: 12006, GPU: 4.9GB\n",
      "Epoch 1, Batch 12010, Loss: 5.1137, Time: 7141.9s, Step: 12011, GPU: 4.9GB\n",
      "Epoch 1, Batch 12015, Loss: 4.1944, Time: 7145.3s, Step: 12016, GPU: 4.9GB\n",
      "Epoch 1, Batch 12020, Loss: 3.4335, Time: 7147.8s, Step: 12021, GPU: 4.9GB\n",
      "Epoch 1, Batch 12025, Loss: 5.1413, Time: 7151.3s, Step: 12026, GPU: 4.9GB\n",
      "Epoch 1, Batch 12030, Loss: 4.2969, Time: 7153.8s, Step: 12031, GPU: 4.9GB\n",
      "Epoch 1, Batch 12035, Loss: 3.9933, Time: 7157.2s, Step: 12036, GPU: 4.9GB\n",
      "Epoch 1, Batch 12040, Loss: 4.7480, Time: 7159.7s, Step: 12041, GPU: 4.9GB\n",
      "Epoch 1, Batch 12045, Loss: 4.8155, Time: 7163.2s, Step: 12046, GPU: 4.9GB\n",
      "Epoch 1, Batch 12050, Loss: 4.5268, Time: 7165.7s, Step: 12051, GPU: 4.9GB\n",
      "Epoch 1, Batch 12055, Loss: 4.2003, Time: 7169.1s, Step: 12056, GPU: 4.9GB\n",
      "Epoch 1, Batch 12060, Loss: 5.4187, Time: 7171.7s, Step: 12061, GPU: 4.9GB\n",
      "Epoch 1, Batch 12065, Loss: 4.6960, Time: 7175.1s, Step: 12066, GPU: 4.9GB\n",
      "Epoch 1, Batch 12070, Loss: 4.3427, Time: 7177.6s, Step: 12071, GPU: 4.9GB\n",
      "Epoch 1, Batch 12075, Loss: 3.5107, Time: 7181.1s, Step: 12076, GPU: 4.9GB\n",
      "Epoch 1, Batch 12080, Loss: 4.2213, Time: 7183.6s, Step: 12081, GPU: 4.9GB\n",
      "Epoch 1, Batch 12085, Loss: 4.8680, Time: 7187.0s, Step: 12086, GPU: 4.9GB\n",
      "Epoch 1, Batch 12090, Loss: 4.1770, Time: 7189.6s, Step: 12091, GPU: 4.9GB\n",
      "Epoch 1, Batch 12095, Loss: 3.4412, Time: 7193.0s, Step: 12096, GPU: 4.9GB\n",
      "Epoch 1, Batch 12100, Loss: 4.6958, Time: 7195.5s, Step: 12101, GPU: 4.9GB\n",
      "Epoch 1, Batch 12105, Loss: 4.1926, Time: 7198.9s, Step: 12106, GPU: 4.9GB\n",
      "\n",
      "🔄 Auto-saving checkpoint at epoch 1, batch 12109...\n",
      "💾 Checkpoint saved to: ./my_model_checkpoints/auto_checkpoint_epoch_1_step_12109.pt\n",
      "✅ Checkpoint saved successfully!\n",
      "\n",
      "Epoch 1, Batch 12110, Loss: 3.9695, Time: 7203.4s, Step: 12111, GPU: 4.9GB\n",
      "Epoch 1, Batch 12115, Loss: 4.3850, Time: 7206.8s, Step: 12116, GPU: 4.9GB\n",
      "Epoch 1, Batch 12120, Loss: 4.4575, Time: 7209.3s, Step: 12121, GPU: 4.9GB\n",
      "Epoch 1, Batch 12125, Loss: 4.2923, Time: 7212.8s, Step: 12126, GPU: 4.9GB\n",
      "Epoch 1, Batch 12130, Loss: 4.8140, Time: 7215.3s, Step: 12131, GPU: 4.9GB\n",
      "Epoch 1, Batch 12135, Loss: 3.9435, Time: 7218.7s, Step: 12136, GPU: 4.9GB\n",
      "Epoch 1, Batch 12140, Loss: 3.5457, Time: 7221.2s, Step: 12141, GPU: 4.9GB\n",
      "Epoch 1, Batch 12145, Loss: 4.0612, Time: 7224.6s, Step: 12146, GPU: 4.9GB\n",
      "Epoch 1, Batch 12150, Loss: 3.9394, Time: 7227.1s, Step: 12151, GPU: 4.9GB\n",
      "Epoch 1, Batch 12155, Loss: 4.0414, Time: 7230.5s, Step: 12156, GPU: 4.9GB\n",
      "Epoch 1, Batch 12160, Loss: 3.9487, Time: 7233.0s, Step: 12161, GPU: 4.9GB\n",
      "Epoch 1, Batch 12165, Loss: 4.4673, Time: 7236.4s, Step: 12166, GPU: 4.9GB\n",
      "Epoch 1, Batch 12170, Loss: 5.0690, Time: 7238.9s, Step: 12171, GPU: 4.9GB\n",
      "Epoch 1, Batch 12175, Loss: 4.3582, Time: 7242.3s, Step: 12176, GPU: 4.9GB\n",
      "Epoch 1, Batch 12180, Loss: 4.3889, Time: 7244.8s, Step: 12181, GPU: 4.9GB\n",
      "Epoch 1, Batch 12185, Loss: 4.4420, Time: 7248.2s, Step: 12186, GPU: 4.9GB\n",
      "Epoch 1, Batch 12190, Loss: 4.2270, Time: 7250.8s, Step: 12191, GPU: 4.9GB\n",
      "Epoch 1, Batch 12195, Loss: 2.7749, Time: 7254.2s, Step: 12196, GPU: 4.9GB\n",
      "Epoch 1, Batch 12200, Loss: 4.6065, Time: 7256.8s, Step: 12201, GPU: 4.9GB\n",
      "Epoch 1, Batch 12205, Loss: 4.9273, Time: 7260.2s, Step: 12206, GPU: 4.9GB\n",
      "Epoch 1, Batch 12210, Loss: 4.2592, Time: 7262.7s, Step: 12211, GPU: 4.9GB\n",
      "Epoch 1, Batch 12215, Loss: 5.2277, Time: 7266.2s, Step: 12216, GPU: 4.9GB\n",
      "Epoch 1, Batch 12220, Loss: 4.6164, Time: 7268.7s, Step: 12221, GPU: 4.9GB\n",
      "Epoch 1, Batch 12225, Loss: 4.2818, Time: 7272.1s, Step: 12226, GPU: 4.9GB\n",
      "Epoch 1, Batch 12230, Loss: 4.5768, Time: 7274.7s, Step: 12231, GPU: 4.9GB\n",
      "Epoch 1, Batch 12235, Loss: 4.1243, Time: 7278.1s, Step: 12236, GPU: 4.9GB\n",
      "Epoch 1, Batch 12240, Loss: 3.7881, Time: 7280.6s, Step: 12241, GPU: 4.9GB\n",
      "Epoch 1, Batch 12245, Loss: 4.1842, Time: 7284.0s, Step: 12246, GPU: 4.9GB\n",
      "Epoch 1, Batch 12250, Loss: 4.7752, Time: 7286.5s, Step: 12251, GPU: 4.9GB\n",
      "Epoch 1, Batch 12255, Loss: 4.0093, Time: 7289.9s, Step: 12256, GPU: 4.9GB\n",
      "Epoch 1, Batch 12260, Loss: 4.6800, Time: 7292.4s, Step: 12261, GPU: 4.9GB\n",
      "Epoch 1, Batch 12265, Loss: 4.9448, Time: 7295.9s, Step: 12266, GPU: 4.9GB\n",
      "Epoch 1, Batch 12270, Loss: 4.2063, Time: 7298.4s, Step: 12271, GPU: 4.9GB\n",
      "Epoch 1, Batch 12275, Loss: 4.2157, Time: 7301.8s, Step: 12276, GPU: 4.9GB\n",
      "Epoch 1, Batch 12280, Loss: 3.8616, Time: 7304.3s, Step: 12281, GPU: 4.9GB\n",
      "Epoch 1, Batch 12285, Loss: 4.3754, Time: 7307.7s, Step: 12286, GPU: 4.9GB\n",
      "Epoch 1, Batch 12290, Loss: 4.7714, Time: 7310.3s, Step: 12291, GPU: 4.9GB\n",
      "Epoch 1, Batch 12295, Loss: 4.9667, Time: 7313.7s, Step: 12296, GPU: 4.9GB\n",
      "Epoch 1, Batch 12300, Loss: 4.5650, Time: 7316.2s, Step: 12301, GPU: 4.9GB\n",
      "Epoch 1, Batch 12305, Loss: 4.7670, Time: 7319.7s, Step: 12306, GPU: 4.9GB\n",
      "Epoch 1, Batch 12310, Loss: 3.9404, Time: 7322.2s, Step: 12311, GPU: 4.9GB\n",
      "Epoch 1, Batch 12315, Loss: 4.3036, Time: 7325.6s, Step: 12316, GPU: 4.9GB\n",
      "Epoch 1, Batch 12320, Loss: 4.0673, Time: 7328.1s, Step: 12321, GPU: 4.9GB\n",
      "Epoch 1, Batch 12325, Loss: 4.2459, Time: 7331.6s, Step: 12326, GPU: 4.9GB\n",
      "Epoch 1, Batch 12330, Loss: 4.0342, Time: 7334.1s, Step: 12331, GPU: 4.9GB\n",
      "Epoch 1, Batch 12335, Loss: 4.4338, Time: 7337.5s, Step: 12336, GPU: 4.9GB\n",
      "Epoch 1, Batch 12340, Loss: 4.3132, Time: 7340.1s, Step: 12341, GPU: 4.9GB\n",
      "Epoch 1, Batch 12345, Loss: 4.4007, Time: 7343.5s, Step: 12346, GPU: 4.9GB\n",
      "Epoch 1, Batch 12350, Loss: 4.1288, Time: 7346.0s, Step: 12351, GPU: 4.9GB\n",
      "Epoch 1, Batch 12355, Loss: 5.0307, Time: 7349.5s, Step: 12356, GPU: 4.9GB\n",
      "Epoch 1, Batch 12360, Loss: 4.3237, Time: 7352.0s, Step: 12361, GPU: 4.9GB\n",
      "Epoch 1, Batch 12365, Loss: 5.4906, Time: 7355.4s, Step: 12366, GPU: 4.9GB\n",
      "Epoch 1, Batch 12370, Loss: 4.5022, Time: 7358.0s, Step: 12371, GPU: 4.9GB\n",
      "Epoch 1, Batch 12375, Loss: 3.8357, Time: 7361.4s, Step: 12376, GPU: 4.9GB\n",
      "Epoch 1, Batch 12380, Loss: 4.0964, Time: 7363.9s, Step: 12381, GPU: 4.9GB\n",
      "Epoch 1, Batch 12385, Loss: 4.4817, Time: 7367.3s, Step: 12386, GPU: 4.9GB\n",
      "Epoch 1, Batch 12390, Loss: 3.1241, Time: 7369.9s, Step: 12391, GPU: 4.9GB\n",
      "Epoch 1, Batch 12395, Loss: 4.6022, Time: 7373.3s, Step: 12396, GPU: 4.9GB\n",
      "Epoch 1, Batch 12400, Loss: 4.8313, Time: 7375.9s, Step: 12401, GPU: 4.9GB\n",
      "Epoch 1, Batch 12405, Loss: 4.2568, Time: 7379.3s, Step: 12406, GPU: 4.9GB\n",
      "Epoch 1, Batch 12410, Loss: 4.8698, Time: 7381.9s, Step: 12411, GPU: 4.9GB\n",
      "Epoch 1, Batch 12415, Loss: 4.9446, Time: 7385.3s, Step: 12416, GPU: 4.9GB\n",
      "Epoch 1, Batch 12420, Loss: 3.8955, Time: 7387.8s, Step: 12421, GPU: 4.9GB\n",
      "Epoch 1, Batch 12425, Loss: 4.1265, Time: 7391.2s, Step: 12426, GPU: 4.9GB\n",
      "Epoch 1, Batch 12430, Loss: 4.4394, Time: 7393.7s, Step: 12431, GPU: 4.9GB\n",
      "Epoch 1, Batch 12435, Loss: 4.2735, Time: 7397.2s, Step: 12436, GPU: 4.9GB\n",
      "Epoch 1, Batch 12440, Loss: 5.2834, Time: 7399.7s, Step: 12441, GPU: 4.9GB\n",
      "Epoch 1, Batch 12445, Loss: 4.3971, Time: 7403.1s, Step: 12446, GPU: 4.9GB\n",
      "Epoch 1, Batch 12450, Loss: 5.1461, Time: 7405.6s, Step: 12451, GPU: 4.9GB\n",
      "Epoch 1, Batch 12455, Loss: 3.8564, Time: 7409.1s, Step: 12456, GPU: 4.9GB\n",
      "Epoch 1, Batch 12460, Loss: 3.5367, Time: 7411.6s, Step: 12461, GPU: 4.9GB\n",
      "Epoch 1, Batch 12465, Loss: 4.2488, Time: 7415.0s, Step: 12466, GPU: 4.9GB\n",
      "Epoch 1, Batch 12470, Loss: 5.1042, Time: 7417.5s, Step: 12471, GPU: 4.9GB\n",
      "Epoch 1, Batch 12475, Loss: 4.4128, Time: 7420.9s, Step: 12476, GPU: 4.9GB\n",
      "Epoch 1, Batch 12480, Loss: 4.3068, Time: 7423.5s, Step: 12481, GPU: 4.9GB\n",
      "Epoch 1, Batch 12485, Loss: 4.1861, Time: 7426.9s, Step: 12486, GPU: 4.9GB\n",
      "Epoch 1, Batch 12490, Loss: 3.9259, Time: 7429.4s, Step: 12491, GPU: 4.9GB\n",
      "Epoch 1, Batch 12495, Loss: 3.8701, Time: 7432.9s, Step: 12496, GPU: 4.9GB\n",
      "Epoch 1, Batch 12500, Loss: 4.1230, Time: 7435.4s, Step: 12501, GPU: 4.9GB\n",
      "Epoch 1, Batch 12505, Loss: 3.8282, Time: 7438.8s, Step: 12506, GPU: 4.9GB\n",
      "Epoch 1, Batch 12510, Loss: 4.0341, Time: 7441.3s, Step: 12511, GPU: 4.9GB\n",
      "Epoch 1, Batch 12515, Loss: 4.2283, Time: 7444.7s, Step: 12516, GPU: 4.9GB\n",
      "Epoch 1, Batch 12520, Loss: 3.7616, Time: 7447.3s, Step: 12521, GPU: 4.9GB\n",
      "Epoch 1, Batch 12525, Loss: 4.4668, Time: 7450.7s, Step: 12526, GPU: 4.9GB\n",
      "Epoch 1, Batch 12530, Loss: 3.6341, Time: 7453.2s, Step: 12531, GPU: 4.9GB\n",
      "Epoch 1, Batch 12535, Loss: 5.0860, Time: 7456.7s, Step: 12536, GPU: 4.9GB\n",
      "Epoch 1, Batch 12540, Loss: 4.9883, Time: 7459.2s, Step: 12541, GPU: 4.9GB\n",
      "Epoch 1, Batch 12545, Loss: 4.5334, Time: 7463.1s, Step: 12546, GPU: 4.9GB\n",
      "Epoch 1, Batch 12550, Loss: 3.6522, Time: 7465.6s, Step: 12551, GPU: 4.9GB\n",
      "Epoch 1, Batch 12555, Loss: 4.2669, Time: 7469.0s, Step: 12556, GPU: 4.9GB\n",
      "Epoch 1, Batch 12560, Loss: 4.3377, Time: 7471.5s, Step: 12561, GPU: 4.9GB\n",
      "Epoch 1, Batch 12565, Loss: 4.0319, Time: 7474.9s, Step: 12566, GPU: 4.9GB\n",
      "Epoch 1, Batch 12570, Loss: 4.2910, Time: 7477.5s, Step: 12571, GPU: 4.9GB\n",
      "Epoch 1, Batch 12575, Loss: 4.0756, Time: 7480.9s, Step: 12576, GPU: 4.9GB\n",
      "Epoch 1, Batch 12580, Loss: 3.9148, Time: 7483.4s, Step: 12581, GPU: 4.9GB\n",
      "Epoch 1, Batch 12585, Loss: 4.1961, Time: 7486.8s, Step: 12586, GPU: 4.9GB\n",
      "Epoch 1, Batch 12590, Loss: 4.2566, Time: 7489.3s, Step: 12591, GPU: 4.9GB\n",
      "Epoch 1, Batch 12595, Loss: 3.7912, Time: 7492.7s, Step: 12596, GPU: 4.9GB\n",
      "Epoch 1, Batch 12600, Loss: 5.5524, Time: 7495.3s, Step: 12601, GPU: 4.9GB\n",
      "Epoch 1, Batch 12605, Loss: 4.3985, Time: 7498.8s, Step: 12606, GPU: 4.9GB\n",
      "Epoch 1, Batch 12610, Loss: 3.5491, Time: 7501.3s, Step: 12611, GPU: 4.9GB\n",
      "Epoch 1, Batch 12615, Loss: 5.3096, Time: 7504.7s, Step: 12616, GPU: 4.9GB\n",
      "Epoch 1, Batch 12620, Loss: 4.0864, Time: 7507.2s, Step: 12621, GPU: 4.9GB\n",
      "Epoch 1, Batch 12625, Loss: 4.2001, Time: 7510.7s, Step: 12626, GPU: 4.9GB\n",
      "Epoch 1, Batch 12630, Loss: 3.2858, Time: 7513.2s, Step: 12631, GPU: 4.9GB\n",
      "Epoch 1, Batch 12635, Loss: 4.6427, Time: 7516.6s, Step: 12636, GPU: 4.9GB\n",
      "Epoch 1, Batch 12640, Loss: 2.9868, Time: 7519.2s, Step: 12641, GPU: 4.9GB\n",
      "Epoch 1, Batch 12645, Loss: 4.2251, Time: 7522.6s, Step: 12646, GPU: 4.9GB\n",
      "Epoch 1, Batch 12650, Loss: 3.6632, Time: 7525.1s, Step: 12651, GPU: 4.9GB\n",
      "Epoch 1, Batch 12655, Loss: 4.3497, Time: 7528.5s, Step: 12656, GPU: 4.9GB\n",
      "Epoch 1, Batch 12660, Loss: 4.3863, Time: 7531.0s, Step: 12661, GPU: 4.9GB\n",
      "Epoch 1, Batch 12665, Loss: 3.9598, Time: 7534.4s, Step: 12666, GPU: 4.9GB\n",
      "Epoch 1, Batch 12670, Loss: 4.5631, Time: 7536.9s, Step: 12671, GPU: 4.9GB\n",
      "Epoch 1, Batch 12675, Loss: 3.7150, Time: 7540.3s, Step: 12676, GPU: 4.9GB\n",
      "Epoch 1, Batch 12680, Loss: 5.4566, Time: 7542.9s, Step: 12681, GPU: 4.9GB\n",
      "Epoch 1, Batch 12685, Loss: 4.7033, Time: 7546.3s, Step: 12686, GPU: 4.9GB\n",
      "Epoch 1, Batch 12690, Loss: 4.4736, Time: 7548.8s, Step: 12691, GPU: 4.9GB\n",
      "Epoch 1, Batch 12695, Loss: 4.3747, Time: 7552.2s, Step: 12696, GPU: 4.9GB\n",
      "Epoch 1, Batch 12700, Loss: 4.9530, Time: 7554.7s, Step: 12701, GPU: 4.9GB\n",
      "Epoch 1, Batch 12705, Loss: 4.3698, Time: 7558.2s, Step: 12706, GPU: 4.9GB\n",
      "Epoch 1, Batch 12710, Loss: 4.6335, Time: 7560.7s, Step: 12711, GPU: 4.9GB\n",
      "Epoch 1, Batch 12715, Loss: 4.3939, Time: 7564.1s, Step: 12716, GPU: 4.9GB\n",
      "Epoch 1, Batch 12720, Loss: 4.3796, Time: 7566.7s, Step: 12721, GPU: 4.9GB\n",
      "Epoch 1, Batch 12725, Loss: 3.8836, Time: 7570.1s, Step: 12726, GPU: 4.9GB\n",
      "Epoch 1, Batch 12730, Loss: 3.9247, Time: 7572.6s, Step: 12731, GPU: 4.9GB\n",
      "Epoch 1, Batch 12735, Loss: 3.8366, Time: 7576.1s, Step: 12736, GPU: 4.9GB\n",
      "Epoch 1, Batch 12740, Loss: 4.2437, Time: 7578.6s, Step: 12741, GPU: 4.9GB\n",
      "Epoch 1, Batch 12745, Loss: 4.1185, Time: 7582.0s, Step: 12746, GPU: 4.9GB\n",
      "Epoch 1, Batch 12750, Loss: 4.7848, Time: 7584.5s, Step: 12751, GPU: 4.9GB\n",
      "Epoch 1, Batch 12755, Loss: 4.6088, Time: 7588.0s, Step: 12756, GPU: 4.9GB\n",
      "Epoch 1, Batch 12760, Loss: 4.5316, Time: 7590.5s, Step: 12761, GPU: 4.9GB\n",
      "Epoch 1, Batch 12765, Loss: 4.5255, Time: 7593.9s, Step: 12766, GPU: 4.9GB\n",
      "Epoch 1, Batch 12770, Loss: 3.7823, Time: 7596.5s, Step: 12771, GPU: 4.9GB\n",
      "Epoch 1, Batch 12775, Loss: 3.8579, Time: 7599.9s, Step: 12776, GPU: 4.9GB\n",
      "Epoch 1, Batch 12780, Loss: 4.8397, Time: 7602.4s, Step: 12781, GPU: 4.9GB\n",
      "Epoch 1, Batch 12785, Loss: 4.2637, Time: 7605.8s, Step: 12786, GPU: 4.9GB\n",
      "Epoch 1, Batch 12790, Loss: 4.2316, Time: 7608.3s, Step: 12791, GPU: 4.9GB\n",
      "Epoch 1, Batch 12795, Loss: 3.5855, Time: 7611.8s, Step: 12796, GPU: 4.9GB\n",
      "Epoch 1, Batch 12800, Loss: 4.4100, Time: 7614.3s, Step: 12801, GPU: 4.9GB\n",
      "Epoch 1, Batch 12805, Loss: 3.8989, Time: 7617.8s, Step: 12806, GPU: 4.9GB\n",
      "Epoch 1, Batch 12810, Loss: 3.8812, Time: 7620.3s, Step: 12811, GPU: 4.9GB\n",
      "Epoch 1, Batch 12815, Loss: 4.2948, Time: 7623.8s, Step: 12816, GPU: 4.9GB\n",
      "Epoch 1, Batch 12820, Loss: 4.8296, Time: 7626.3s, Step: 12821, GPU: 4.9GB\n",
      "Epoch 1, Batch 12825, Loss: 4.8811, Time: 7629.8s, Step: 12826, GPU: 4.9GB\n",
      "Epoch 1, Batch 12830, Loss: 4.3985, Time: 7632.4s, Step: 12831, GPU: 4.9GB\n",
      "Epoch 1, Batch 12835, Loss: 4.4516, Time: 7635.8s, Step: 12836, GPU: 4.9GB\n",
      "Epoch 1, Batch 12840, Loss: 4.8276, Time: 7638.3s, Step: 12841, GPU: 4.9GB\n",
      "Epoch 1, Batch 12845, Loss: 3.9699, Time: 7641.8s, Step: 12846, GPU: 4.9GB\n",
      "Epoch 1, Batch 12850, Loss: 4.1820, Time: 7644.3s, Step: 12851, GPU: 4.9GB\n",
      "Epoch 1, Batch 12855, Loss: 3.9745, Time: 7647.8s, Step: 12856, GPU: 4.9GB\n",
      "Epoch 1, Batch 12860, Loss: 4.4480, Time: 7650.3s, Step: 12861, GPU: 4.9GB\n",
      "Epoch 1, Batch 12865, Loss: 4.0558, Time: 7653.7s, Step: 12866, GPU: 4.9GB\n",
      "Epoch 1, Batch 12870, Loss: 4.2063, Time: 7656.3s, Step: 12871, GPU: 4.9GB\n",
      "Epoch 1, Batch 12875, Loss: 4.4570, Time: 7659.7s, Step: 12876, GPU: 4.9GB\n",
      "Epoch 1, Batch 12880, Loss: 3.9509, Time: 7662.2s, Step: 12881, GPU: 4.9GB\n",
      "Epoch 1, Batch 12885, Loss: 4.5608, Time: 7665.6s, Step: 12886, GPU: 4.9GB\n",
      "Epoch 1, Batch 12890, Loss: 4.2888, Time: 7668.2s, Step: 12891, GPU: 4.9GB\n",
      "Epoch 1, Batch 12895, Loss: 4.3795, Time: 7671.6s, Step: 12896, GPU: 4.9GB\n",
      "Epoch 1, Batch 12900, Loss: 4.3040, Time: 7674.1s, Step: 12901, GPU: 4.9GB\n",
      "Epoch 1, Batch 12905, Loss: 3.3112, Time: 7677.6s, Step: 12906, GPU: 4.9GB\n",
      "Epoch 1, Batch 12910, Loss: 3.8047, Time: 7680.1s, Step: 12911, GPU: 4.9GB\n",
      "Epoch 1, Batch 12915, Loss: 4.7605, Time: 7683.5s, Step: 12916, GPU: 4.9GB\n",
      "Epoch 1, Batch 12920, Loss: 4.2013, Time: 7686.1s, Step: 12921, GPU: 4.9GB\n",
      "Epoch 1, Batch 12925, Loss: 3.5729, Time: 7689.5s, Step: 12926, GPU: 4.9GB\n",
      "Epoch 1, Batch 12930, Loss: 4.2565, Time: 7692.1s, Step: 12931, GPU: 4.9GB\n",
      "Epoch 1, Batch 12935, Loss: 4.5326, Time: 7695.5s, Step: 12936, GPU: 4.9GB\n",
      "Epoch 1, Batch 12940, Loss: 3.4824, Time: 7698.1s, Step: 12941, GPU: 4.9GB\n",
      "Epoch 1, Batch 12945, Loss: 4.2250, Time: 7701.5s, Step: 12946, GPU: 4.9GB\n",
      "Epoch 1, Batch 12950, Loss: 4.3758, Time: 7705.2s, Step: 12951, GPU: 4.9GB\n",
      "Epoch 1, Batch 12955, Loss: 3.9359, Time: 7708.7s, Step: 12956, GPU: 4.9GB\n",
      "Epoch 1, Batch 12960, Loss: 4.8242, Time: 7711.2s, Step: 12961, GPU: 4.9GB\n",
      "Epoch 1, Batch 12965, Loss: 4.0175, Time: 7714.7s, Step: 12966, GPU: 4.9GB\n",
      "Epoch 1, Batch 12970, Loss: 4.1398, Time: 7717.2s, Step: 12971, GPU: 4.9GB\n",
      "Epoch 1, Batch 12975, Loss: 5.6645, Time: 7720.7s, Step: 12976, GPU: 4.9GB\n",
      "Epoch 1, Batch 12980, Loss: 4.7230, Time: 7723.2s, Step: 12981, GPU: 4.9GB\n",
      "Epoch 1, Batch 12985, Loss: 4.0512, Time: 7726.7s, Step: 12986, GPU: 4.9GB\n",
      "Epoch 1, Batch 12990, Loss: 4.0761, Time: 7729.2s, Step: 12991, GPU: 4.9GB\n",
      "Epoch 1, Batch 12995, Loss: 3.8413, Time: 7732.7s, Step: 12996, GPU: 4.9GB\n",
      "Epoch 1, Batch 13000, Loss: 4.3789, Time: 7735.3s, Step: 13001, GPU: 4.9GB\n",
      "Epoch 1, Batch 13005, Loss: 4.2368, Time: 7738.7s, Step: 13006, GPU: 4.9GB\n",
      "Epoch 1, Batch 13010, Loss: 4.1239, Time: 7741.2s, Step: 13011, GPU: 4.9GB\n",
      "Epoch 1, Batch 13015, Loss: 3.8545, Time: 7744.6s, Step: 13016, GPU: 4.9GB\n",
      "Epoch 1, Batch 13020, Loss: 3.8513, Time: 7747.1s, Step: 13021, GPU: 4.9GB\n",
      "Epoch 1, Batch 13025, Loss: 4.8801, Time: 7750.5s, Step: 13026, GPU: 4.9GB\n",
      "Epoch 1, Batch 13030, Loss: 3.9693, Time: 7753.0s, Step: 13031, GPU: 4.9GB\n",
      "Epoch 1, Batch 13035, Loss: 4.5078, Time: 7756.4s, Step: 13036, GPU: 4.9GB\n",
      "Epoch 1, Batch 13040, Loss: 3.7053, Time: 7758.9s, Step: 13041, GPU: 4.9GB\n",
      "Epoch 1, Batch 13045, Loss: 3.7965, Time: 7762.4s, Step: 13046, GPU: 4.9GB\n",
      "Epoch 1, Batch 13050, Loss: 4.0804, Time: 7764.9s, Step: 13051, GPU: 4.9GB\n",
      "Epoch 1, Batch 13055, Loss: 4.4985, Time: 7768.3s, Step: 13056, GPU: 4.9GB\n",
      "Epoch 1, Batch 13060, Loss: 5.2169, Time: 7770.8s, Step: 13061, GPU: 4.9GB\n",
      "Epoch 1, Batch 13065, Loss: 3.4919, Time: 7774.2s, Step: 13066, GPU: 4.9GB\n",
      "Epoch 1, Batch 13070, Loss: 4.2029, Time: 7776.7s, Step: 13071, GPU: 4.9GB\n",
      "Epoch 1, Batch 13075, Loss: 3.5337, Time: 7780.1s, Step: 13076, GPU: 4.9GB\n",
      "Epoch 1, Batch 13080, Loss: 4.4432, Time: 7782.7s, Step: 13081, GPU: 4.9GB\n",
      "Epoch 1, Batch 13085, Loss: 4.3041, Time: 7786.1s, Step: 13086, GPU: 4.9GB\n",
      "Epoch 1, Batch 13090, Loss: 4.5139, Time: 7788.6s, Step: 13091, GPU: 4.9GB\n",
      "Epoch 1, Batch 13095, Loss: 3.7927, Time: 7792.0s, Step: 13096, GPU: 4.9GB\n",
      "Epoch 1, Batch 13100, Loss: 4.4260, Time: 7794.5s, Step: 13101, GPU: 4.9GB\n",
      "Epoch 1, Batch 13105, Loss: 4.2566, Time: 7797.9s, Step: 13106, GPU: 4.9GB\n",
      "Epoch 1, Batch 13110, Loss: 5.4280, Time: 7800.5s, Step: 13111, GPU: 4.9GB\n",
      "Epoch 1, Batch 13115, Loss: 5.1295, Time: 7803.9s, Step: 13116, GPU: 4.9GB\n",
      "Epoch 1, Batch 13120, Loss: 3.7940, Time: 7806.4s, Step: 13121, GPU: 4.9GB\n",
      "Epoch 1, Batch 13125, Loss: 4.7626, Time: 7809.9s, Step: 13126, GPU: 4.9GB\n",
      "Epoch 1, Batch 13130, Loss: 4.3521, Time: 7812.4s, Step: 13131, GPU: 4.9GB\n",
      "Epoch 1, Batch 13135, Loss: 3.5738, Time: 7815.8s, Step: 13136, GPU: 4.9GB\n",
      "Epoch 1, Batch 13140, Loss: 4.0774, Time: 7818.3s, Step: 13141, GPU: 4.9GB\n",
      "Epoch 1, Batch 13145, Loss: 4.5228, Time: 7821.7s, Step: 13146, GPU: 4.9GB\n",
      "Epoch 1, Batch 13150, Loss: 5.1773, Time: 7824.3s, Step: 13151, GPU: 4.9GB\n",
      "Epoch 1, Batch 13155, Loss: 4.4809, Time: 7827.7s, Step: 13156, GPU: 4.9GB\n",
      "Epoch 1, Batch 13160, Loss: 4.1807, Time: 7830.2s, Step: 13161, GPU: 4.9GB\n",
      "Epoch 1, Batch 13165, Loss: 4.0297, Time: 7833.6s, Step: 13166, GPU: 4.9GB\n",
      "Epoch 1, Batch 13170, Loss: 3.9248, Time: 7836.2s, Step: 13171, GPU: 4.9GB\n",
      "Epoch 1, Batch 13175, Loss: 4.6511, Time: 7839.6s, Step: 13176, GPU: 4.9GB\n",
      "Epoch 1, Batch 13180, Loss: 4.3717, Time: 7842.1s, Step: 13181, GPU: 4.9GB\n",
      "Epoch 1, Batch 13185, Loss: 4.2914, Time: 7845.5s, Step: 13186, GPU: 4.9GB\n",
      "Epoch 1, Batch 13190, Loss: 3.6217, Time: 7848.1s, Step: 13191, GPU: 4.9GB\n",
      "Epoch 1, Batch 13195, Loss: 4.3895, Time: 7851.5s, Step: 13196, GPU: 4.9GB\n",
      "Epoch 1, Batch 13200, Loss: 4.0840, Time: 7854.1s, Step: 13201, GPU: 4.9GB\n",
      "Epoch 1, Batch 13205, Loss: 4.2715, Time: 7857.5s, Step: 13206, GPU: 4.9GB\n",
      "Epoch 1, Batch 13210, Loss: 4.1393, Time: 7860.1s, Step: 13211, GPU: 4.9GB\n",
      "Epoch 1, Batch 13215, Loss: 3.9356, Time: 7863.5s, Step: 13216, GPU: 4.9GB\n",
      "Epoch 1, Batch 13220, Loss: 3.5668, Time: 7866.0s, Step: 13221, GPU: 4.9GB\n",
      "Epoch 1, Batch 13225, Loss: 4.5437, Time: 7869.5s, Step: 13226, GPU: 4.9GB\n",
      "Epoch 1, Batch 13230, Loss: 3.8119, Time: 7872.0s, Step: 13231, GPU: 4.9GB\n",
      "Epoch 1, Batch 13235, Loss: 4.6136, Time: 7875.4s, Step: 13236, GPU: 4.9GB\n",
      "Epoch 1, Batch 13240, Loss: 3.8553, Time: 7877.9s, Step: 13241, GPU: 4.9GB\n",
      "Epoch 1, Batch 13245, Loss: 4.2777, Time: 7881.3s, Step: 13246, GPU: 4.9GB\n",
      "Epoch 1, Batch 13250, Loss: 5.2834, Time: 7883.8s, Step: 13251, GPU: 4.9GB\n",
      "Epoch 1, Batch 13255, Loss: 4.1668, Time: 7887.3s, Step: 13256, GPU: 4.9GB\n",
      "Epoch 1, Batch 13260, Loss: 3.6487, Time: 7889.8s, Step: 13261, GPU: 4.9GB\n",
      "Epoch 1, Batch 13265, Loss: 3.8904, Time: 7893.3s, Step: 13266, GPU: 4.9GB\n",
      "Epoch 1, Batch 13270, Loss: 4.7720, Time: 7895.8s, Step: 13271, GPU: 4.9GB\n",
      "Epoch 1, Batch 13275, Loss: 4.0938, Time: 7899.2s, Step: 13276, GPU: 4.9GB\n",
      "Epoch 1, Batch 13280, Loss: 4.2090, Time: 7901.7s, Step: 13281, GPU: 4.9GB\n",
      "Epoch 1, Batch 13285, Loss: 3.5900, Time: 7905.2s, Step: 13286, GPU: 4.9GB\n",
      "Epoch 1, Batch 13290, Loss: 3.5271, Time: 7907.7s, Step: 13291, GPU: 4.9GB\n",
      "Epoch 1, Batch 13295, Loss: 4.3134, Time: 7911.1s, Step: 13296, GPU: 4.9GB\n",
      "Epoch 1, Batch 13300, Loss: 3.4306, Time: 7913.6s, Step: 13301, GPU: 4.9GB\n",
      "Epoch 1, Batch 13305, Loss: 4.5129, Time: 7917.0s, Step: 13306, GPU: 4.9GB\n",
      "Epoch 1, Batch 13310, Loss: 4.6200, Time: 7919.6s, Step: 13311, GPU: 4.9GB\n",
      "Epoch 1, Batch 13315, Loss: 3.8216, Time: 7923.0s, Step: 13316, GPU: 4.9GB\n",
      "Epoch 1, Batch 13320, Loss: 5.0516, Time: 7925.5s, Step: 13321, GPU: 4.9GB\n",
      "Epoch 1, Batch 13325, Loss: 4.2842, Time: 7928.9s, Step: 13326, GPU: 4.9GB\n",
      "Epoch 1, Batch 13330, Loss: 4.9295, Time: 7931.5s, Step: 13331, GPU: 4.9GB\n",
      "Epoch 1, Batch 13335, Loss: 3.4295, Time: 7934.9s, Step: 13336, GPU: 4.9GB\n",
      "Epoch 1, Batch 13340, Loss: 4.3279, Time: 7937.5s, Step: 13341, GPU: 4.9GB\n",
      "Epoch 1, Batch 13345, Loss: 5.1317, Time: 7940.9s, Step: 13346, GPU: 4.9GB\n",
      "Epoch 1, Batch 13350, Loss: 4.3108, Time: 7943.4s, Step: 13351, GPU: 4.9GB\n",
      "Epoch 1, Batch 13355, Loss: 4.3378, Time: 7946.9s, Step: 13356, GPU: 4.9GB\n",
      "Epoch 1, Batch 13360, Loss: 4.4431, Time: 7949.4s, Step: 13361, GPU: 4.9GB\n",
      "Epoch 1, Batch 13365, Loss: 4.2859, Time: 7952.8s, Step: 13366, GPU: 4.9GB\n",
      "Epoch 1, Batch 13370, Loss: 3.5822, Time: 7955.4s, Step: 13371, GPU: 4.9GB\n",
      "Epoch 1, Batch 13375, Loss: 4.2104, Time: 7958.8s, Step: 13376, GPU: 4.9GB\n",
      "Epoch 1, Batch 13380, Loss: 3.3115, Time: 7961.3s, Step: 13381, GPU: 4.9GB\n",
      "Epoch 1, Batch 13385, Loss: 4.7815, Time: 7964.7s, Step: 13386, GPU: 4.9GB\n",
      "Epoch 1, Batch 13390, Loss: 3.5808, Time: 7967.2s, Step: 13391, GPU: 4.9GB\n",
      "Epoch 1, Batch 13395, Loss: 4.2853, Time: 7970.7s, Step: 13396, GPU: 4.9GB\n",
      "Epoch 1, Batch 13400, Loss: 4.1861, Time: 7973.2s, Step: 13401, GPU: 4.9GB\n",
      "Epoch 1, Batch 13405, Loss: 3.0015, Time: 7976.6s, Step: 13406, GPU: 4.9GB\n",
      "Epoch 1, Batch 13410, Loss: 3.9179, Time: 7979.2s, Step: 13411, GPU: 4.9GB\n",
      "Epoch 1, Batch 13415, Loss: 2.7634, Time: 7982.6s, Step: 13416, GPU: 4.9GB\n",
      "Epoch 1, Batch 13420, Loss: 3.8019, Time: 7985.1s, Step: 13421, GPU: 4.9GB\n",
      "Epoch 1, Batch 13425, Loss: 4.3402, Time: 7988.5s, Step: 13426, GPU: 4.9GB\n",
      "Epoch 1, Batch 13430, Loss: 4.8455, Time: 7991.0s, Step: 13431, GPU: 4.9GB\n",
      "Epoch 1, Batch 13435, Loss: 3.7647, Time: 7994.4s, Step: 13436, GPU: 4.9GB\n",
      "Epoch 1, Batch 13440, Loss: 3.9367, Time: 7996.9s, Step: 13441, GPU: 4.9GB\n",
      "Epoch 1, Batch 13445, Loss: 3.5496, Time: 8000.3s, Step: 13446, GPU: 4.9GB\n",
      "Epoch 1, Batch 13450, Loss: 3.9606, Time: 8002.8s, Step: 13451, GPU: 4.9GB\n",
      "Epoch 1, Batch 13455, Loss: 4.1461, Time: 8006.2s, Step: 13456, GPU: 4.9GB\n",
      "Epoch 1, Batch 13460, Loss: 4.3754, Time: 8008.7s, Step: 13461, GPU: 4.9GB\n",
      "Epoch 1, Batch 13465, Loss: 4.1064, Time: 8012.1s, Step: 13466, GPU: 4.9GB\n",
      "Epoch 1, Batch 13470, Loss: 3.7908, Time: 8014.6s, Step: 13471, GPU: 4.9GB\n",
      "Epoch 1, Batch 13475, Loss: 4.9880, Time: 8018.0s, Step: 13476, GPU: 4.9GB\n",
      "Epoch 1, Batch 13480, Loss: 4.2350, Time: 8020.5s, Step: 13481, GPU: 4.9GB\n",
      "Epoch 1, Batch 13485, Loss: 3.8525, Time: 8023.9s, Step: 13486, GPU: 4.9GB\n",
      "Epoch 1, Batch 13490, Loss: 4.5702, Time: 8026.4s, Step: 13491, GPU: 4.9GB\n",
      "Epoch 1, Batch 13495, Loss: 4.1495, Time: 8029.8s, Step: 13496, GPU: 4.9GB\n",
      "Epoch 1, Batch 13500, Loss: 4.7612, Time: 8032.3s, Step: 13501, GPU: 4.9GB\n",
      "Epoch 1, Batch 13505, Loss: 4.9684, Time: 8035.8s, Step: 13506, GPU: 4.9GB\n",
      "Epoch 1, Batch 13510, Loss: 3.8079, Time: 8038.3s, Step: 13511, GPU: 4.9GB\n",
      "Epoch 1, Batch 13515, Loss: 4.8152, Time: 8041.7s, Step: 13516, GPU: 4.9GB\n",
      "Epoch 1, Batch 13520, Loss: 4.3600, Time: 8044.2s, Step: 13521, GPU: 4.9GB\n",
      "Epoch 1, Batch 13525, Loss: 4.0484, Time: 8047.6s, Step: 13526, GPU: 4.9GB\n",
      "Epoch 1, Batch 13530, Loss: 3.9656, Time: 8050.2s, Step: 13531, GPU: 4.9GB\n",
      "Epoch 1, Batch 13535, Loss: 3.8953, Time: 8053.6s, Step: 13536, GPU: 4.9GB\n",
      "Epoch 1, Batch 13540, Loss: 3.3430, Time: 8056.1s, Step: 13541, GPU: 4.9GB\n",
      "Epoch 1, Batch 13545, Loss: 5.0766, Time: 8059.5s, Step: 13546, GPU: 4.9GB\n",
      "Epoch 1, Batch 13550, Loss: 3.9080, Time: 8062.0s, Step: 13551, GPU: 4.9GB\n",
      "Epoch 1, Batch 13555, Loss: 4.6862, Time: 8065.4s, Step: 13556, GPU: 4.9GB\n",
      "Epoch 1, Batch 13560, Loss: 3.7456, Time: 8068.0s, Step: 13561, GPU: 4.9GB\n",
      "Epoch 1, Batch 13565, Loss: 3.6125, Time: 8071.4s, Step: 13566, GPU: 4.9GB\n",
      "Epoch 1, Batch 13570, Loss: 3.5211, Time: 8073.9s, Step: 13571, GPU: 4.9GB\n",
      "Epoch 1, Batch 13575, Loss: 5.3623, Time: 8077.3s, Step: 13576, GPU: 4.9GB\n",
      "Epoch 1, Batch 13580, Loss: 4.2893, Time: 8079.8s, Step: 13581, GPU: 4.9GB\n",
      "Epoch 1, Batch 13585, Loss: 4.4494, Time: 8083.2s, Step: 13586, GPU: 4.9GB\n",
      "Epoch 1, Batch 13590, Loss: 4.4633, Time: 8085.8s, Step: 13591, GPU: 4.9GB\n",
      "Epoch 1, Batch 13595, Loss: 3.7015, Time: 8089.2s, Step: 13596, GPU: 4.9GB\n",
      "Epoch 1, Batch 13600, Loss: 4.7710, Time: 8091.8s, Step: 13601, GPU: 4.9GB\n",
      "Epoch 1, Batch 13605, Loss: 3.9928, Time: 8095.2s, Step: 13606, GPU: 4.9GB\n",
      "Epoch 1, Batch 13610, Loss: 4.0812, Time: 8097.7s, Step: 13611, GPU: 4.9GB\n",
      "Epoch 1, Batch 13615, Loss: 4.3360, Time: 8101.1s, Step: 13616, GPU: 4.9GB\n",
      "Epoch 1, Batch 13620, Loss: 4.1910, Time: 8103.6s, Step: 13621, GPU: 4.9GB\n",
      "Epoch 1, Batch 13625, Loss: 4.3298, Time: 8107.0s, Step: 13626, GPU: 4.9GB\n",
      "Epoch 1, Batch 13630, Loss: 3.4168, Time: 8109.5s, Step: 13631, GPU: 4.9GB\n",
      "Epoch 1, Batch 13635, Loss: 4.3112, Time: 8113.0s, Step: 13636, GPU: 4.9GB\n",
      "Epoch 1, Batch 13640, Loss: 4.4606, Time: 8115.5s, Step: 13641, GPU: 4.9GB\n",
      "Epoch 1, Batch 13645, Loss: 3.9104, Time: 8118.9s, Step: 13646, GPU: 4.9GB\n",
      "Epoch 1, Batch 13650, Loss: 3.6911, Time: 8121.4s, Step: 13651, GPU: 4.9GB\n",
      "Epoch 1, Batch 13655, Loss: 4.3043, Time: 8124.8s, Step: 13656, GPU: 4.9GB\n",
      "Epoch 1, Batch 13660, Loss: 3.8235, Time: 8127.3s, Step: 13661, GPU: 4.9GB\n",
      "Epoch 1, Batch 13665, Loss: 4.2865, Time: 8130.7s, Step: 13666, GPU: 4.9GB\n",
      "Epoch 1, Batch 13670, Loss: 5.1201, Time: 8133.3s, Step: 13671, GPU: 4.9GB\n",
      "Epoch 1, Batch 13675, Loss: 3.3245, Time: 8136.7s, Step: 13676, GPU: 4.9GB\n",
      "Epoch 1, Batch 13680, Loss: 4.0787, Time: 8139.2s, Step: 13681, GPU: 4.9GB\n",
      "Epoch 1, Batch 13685, Loss: 4.9727, Time: 8142.6s, Step: 13686, GPU: 4.9GB\n",
      "Epoch 1, Batch 13690, Loss: 4.6502, Time: 8145.1s, Step: 13691, GPU: 4.9GB\n",
      "Epoch 1, Batch 13695, Loss: 4.8577, Time: 8148.5s, Step: 13696, GPU: 4.9GB\n",
      "Epoch 1, Batch 13700, Loss: 3.5601, Time: 8151.0s, Step: 13701, GPU: 4.9GB\n",
      "Epoch 1, Batch 13705, Loss: 3.4142, Time: 8154.4s, Step: 13706, GPU: 4.9GB\n",
      "Epoch 1, Batch 13710, Loss: 4.2927, Time: 8156.9s, Step: 13711, GPU: 4.9GB\n",
      "Epoch 1, Batch 13715, Loss: 3.5770, Time: 8160.3s, Step: 13716, GPU: 4.9GB\n",
      "Epoch 1, Batch 13720, Loss: 4.2394, Time: 8162.8s, Step: 13721, GPU: 4.9GB\n",
      "Epoch 1, Batch 13725, Loss: 4.0508, Time: 8166.2s, Step: 13726, GPU: 4.9GB\n",
      "Epoch 1, Batch 13730, Loss: 4.3510, Time: 8168.8s, Step: 13731, GPU: 4.9GB\n",
      "Epoch 1, Batch 13735, Loss: 4.5983, Time: 8172.2s, Step: 13736, GPU: 4.9GB\n",
      "Epoch 1, Batch 13740, Loss: 4.0921, Time: 8174.7s, Step: 13741, GPU: 4.9GB\n",
      "Epoch 1, Batch 13745, Loss: 4.1318, Time: 8178.1s, Step: 13746, GPU: 4.9GB\n",
      "Epoch 1, Batch 13750, Loss: 3.8853, Time: 8180.6s, Step: 13751, GPU: 4.9GB\n",
      "Epoch 1, Batch 13755, Loss: 3.5855, Time: 8184.1s, Step: 13756, GPU: 4.9GB\n",
      "Epoch 1, Batch 13760, Loss: 4.1096, Time: 8186.6s, Step: 13761, GPU: 4.9GB\n",
      "Epoch 1, Batch 13765, Loss: 3.3443, Time: 8190.0s, Step: 13766, GPU: 4.9GB\n",
      "Epoch 1, Batch 13770, Loss: 4.7538, Time: 8192.5s, Step: 13771, GPU: 4.9GB\n",
      "Epoch 1, Batch 13775, Loss: 4.4112, Time: 8196.0s, Step: 13776, GPU: 4.9GB\n",
      "Epoch 1, Batch 13780, Loss: 4.0629, Time: 8198.5s, Step: 13781, GPU: 4.9GB\n",
      "Epoch 1, Batch 13785, Loss: 4.1873, Time: 8201.9s, Step: 13786, GPU: 4.9GB\n",
      "Epoch 1, Batch 13790, Loss: 3.7865, Time: 8204.4s, Step: 13791, GPU: 4.9GB\n",
      "Epoch 1, Batch 13795, Loss: 4.5904, Time: 8207.8s, Step: 13796, GPU: 4.9GB\n",
      "Epoch 1, Batch 13800, Loss: 4.1895, Time: 8210.4s, Step: 13801, GPU: 4.9GB\n",
      "Epoch 1, Batch 13805, Loss: 4.4312, Time: 8213.9s, Step: 13806, GPU: 4.9GB\n",
      "Epoch 1, Batch 13810, Loss: 4.0955, Time: 8216.4s, Step: 13811, GPU: 4.9GB\n",
      "Epoch 1, Batch 13815, Loss: 4.5067, Time: 8219.8s, Step: 13816, GPU: 4.9GB\n",
      "Epoch 1, Batch 13820, Loss: 4.1049, Time: 8222.3s, Step: 13821, GPU: 4.9GB\n",
      "Epoch 1, Batch 13825, Loss: 4.1510, Time: 8225.7s, Step: 13826, GPU: 4.9GB\n",
      "Epoch 1, Batch 13830, Loss: 4.3968, Time: 8228.3s, Step: 13831, GPU: 4.9GB\n",
      "Epoch 1, Batch 13835, Loss: 4.0461, Time: 8231.7s, Step: 13836, GPU: 4.9GB\n",
      "Epoch 1, Batch 13840, Loss: 4.9474, Time: 8234.2s, Step: 13841, GPU: 4.9GB\n",
      "Epoch 1, Batch 13845, Loss: 3.9659, Time: 8237.7s, Step: 13846, GPU: 4.9GB\n",
      "Epoch 1, Batch 13850, Loss: 3.1501, Time: 8240.2s, Step: 13851, GPU: 4.9GB\n",
      "Epoch 1, Batch 13855, Loss: 4.3779, Time: 8243.6s, Step: 13856, GPU: 4.9GB\n",
      "Epoch 1, Batch 13860, Loss: 4.9462, Time: 8246.1s, Step: 13861, GPU: 4.9GB\n",
      "Epoch 1, Batch 13865, Loss: 4.0681, Time: 8249.5s, Step: 13866, GPU: 4.9GB\n",
      "Epoch 1, Batch 13870, Loss: 4.6441, Time: 8252.0s, Step: 13871, GPU: 4.9GB\n",
      "Epoch 1, Batch 13875, Loss: 3.7301, Time: 8255.4s, Step: 13876, GPU: 4.9GB\n",
      "Epoch 1, Batch 13880, Loss: 4.3764, Time: 8257.9s, Step: 13881, GPU: 4.9GB\n",
      "Epoch 1, Batch 13885, Loss: 4.2153, Time: 8261.4s, Step: 13886, GPU: 4.9GB\n",
      "Epoch 1, Batch 13890, Loss: 4.1271, Time: 8263.9s, Step: 13891, GPU: 4.9GB\n",
      "Epoch 1, Batch 13895, Loss: 4.3706, Time: 8267.3s, Step: 13896, GPU: 4.9GB\n",
      "Epoch 1, Batch 13900, Loss: 4.1287, Time: 8269.8s, Step: 13901, GPU: 4.9GB\n",
      "Epoch 1, Batch 13905, Loss: 4.0736, Time: 8273.2s, Step: 13906, GPU: 4.9GB\n",
      "Epoch 1, Batch 13910, Loss: 2.8774, Time: 8275.7s, Step: 13911, GPU: 4.9GB\n",
      "Epoch 1, Batch 13915, Loss: 4.6581, Time: 8279.2s, Step: 13916, GPU: 4.9GB\n",
      "Epoch 1, Batch 13920, Loss: 3.8421, Time: 8281.7s, Step: 13921, GPU: 4.9GB\n",
      "Epoch 1, Batch 13925, Loss: 4.5042, Time: 8285.1s, Step: 13926, GPU: 4.9GB\n",
      "Epoch 1, Batch 13930, Loss: 4.2827, Time: 8287.6s, Step: 13931, GPU: 4.9GB\n",
      "Epoch 1, Batch 13935, Loss: 4.0312, Time: 8291.1s, Step: 13936, GPU: 4.9GB\n",
      "Epoch 1, Batch 13940, Loss: 4.9188, Time: 8293.6s, Step: 13941, GPU: 4.9GB\n",
      "Epoch 1, Batch 13945, Loss: 3.7088, Time: 8297.0s, Step: 13946, GPU: 4.9GB\n",
      "Epoch 1, Batch 13950, Loss: 4.2643, Time: 8299.5s, Step: 13951, GPU: 4.9GB\n",
      "Epoch 1, Batch 13955, Loss: 4.2868, Time: 8302.9s, Step: 13956, GPU: 4.9GB\n",
      "Epoch 1, Batch 13960, Loss: 3.6961, Time: 8305.4s, Step: 13961, GPU: 4.9GB\n",
      "Epoch 1, Batch 13965, Loss: 4.5638, Time: 8308.8s, Step: 13966, GPU: 4.9GB\n",
      "Epoch 1, Batch 13970, Loss: 4.5965, Time: 8311.3s, Step: 13971, GPU: 4.9GB\n",
      "Epoch 1, Batch 13975, Loss: 4.6322, Time: 8314.7s, Step: 13976, GPU: 4.9GB\n",
      "Epoch 1, Batch 13980, Loss: 3.8586, Time: 8317.2s, Step: 13981, GPU: 4.9GB\n",
      "Epoch 1, Batch 13985, Loss: 4.4046, Time: 8320.6s, Step: 13986, GPU: 4.9GB\n",
      "Epoch 1, Batch 13990, Loss: 3.9104, Time: 8323.1s, Step: 13991, GPU: 4.9GB\n",
      "Epoch 1, Batch 13995, Loss: 3.5469, Time: 8326.5s, Step: 13996, GPU: 4.9GB\n",
      "Epoch 1, Batch 14000, Loss: 4.6715, Time: 8329.1s, Step: 14001, GPU: 4.9GB\n",
      "Epoch 1, Batch 14005, Loss: 3.9040, Time: 8332.5s, Step: 14006, GPU: 4.9GB\n",
      "Epoch 1, Batch 14010, Loss: 4.9598, Time: 8335.0s, Step: 14011, GPU: 4.9GB\n",
      "Epoch 1, Batch 14015, Loss: 4.4468, Time: 8338.4s, Step: 14016, GPU: 4.9GB\n",
      "Epoch 1, Batch 14020, Loss: 4.4456, Time: 8340.9s, Step: 14021, GPU: 4.9GB\n",
      "Epoch 1, Batch 14025, Loss: 4.3254, Time: 8344.3s, Step: 14026, GPU: 4.9GB\n",
      "Epoch 1, Batch 14030, Loss: 3.9159, Time: 8346.8s, Step: 14031, GPU: 4.9GB\n",
      "Epoch 1, Batch 14035, Loss: 4.5356, Time: 8350.1s, Step: 14036, GPU: 4.9GB\n",
      "Epoch 1, Batch 14040, Loss: 3.9971, Time: 8352.7s, Step: 14041, GPU: 4.9GB\n",
      "Epoch 1, Batch 14045, Loss: 4.3842, Time: 8356.1s, Step: 14046, GPU: 4.9GB\n",
      "Epoch 1, Batch 14050, Loss: 4.1081, Time: 8358.6s, Step: 14051, GPU: 4.9GB\n",
      "Epoch 1, Batch 14055, Loss: 3.6144, Time: 8362.1s, Step: 14056, GPU: 4.9GB\n",
      "Epoch 1, Batch 14060, Loss: 4.0152, Time: 8364.6s, Step: 14061, GPU: 4.9GB\n",
      "Epoch 1, Batch 14065, Loss: 5.1905, Time: 8368.0s, Step: 14066, GPU: 4.9GB\n",
      "Epoch 1, Batch 14070, Loss: 4.1021, Time: 8370.5s, Step: 14071, GPU: 4.9GB\n",
      "Epoch 1, Batch 14075, Loss: 4.4494, Time: 8373.9s, Step: 14076, GPU: 4.9GB\n",
      "Epoch 1, Batch 14080, Loss: 5.1903, Time: 8376.4s, Step: 14081, GPU: 4.9GB\n",
      "Epoch 1, Batch 14085, Loss: 3.2859, Time: 8379.8s, Step: 14086, GPU: 4.9GB\n",
      "Epoch 1, Batch 14090, Loss: 3.9389, Time: 8382.3s, Step: 14091, GPU: 4.9GB\n",
      "Epoch 1, Batch 14095, Loss: 4.9062, Time: 8385.8s, Step: 14096, GPU: 4.9GB\n",
      "Epoch 1, Batch 14100, Loss: 2.6954, Time: 8388.3s, Step: 14101, GPU: 4.9GB\n",
      "Epoch 1, Batch 14105, Loss: 4.9710, Time: 8391.7s, Step: 14106, GPU: 4.9GB\n",
      "Epoch 1, Batch 14110, Loss: 4.7201, Time: 8394.2s, Step: 14111, GPU: 4.9GB\n",
      "Epoch 1, Batch 14115, Loss: 3.5204, Time: 8397.6s, Step: 14116, GPU: 4.9GB\n",
      "Epoch 1, Batch 14120, Loss: 4.0023, Time: 8400.2s, Step: 14121, GPU: 4.9GB\n",
      "Epoch 1, Batch 14125, Loss: 3.9927, Time: 8403.6s, Step: 14126, GPU: 4.9GB\n",
      "Epoch 1, Batch 14130, Loss: 3.9789, Time: 8406.1s, Step: 14131, GPU: 4.9GB\n",
      "Epoch 1, Batch 14135, Loss: 4.5768, Time: 8409.5s, Step: 14136, GPU: 4.9GB\n",
      "Epoch 1, Batch 14140, Loss: 4.1917, Time: 8412.0s, Step: 14141, GPU: 4.9GB\n",
      "Epoch 1, Batch 14145, Loss: 3.7993, Time: 8415.4s, Step: 14146, GPU: 4.9GB\n",
      "Epoch 1, Batch 14150, Loss: 4.0962, Time: 8417.9s, Step: 14151, GPU: 4.9GB\n",
      "Epoch 1, Batch 14155, Loss: 3.8982, Time: 8421.3s, Step: 14156, GPU: 4.9GB\n",
      "Epoch 1, Batch 14160, Loss: 5.2615, Time: 8423.9s, Step: 14161, GPU: 4.9GB\n",
      "Epoch 1, Batch 14165, Loss: 3.6555, Time: 8427.2s, Step: 14166, GPU: 4.9GB\n",
      "Epoch 1, Batch 14170, Loss: 4.1183, Time: 8429.8s, Step: 14171, GPU: 4.9GB\n",
      "Epoch 1, Batch 14175, Loss: 3.9144, Time: 8433.2s, Step: 14176, GPU: 4.9GB\n",
      "Epoch 1, Batch 14180, Loss: 4.3897, Time: 8435.8s, Step: 14181, GPU: 4.9GB\n",
      "Epoch 1, Batch 14185, Loss: 4.0742, Time: 8439.2s, Step: 14186, GPU: 4.9GB\n",
      "Epoch 1, Batch 14190, Loss: 4.6421, Time: 8441.7s, Step: 14191, GPU: 4.9GB\n",
      "Epoch 1, Batch 14195, Loss: 4.9234, Time: 8445.1s, Step: 14196, GPU: 4.9GB\n",
      "Epoch 1, Batch 14200, Loss: 5.0010, Time: 8447.7s, Step: 14201, GPU: 4.9GB\n",
      "Epoch 1, Batch 14205, Loss: 3.6769, Time: 8451.1s, Step: 14206, GPU: 4.9GB\n",
      "Epoch 1, Batch 14210, Loss: 3.7873, Time: 8453.7s, Step: 14211, GPU: 4.9GB\n",
      "Epoch 1, Batch 14215, Loss: 4.0944, Time: 8457.1s, Step: 14216, GPU: 4.9GB\n",
      "Epoch 1, Batch 14220, Loss: 4.3215, Time: 8459.7s, Step: 14221, GPU: 4.9GB\n",
      "Epoch 1, Batch 14225, Loss: 3.9805, Time: 8463.1s, Step: 14226, GPU: 4.9GB\n",
      "Epoch 1, Batch 14230, Loss: 4.5522, Time: 8465.6s, Step: 14231, GPU: 4.9GB\n",
      "Epoch 1, Batch 14235, Loss: 4.5875, Time: 8469.1s, Step: 14236, GPU: 4.9GB\n",
      "Epoch 1, Batch 14240, Loss: 5.6884, Time: 8471.6s, Step: 14241, GPU: 4.9GB\n",
      "Epoch 1, Batch 14245, Loss: 5.2125, Time: 8475.1s, Step: 14246, GPU: 4.9GB\n",
      "Epoch 1, Batch 14250, Loss: 5.0181, Time: 8477.6s, Step: 14251, GPU: 4.9GB\n",
      "Epoch 1, Batch 14255, Loss: 4.2907, Time: 8481.0s, Step: 14256, GPU: 4.9GB\n",
      "Epoch 1, Batch 14260, Loss: 4.6737, Time: 8483.5s, Step: 14261, GPU: 4.9GB\n",
      "Epoch 1, Batch 14265, Loss: 3.4011, Time: 8486.9s, Step: 14266, GPU: 4.9GB\n",
      "Epoch 1, Batch 14270, Loss: 3.6786, Time: 8489.5s, Step: 14271, GPU: 4.9GB\n",
      "Epoch 1, Batch 14275, Loss: 4.0350, Time: 8492.9s, Step: 14276, GPU: 4.9GB\n",
      "Epoch 1, Batch 14280, Loss: 4.3137, Time: 8495.4s, Step: 14281, GPU: 4.9GB\n",
      "Epoch 1, Batch 14285, Loss: 4.1061, Time: 8498.8s, Step: 14286, GPU: 4.9GB\n",
      "Epoch 1, Batch 14290, Loss: 5.2262, Time: 8501.4s, Step: 14291, GPU: 4.9GB\n",
      "Epoch 1, Batch 14295, Loss: 4.3678, Time: 8504.9s, Step: 14296, GPU: 4.9GB\n",
      "Epoch 1, Batch 14300, Loss: 4.1706, Time: 8507.5s, Step: 14301, GPU: 4.9GB\n",
      "Epoch 1, Batch 14305, Loss: 4.1057, Time: 8510.9s, Step: 14306, GPU: 4.9GB\n",
      "Epoch 1, Batch 14310, Loss: 3.8909, Time: 8513.4s, Step: 14311, GPU: 4.9GB\n",
      "Epoch 1, Batch 14315, Loss: 3.8474, Time: 8516.9s, Step: 14316, GPU: 4.9GB\n",
      "Epoch 1, Batch 14320, Loss: 3.9536, Time: 8519.4s, Step: 14321, GPU: 4.9GB\n",
      "Epoch 1, Batch 14325, Loss: 3.0800, Time: 8522.8s, Step: 14326, GPU: 4.9GB\n",
      "Epoch 1, Batch 14330, Loss: 3.7994, Time: 8525.3s, Step: 14331, GPU: 4.9GB\n",
      "Epoch 1, Batch 14335, Loss: 4.5682, Time: 8528.8s, Step: 14336, GPU: 4.9GB\n",
      "Epoch 1, Batch 14340, Loss: 3.3904, Time: 8531.3s, Step: 14341, GPU: 4.9GB\n",
      "Epoch 1, Batch 14345, Loss: 4.2539, Time: 8534.7s, Step: 14346, GPU: 4.9GB\n",
      "Epoch 1, Batch 14350, Loss: 4.9503, Time: 8537.2s, Step: 14351, GPU: 4.9GB\n",
      "Epoch 1, Batch 14355, Loss: 4.4347, Time: 8540.6s, Step: 14356, GPU: 4.9GB\n",
      "Epoch 1, Batch 14360, Loss: 4.3759, Time: 8543.1s, Step: 14361, GPU: 4.9GB\n",
      "Epoch 1, Batch 14365, Loss: 4.1168, Time: 8546.5s, Step: 14366, GPU: 4.9GB\n",
      "Epoch 1, Batch 14370, Loss: 3.8376, Time: 8549.1s, Step: 14371, GPU: 4.9GB\n",
      "Epoch 1, Batch 14375, Loss: 4.2087, Time: 8552.5s, Step: 14376, GPU: 4.9GB\n",
      "Epoch 1, Batch 14380, Loss: 4.9573, Time: 8555.0s, Step: 14381, GPU: 4.9GB\n",
      "Epoch 1, Batch 14385, Loss: 4.1519, Time: 8558.4s, Step: 14386, GPU: 4.9GB\n",
      "Epoch 1, Batch 14390, Loss: 4.0151, Time: 8560.9s, Step: 14391, GPU: 4.9GB\n",
      "Epoch 1, Batch 14395, Loss: 4.8948, Time: 8564.4s, Step: 14396, GPU: 4.9GB\n",
      "Epoch 1, Batch 14400, Loss: 4.2372, Time: 8566.9s, Step: 14401, GPU: 4.9GB\n",
      "Epoch 1, Batch 14405, Loss: 5.2207, Time: 8570.3s, Step: 14406, GPU: 4.9GB\n",
      "Epoch 1, Batch 14410, Loss: 3.2428, Time: 8572.9s, Step: 14411, GPU: 4.9GB\n",
      "Epoch 1, Batch 14415, Loss: 4.1527, Time: 8576.3s, Step: 14416, GPU: 4.9GB\n",
      "Epoch 1, Batch 14420, Loss: 4.2282, Time: 8578.8s, Step: 14421, GPU: 4.9GB\n",
      "Epoch 1, Batch 14425, Loss: 4.2767, Time: 8582.2s, Step: 14426, GPU: 4.9GB\n",
      "Epoch 1, Batch 14430, Loss: 3.9294, Time: 8584.7s, Step: 14431, GPU: 4.9GB\n",
      "Epoch 1, Batch 14435, Loss: 4.4879, Time: 8588.1s, Step: 14436, GPU: 4.9GB\n",
      "Epoch 1, Batch 14440, Loss: 4.4479, Time: 8590.6s, Step: 14441, GPU: 4.9GB\n",
      "Epoch 1, Batch 14445, Loss: 3.7559, Time: 8594.1s, Step: 14446, GPU: 4.9GB\n",
      "Epoch 1, Batch 14450, Loss: 3.6609, Time: 8596.6s, Step: 14451, GPU: 4.9GB\n",
      "Epoch 1, Batch 14455, Loss: 4.3277, Time: 8600.0s, Step: 14456, GPU: 4.9GB\n",
      "Epoch 1, Batch 14460, Loss: 4.1464, Time: 8602.6s, Step: 14461, GPU: 4.9GB\n",
      "Epoch 1, Batch 14465, Loss: 5.3770, Time: 8606.0s, Step: 14466, GPU: 4.9GB\n",
      "Epoch 1, Batch 14470, Loss: 4.5549, Time: 8608.5s, Step: 14471, GPU: 4.9GB\n",
      "Epoch 1, Batch 14475, Loss: 4.4518, Time: 8612.0s, Step: 14476, GPU: 4.9GB\n",
      "Epoch 1, Batch 14480, Loss: 4.4612, Time: 8614.5s, Step: 14481, GPU: 4.9GB\n",
      "Epoch 1, Batch 14485, Loss: 4.3534, Time: 8617.9s, Step: 14486, GPU: 4.9GB\n",
      "Epoch 1, Batch 14490, Loss: 3.8411, Time: 8620.4s, Step: 14491, GPU: 4.9GB\n",
      "Epoch 1, Batch 14495, Loss: 4.4418, Time: 8623.8s, Step: 14496, GPU: 4.9GB\n",
      "Epoch 1, Batch 14500, Loss: 4.3926, Time: 8626.3s, Step: 14501, GPU: 4.9GB\n",
      "Epoch 1, Batch 14505, Loss: 3.2825, Time: 8629.7s, Step: 14506, GPU: 4.9GB\n",
      "Epoch 1, Batch 14510, Loss: 3.9553, Time: 8632.3s, Step: 14511, GPU: 4.9GB\n",
      "Epoch 1, Batch 14515, Loss: 3.8524, Time: 8635.7s, Step: 14516, GPU: 4.9GB\n",
      "Epoch 1, Batch 14520, Loss: 5.0294, Time: 8638.2s, Step: 14521, GPU: 4.9GB\n",
      "Epoch 1, Batch 14525, Loss: 4.4952, Time: 8641.6s, Step: 14526, GPU: 4.9GB\n",
      "Epoch 1, Batch 14530, Loss: 4.7590, Time: 8644.2s, Step: 14531, GPU: 4.9GB\n",
      "Epoch 1, Batch 14535, Loss: 3.7274, Time: 8647.6s, Step: 14536, GPU: 4.9GB\n",
      "Epoch 1, Batch 14540, Loss: 4.7039, Time: 8650.1s, Step: 14541, GPU: 4.9GB\n",
      "Epoch 1, Batch 14545, Loss: 4.5402, Time: 8653.5s, Step: 14546, GPU: 4.9GB\n",
      "Epoch 1, Batch 14550, Loss: 4.0103, Time: 8656.0s, Step: 14551, GPU: 4.9GB\n",
      "Epoch 1, Batch 14555, Loss: 4.7494, Time: 8659.4s, Step: 14556, GPU: 4.9GB\n",
      "Epoch 1, Batch 14560, Loss: 4.4682, Time: 8661.9s, Step: 14561, GPU: 4.9GB\n",
      "Epoch 1, Batch 14565, Loss: 3.7995, Time: 8665.3s, Step: 14566, GPU: 4.9GB\n",
      "Epoch 1, Batch 14570, Loss: 4.4904, Time: 8667.9s, Step: 14571, GPU: 4.9GB\n",
      "Epoch 1, Batch 14575, Loss: 4.2163, Time: 8671.3s, Step: 14576, GPU: 4.9GB\n",
      "Epoch 1, Batch 14580, Loss: 4.3070, Time: 8673.8s, Step: 14581, GPU: 4.9GB\n",
      "Epoch 1, Batch 14585, Loss: 5.5269, Time: 8677.2s, Step: 14586, GPU: 4.9GB\n",
      "Epoch 1, Batch 14590, Loss: 4.1000, Time: 8679.7s, Step: 14591, GPU: 4.9GB\n",
      "Epoch 1, Batch 14595, Loss: 4.0140, Time: 8683.1s, Step: 14596, GPU: 4.9GB\n",
      "Epoch 1, Batch 14600, Loss: 3.2363, Time: 8685.7s, Step: 14601, GPU: 4.9GB\n",
      "Epoch 1, Batch 14605, Loss: 4.0565, Time: 8689.2s, Step: 14606, GPU: 4.9GB\n",
      "Epoch 1, Batch 14610, Loss: 4.6199, Time: 8691.7s, Step: 14611, GPU: 4.9GB\n",
      "Epoch 1, Batch 14615, Loss: 3.9172, Time: 8695.1s, Step: 14616, GPU: 4.9GB\n",
      "Epoch 1, Batch 14620, Loss: 3.8713, Time: 8697.6s, Step: 14621, GPU: 4.9GB\n",
      "Epoch 1, Batch 14625, Loss: 4.0369, Time: 8701.0s, Step: 14626, GPU: 4.9GB\n",
      "Epoch 1, Batch 14630, Loss: 3.8353, Time: 8703.6s, Step: 14631, GPU: 4.9GB\n",
      "Epoch 1, Batch 14635, Loss: 4.3688, Time: 8707.0s, Step: 14636, GPU: 4.9GB\n",
      "Epoch 1, Batch 14640, Loss: 3.6760, Time: 8709.5s, Step: 14641, GPU: 4.9GB\n",
      "Epoch 1, Batch 14645, Loss: 4.2752, Time: 8712.9s, Step: 14646, GPU: 4.9GB\n",
      "Epoch 1, Batch 14650, Loss: 4.3157, Time: 8715.4s, Step: 14651, GPU: 4.9GB\n",
      "Epoch 1, Batch 14655, Loss: 4.6304, Time: 8718.9s, Step: 14656, GPU: 4.9GB\n",
      "Epoch 1, Batch 14660, Loss: 4.3440, Time: 8721.7s, Step: 14661, GPU: 4.9GB\n",
      "Epoch 1, Batch 14665, Loss: 3.7443, Time: 8725.1s, Step: 14666, GPU: 4.9GB\n",
      "Epoch 1, Batch 14670, Loss: 4.7094, Time: 8727.6s, Step: 14671, GPU: 4.9GB\n",
      "Epoch 1, Batch 14675, Loss: 3.1560, Time: 8731.0s, Step: 14676, GPU: 4.9GB\n",
      "Epoch 1, Batch 14680, Loss: 4.1707, Time: 8733.6s, Step: 14681, GPU: 4.9GB\n",
      "Epoch 1, Batch 14685, Loss: 4.6492, Time: 8737.0s, Step: 14686, GPU: 4.9GB\n",
      "Epoch 1, Batch 14690, Loss: 5.3127, Time: 8739.5s, Step: 14691, GPU: 4.9GB\n",
      "Epoch 1, Batch 14695, Loss: 4.0003, Time: 8742.9s, Step: 14696, GPU: 4.9GB\n",
      "Epoch 1, Batch 14700, Loss: 3.9046, Time: 8745.4s, Step: 14701, GPU: 4.9GB\n",
      "Epoch 1, Batch 14705, Loss: 4.3219, Time: 8748.9s, Step: 14706, GPU: 4.9GB\n",
      "Epoch 1, Batch 14710, Loss: 4.1908, Time: 8751.4s, Step: 14711, GPU: 4.9GB\n",
      "Epoch 1, Batch 14715, Loss: 4.4271, Time: 8754.8s, Step: 14716, GPU: 4.9GB\n",
      "Epoch 1, Batch 14720, Loss: 3.8480, Time: 8757.3s, Step: 14721, GPU: 4.9GB\n",
      "Epoch 1, Batch 14725, Loss: 4.6293, Time: 8760.8s, Step: 14726, GPU: 4.9GB\n",
      "Epoch 1, Batch 14730, Loss: 3.8960, Time: 8763.3s, Step: 14731, GPU: 4.9GB\n",
      "Epoch 1, Batch 14735, Loss: 4.8307, Time: 8766.7s, Step: 14736, GPU: 4.9GB\n",
      "Epoch 1, Batch 14740, Loss: 4.9876, Time: 8769.3s, Step: 14741, GPU: 4.9GB\n",
      "Epoch 1, Batch 14745, Loss: 4.4165, Time: 8772.7s, Step: 14746, GPU: 4.9GB\n",
      "Epoch 1, Batch 14750, Loss: 4.8133, Time: 8775.2s, Step: 14751, GPU: 4.9GB\n",
      "Epoch 1, Batch 14755, Loss: 4.2693, Time: 8778.6s, Step: 14756, GPU: 4.9GB\n",
      "Epoch 1, Batch 14760, Loss: 4.0961, Time: 8781.1s, Step: 14761, GPU: 4.9GB\n",
      "Epoch 1, Batch 14765, Loss: 3.4667, Time: 8784.5s, Step: 14766, GPU: 4.9GB\n",
      "Epoch 1, Batch 14770, Loss: 3.8871, Time: 8787.0s, Step: 14771, GPU: 4.9GB\n",
      "Epoch 1, Batch 14775, Loss: 3.4806, Time: 8790.5s, Step: 14776, GPU: 4.9GB\n",
      "Epoch 1, Batch 14780, Loss: 4.3534, Time: 8793.0s, Step: 14781, GPU: 4.9GB\n",
      "Epoch 1, Batch 14785, Loss: 4.2812, Time: 8796.4s, Step: 14786, GPU: 4.9GB\n",
      "Epoch 1, Batch 14790, Loss: 4.0303, Time: 8798.9s, Step: 14791, GPU: 4.9GB\n",
      "Epoch 1, Batch 14795, Loss: 4.1274, Time: 8802.4s, Step: 14796, GPU: 4.9GB\n",
      "Epoch 1, Batch 14800, Loss: 4.1248, Time: 8805.0s, Step: 14801, GPU: 4.9GB\n",
      "Epoch 1, Batch 14805, Loss: 4.6944, Time: 8808.4s, Step: 14806, GPU: 4.9GB\n",
      "Epoch 1, Batch 14810, Loss: 3.9096, Time: 8810.9s, Step: 14811, GPU: 4.9GB\n",
      "Epoch 1, Batch 14815, Loss: 4.3577, Time: 8814.4s, Step: 14816, GPU: 4.9GB\n",
      "Epoch 1, Batch 14820, Loss: 4.0212, Time: 8816.9s, Step: 14821, GPU: 4.9GB\n",
      "Epoch 1, Batch 14825, Loss: 3.9158, Time: 8820.3s, Step: 14826, GPU: 4.9GB\n",
      "Epoch 1, Batch 14830, Loss: 3.6883, Time: 8822.8s, Step: 14831, GPU: 4.9GB\n",
      "Epoch 1, Batch 14835, Loss: 4.0539, Time: 8826.2s, Step: 14836, GPU: 4.9GB\n",
      "Epoch 1, Batch 14840, Loss: 4.8659, Time: 8828.7s, Step: 14841, GPU: 4.9GB\n",
      "Epoch 1, Batch 14845, Loss: 3.3977, Time: 8832.1s, Step: 14846, GPU: 4.9GB\n",
      "Epoch 1, Batch 14850, Loss: 4.9451, Time: 8834.7s, Step: 14851, GPU: 4.9GB\n",
      "Epoch 1, Batch 14855, Loss: 4.0625, Time: 8838.1s, Step: 14856, GPU: 4.9GB\n",
      "Epoch 1, Batch 14860, Loss: 4.1237, Time: 8840.6s, Step: 14861, GPU: 4.9GB\n",
      "Epoch 1, Batch 14865, Loss: 3.1196, Time: 8844.1s, Step: 14866, GPU: 4.9GB\n",
      "Epoch 1, Batch 14870, Loss: 4.0211, Time: 8846.6s, Step: 14871, GPU: 4.9GB\n",
      "Epoch 1, Batch 14875, Loss: 4.2329, Time: 8850.0s, Step: 14876, GPU: 4.9GB\n",
      "Epoch 1, Batch 14880, Loss: 4.4472, Time: 8852.5s, Step: 14881, GPU: 4.9GB\n",
      "Epoch 1, Batch 14885, Loss: 5.2940, Time: 8855.9s, Step: 14886, GPU: 4.9GB\n",
      "Epoch 1, Batch 14890, Loss: 4.8190, Time: 8858.4s, Step: 14891, GPU: 4.9GB\n",
      "Epoch 1, Batch 14895, Loss: 3.4765, Time: 8861.9s, Step: 14896, GPU: 4.9GB\n",
      "Epoch 1, Batch 14900, Loss: 3.8266, Time: 8864.4s, Step: 14901, GPU: 4.9GB\n",
      "Epoch 1, Batch 14905, Loss: 4.3621, Time: 8867.8s, Step: 14906, GPU: 4.9GB\n",
      "Epoch 1, Batch 14910, Loss: 3.7908, Time: 8870.3s, Step: 14911, GPU: 4.9GB\n",
      "Epoch 1, Batch 14915, Loss: 4.9582, Time: 8873.8s, Step: 14916, GPU: 4.9GB\n",
      "Epoch 1, Batch 14920, Loss: 3.5670, Time: 8876.3s, Step: 14921, GPU: 4.9GB\n",
      "Epoch 1, Batch 14925, Loss: 2.8137, Time: 8879.7s, Step: 14926, GPU: 4.9GB\n",
      "Epoch 1, Batch 14930, Loss: 4.8885, Time: 8882.2s, Step: 14931, GPU: 4.9GB\n",
      "Epoch 1, Batch 14935, Loss: 4.2677, Time: 8885.7s, Step: 14936, GPU: 4.9GB\n",
      "Epoch 1, Batch 14940, Loss: 4.1832, Time: 8888.2s, Step: 14941, GPU: 4.9GB\n",
      "Epoch 1, Batch 14945, Loss: 3.4114, Time: 8891.6s, Step: 14946, GPU: 4.9GB\n",
      "Epoch 1, Batch 14950, Loss: 4.0017, Time: 8894.2s, Step: 14951, GPU: 4.9GB\n",
      "Epoch 1, Batch 14955, Loss: 3.7216, Time: 8897.7s, Step: 14956, GPU: 4.9GB\n",
      "Epoch 1, Batch 14960, Loss: 4.3973, Time: 8900.3s, Step: 14961, GPU: 4.9GB\n",
      "Epoch 1, Batch 14965, Loss: 4.3808, Time: 8903.7s, Step: 14966, GPU: 4.9GB\n",
      "Epoch 1, Batch 14970, Loss: 4.3933, Time: 8906.2s, Step: 14971, GPU: 4.9GB\n",
      "Epoch 1, Batch 14975, Loss: 4.3531, Time: 8909.6s, Step: 14976, GPU: 4.9GB\n",
      "Epoch 1, Batch 14980, Loss: 4.2894, Time: 8912.2s, Step: 14981, GPU: 4.9GB\n",
      "Epoch 1, Batch 14985, Loss: 4.0917, Time: 8915.6s, Step: 14986, GPU: 4.9GB\n",
      "Epoch 1, Batch 14990, Loss: 4.6554, Time: 8918.1s, Step: 14991, GPU: 4.9GB\n",
      "Epoch 1, Batch 14995, Loss: 4.0340, Time: 8921.5s, Step: 14996, GPU: 4.9GB\n",
      "Epoch 1, Batch 15000, Loss: 4.2198, Time: 8924.1s, Step: 15001, GPU: 4.9GB\n",
      "Epoch 1, Batch 15005, Loss: 4.6101, Time: 8927.6s, Step: 15006, GPU: 4.9GB\n",
      "Epoch 1, Batch 15010, Loss: 4.2196, Time: 8930.1s, Step: 15011, GPU: 4.9GB\n",
      "Epoch 1, Batch 15015, Loss: 4.8275, Time: 8933.5s, Step: 15016, GPU: 4.9GB\n",
      "Epoch 1, Batch 15020, Loss: 4.9265, Time: 8936.0s, Step: 15021, GPU: 4.9GB\n",
      "Epoch 1, Batch 15025, Loss: 4.2083, Time: 8939.4s, Step: 15026, GPU: 4.9GB\n",
      "Epoch 1, Batch 15030, Loss: 4.0064, Time: 8941.9s, Step: 15031, GPU: 4.9GB\n",
      "Epoch 1, Batch 15035, Loss: 3.6953, Time: 8945.3s, Step: 15036, GPU: 4.9GB\n",
      "Epoch 1, Batch 15040, Loss: 4.6431, Time: 8947.9s, Step: 15041, GPU: 4.9GB\n",
      "Epoch 1, Batch 15045, Loss: 4.4003, Time: 8951.3s, Step: 15046, GPU: 4.9GB\n",
      "Epoch 1, Batch 15050, Loss: 4.0361, Time: 8953.8s, Step: 15051, GPU: 4.9GB\n",
      "Epoch 1, Batch 15055, Loss: 3.2805, Time: 8957.2s, Step: 15056, GPU: 4.9GB\n",
      "Epoch 1, Batch 15060, Loss: 4.0700, Time: 8959.7s, Step: 15061, GPU: 4.9GB\n",
      "Epoch 1, Batch 15065, Loss: 3.7928, Time: 8963.1s, Step: 15066, GPU: 4.9GB\n",
      "Epoch 1, Batch 15070, Loss: 4.0377, Time: 8965.6s, Step: 15071, GPU: 4.9GB\n",
      "Epoch 1, Batch 15075, Loss: 4.5585, Time: 8969.0s, Step: 15076, GPU: 4.9GB\n",
      "Epoch 1, Batch 15080, Loss: 3.9890, Time: 8971.5s, Step: 15081, GPU: 4.9GB\n",
      "Epoch 1, Batch 15085, Loss: 4.5839, Time: 8974.9s, Step: 15086, GPU: 4.9GB\n",
      "Epoch 1, Batch 15090, Loss: 5.1077, Time: 8977.4s, Step: 15091, GPU: 4.9GB\n",
      "Epoch 1, Batch 15095, Loss: 3.6360, Time: 8980.8s, Step: 15096, GPU: 4.9GB\n",
      "Epoch 1, Batch 15100, Loss: 4.1779, Time: 8983.3s, Step: 15101, GPU: 4.9GB\n",
      "Epoch 1, Batch 15105, Loss: 4.3645, Time: 8987.0s, Step: 15106, GPU: 4.9GB\n",
      "Epoch 1, Batch 15110, Loss: 5.0479, Time: 8989.5s, Step: 15111, GPU: 4.9GB\n",
      "Epoch 1, Batch 15115, Loss: 4.8153, Time: 8992.9s, Step: 15116, GPU: 4.9GB\n",
      "Epoch 1, Batch 15120, Loss: 3.7340, Time: 8995.4s, Step: 15121, GPU: 4.9GB\n",
      "Epoch 1, Batch 15125, Loss: 4.8732, Time: 8998.8s, Step: 15126, GPU: 4.9GB\n",
      "Epoch 1, Batch 15130, Loss: 4.3405, Time: 9001.3s, Step: 15131, GPU: 4.9GB\n",
      "Epoch 1, Batch 15135, Loss: 4.0871, Time: 9004.8s, Step: 15136, GPU: 4.9GB\n",
      "Epoch 1, Batch 15140, Loss: 4.5363, Time: 9007.3s, Step: 15141, GPU: 4.9GB\n",
      "Epoch 1, Batch 15145, Loss: 4.2665, Time: 9010.7s, Step: 15146, GPU: 4.9GB\n",
      "Epoch 1, Batch 15150, Loss: 3.6287, Time: 9013.3s, Step: 15151, GPU: 4.9GB\n",
      "Epoch 1, Batch 15155, Loss: 4.5192, Time: 9016.7s, Step: 15156, GPU: 4.9GB\n",
      "Epoch 1, Batch 15160, Loss: 4.6860, Time: 9019.2s, Step: 15161, GPU: 4.9GB\n",
      "Epoch 1, Batch 15165, Loss: 3.8375, Time: 9022.6s, Step: 15166, GPU: 4.9GB\n",
      "Epoch 1, Batch 15170, Loss: 3.9399, Time: 9025.1s, Step: 15171, GPU: 4.9GB\n",
      "Epoch 1, Batch 15175, Loss: 3.9088, Time: 9028.5s, Step: 15176, GPU: 4.9GB\n",
      "Epoch 1, Batch 15180, Loss: 4.0435, Time: 9031.1s, Step: 15181, GPU: 4.9GB\n",
      "Epoch 1, Batch 15185, Loss: 4.8575, Time: 9034.6s, Step: 15186, GPU: 4.9GB\n",
      "Epoch 1, Batch 15190, Loss: 3.5584, Time: 9037.1s, Step: 15191, GPU: 4.9GB\n",
      "Epoch 1, Batch 15195, Loss: 3.7497, Time: 9040.6s, Step: 15196, GPU: 4.9GB\n",
      "Epoch 1, Batch 15200, Loss: 4.2443, Time: 9043.2s, Step: 15201, GPU: 4.9GB\n",
      "Epoch 1, Batch 15205, Loss: 4.1300, Time: 9046.7s, Step: 15206, GPU: 4.9GB\n",
      "Epoch 1, Batch 15210, Loss: 4.0465, Time: 9049.2s, Step: 15211, GPU: 4.9GB\n",
      "Epoch 1, Batch 15215, Loss: 4.2071, Time: 9052.6s, Step: 15216, GPU: 4.9GB\n",
      "Epoch 1, Batch 15220, Loss: 3.6646, Time: 9055.1s, Step: 15221, GPU: 4.9GB\n",
      "Epoch 1, Batch 15225, Loss: 3.9515, Time: 9058.6s, Step: 15226, GPU: 4.9GB\n",
      "Epoch 1, Batch 15230, Loss: 4.7398, Time: 9061.1s, Step: 15231, GPU: 4.9GB\n",
      "Epoch 1, Batch 15235, Loss: 4.5049, Time: 9064.5s, Step: 15236, GPU: 4.9GB\n",
      "Epoch 1, Batch 15240, Loss: 4.6381, Time: 9067.0s, Step: 15241, GPU: 4.9GB\n",
      "Epoch 1, Batch 15245, Loss: 4.9729, Time: 9070.4s, Step: 15246, GPU: 4.9GB\n",
      "Epoch 1, Batch 15250, Loss: 4.1146, Time: 9072.9s, Step: 15251, GPU: 4.9GB\n",
      "Epoch 1, Batch 15255, Loss: 4.8452, Time: 9076.3s, Step: 15256, GPU: 4.9GB\n",
      "Epoch 1, Batch 15260, Loss: 4.7212, Time: 9078.9s, Step: 15261, GPU: 4.9GB\n",
      "Epoch 1, Batch 15265, Loss: 4.7048, Time: 9082.3s, Step: 15266, GPU: 4.9GB\n",
      "Epoch 1, Batch 15270, Loss: 4.8343, Time: 9084.8s, Step: 15271, GPU: 4.9GB\n",
      "Epoch 1, Batch 15275, Loss: 3.7630, Time: 9088.2s, Step: 15276, GPU: 4.9GB\n",
      "Epoch 1, Batch 15280, Loss: 3.7351, Time: 9090.8s, Step: 15281, GPU: 4.9GB\n",
      "Epoch 1, Batch 15285, Loss: 4.0412, Time: 9094.2s, Step: 15286, GPU: 4.9GB\n",
      "Epoch 1, Batch 15290, Loss: 4.0058, Time: 9096.7s, Step: 15291, GPU: 4.9GB\n",
      "Epoch 1, Batch 15295, Loss: 3.1312, Time: 9100.1s, Step: 15296, GPU: 4.9GB\n",
      "Epoch 1, Batch 15300, Loss: 4.8736, Time: 9102.7s, Step: 15301, GPU: 4.9GB\n",
      "Epoch 1, Batch 15305, Loss: 3.6332, Time: 9106.1s, Step: 15306, GPU: 4.9GB\n",
      "Epoch 1, Batch 15310, Loss: 4.3277, Time: 9108.6s, Step: 15311, GPU: 4.9GB\n",
      "Epoch 1, Batch 15315, Loss: 3.8162, Time: 9112.0s, Step: 15316, GPU: 4.9GB\n",
      "Epoch 1, Batch 15320, Loss: 4.0168, Time: 9114.5s, Step: 15321, GPU: 4.9GB\n",
      "Epoch 1, Batch 15325, Loss: 3.4885, Time: 9117.9s, Step: 15326, GPU: 4.9GB\n",
      "Epoch 1, Batch 15330, Loss: 4.0989, Time: 9120.5s, Step: 15331, GPU: 4.9GB\n",
      "Epoch 1, Batch 15335, Loss: 4.4174, Time: 9123.9s, Step: 15336, GPU: 4.9GB\n",
      "Epoch 1, Batch 15340, Loss: 3.8527, Time: 9126.4s, Step: 15341, GPU: 4.9GB\n",
      "Epoch 1, Batch 15345, Loss: 4.0710, Time: 9129.9s, Step: 15346, GPU: 4.9GB\n",
      "Epoch 1, Batch 15350, Loss: 3.0380, Time: 9132.4s, Step: 15351, GPU: 4.9GB\n",
      "Epoch 1, Batch 15355, Loss: 3.9530, Time: 9135.8s, Step: 15356, GPU: 4.9GB\n",
      "Epoch 1, Batch 15360, Loss: 4.1230, Time: 9138.3s, Step: 15361, GPU: 4.9GB\n",
      "Epoch 1, Batch 15365, Loss: 3.2301, Time: 9141.8s, Step: 15366, GPU: 4.9GB\n",
      "Epoch 1, Batch 15370, Loss: 4.7638, Time: 9144.3s, Step: 15371, GPU: 4.9GB\n",
      "Epoch 1, Batch 15375, Loss: 4.3057, Time: 9147.7s, Step: 15376, GPU: 4.9GB\n",
      "Epoch 1, Batch 15380, Loss: 3.6448, Time: 9150.2s, Step: 15381, GPU: 4.9GB\n",
      "Epoch 1, Batch 15385, Loss: 4.3084, Time: 9153.7s, Step: 15386, GPU: 4.9GB\n",
      "Epoch 1, Batch 15390, Loss: 4.7199, Time: 9156.2s, Step: 15391, GPU: 4.9GB\n",
      "Epoch 1, Batch 15395, Loss: 3.6734, Time: 9159.6s, Step: 15396, GPU: 4.9GB\n",
      "Epoch 1, Batch 15400, Loss: 5.1770, Time: 9162.2s, Step: 15401, GPU: 4.9GB\n",
      "Epoch 1, Batch 15405, Loss: 4.0131, Time: 9165.7s, Step: 15406, GPU: 4.9GB\n",
      "Epoch 1, Batch 15410, Loss: 3.9512, Time: 9168.2s, Step: 15411, GPU: 4.9GB\n",
      "Epoch 1, Batch 15415, Loss: 3.8298, Time: 9171.6s, Step: 15416, GPU: 4.9GB\n",
      "Epoch 1, Batch 15420, Loss: 4.4452, Time: 9174.1s, Step: 15421, GPU: 4.9GB\n",
      "Epoch 1, Batch 15425, Loss: 4.1076, Time: 9177.5s, Step: 15426, GPU: 4.9GB\n",
      "Epoch 1, Batch 15430, Loss: 4.9424, Time: 9180.1s, Step: 15431, GPU: 4.9GB\n",
      "Epoch 1, Batch 15435, Loss: 4.7648, Time: 9183.5s, Step: 15436, GPU: 4.9GB\n",
      "Epoch 1, Batch 15440, Loss: 4.2364, Time: 9186.0s, Step: 15441, GPU: 4.9GB\n",
      "Epoch 1, Batch 15445, Loss: 3.9017, Time: 9189.4s, Step: 15446, GPU: 4.9GB\n",
      "Epoch 1, Batch 15450, Loss: 4.4394, Time: 9192.0s, Step: 15451, GPU: 4.9GB\n",
      "Epoch 1, Batch 15455, Loss: 3.9553, Time: 9195.4s, Step: 15456, GPU: 4.9GB\n",
      "Epoch 1, Batch 15460, Loss: 3.7983, Time: 9197.9s, Step: 15461, GPU: 4.9GB\n",
      "Epoch 1, Batch 15465, Loss: 4.0197, Time: 9201.4s, Step: 15466, GPU: 4.9GB\n",
      "Epoch 1, Batch 15470, Loss: 3.6511, Time: 9203.9s, Step: 15471, GPU: 4.9GB\n",
      "Epoch 1, Batch 15475, Loss: 3.7211, Time: 9207.4s, Step: 15476, GPU: 4.9GB\n",
      "Epoch 1, Batch 15480, Loss: 4.0588, Time: 9209.9s, Step: 15481, GPU: 4.9GB\n",
      "Epoch 1, Batch 15485, Loss: 4.1056, Time: 9213.3s, Step: 15486, GPU: 4.9GB\n",
      "Epoch 1, Batch 15490, Loss: 3.4051, Time: 9215.8s, Step: 15491, GPU: 4.9GB\n",
      "Epoch 1, Batch 15495, Loss: 2.8040, Time: 9219.2s, Step: 15496, GPU: 4.9GB\n",
      "Epoch 1, Batch 15500, Loss: 4.1769, Time: 9221.8s, Step: 15501, GPU: 4.9GB\n",
      "Epoch 1, Batch 15505, Loss: 4.1175, Time: 9225.2s, Step: 15506, GPU: 4.9GB\n",
      "Epoch 1, Batch 15510, Loss: 3.9933, Time: 9227.7s, Step: 15511, GPU: 4.9GB\n",
      "Epoch 1, Batch 15515, Loss: 4.1512, Time: 9231.1s, Step: 15516, GPU: 4.9GB\n",
      "Epoch 1, Batch 15520, Loss: 4.6332, Time: 9233.6s, Step: 15521, GPU: 4.9GB\n",
      "Epoch 1, Batch 15525, Loss: 3.5828, Time: 9237.0s, Step: 15526, GPU: 4.9GB\n",
      "Epoch 1, Batch 15530, Loss: 4.0580, Time: 9239.5s, Step: 15531, GPU: 4.9GB\n",
      "Epoch 1, Batch 15535, Loss: 3.9415, Time: 9242.9s, Step: 15536, GPU: 4.9GB\n",
      "Epoch 1, Batch 15540, Loss: 4.0365, Time: 9245.4s, Step: 15541, GPU: 4.9GB\n",
      "Epoch 1, Batch 15545, Loss: 4.4191, Time: 9248.8s, Step: 15546, GPU: 4.9GB\n",
      "Epoch 1, Batch 15550, Loss: 4.5625, Time: 9251.3s, Step: 15551, GPU: 4.9GB\n",
      "Epoch 1, Batch 15555, Loss: 3.5230, Time: 9254.7s, Step: 15556, GPU: 4.9GB\n",
      "Epoch 1, Batch 15560, Loss: 4.1958, Time: 9257.3s, Step: 15561, GPU: 4.9GB\n",
      "Epoch 1, Batch 15565, Loss: 4.1632, Time: 9260.7s, Step: 15566, GPU: 4.9GB\n",
      "Epoch 1, Batch 15570, Loss: 4.5178, Time: 9263.2s, Step: 15571, GPU: 4.9GB\n",
      "Epoch 1, Batch 15575, Loss: 3.9873, Time: 9266.6s, Step: 15576, GPU: 4.9GB\n",
      "Epoch 1, Batch 15580, Loss: 4.5806, Time: 9269.1s, Step: 15581, GPU: 4.9GB\n",
      "Epoch 1, Batch 15585, Loss: 4.2624, Time: 9272.5s, Step: 15586, GPU: 4.9GB\n",
      "Epoch 1, Batch 15590, Loss: 4.8719, Time: 9275.0s, Step: 15591, GPU: 4.9GB\n",
      "Epoch 1, Batch 15595, Loss: 4.2566, Time: 9278.4s, Step: 15596, GPU: 4.9GB\n",
      "Epoch 1, Batch 15600, Loss: 4.1912, Time: 9281.0s, Step: 15601, GPU: 4.9GB\n",
      "Epoch 1, Batch 15605, Loss: 2.8788, Time: 9284.4s, Step: 15606, GPU: 4.9GB\n",
      "Epoch 1, Batch 15610, Loss: 4.2507, Time: 9286.9s, Step: 15611, GPU: 4.9GB\n",
      "Epoch 1, Batch 15615, Loss: 4.4437, Time: 9290.3s, Step: 15616, GPU: 4.9GB\n",
      "Epoch 1, Batch 15620, Loss: 3.8494, Time: 9292.8s, Step: 15621, GPU: 4.9GB\n",
      "Epoch 1, Batch 15625, Loss: 4.3673, Time: 9296.3s, Step: 15626, GPU: 4.9GB\n",
      "Epoch 1, Batch 15630, Loss: 3.9540, Time: 9298.8s, Step: 15631, GPU: 4.9GB\n",
      "Epoch 1, Batch 15635, Loss: 3.8260, Time: 9302.2s, Step: 15636, GPU: 4.9GB\n",
      "Epoch 1, Batch 15640, Loss: 4.6622, Time: 9304.7s, Step: 15641, GPU: 4.9GB\n",
      "Epoch 1, Batch 15645, Loss: 4.4485, Time: 9308.1s, Step: 15646, GPU: 4.9GB\n",
      "Epoch 1, Batch 15650, Loss: 4.5206, Time: 9310.7s, Step: 15651, GPU: 4.9GB\n",
      "Epoch 1, Batch 15655, Loss: 3.1842, Time: 9314.1s, Step: 15656, GPU: 4.9GB\n",
      "Epoch 1, Batch 15660, Loss: 4.5953, Time: 9316.7s, Step: 15661, GPU: 4.9GB\n",
      "Epoch 1, Batch 15665, Loss: 4.2583, Time: 9320.1s, Step: 15666, GPU: 4.9GB\n",
      "Epoch 1, Batch 15670, Loss: 3.7678, Time: 9322.7s, Step: 15671, GPU: 4.9GB\n",
      "Epoch 1, Batch 15675, Loss: 4.6954, Time: 9326.1s, Step: 15676, GPU: 4.9GB\n",
      "Epoch 1, Batch 15680, Loss: 4.2201, Time: 9328.7s, Step: 15681, GPU: 4.9GB\n",
      "Epoch 1, Batch 15685, Loss: 4.0331, Time: 9332.1s, Step: 15686, GPU: 4.9GB\n",
      "Epoch 1, Batch 15690, Loss: 3.8682, Time: 9334.6s, Step: 15691, GPU: 4.9GB\n",
      "Epoch 1, Batch 15695, Loss: 4.1118, Time: 9338.1s, Step: 15696, GPU: 4.9GB\n",
      "Epoch 1, Batch 15700, Loss: 4.5033, Time: 9340.6s, Step: 15701, GPU: 4.9GB\n",
      "Epoch 1, Batch 15705, Loss: 4.6572, Time: 9344.0s, Step: 15706, GPU: 4.9GB\n",
      "Epoch 1, Batch 15710, Loss: 4.2593, Time: 9346.6s, Step: 15711, GPU: 4.9GB\n",
      "Epoch 1, Batch 15715, Loss: 4.4792, Time: 9350.0s, Step: 15716, GPU: 4.9GB\n",
      "Epoch 1, Batch 15720, Loss: 4.0090, Time: 9352.5s, Step: 15721, GPU: 4.9GB\n",
      "Epoch 1, Batch 15725, Loss: 4.3608, Time: 9356.0s, Step: 15726, GPU: 4.9GB\n",
      "Epoch 1, Batch 15730, Loss: 4.7384, Time: 9358.5s, Step: 15731, GPU: 4.9GB\n",
      "Epoch 1, Batch 15735, Loss: 3.1999, Time: 9361.9s, Step: 15736, GPU: 4.9GB\n",
      "Epoch 1, Batch 15740, Loss: 4.6105, Time: 9364.5s, Step: 15741, GPU: 4.9GB\n",
      "Epoch 1, Batch 15745, Loss: 3.7348, Time: 9367.9s, Step: 15746, GPU: 4.9GB\n",
      "Epoch 1, Batch 15750, Loss: 4.0859, Time: 9370.5s, Step: 15751, GPU: 4.9GB\n",
      "Epoch 1, Batch 15755, Loss: 4.2421, Time: 9373.9s, Step: 15756, GPU: 4.9GB\n",
      "Epoch 1, Batch 15760, Loss: 3.9060, Time: 9376.5s, Step: 15761, GPU: 4.9GB\n",
      "Epoch 1, Batch 15765, Loss: 3.8104, Time: 9379.9s, Step: 15766, GPU: 4.9GB\n",
      "Epoch 1, Batch 15770, Loss: 4.4006, Time: 9382.4s, Step: 15771, GPU: 4.9GB\n",
      "Epoch 1, Batch 15775, Loss: 3.9790, Time: 9385.9s, Step: 15776, GPU: 4.9GB\n",
      "Epoch 1, Batch 15780, Loss: 3.9923, Time: 9388.4s, Step: 15781, GPU: 4.9GB\n",
      "Epoch 1, Batch 15785, Loss: 3.0029, Time: 9391.8s, Step: 15786, GPU: 4.9GB\n",
      "Epoch 1, Batch 15790, Loss: 3.6936, Time: 9394.3s, Step: 15791, GPU: 4.9GB\n",
      "Epoch 1, Batch 15795, Loss: 3.8152, Time: 9397.7s, Step: 15796, GPU: 4.9GB\n",
      "Epoch 1, Batch 15800, Loss: 3.9146, Time: 9400.3s, Step: 15801, GPU: 4.9GB\n",
      "Epoch 1, Batch 15805, Loss: 5.1511, Time: 9403.8s, Step: 15806, GPU: 4.9GB\n",
      "Epoch 1, Batch 15810, Loss: 3.7499, Time: 9406.3s, Step: 15811, GPU: 4.9GB\n",
      "Epoch 1, Batch 15815, Loss: 4.2452, Time: 9409.7s, Step: 15816, GPU: 4.9GB\n",
      "Epoch 1, Batch 15820, Loss: 3.4363, Time: 9412.2s, Step: 15821, GPU: 4.9GB\n",
      "Epoch 1, Batch 15825, Loss: 4.9897, Time: 9415.7s, Step: 15826, GPU: 4.9GB\n",
      "Epoch 1, Batch 15830, Loss: 5.3462, Time: 9418.2s, Step: 15831, GPU: 4.9GB\n",
      "Epoch 1, Batch 15835, Loss: 3.9522, Time: 9421.6s, Step: 15836, GPU: 4.9GB\n",
      "Epoch 1, Batch 15840, Loss: 4.3163, Time: 9424.2s, Step: 15841, GPU: 4.9GB\n",
      "Epoch 1, Batch 15845, Loss: 3.5719, Time: 9427.6s, Step: 15846, GPU: 4.9GB\n",
      "Epoch 1, Batch 15850, Loss: 4.5063, Time: 9430.1s, Step: 15851, GPU: 4.9GB\n",
      "Epoch 1, Batch 15855, Loss: 3.6847, Time: 9433.6s, Step: 15856, GPU: 4.9GB\n",
      "Epoch 1, Batch 15860, Loss: 4.4493, Time: 9436.1s, Step: 15861, GPU: 4.9GB\n",
      "Epoch 1, Batch 15865, Loss: 4.6570, Time: 9439.5s, Step: 15866, GPU: 4.9GB\n",
      "Epoch 1, Batch 15870, Loss: 4.2172, Time: 9442.0s, Step: 15871, GPU: 4.9GB\n",
      "Epoch 1, Batch 15875, Loss: 4.3287, Time: 9445.4s, Step: 15876, GPU: 4.9GB\n",
      "Epoch 1, Batch 15880, Loss: 4.5397, Time: 9448.0s, Step: 15881, GPU: 4.9GB\n",
      "Epoch 1, Batch 15885, Loss: 3.4819, Time: 9451.4s, Step: 15886, GPU: 4.9GB\n",
      "Epoch 1, Batch 15890, Loss: 3.9726, Time: 9453.9s, Step: 15891, GPU: 4.9GB\n",
      "Epoch 1, Batch 15895, Loss: 3.9657, Time: 9457.4s, Step: 15896, GPU: 4.9GB\n",
      "Epoch 1, Batch 15900, Loss: 2.7167, Time: 9459.9s, Step: 15901, GPU: 4.9GB\n",
      "Epoch 1, Batch 15905, Loss: 4.5335, Time: 9463.3s, Step: 15906, GPU: 4.9GB\n",
      "Epoch 1, Batch 15910, Loss: 4.1656, Time: 9465.8s, Step: 15911, GPU: 4.9GB\n",
      "Epoch 1, Batch 15915, Loss: 4.4172, Time: 9469.3s, Step: 15916, GPU: 4.9GB\n",
      "Epoch 1, Batch 15920, Loss: 4.4557, Time: 9471.8s, Step: 15921, GPU: 4.9GB\n",
      "Epoch 1, Batch 15925, Loss: 3.7411, Time: 9475.2s, Step: 15926, GPU: 4.9GB\n",
      "Epoch 1, Batch 15930, Loss: 4.9572, Time: 9477.7s, Step: 15931, GPU: 4.9GB\n",
      "Epoch 1, Batch 15935, Loss: 4.1416, Time: 9481.1s, Step: 15936, GPU: 4.9GB\n",
      "Epoch 1, Batch 15940, Loss: 4.3770, Time: 9483.7s, Step: 15941, GPU: 4.9GB\n",
      "Epoch 1, Batch 15945, Loss: 5.2856, Time: 9487.1s, Step: 15946, GPU: 4.9GB\n",
      "Epoch 1, Batch 15950, Loss: 4.2864, Time: 9489.6s, Step: 15951, GPU: 4.9GB\n",
      "Epoch 1, Batch 15955, Loss: 3.8966, Time: 9493.0s, Step: 15956, GPU: 4.9GB\n",
      "Epoch 1, Batch 15960, Loss: 4.3608, Time: 9495.5s, Step: 15961, GPU: 4.9GB\n",
      "Epoch 1, Batch 15965, Loss: 3.5817, Time: 9499.0s, Step: 15966, GPU: 4.9GB\n",
      "Epoch 1, Batch 15970, Loss: 4.3855, Time: 9501.5s, Step: 15971, GPU: 4.9GB\n",
      "Epoch 1, Batch 15975, Loss: 3.8864, Time: 9504.9s, Step: 15976, GPU: 4.9GB\n",
      "Epoch 1, Batch 15980, Loss: 3.8579, Time: 9507.4s, Step: 15981, GPU: 4.9GB\n",
      "Epoch 1, Batch 15985, Loss: 4.3840, Time: 9510.8s, Step: 15986, GPU: 4.9GB\n",
      "Epoch 1, Batch 15990, Loss: 3.9670, Time: 9513.3s, Step: 15991, GPU: 4.9GB\n",
      "Epoch 1, Batch 15995, Loss: 4.2012, Time: 9516.8s, Step: 15996, GPU: 4.9GB\n",
      "Epoch 1, Batch 16000, Loss: 4.1443, Time: 9519.4s, Step: 16001, GPU: 4.9GB\n",
      "Epoch 1, Batch 16005, Loss: 5.0715, Time: 9522.8s, Step: 16006, GPU: 4.9GB\n",
      "Epoch 1, Batch 16010, Loss: 4.9860, Time: 9525.3s, Step: 16011, GPU: 4.9GB\n",
      "Epoch 1, Batch 16015, Loss: 4.5944, Time: 9528.7s, Step: 16016, GPU: 4.9GB\n",
      "Epoch 1, Batch 16020, Loss: 4.3064, Time: 9531.2s, Step: 16021, GPU: 4.9GB\n",
      "Epoch 1, Batch 16025, Loss: 5.2032, Time: 9534.6s, Step: 16026, GPU: 4.9GB\n",
      "Epoch 1, Batch 16030, Loss: 4.1740, Time: 9537.1s, Step: 16031, GPU: 4.9GB\n",
      "Epoch 1, Batch 16035, Loss: 3.9723, Time: 9540.5s, Step: 16036, GPU: 4.9GB\n",
      "Epoch 1, Batch 16040, Loss: 4.7242, Time: 9543.1s, Step: 16041, GPU: 4.9GB\n",
      "Epoch 1, Batch 16045, Loss: 4.3903, Time: 9546.5s, Step: 16046, GPU: 4.9GB\n",
      "Epoch 1, Batch 16050, Loss: 4.5302, Time: 9549.0s, Step: 16051, GPU: 4.9GB\n",
      "Epoch 1, Batch 16055, Loss: 3.9300, Time: 9552.4s, Step: 16056, GPU: 4.9GB\n",
      "Epoch 1, Batch 16060, Loss: 4.0183, Time: 9554.9s, Step: 16061, GPU: 4.9GB\n",
      "Epoch 1, Batch 16065, Loss: 3.1492, Time: 9558.4s, Step: 16066, GPU: 4.9GB\n",
      "Epoch 1, Batch 16070, Loss: 4.3127, Time: 9560.9s, Step: 16071, GPU: 4.9GB\n",
      "Epoch 1, Batch 16075, Loss: 3.4750, Time: 9564.3s, Step: 16076, GPU: 4.9GB\n",
      "Epoch 1, Batch 16080, Loss: 4.1501, Time: 9566.8s, Step: 16081, GPU: 4.9GB\n",
      "Epoch 1, Batch 16085, Loss: 5.9254, Time: 9570.2s, Step: 16086, GPU: 4.9GB\n",
      "Epoch 1, Batch 16090, Loss: 4.3390, Time: 9572.8s, Step: 16091, GPU: 4.9GB\n",
      "Epoch 1, Batch 16095, Loss: 3.7997, Time: 9576.2s, Step: 16096, GPU: 4.9GB\n",
      "Epoch 1, Batch 16100, Loss: 4.8811, Time: 9578.7s, Step: 16101, GPU: 4.9GB\n",
      "Epoch 1, Batch 16105, Loss: 3.4038, Time: 9582.1s, Step: 16106, GPU: 4.9GB\n",
      "Epoch 1, Batch 16110, Loss: 4.2201, Time: 9584.6s, Step: 16111, GPU: 4.9GB\n",
      "Epoch 1, Batch 16115, Loss: 3.8561, Time: 9588.0s, Step: 16116, GPU: 4.9GB\n",
      "Epoch 1, Batch 16120, Loss: 4.6911, Time: 9590.5s, Step: 16121, GPU: 4.9GB\n",
      "Epoch 1, Batch 16125, Loss: 3.7726, Time: 9593.9s, Step: 16126, GPU: 4.9GB\n",
      "Epoch 1, Batch 16130, Loss: 4.3108, Time: 9596.5s, Step: 16131, GPU: 4.9GB\n",
      "Epoch 1, Batch 16135, Loss: 4.0444, Time: 9599.9s, Step: 16136, GPU: 4.9GB\n",
      "Epoch 1, Batch 16140, Loss: 3.6550, Time: 9602.4s, Step: 16141, GPU: 4.9GB\n",
      "Epoch 1, Batch 16145, Loss: 3.7741, Time: 9605.8s, Step: 16146, GPU: 4.9GB\n",
      "Epoch 1, Batch 16150, Loss: 3.9122, Time: 9608.4s, Step: 16151, GPU: 4.9GB\n",
      "Epoch 1, Batch 16155, Loss: 4.0605, Time: 9611.8s, Step: 16156, GPU: 4.9GB\n",
      "Epoch 1, Batch 16160, Loss: 4.1639, Time: 9614.3s, Step: 16161, GPU: 4.9GB\n",
      "Epoch 1, Batch 16165, Loss: 4.8764, Time: 9617.7s, Step: 16166, GPU: 4.9GB\n",
      "Epoch 1, Batch 16170, Loss: 4.4665, Time: 9620.2s, Step: 16171, GPU: 4.9GB\n",
      "Epoch 1, Batch 16175, Loss: 4.6312, Time: 9623.6s, Step: 16176, GPU: 4.9GB\n",
      "Epoch 1, Batch 16180, Loss: 3.2106, Time: 9626.1s, Step: 16181, GPU: 4.9GB\n",
      "Epoch 1, Batch 16185, Loss: 3.2154, Time: 9629.6s, Step: 16186, GPU: 4.9GB\n",
      "Epoch 1, Batch 16190, Loss: 4.5173, Time: 9632.1s, Step: 16191, GPU: 4.9GB\n",
      "Epoch 1, Batch 16195, Loss: 3.8344, Time: 9635.5s, Step: 16196, GPU: 4.9GB\n",
      "Epoch 1, Batch 16200, Loss: 3.5627, Time: 9638.1s, Step: 16201, GPU: 4.9GB\n",
      "Epoch 1, Batch 16205, Loss: 4.5916, Time: 9641.5s, Step: 16206, GPU: 4.9GB\n",
      "Epoch 1, Batch 16210, Loss: 3.9437, Time: 9644.0s, Step: 16211, GPU: 4.9GB\n",
      "Epoch 1, Batch 16215, Loss: 3.4280, Time: 9647.4s, Step: 16216, GPU: 4.9GB\n",
      "Epoch 1, Batch 16220, Loss: 3.6390, Time: 9649.9s, Step: 16221, GPU: 4.9GB\n",
      "Epoch 1, Batch 16225, Loss: 3.6216, Time: 9653.3s, Step: 16226, GPU: 4.9GB\n",
      "Epoch 1, Batch 16230, Loss: 4.0654, Time: 9655.8s, Step: 16231, GPU: 4.9GB\n",
      "Epoch 1, Batch 16235, Loss: 4.5998, Time: 9659.3s, Step: 16236, GPU: 4.9GB\n",
      "Epoch 1, Batch 16240, Loss: 3.9245, Time: 9661.8s, Step: 16241, GPU: 4.9GB\n",
      "Epoch 1, Batch 16245, Loss: 4.4908, Time: 9665.2s, Step: 16246, GPU: 4.9GB\n",
      "Epoch 1, Batch 16250, Loss: 4.7815, Time: 9667.7s, Step: 16251, GPU: 4.9GB\n",
      "Epoch 1, Batch 16255, Loss: 2.9929, Time: 9671.1s, Step: 16256, GPU: 4.9GB\n",
      "Epoch 1, Batch 16260, Loss: 4.5779, Time: 9673.6s, Step: 16261, GPU: 4.9GB\n",
      "Epoch 1, Batch 16265, Loss: 4.3485, Time: 9677.0s, Step: 16266, GPU: 4.9GB\n",
      "Epoch 1, Batch 16270, Loss: 4.5521, Time: 9679.5s, Step: 16271, GPU: 4.9GB\n",
      "Epoch 1, Batch 16275, Loss: 3.9138, Time: 9682.9s, Step: 16276, GPU: 4.9GB\n",
      "Epoch 1, Batch 16280, Loss: 3.9510, Time: 9685.5s, Step: 16281, GPU: 4.9GB\n",
      "Epoch 1, Batch 16285, Loss: 4.5856, Time: 9688.9s, Step: 16286, GPU: 4.9GB\n",
      "Epoch 1, Batch 16290, Loss: 3.9246, Time: 9691.4s, Step: 16291, GPU: 4.9GB\n",
      "Epoch 1, Batch 16295, Loss: 4.2202, Time: 9694.8s, Step: 16296, GPU: 4.9GB\n",
      "Epoch 1, Batch 16300, Loss: 3.6398, Time: 9697.3s, Step: 16301, GPU: 4.9GB\n",
      "Epoch 1, Batch 16305, Loss: 3.4267, Time: 9700.7s, Step: 16306, GPU: 4.9GB\n",
      "Epoch 1, Batch 16310, Loss: 3.3572, Time: 9703.2s, Step: 16311, GPU: 4.9GB\n",
      "Epoch 1, Batch 16315, Loss: 4.1781, Time: 9706.6s, Step: 16316, GPU: 4.9GB\n",
      "Epoch 1, Batch 16320, Loss: 4.3516, Time: 9709.1s, Step: 16321, GPU: 4.9GB\n",
      "Epoch 1, Batch 16325, Loss: 4.4831, Time: 9712.5s, Step: 16326, GPU: 4.9GB\n",
      "Epoch 1, Batch 16330, Loss: 4.0599, Time: 9715.1s, Step: 16331, GPU: 4.9GB\n",
      "Epoch 1, Batch 16335, Loss: 4.1670, Time: 9718.5s, Step: 16336, GPU: 4.9GB\n",
      "Epoch 1, Batch 16340, Loss: 4.5453, Time: 9721.0s, Step: 16341, GPU: 4.9GB\n",
      "Epoch 1, Batch 16345, Loss: 3.8611, Time: 9724.4s, Step: 16346, GPU: 4.9GB\n",
      "Epoch 1, Batch 16350, Loss: 4.2749, Time: 9726.9s, Step: 16351, GPU: 4.9GB\n",
      "Epoch 1, Batch 16355, Loss: 4.2454, Time: 9730.4s, Step: 16356, GPU: 4.9GB\n",
      "Epoch 1, Batch 16360, Loss: 3.7415, Time: 9732.9s, Step: 16361, GPU: 4.9GB\n",
      "Epoch 1, Batch 16365, Loss: 4.5145, Time: 9736.3s, Step: 16366, GPU: 4.9GB\n",
      "Epoch 1, Batch 16370, Loss: 4.0756, Time: 9738.9s, Step: 16371, GPU: 4.9GB\n",
      "Epoch 1, Batch 16375, Loss: 3.8791, Time: 9742.3s, Step: 16376, GPU: 4.9GB\n",
      "Epoch 1, Batch 16380, Loss: 3.8501, Time: 9744.8s, Step: 16381, GPU: 4.9GB\n",
      "Epoch 1, Batch 16385, Loss: 3.9674, Time: 9748.2s, Step: 16386, GPU: 4.9GB\n",
      "Epoch 1, Batch 16390, Loss: 3.6852, Time: 9750.8s, Step: 16391, GPU: 4.9GB\n",
      "Epoch 1, Batch 16395, Loss: 4.4476, Time: 9754.2s, Step: 16396, GPU: 4.9GB\n",
      "Epoch 1, Batch 16400, Loss: 3.7063, Time: 9756.8s, Step: 16401, GPU: 4.9GB\n",
      "Epoch 1, Batch 16405, Loss: 3.3848, Time: 9760.2s, Step: 16406, GPU: 4.9GB\n",
      "Epoch 1, Batch 16410, Loss: 3.7194, Time: 9762.7s, Step: 16411, GPU: 4.9GB\n",
      "Epoch 1, Batch 16415, Loss: 3.7718, Time: 9766.1s, Step: 16416, GPU: 4.9GB\n",
      "Epoch 1, Batch 16420, Loss: 4.0355, Time: 9768.7s, Step: 16421, GPU: 4.9GB\n",
      "Epoch 1, Batch 16425, Loss: 3.6893, Time: 9772.1s, Step: 16426, GPU: 4.9GB\n",
      "Epoch 1, Batch 16430, Loss: 4.0587, Time: 9774.6s, Step: 16431, GPU: 4.9GB\n",
      "Epoch 1, Batch 16435, Loss: 4.3278, Time: 9778.0s, Step: 16436, GPU: 4.9GB\n",
      "Epoch 1, Batch 16440, Loss: 3.9209, Time: 9780.5s, Step: 16441, GPU: 4.9GB\n",
      "Epoch 1, Batch 16445, Loss: 3.6921, Time: 9784.0s, Step: 16446, GPU: 4.9GB\n",
      "Epoch 1, Batch 16450, Loss: 4.1129, Time: 9786.5s, Step: 16451, GPU: 4.9GB\n",
      "Epoch 1, Batch 16455, Loss: 3.4553, Time: 9789.9s, Step: 16456, GPU: 4.9GB\n",
      "Epoch 1, Batch 16460, Loss: 3.0167, Time: 9792.4s, Step: 16461, GPU: 4.9GB\n",
      "Epoch 1, Batch 16465, Loss: 4.6509, Time: 9795.8s, Step: 16466, GPU: 4.9GB\n",
      "Epoch 1, Batch 16470, Loss: 4.6197, Time: 9798.3s, Step: 16471, GPU: 4.9GB\n",
      "Epoch 1, Batch 16475, Loss: 4.1435, Time: 9801.8s, Step: 16476, GPU: 4.9GB\n",
      "Epoch 1, Batch 16480, Loss: 4.5608, Time: 9804.3s, Step: 16481, GPU: 4.9GB\n",
      "Epoch 1, Batch 16485, Loss: 4.0635, Time: 9807.7s, Step: 16486, GPU: 4.9GB\n",
      "Epoch 1, Batch 16490, Loss: 4.6718, Time: 9810.2s, Step: 16491, GPU: 4.9GB\n",
      "Epoch 1, Batch 16495, Loss: 4.0257, Time: 9813.6s, Step: 16496, GPU: 4.9GB\n",
      "Epoch 1, Batch 16500, Loss: 4.7228, Time: 9816.2s, Step: 16501, GPU: 4.9GB\n",
      "Epoch 1, Batch 16505, Loss: 4.7966, Time: 9819.6s, Step: 16506, GPU: 4.9GB\n",
      "Epoch 1, Batch 16510, Loss: 4.0056, Time: 9822.1s, Step: 16511, GPU: 4.9GB\n",
      "Epoch 1, Batch 16515, Loss: 4.6851, Time: 9825.5s, Step: 16516, GPU: 4.9GB\n",
      "Epoch 1, Batch 16520, Loss: 4.1446, Time: 9828.0s, Step: 16521, GPU: 4.9GB\n",
      "Epoch 1, Batch 16525, Loss: 3.5308, Time: 9831.4s, Step: 16526, GPU: 4.9GB\n",
      "Epoch 1, Batch 16530, Loss: 3.7876, Time: 9834.0s, Step: 16531, GPU: 4.9GB\n",
      "Epoch 1, Batch 16535, Loss: 4.3771, Time: 9837.4s, Step: 16536, GPU: 4.9GB\n",
      "Epoch 1, Batch 16540, Loss: 4.4848, Time: 9839.9s, Step: 16541, GPU: 4.9GB\n",
      "Epoch 1, Batch 16545, Loss: 3.8684, Time: 9843.3s, Step: 16546, GPU: 4.9GB\n",
      "Epoch 1, Batch 16550, Loss: 3.5201, Time: 9845.8s, Step: 16551, GPU: 4.9GB\n",
      "Epoch 1, Batch 16555, Loss: 4.3111, Time: 9849.3s, Step: 16556, GPU: 4.9GB\n",
      "Epoch 1, Batch 16560, Loss: 3.3274, Time: 9851.8s, Step: 16561, GPU: 4.9GB\n",
      "Epoch 1, Batch 16565, Loss: 3.6522, Time: 9855.2s, Step: 16566, GPU: 4.9GB\n",
      "Epoch 1, Batch 16570, Loss: 4.6210, Time: 9857.7s, Step: 16571, GPU: 4.9GB\n",
      "Epoch 1, Batch 16575, Loss: 4.2221, Time: 9861.1s, Step: 16576, GPU: 4.9GB\n",
      "Epoch 1, Batch 16580, Loss: 4.7019, Time: 9863.6s, Step: 16581, GPU: 4.9GB\n",
      "Epoch 1, Batch 16585, Loss: 4.1878, Time: 9867.0s, Step: 16586, GPU: 4.9GB\n",
      "Epoch 1, Batch 16590, Loss: 4.8776, Time: 9869.6s, Step: 16591, GPU: 4.9GB\n",
      "Epoch 1, Batch 16595, Loss: 3.2719, Time: 9873.0s, Step: 16596, GPU: 4.9GB\n",
      "Epoch 1, Batch 16600, Loss: 4.1341, Time: 9875.6s, Step: 16601, GPU: 4.9GB\n",
      "Epoch 1, Batch 16605, Loss: 4.0574, Time: 9879.0s, Step: 16606, GPU: 4.9GB\n",
      "Epoch 1, Batch 16610, Loss: 4.0289, Time: 9881.5s, Step: 16611, GPU: 4.9GB\n",
      "Epoch 1, Batch 16615, Loss: 4.6431, Time: 9884.9s, Step: 16616, GPU: 4.9GB\n",
      "Epoch 1, Batch 16620, Loss: 4.0887, Time: 9887.4s, Step: 16621, GPU: 4.9GB\n",
      "Epoch 1, Batch 16625, Loss: 3.8742, Time: 9890.8s, Step: 16626, GPU: 4.9GB\n",
      "Epoch 1, Batch 16630, Loss: 4.1881, Time: 9893.4s, Step: 16631, GPU: 4.9GB\n",
      "Epoch 1, Batch 16635, Loss: 3.4633, Time: 9896.8s, Step: 16636, GPU: 4.9GB\n",
      "Epoch 1, Batch 16640, Loss: 3.9925, Time: 9899.3s, Step: 16641, GPU: 4.9GB\n",
      "Epoch 1, Batch 16645, Loss: 4.0797, Time: 9902.7s, Step: 16646, GPU: 4.9GB\n",
      "Epoch 1, Batch 16650, Loss: 3.4562, Time: 9905.2s, Step: 16651, GPU: 4.9GB\n",
      "Epoch 1, Batch 16655, Loss: 4.0946, Time: 9908.6s, Step: 16656, GPU: 4.9GB\n",
      "Epoch 1, Batch 16660, Loss: 2.7919, Time: 9911.2s, Step: 16661, GPU: 4.9GB\n",
      "Epoch 1, Batch 16665, Loss: 4.5352, Time: 9914.6s, Step: 16666, GPU: 4.9GB\n",
      "Epoch 1, Batch 16670, Loss: 3.4031, Time: 9917.2s, Step: 16671, GPU: 4.9GB\n",
      "Epoch 1, Batch 16675, Loss: 4.0469, Time: 9920.6s, Step: 16676, GPU: 4.9GB\n",
      "Epoch 1, Batch 16680, Loss: 3.9448, Time: 9923.1s, Step: 16681, GPU: 4.9GB\n",
      "Epoch 1, Batch 16685, Loss: 4.1161, Time: 9926.5s, Step: 16686, GPU: 4.9GB\n",
      "Epoch 1, Batch 16690, Loss: 4.5440, Time: 9929.1s, Step: 16691, GPU: 4.9GB\n",
      "Epoch 1, Batch 16695, Loss: 4.3866, Time: 9932.5s, Step: 16696, GPU: 4.9GB\n",
      "Epoch 1, Batch 16700, Loss: 3.7991, Time: 9935.1s, Step: 16701, GPU: 4.9GB\n",
      "Epoch 1, Batch 16705, Loss: 4.1033, Time: 9938.5s, Step: 16706, GPU: 4.9GB\n",
      "Epoch 1, Batch 16710, Loss: 3.9236, Time: 9941.1s, Step: 16711, GPU: 4.9GB\n",
      "Epoch 1, Batch 16715, Loss: 4.3305, Time: 9944.5s, Step: 16716, GPU: 4.9GB\n",
      "Epoch 1, Batch 16720, Loss: 3.6937, Time: 9947.0s, Step: 16721, GPU: 4.9GB\n",
      "Epoch 1, Batch 16725, Loss: 3.5569, Time: 9950.5s, Step: 16726, GPU: 4.9GB\n",
      "Epoch 1, Batch 16730, Loss: 4.2860, Time: 9953.0s, Step: 16731, GPU: 4.9GB\n",
      "Epoch 1, Batch 16735, Loss: 4.9365, Time: 9956.4s, Step: 16736, GPU: 4.9GB\n",
      "Epoch 1, Batch 16740, Loss: 4.6915, Time: 9959.0s, Step: 16741, GPU: 4.9GB\n",
      "Epoch 1, Batch 16745, Loss: 3.9696, Time: 9962.4s, Step: 16746, GPU: 4.9GB\n",
      "Epoch 1, Batch 16750, Loss: 3.9398, Time: 9964.9s, Step: 16751, GPU: 4.9GB\n",
      "Epoch 1, Batch 16755, Loss: 4.2992, Time: 9968.3s, Step: 16756, GPU: 4.9GB\n",
      "Epoch 1, Batch 16760, Loss: 4.5464, Time: 9970.8s, Step: 16761, GPU: 4.9GB\n",
      "Epoch 1, Batch 16765, Loss: 3.8767, Time: 9974.3s, Step: 16766, GPU: 4.9GB\n",
      "Epoch 1, Batch 16770, Loss: 4.1144, Time: 9976.8s, Step: 16771, GPU: 4.9GB\n",
      "Epoch 1, Batch 16775, Loss: 3.2485, Time: 9980.3s, Step: 16776, GPU: 4.9GB\n",
      "Epoch 1, Batch 16780, Loss: 3.4944, Time: 9982.8s, Step: 16781, GPU: 4.9GB\n",
      "Epoch 1, Batch 16785, Loss: 3.7668, Time: 9986.2s, Step: 16786, GPU: 4.9GB\n",
      "Epoch 1, Batch 16790, Loss: 5.1840, Time: 9988.7s, Step: 16791, GPU: 4.9GB\n",
      "Epoch 1, Batch 16795, Loss: 4.3938, Time: 9992.1s, Step: 16796, GPU: 4.9GB\n",
      "Epoch 1, Batch 16800, Loss: 4.2637, Time: 9994.7s, Step: 16801, GPU: 4.9GB\n",
      "Epoch 1, Batch 16805, Loss: 4.7496, Time: 9998.1s, Step: 16806, GPU: 4.9GB\n",
      "Epoch 1, Batch 16810, Loss: 4.1168, Time: 10000.6s, Step: 16811, GPU: 4.9GB\n",
      "Epoch 1, Batch 16815, Loss: 3.7896, Time: 10004.1s, Step: 16816, GPU: 4.9GB\n",
      "Epoch 1, Batch 16820, Loss: 3.7527, Time: 10006.6s, Step: 16821, GPU: 4.9GB\n",
      "Epoch 1, Batch 16825, Loss: 3.6250, Time: 10010.0s, Step: 16826, GPU: 4.9GB\n",
      "Epoch 1, Batch 16830, Loss: 3.7568, Time: 10012.5s, Step: 16831, GPU: 4.9GB\n",
      "Epoch 1, Batch 16835, Loss: 5.1780, Time: 10016.0s, Step: 16836, GPU: 4.9GB\n",
      "Epoch 1, Batch 16840, Loss: 3.8023, Time: 10018.5s, Step: 16841, GPU: 4.9GB\n",
      "Epoch 1, Batch 16845, Loss: 6.0061, Time: 10021.9s, Step: 16846, GPU: 4.9GB\n",
      "Epoch 1, Batch 16850, Loss: 3.4534, Time: 10024.4s, Step: 16851, GPU: 4.9GB\n",
      "Epoch 1, Batch 16855, Loss: 3.9431, Time: 10027.9s, Step: 16856, GPU: 4.9GB\n",
      "Epoch 1, Batch 16860, Loss: 4.2266, Time: 10030.4s, Step: 16861, GPU: 4.9GB\n",
      "Epoch 1, Batch 16865, Loss: 4.3331, Time: 10033.8s, Step: 16866, GPU: 4.9GB\n",
      "Epoch 1, Batch 16870, Loss: 4.4555, Time: 10036.4s, Step: 16871, GPU: 4.9GB\n",
      "Epoch 1, Batch 16875, Loss: 3.8490, Time: 10039.8s, Step: 16876, GPU: 4.9GB\n",
      "Epoch 1, Batch 16880, Loss: 4.2458, Time: 10042.3s, Step: 16881, GPU: 4.9GB\n",
      "Epoch 1, Batch 16885, Loss: 6.6027, Time: 10045.8s, Step: 16886, GPU: 4.9GB\n",
      "Epoch 1, Batch 16890, Loss: 4.0920, Time: 10048.3s, Step: 16891, GPU: 4.9GB\n",
      "Epoch 1, Batch 16895, Loss: 4.0969, Time: 10051.7s, Step: 16896, GPU: 4.9GB\n",
      "Epoch 1, Batch 16900, Loss: 3.4928, Time: 10054.2s, Step: 16901, GPU: 4.9GB\n",
      "Epoch 1, Batch 16905, Loss: 3.7017, Time: 10057.7s, Step: 16906, GPU: 4.9GB\n",
      "Epoch 1, Batch 16910, Loss: 3.8430, Time: 10060.2s, Step: 16911, GPU: 4.9GB\n",
      "Epoch 1, Batch 16915, Loss: 3.2039, Time: 10063.6s, Step: 16916, GPU: 4.9GB\n",
      "Epoch 1, Batch 16920, Loss: 4.5585, Time: 10066.2s, Step: 16921, GPU: 4.9GB\n",
      "Epoch 1, Batch 16925, Loss: 5.1184, Time: 10069.6s, Step: 16926, GPU: 4.9GB\n",
      "Epoch 1, Batch 16930, Loss: 3.7215, Time: 10072.2s, Step: 16931, GPU: 4.9GB\n",
      "Epoch 1, Batch 16935, Loss: 5.1127, Time: 10075.6s, Step: 16936, GPU: 4.9GB\n",
      "Epoch 1, Batch 16940, Loss: 4.0906, Time: 10078.1s, Step: 16941, GPU: 4.9GB\n",
      "Epoch 1, Batch 16945, Loss: 3.6175, Time: 10081.5s, Step: 16946, GPU: 4.9GB\n",
      "Epoch 1, Batch 16950, Loss: 3.8962, Time: 10084.0s, Step: 16951, GPU: 4.9GB\n",
      "Epoch 1, Batch 16955, Loss: 3.8071, Time: 10087.4s, Step: 16956, GPU: 4.9GB\n",
      "Epoch 1, Batch 16960, Loss: 3.5348, Time: 10089.9s, Step: 16961, GPU: 4.9GB\n",
      "Epoch 1, Batch 16965, Loss: 5.1593, Time: 10093.4s, Step: 16966, GPU: 4.9GB\n",
      "Epoch 1, Batch 16970, Loss: 4.2988, Time: 10095.9s, Step: 16971, GPU: 4.9GB\n",
      "Epoch 1, Batch 16975, Loss: 4.4064, Time: 10099.3s, Step: 16976, GPU: 4.9GB\n",
      "Epoch 1, Batch 16980, Loss: 4.3495, Time: 10101.8s, Step: 16981, GPU: 4.9GB\n",
      "Epoch 1, Batch 16985, Loss: 4.5974, Time: 10105.2s, Step: 16986, GPU: 4.9GB\n",
      "Epoch 1, Batch 16990, Loss: 4.1500, Time: 10107.8s, Step: 16991, GPU: 4.9GB\n",
      "Epoch 1, Batch 16995, Loss: 4.7503, Time: 10111.2s, Step: 16996, GPU: 4.9GB\n",
      "Epoch 1, Batch 17000, Loss: 3.8006, Time: 10113.8s, Step: 17001, GPU: 4.9GB\n",
      "Epoch 1, Batch 17005, Loss: 3.2582, Time: 10117.2s, Step: 17006, GPU: 4.9GB\n",
      "Epoch 1, Batch 17010, Loss: 4.6109, Time: 10119.7s, Step: 17011, GPU: 4.9GB\n",
      "Epoch 1, Batch 17015, Loss: 3.3941, Time: 10123.1s, Step: 17016, GPU: 4.9GB\n",
      "Epoch 1, Batch 17020, Loss: 4.3554, Time: 10125.7s, Step: 17021, GPU: 4.9GB\n",
      "Epoch 1, Batch 17025, Loss: 3.9603, Time: 10129.1s, Step: 17026, GPU: 4.9GB\n",
      "Epoch 1, Batch 17030, Loss: 3.8477, Time: 10131.6s, Step: 17031, GPU: 4.9GB\n",
      "Epoch 1, Batch 17035, Loss: 4.2739, Time: 10135.0s, Step: 17036, GPU: 4.9GB\n",
      "Epoch 1, Batch 17040, Loss: 4.2213, Time: 10137.6s, Step: 17041, GPU: 4.9GB\n",
      "Epoch 1, Batch 17045, Loss: 4.6311, Time: 10141.0s, Step: 17046, GPU: 4.9GB\n",
      "Epoch 1, Batch 17050, Loss: 3.8604, Time: 10143.5s, Step: 17051, GPU: 4.9GB\n",
      "Epoch 1, Batch 17055, Loss: 3.4706, Time: 10146.9s, Step: 17056, GPU: 4.9GB\n",
      "Epoch 1, Batch 17060, Loss: 4.6305, Time: 10149.4s, Step: 17061, GPU: 4.9GB\n",
      "Epoch 1, Batch 17065, Loss: 3.8005, Time: 10152.8s, Step: 17066, GPU: 4.9GB\n",
      "Epoch 1, Batch 17070, Loss: 4.1179, Time: 10155.4s, Step: 17071, GPU: 4.9GB\n",
      "Epoch 1, Batch 17075, Loss: 4.0664, Time: 10158.8s, Step: 17076, GPU: 4.9GB\n",
      "Epoch 1, Batch 17080, Loss: 4.2182, Time: 10161.3s, Step: 17081, GPU: 4.9GB\n",
      "Epoch 1, Batch 17085, Loss: 3.8506, Time: 10164.7s, Step: 17086, GPU: 4.9GB\n",
      "Epoch 1, Batch 17090, Loss: 3.7426, Time: 10167.2s, Step: 17091, GPU: 4.9GB\n",
      "Epoch 1, Batch 17095, Loss: 4.2466, Time: 10170.7s, Step: 17096, GPU: 4.9GB\n",
      "Epoch 1, Batch 17100, Loss: 4.0205, Time: 10173.2s, Step: 17101, GPU: 4.9GB\n",
      "Epoch 1, Batch 17105, Loss: 4.5771, Time: 10176.6s, Step: 17106, GPU: 4.9GB\n",
      "Epoch 1, Batch 17110, Loss: 3.9250, Time: 10179.1s, Step: 17111, GPU: 4.9GB\n",
      "Epoch 1, Batch 17115, Loss: 3.8534, Time: 10182.6s, Step: 17116, GPU: 4.9GB\n",
      "Epoch 1, Batch 17120, Loss: 4.8600, Time: 10185.1s, Step: 17121, GPU: 4.9GB\n",
      "Epoch 1, Batch 17125, Loss: 3.8703, Time: 10188.5s, Step: 17126, GPU: 4.9GB\n",
      "Epoch 1, Batch 17130, Loss: 3.6183, Time: 10191.0s, Step: 17131, GPU: 4.9GB\n",
      "Epoch 1, Batch 17135, Loss: 4.1855, Time: 10194.4s, Step: 17136, GPU: 4.9GB\n",
      "Epoch 1, Batch 17140, Loss: 4.0161, Time: 10197.0s, Step: 17141, GPU: 4.9GB\n",
      "Epoch 1, Batch 17145, Loss: 3.7206, Time: 10200.4s, Step: 17146, GPU: 4.9GB\n",
      "Epoch 1, Batch 17150, Loss: 3.6431, Time: 10202.9s, Step: 17151, GPU: 4.9GB\n",
      "Epoch 1, Batch 17155, Loss: 3.8595, Time: 10206.4s, Step: 17156, GPU: 4.9GB\n",
      "Epoch 1, Batch 17160, Loss: 4.7587, Time: 10208.9s, Step: 17161, GPU: 4.9GB\n",
      "Epoch 1, Batch 17165, Loss: 4.2820, Time: 10212.3s, Step: 17166, GPU: 4.9GB\n",
      "Epoch 1, Batch 17170, Loss: 4.0274, Time: 10214.9s, Step: 17171, GPU: 4.9GB\n",
      "Epoch 1, Batch 17175, Loss: 3.9804, Time: 10218.3s, Step: 17176, GPU: 4.9GB\n",
      "Epoch 1, Batch 17180, Loss: 3.4043, Time: 10220.8s, Step: 17181, GPU: 4.9GB\n",
      "Epoch 1, Batch 17185, Loss: 4.3822, Time: 10224.3s, Step: 17186, GPU: 4.9GB\n",
      "Epoch 1, Batch 17190, Loss: 4.6480, Time: 10226.8s, Step: 17191, GPU: 4.9GB\n",
      "Epoch 1, Batch 17195, Loss: 3.6134, Time: 10230.2s, Step: 17196, GPU: 4.9GB\n",
      "Epoch 1, Batch 17200, Loss: 4.4584, Time: 10232.8s, Step: 17201, GPU: 4.9GB\n",
      "Epoch 1, Batch 17205, Loss: 3.9872, Time: 10236.2s, Step: 17206, GPU: 4.9GB\n",
      "Epoch 1, Batch 17210, Loss: 4.1117, Time: 10238.8s, Step: 17211, GPU: 4.9GB\n",
      "Epoch 1, Batch 17215, Loss: 4.1200, Time: 10242.3s, Step: 17216, GPU: 4.9GB\n",
      "Epoch 1, Batch 17220, Loss: 3.8874, Time: 10244.8s, Step: 17221, GPU: 4.9GB\n",
      "Epoch 1, Batch 17225, Loss: 4.4661, Time: 10248.2s, Step: 17226, GPU: 4.9GB\n",
      "Epoch 1, Batch 17230, Loss: 4.1547, Time: 10250.7s, Step: 17231, GPU: 4.9GB\n",
      "Epoch 1, Batch 17235, Loss: 3.9415, Time: 10254.1s, Step: 17236, GPU: 4.9GB\n",
      "Epoch 1, Batch 17240, Loss: 3.5113, Time: 10256.6s, Step: 17241, GPU: 4.9GB\n",
      "Epoch 1, Batch 17245, Loss: 4.0684, Time: 10260.0s, Step: 17246, GPU: 4.9GB\n",
      "Epoch 1, Batch 17250, Loss: 5.1446, Time: 10262.6s, Step: 17251, GPU: 4.9GB\n",
      "Epoch 1, Batch 17255, Loss: 4.2013, Time: 10265.9s, Step: 17256, GPU: 4.9GB\n",
      "Epoch 1, Batch 17260, Loss: 3.8686, Time: 10268.5s, Step: 17261, GPU: 4.9GB\n",
      "Epoch 1, Batch 17265, Loss: 3.1179, Time: 10271.9s, Step: 17266, GPU: 4.9GB\n",
      "Epoch 1, Batch 17270, Loss: 3.9750, Time: 10274.4s, Step: 17271, GPU: 4.9GB\n",
      "Epoch 1, Batch 17275, Loss: 4.0082, Time: 10277.8s, Step: 17276, GPU: 4.9GB\n",
      "Epoch 1, Batch 17280, Loss: 4.5644, Time: 10280.3s, Step: 17281, GPU: 4.9GB\n",
      "Epoch 1, Batch 17285, Loss: 5.0479, Time: 10283.8s, Step: 17286, GPU: 4.9GB\n",
      "Epoch 1, Batch 17290, Loss: 4.4225, Time: 10286.3s, Step: 17291, GPU: 4.9GB\n",
      "Epoch 1, Batch 17295, Loss: 4.1380, Time: 10289.7s, Step: 17296, GPU: 4.9GB\n",
      "Epoch 1, Batch 17300, Loss: 4.1113, Time: 10292.2s, Step: 17301, GPU: 4.9GB\n",
      "Epoch 1, Batch 17305, Loss: 3.9505, Time: 10295.6s, Step: 17306, GPU: 4.9GB\n",
      "Epoch 1, Batch 17310, Loss: 4.5603, Time: 10298.2s, Step: 17311, GPU: 4.9GB\n",
      "Epoch 1, Batch 17315, Loss: 4.7805, Time: 10301.6s, Step: 17316, GPU: 4.9GB\n",
      "Epoch 1, Batch 17320, Loss: 3.9459, Time: 10304.1s, Step: 17321, GPU: 4.9GB\n",
      "Epoch 1, Batch 17325, Loss: 3.9570, Time: 10307.5s, Step: 17326, GPU: 4.9GB\n",
      "Epoch 1, Batch 17330, Loss: 4.4606, Time: 10310.0s, Step: 17331, GPU: 4.9GB\n",
      "Epoch 1, Batch 17335, Loss: 3.8322, Time: 10313.4s, Step: 17336, GPU: 4.9GB\n",
      "Epoch 1, Batch 17340, Loss: 3.8922, Time: 10315.9s, Step: 17341, GPU: 4.9GB\n",
      "Epoch 1, Batch 17345, Loss: 4.8082, Time: 10319.3s, Step: 17346, GPU: 4.9GB\n",
      "Epoch 1, Batch 17350, Loss: 3.7127, Time: 10321.8s, Step: 17351, GPU: 4.9GB\n",
      "Epoch 1, Batch 17355, Loss: 3.7837, Time: 10325.2s, Step: 17356, GPU: 4.9GB\n",
      "Epoch 1, Batch 17360, Loss: 3.9615, Time: 10327.8s, Step: 17361, GPU: 4.9GB\n",
      "Epoch 1, Batch 17365, Loss: 3.9712, Time: 10331.2s, Step: 17366, GPU: 4.9GB\n",
      "Epoch 1, Batch 17370, Loss: 4.0437, Time: 10333.7s, Step: 17371, GPU: 4.9GB\n",
      "Epoch 1, Batch 17375, Loss: 3.8803, Time: 10337.2s, Step: 17376, GPU: 4.9GB\n",
      "Epoch 1, Batch 17380, Loss: 4.1503, Time: 10339.7s, Step: 17381, GPU: 4.9GB\n",
      "Epoch 1, Batch 17385, Loss: 4.4843, Time: 10343.1s, Step: 17386, GPU: 4.9GB\n",
      "Epoch 1, Batch 17390, Loss: 3.4451, Time: 10345.6s, Step: 17391, GPU: 4.9GB\n",
      "Epoch 1, Batch 17395, Loss: 3.9804, Time: 10349.1s, Step: 17396, GPU: 4.9GB\n",
      "Epoch 1, Batch 17400, Loss: 3.9130, Time: 10351.7s, Step: 17401, GPU: 4.9GB\n",
      "Epoch 1, Batch 17405, Loss: 4.5891, Time: 10355.1s, Step: 17406, GPU: 4.9GB\n",
      "Epoch 1, Batch 17410, Loss: 4.4287, Time: 10357.6s, Step: 17411, GPU: 4.9GB\n",
      "Epoch 1, Batch 17415, Loss: 4.6129, Time: 10361.1s, Step: 17416, GPU: 4.9GB\n",
      "Epoch 1, Batch 17420, Loss: 3.7410, Time: 10363.6s, Step: 17421, GPU: 4.9GB\n",
      "Epoch 1, Batch 17425, Loss: 5.0371, Time: 10367.0s, Step: 17426, GPU: 4.9GB\n",
      "Epoch 1, Batch 17430, Loss: 3.7162, Time: 10369.5s, Step: 17431, GPU: 4.9GB\n",
      "Epoch 1, Batch 17435, Loss: 3.9834, Time: 10372.9s, Step: 17436, GPU: 4.9GB\n",
      "Epoch 1, Batch 17440, Loss: 4.6144, Time: 10375.5s, Step: 17441, GPU: 4.9GB\n",
      "Epoch 1, Batch 17445, Loss: 3.9008, Time: 10378.9s, Step: 17446, GPU: 4.9GB\n",
      "Epoch 1, Batch 17450, Loss: 4.0154, Time: 10381.4s, Step: 17451, GPU: 4.9GB\n",
      "Epoch 1, Batch 17455, Loss: 3.2353, Time: 10384.8s, Step: 17456, GPU: 4.9GB\n",
      "Epoch 1, Batch 17460, Loss: 4.1521, Time: 10387.3s, Step: 17461, GPU: 4.9GB\n",
      "Epoch 1, Batch 17465, Loss: 3.3494, Time: 10390.7s, Step: 17466, GPU: 4.9GB\n",
      "Epoch 1, Batch 17470, Loss: 4.1638, Time: 10393.2s, Step: 17471, GPU: 4.9GB\n",
      "Epoch 1, Batch 17475, Loss: 3.5421, Time: 10396.6s, Step: 17476, GPU: 4.9GB\n",
      "Epoch 1, Batch 17480, Loss: 3.9587, Time: 10399.1s, Step: 17481, GPU: 4.9GB\n",
      "Epoch 1, Batch 17485, Loss: 4.2375, Time: 10402.5s, Step: 17486, GPU: 4.9GB\n",
      "Epoch 1, Batch 17490, Loss: 3.4945, Time: 10405.0s, Step: 17491, GPU: 4.9GB\n",
      "Epoch 1, Batch 17495, Loss: 3.9634, Time: 10408.4s, Step: 17496, GPU: 4.9GB\n",
      "Epoch 1, Batch 17500, Loss: 4.1644, Time: 10410.9s, Step: 17501, GPU: 4.9GB\n",
      "Epoch 1, Batch 17505, Loss: 4.2906, Time: 10414.3s, Step: 17506, GPU: 4.9GB\n",
      "Epoch 1, Batch 17510, Loss: 4.7174, Time: 10416.8s, Step: 17511, GPU: 4.9GB\n",
      "Epoch 1, Batch 17515, Loss: 3.8572, Time: 10420.2s, Step: 17516, GPU: 4.9GB\n",
      "Epoch 1, Batch 17520, Loss: 3.6587, Time: 10422.8s, Step: 17521, GPU: 4.9GB\n",
      "Epoch 1, Batch 17525, Loss: 3.0568, Time: 10426.2s, Step: 17526, GPU: 4.9GB\n",
      "Epoch 1, Batch 17530, Loss: 3.9666, Time: 10428.7s, Step: 17531, GPU: 4.9GB\n",
      "Epoch 1, Batch 17535, Loss: 3.9704, Time: 10432.1s, Step: 17536, GPU: 4.9GB\n",
      "Epoch 1, Batch 17540, Loss: 3.6832, Time: 10434.6s, Step: 17541, GPU: 4.9GB\n",
      "Epoch 1, Batch 17545, Loss: 3.9815, Time: 10438.0s, Step: 17546, GPU: 4.9GB\n",
      "Epoch 1, Batch 17550, Loss: 2.9726, Time: 10440.5s, Step: 17551, GPU: 4.9GB\n",
      "Epoch 1, Batch 17555, Loss: 4.4554, Time: 10443.9s, Step: 17556, GPU: 4.9GB\n",
      "Epoch 1, Batch 17560, Loss: 5.0357, Time: 10446.4s, Step: 17561, GPU: 4.9GB\n",
      "Epoch 1, Batch 17565, Loss: 3.4813, Time: 10449.9s, Step: 17566, GPU: 4.9GB\n",
      "Epoch 1, Batch 17570, Loss: 4.1137, Time: 10452.4s, Step: 17571, GPU: 4.9GB\n",
      "Epoch 1, Batch 17575, Loss: 4.5265, Time: 10456.0s, Step: 17576, GPU: 4.9GB\n",
      "Epoch 1, Batch 17580, Loss: 3.2702, Time: 10458.5s, Step: 17581, GPU: 4.9GB\n",
      "Epoch 1, Batch 17585, Loss: 2.3969, Time: 10461.9s, Step: 17586, GPU: 4.9GB\n",
      "Epoch 1, Batch 17590, Loss: 3.9438, Time: 10464.4s, Step: 17591, GPU: 4.9GB\n",
      "Epoch 1, Batch 17595, Loss: 3.9855, Time: 10467.8s, Step: 17596, GPU: 4.9GB\n",
      "Epoch 1, Batch 17600, Loss: 3.6912, Time: 10470.4s, Step: 17601, GPU: 4.9GB\n",
      "Epoch 1, Batch 17605, Loss: 4.1547, Time: 10473.9s, Step: 17606, GPU: 4.9GB\n",
      "Epoch 1, Batch 17610, Loss: 4.3277, Time: 10476.4s, Step: 17611, GPU: 4.9GB\n",
      "Epoch 1, Batch 17615, Loss: 3.5977, Time: 10479.8s, Step: 17616, GPU: 4.9GB\n",
      "Epoch 1, Batch 17620, Loss: 4.0749, Time: 10482.3s, Step: 17621, GPU: 4.9GB\n",
      "Epoch 1, Batch 17625, Loss: 3.8797, Time: 10485.7s, Step: 17626, GPU: 4.9GB\n",
      "Epoch 1, Batch 17630, Loss: 3.8921, Time: 10488.2s, Step: 17631, GPU: 4.9GB\n",
      "Epoch 1, Batch 17635, Loss: 4.7364, Time: 10491.7s, Step: 17636, GPU: 4.9GB\n",
      "Epoch 1, Batch 17640, Loss: 3.6877, Time: 10494.2s, Step: 17641, GPU: 4.9GB\n",
      "Epoch 1, Batch 17645, Loss: 4.0507, Time: 10497.6s, Step: 17646, GPU: 4.9GB\n",
      "Epoch 1, Batch 17650, Loss: 3.6402, Time: 10500.1s, Step: 17651, GPU: 4.9GB\n",
      "Epoch 1, Batch 17655, Loss: 3.8537, Time: 10503.5s, Step: 17656, GPU: 4.9GB\n",
      "Epoch 1, Batch 17660, Loss: 3.7515, Time: 10506.1s, Step: 17661, GPU: 4.9GB\n",
      "Epoch 1, Batch 17665, Loss: 4.0069, Time: 10509.5s, Step: 17666, GPU: 4.9GB\n",
      "Epoch 1, Batch 17670, Loss: 4.1522, Time: 10512.0s, Step: 17671, GPU: 4.9GB\n",
      "Epoch 1, Batch 17675, Loss: 3.8271, Time: 10515.4s, Step: 17676, GPU: 4.9GB\n",
      "Epoch 1, Batch 17680, Loss: 4.3598, Time: 10517.9s, Step: 17681, GPU: 4.9GB\n",
      "Epoch 1, Batch 17685, Loss: 3.0471, Time: 10521.3s, Step: 17686, GPU: 4.9GB\n",
      "Epoch 1, Batch 17690, Loss: 3.7892, Time: 10523.8s, Step: 17691, GPU: 4.9GB\n",
      "Epoch 1, Batch 17695, Loss: 4.3632, Time: 10527.3s, Step: 17696, GPU: 4.9GB\n",
      "Epoch 1, Batch 17700, Loss: 5.0757, Time: 10529.8s, Step: 17701, GPU: 4.9GB\n",
      "Epoch 1, Batch 17705, Loss: 3.6877, Time: 10533.2s, Step: 17706, GPU: 4.9GB\n",
      "Epoch 1, Batch 17710, Loss: 3.9095, Time: 10535.8s, Step: 17711, GPU: 4.9GB\n",
      "Epoch 1, Batch 17715, Loss: 4.0129, Time: 10539.2s, Step: 17716, GPU: 4.9GB\n",
      "Epoch 1, Batch 17720, Loss: 3.2240, Time: 10541.7s, Step: 17721, GPU: 4.9GB\n",
      "Epoch 1, Batch 17725, Loss: 4.5910, Time: 10545.2s, Step: 17726, GPU: 4.9GB\n",
      "Epoch 1, Batch 17730, Loss: 4.3781, Time: 10547.7s, Step: 17731, GPU: 4.9GB\n",
      "Epoch 1, Batch 17735, Loss: 3.9814, Time: 10551.1s, Step: 17736, GPU: 4.9GB\n",
      "Epoch 1, Batch 17740, Loss: 2.7795, Time: 10553.6s, Step: 17741, GPU: 4.9GB\n",
      "Epoch 1, Batch 17745, Loss: 4.0246, Time: 10557.1s, Step: 17746, GPU: 4.9GB\n",
      "Epoch 1, Batch 17750, Loss: 4.1588, Time: 10559.6s, Step: 17751, GPU: 4.9GB\n",
      "Epoch 1, Batch 17755, Loss: 3.5498, Time: 10563.0s, Step: 17756, GPU: 4.9GB\n",
      "Epoch 1, Batch 17760, Loss: 3.8667, Time: 10565.5s, Step: 17761, GPU: 4.9GB\n",
      "Epoch 1, Batch 17765, Loss: 3.7348, Time: 10569.0s, Step: 17766, GPU: 4.9GB\n",
      "Epoch 1, Batch 17770, Loss: 3.7276, Time: 10571.5s, Step: 17771, GPU: 4.9GB\n",
      "Epoch 1, Batch 17775, Loss: 3.6878, Time: 10574.9s, Step: 17776, GPU: 4.9GB\n",
      "Epoch 1, Batch 17780, Loss: 3.9499, Time: 10577.4s, Step: 17781, GPU: 4.9GB\n",
      "Epoch 1, Batch 17785, Loss: 4.6859, Time: 10580.9s, Step: 17786, GPU: 4.9GB\n",
      "Epoch 1, Batch 17790, Loss: 3.9193, Time: 10583.4s, Step: 17791, GPU: 4.9GB\n",
      "Epoch 1, Batch 17795, Loss: 3.9777, Time: 10586.9s, Step: 17796, GPU: 4.9GB\n",
      "Epoch 1, Batch 17800, Loss: 3.6870, Time: 10589.5s, Step: 17801, GPU: 4.9GB\n",
      "Epoch 1, Batch 17805, Loss: 4.5105, Time: 10592.9s, Step: 17806, GPU: 4.9GB\n",
      "Epoch 1, Batch 17810, Loss: 4.3279, Time: 10595.5s, Step: 17811, GPU: 4.9GB\n",
      "Epoch 1, Batch 17815, Loss: 4.2171, Time: 10598.9s, Step: 17816, GPU: 4.9GB\n",
      "Epoch 1, Batch 17820, Loss: 4.4265, Time: 10601.4s, Step: 17821, GPU: 4.9GB\n",
      "Epoch 1, Batch 17825, Loss: 4.7739, Time: 10604.8s, Step: 17826, GPU: 4.9GB\n",
      "Epoch 1, Batch 17830, Loss: 4.2122, Time: 10607.4s, Step: 17831, GPU: 4.9GB\n",
      "Epoch 1, Batch 17835, Loss: 3.7959, Time: 10610.8s, Step: 17836, GPU: 4.9GB\n",
      "Epoch 1, Batch 17840, Loss: 2.8871, Time: 10613.4s, Step: 17841, GPU: 4.9GB\n",
      "Epoch 1, Batch 17845, Loss: 3.7992, Time: 10616.8s, Step: 17846, GPU: 4.9GB\n",
      "Epoch 1, Batch 17850, Loss: 5.2607, Time: 10619.3s, Step: 17851, GPU: 4.9GB\n",
      "Epoch 1, Batch 17855, Loss: 3.8722, Time: 10622.7s, Step: 17856, GPU: 4.9GB\n",
      "Epoch 1, Batch 17860, Loss: 4.0274, Time: 10625.3s, Step: 17861, GPU: 4.9GB\n",
      "Epoch 1, Batch 17865, Loss: 3.9359, Time: 10628.7s, Step: 17866, GPU: 4.9GB\n",
      "Epoch 1, Batch 17870, Loss: 5.1057, Time: 10631.2s, Step: 17871, GPU: 4.9GB\n",
      "Epoch 1, Batch 17875, Loss: 4.6220, Time: 10634.6s, Step: 17876, GPU: 4.9GB\n",
      "Epoch 1, Batch 17880, Loss: 4.4376, Time: 10637.1s, Step: 17881, GPU: 4.9GB\n",
      "Epoch 1, Batch 17885, Loss: 4.4890, Time: 10640.6s, Step: 17886, GPU: 4.9GB\n",
      "Epoch 1, Batch 17890, Loss: 4.0838, Time: 10643.1s, Step: 17891, GPU: 4.9GB\n",
      "Epoch 1, Batch 17895, Loss: 4.0610, Time: 10646.5s, Step: 17896, GPU: 4.9GB\n",
      "Epoch 1, Batch 17900, Loss: 4.6043, Time: 10649.0s, Step: 17901, GPU: 4.9GB\n",
      "Epoch 1, Batch 17905, Loss: 4.3597, Time: 10652.4s, Step: 17906, GPU: 4.9GB\n",
      "Epoch 1, Batch 17910, Loss: 5.1164, Time: 10655.0s, Step: 17911, GPU: 4.9GB\n",
      "Epoch 1, Batch 17915, Loss: 4.8506, Time: 10658.4s, Step: 17916, GPU: 4.9GB\n",
      "Epoch 1, Batch 17920, Loss: 3.6745, Time: 10660.9s, Step: 17921, GPU: 4.9GB\n",
      "Epoch 1, Batch 17925, Loss: 2.7806, Time: 10664.4s, Step: 17926, GPU: 4.9GB\n",
      "Epoch 1, Batch 17930, Loss: 4.3969, Time: 10666.9s, Step: 17931, GPU: 4.9GB\n",
      "Epoch 1, Batch 17935, Loss: 4.6180, Time: 10670.3s, Step: 17936, GPU: 4.9GB\n",
      "Epoch 1, Batch 17940, Loss: 3.2564, Time: 10672.8s, Step: 17941, GPU: 4.9GB\n",
      "Epoch 1, Batch 17945, Loss: 3.4199, Time: 10676.2s, Step: 17946, GPU: 4.9GB\n",
      "Epoch 1, Batch 17950, Loss: 4.0741, Time: 10678.7s, Step: 17951, GPU: 4.9GB\n",
      "Epoch 1, Batch 17955, Loss: 5.0512, Time: 10682.2s, Step: 17956, GPU: 4.9GB\n",
      "Epoch 1, Batch 17960, Loss: 3.7929, Time: 10684.7s, Step: 17961, GPU: 4.9GB\n",
      "Epoch 1, Batch 17965, Loss: 4.4316, Time: 10688.1s, Step: 17966, GPU: 4.9GB\n",
      "Epoch 1, Batch 17970, Loss: 4.1250, Time: 10690.7s, Step: 17971, GPU: 4.9GB\n",
      "Epoch 1, Batch 17975, Loss: 4.2097, Time: 10694.1s, Step: 17976, GPU: 4.9GB\n",
      "Epoch 1, Batch 17980, Loss: 3.2399, Time: 10696.6s, Step: 17981, GPU: 4.9GB\n",
      "Epoch 1, Batch 17985, Loss: 4.0376, Time: 10700.1s, Step: 17986, GPU: 4.9GB\n",
      "Epoch 1, Batch 17990, Loss: 3.4525, Time: 10702.6s, Step: 17991, GPU: 4.9GB\n",
      "Epoch 1, Batch 17995, Loss: 3.8378, Time: 10706.1s, Step: 17996, GPU: 4.9GB\n",
      "Epoch 1, Batch 18000, Loss: 3.9496, Time: 10708.7s, Step: 18001, GPU: 4.9GB\n",
      "Epoch 1, Batch 18005, Loss: 4.8988, Time: 10712.1s, Step: 18006, GPU: 4.9GB\n",
      "Epoch 1, Batch 18010, Loss: 3.1451, Time: 10714.7s, Step: 18011, GPU: 4.9GB\n",
      "Epoch 1, Batch 18015, Loss: 4.2564, Time: 10718.6s, Step: 18016, GPU: 4.9GB\n",
      "Epoch 1, Batch 18020, Loss: 4.4026, Time: 10721.1s, Step: 18021, GPU: 4.9GB\n",
      "Epoch 1, Batch 18025, Loss: 4.2532, Time: 10724.5s, Step: 18026, GPU: 4.9GB\n",
      "Epoch 1, Batch 18030, Loss: 3.8116, Time: 10727.0s, Step: 18031, GPU: 4.9GB\n",
      "Epoch 1, Batch 18035, Loss: 2.9157, Time: 10730.5s, Step: 18036, GPU: 4.9GB\n",
      "Epoch 1, Batch 18040, Loss: 4.0384, Time: 10733.0s, Step: 18041, GPU: 4.9GB\n",
      "Epoch 1, Batch 18045, Loss: 3.2559, Time: 10736.4s, Step: 18046, GPU: 4.9GB\n",
      "Epoch 1, Batch 18050, Loss: 3.4722, Time: 10738.9s, Step: 18051, GPU: 4.9GB\n",
      "Epoch 1, Batch 18055, Loss: 3.9430, Time: 10742.4s, Step: 18056, GPU: 4.9GB\n",
      "Epoch 1, Batch 18060, Loss: 4.3971, Time: 10744.9s, Step: 18061, GPU: 4.9GB\n",
      "Epoch 1, Batch 18065, Loss: 4.7662, Time: 10748.3s, Step: 18066, GPU: 4.9GB\n",
      "Epoch 1, Batch 18070, Loss: 3.9320, Time: 10750.8s, Step: 18071, GPU: 4.9GB\n",
      "Epoch 1, Batch 18075, Loss: 3.6212, Time: 10754.2s, Step: 18076, GPU: 4.9GB\n",
      "Epoch 1, Batch 18080, Loss: 3.3924, Time: 10756.7s, Step: 18081, GPU: 4.9GB\n",
      "Epoch 1, Batch 18085, Loss: 4.4858, Time: 10760.2s, Step: 18086, GPU: 4.9GB\n",
      "Epoch 1, Batch 18090, Loss: 3.6002, Time: 10762.7s, Step: 18091, GPU: 4.9GB\n",
      "Epoch 1, Batch 18095, Loss: 2.8019, Time: 10766.1s, Step: 18096, GPU: 4.9GB\n",
      "Epoch 1, Batch 18100, Loss: 4.2083, Time: 10768.7s, Step: 18101, GPU: 4.9GB\n",
      "Epoch 1, Batch 18105, Loss: 3.8917, Time: 10772.1s, Step: 18106, GPU: 4.9GB\n",
      "Epoch 1, Batch 18110, Loss: 3.9207, Time: 10774.6s, Step: 18111, GPU: 4.9GB\n",
      "Epoch 1, Batch 18115, Loss: 3.7768, Time: 10778.0s, Step: 18116, GPU: 4.9GB\n",
      "Epoch 1, Batch 18120, Loss: 3.2811, Time: 10780.6s, Step: 18121, GPU: 4.9GB\n",
      "Epoch 1, Batch 18125, Loss: 4.3452, Time: 10784.0s, Step: 18126, GPU: 4.9GB\n",
      "Epoch 1, Batch 18130, Loss: 3.9006, Time: 10786.5s, Step: 18131, GPU: 4.9GB\n",
      "Epoch 1, Batch 18135, Loss: 4.1647, Time: 10789.9s, Step: 18136, GPU: 4.9GB\n",
      "Epoch 1, Batch 18140, Loss: 4.0815, Time: 10792.5s, Step: 18141, GPU: 4.9GB\n",
      "Epoch 1, Batch 18145, Loss: 3.9313, Time: 10795.9s, Step: 18146, GPU: 4.9GB\n",
      "Epoch 1, Batch 18150, Loss: 5.0735, Time: 10798.5s, Step: 18151, GPU: 4.9GB\n",
      "Epoch 1, Batch 18155, Loss: 4.6423, Time: 10801.9s, Step: 18156, GPU: 4.9GB\n",
      "Epoch 1, Batch 18160, Loss: 3.1954, Time: 10804.4s, Step: 18161, GPU: 4.9GB\n",
      "Epoch 1, Batch 18165, Loss: 2.9574, Time: 10807.9s, Step: 18166, GPU: 4.9GB\n",
      "Epoch 1, Batch 18170, Loss: 4.2538, Time: 10810.4s, Step: 18171, GPU: 4.9GB\n",
      "Epoch 1, Batch 18175, Loss: 4.1782, Time: 10813.8s, Step: 18176, GPU: 4.9GB\n",
      "Epoch 1, Batch 18180, Loss: 4.5618, Time: 10816.4s, Step: 18181, GPU: 4.9GB\n",
      "Epoch 1, Batch 18185, Loss: 4.0376, Time: 10819.8s, Step: 18186, GPU: 4.9GB\n",
      "Epoch 1, Batch 18190, Loss: 4.1703, Time: 10822.4s, Step: 18191, GPU: 4.9GB\n",
      "Epoch 1, Batch 18195, Loss: 4.4203, Time: 10825.9s, Step: 18196, GPU: 4.9GB\n",
      "Epoch 1, Batch 18200, Loss: 4.2146, Time: 10828.5s, Step: 18201, GPU: 4.9GB\n",
      "Epoch 1, Batch 18205, Loss: 4.7390, Time: 10831.9s, Step: 18206, GPU: 4.9GB\n",
      "Epoch 1, Batch 18210, Loss: 3.9712, Time: 10834.4s, Step: 18211, GPU: 4.9GB\n",
      "Epoch 1, Batch 18215, Loss: 3.7427, Time: 10837.8s, Step: 18216, GPU: 4.9GB\n",
      "Epoch 1, Batch 18220, Loss: 3.8070, Time: 10840.4s, Step: 18221, GPU: 4.9GB\n",
      "Epoch 1, Batch 18225, Loss: 3.3670, Time: 10843.8s, Step: 18226, GPU: 4.9GB\n",
      "Epoch 1, Batch 18230, Loss: 3.4851, Time: 10846.4s, Step: 18231, GPU: 4.9GB\n",
      "Epoch 1, Batch 18235, Loss: 3.9470, Time: 10849.8s, Step: 18236, GPU: 4.9GB\n",
      "Epoch 1, Batch 18240, Loss: 4.4689, Time: 10852.4s, Step: 18241, GPU: 4.9GB\n",
      "Epoch 1, Batch 18245, Loss: 4.0178, Time: 10855.8s, Step: 18246, GPU: 4.9GB\n",
      "Epoch 1, Batch 18250, Loss: 4.7753, Time: 10858.3s, Step: 18251, GPU: 4.9GB\n",
      "Epoch 1, Batch 18255, Loss: 3.8732, Time: 10861.7s, Step: 18256, GPU: 4.9GB\n",
      "Epoch 1, Batch 18260, Loss: 3.9751, Time: 10864.2s, Step: 18261, GPU: 4.9GB\n",
      "Epoch 1, Batch 18265, Loss: 4.2085, Time: 10867.6s, Step: 18266, GPU: 4.9GB\n",
      "Epoch 1, Batch 18270, Loss: 4.2737, Time: 10870.2s, Step: 18271, GPU: 4.9GB\n",
      "Epoch 1, Batch 18275, Loss: 3.7363, Time: 10873.7s, Step: 18276, GPU: 4.9GB\n",
      "Epoch 1, Batch 18280, Loss: 4.1746, Time: 10876.2s, Step: 18281, GPU: 4.9GB\n",
      "Epoch 1, Batch 18285, Loss: 5.4321, Time: 10879.6s, Step: 18286, GPU: 4.9GB\n",
      "Epoch 1, Batch 18290, Loss: 3.8596, Time: 10882.2s, Step: 18291, GPU: 4.9GB\n",
      "Epoch 1, Batch 18295, Loss: 4.7530, Time: 10885.6s, Step: 18296, GPU: 4.9GB\n",
      "Epoch 1, Batch 18300, Loss: 3.7418, Time: 10888.1s, Step: 18301, GPU: 4.9GB\n",
      "Epoch 1, Batch 18305, Loss: 3.6504, Time: 10891.6s, Step: 18306, GPU: 4.9GB\n",
      "Epoch 1, Batch 18310, Loss: 3.9184, Time: 10894.1s, Step: 18311, GPU: 4.9GB\n",
      "Epoch 1, Batch 18315, Loss: 4.6907, Time: 10897.5s, Step: 18316, GPU: 4.9GB\n",
      "Epoch 1, Batch 18320, Loss: 4.0296, Time: 10900.0s, Step: 18321, GPU: 4.9GB\n",
      "Epoch 1, Batch 18325, Loss: 4.5743, Time: 10903.4s, Step: 18326, GPU: 4.9GB\n",
      "Epoch 1, Batch 18330, Loss: 4.0157, Time: 10905.9s, Step: 18331, GPU: 4.9GB\n",
      "Epoch 1, Batch 18335, Loss: 4.2027, Time: 10909.3s, Step: 18336, GPU: 4.9GB\n",
      "Epoch 1, Batch 18340, Loss: 4.3553, Time: 10911.8s, Step: 18341, GPU: 4.9GB\n",
      "Epoch 1, Batch 18345, Loss: 3.4954, Time: 10915.2s, Step: 18346, GPU: 4.9GB\n",
      "Epoch 1, Batch 18350, Loss: 3.7229, Time: 10917.7s, Step: 18351, GPU: 4.9GB\n",
      "Epoch 1, Batch 18355, Loss: 3.6473, Time: 10921.2s, Step: 18356, GPU: 4.9GB\n",
      "Epoch 1, Batch 18360, Loss: 3.2725, Time: 10923.7s, Step: 18361, GPU: 4.9GB\n",
      "Epoch 1, Batch 18365, Loss: 5.1328, Time: 10927.1s, Step: 18366, GPU: 4.9GB\n",
      "Epoch 1, Batch 18370, Loss: 3.6121, Time: 10929.6s, Step: 18371, GPU: 4.9GB\n",
      "Epoch 1, Batch 18375, Loss: 3.9641, Time: 10933.1s, Step: 18376, GPU: 4.9GB\n",
      "Epoch 1, Batch 18380, Loss: 4.2884, Time: 10935.6s, Step: 18381, GPU: 4.9GB\n",
      "Epoch 1, Batch 18385, Loss: 4.1693, Time: 10939.0s, Step: 18386, GPU: 4.9GB\n",
      "Epoch 1, Batch 18390, Loss: 4.9726, Time: 10941.5s, Step: 18391, GPU: 4.9GB\n",
      "Epoch 1, Batch 18395, Loss: 3.7634, Time: 10944.9s, Step: 18396, GPU: 4.9GB\n",
      "Epoch 1, Batch 18400, Loss: 3.6162, Time: 10947.5s, Step: 18401, GPU: 4.9GB\n",
      "Epoch 1, Batch 18405, Loss: 5.0868, Time: 10950.9s, Step: 18406, GPU: 4.9GB\n",
      "Epoch 1, Batch 18410, Loss: 3.3202, Time: 10953.4s, Step: 18411, GPU: 4.9GB\n",
      "Epoch 1, Batch 18415, Loss: 4.0050, Time: 10956.9s, Step: 18416, GPU: 4.9GB\n",
      "Epoch 1, Batch 18420, Loss: 3.7576, Time: 10959.4s, Step: 18421, GPU: 4.9GB\n",
      "Epoch 1, Batch 18425, Loss: 3.9355, Time: 10962.8s, Step: 18426, GPU: 4.9GB\n",
      "Epoch 1, Batch 18430, Loss: 3.1819, Time: 10965.3s, Step: 18431, GPU: 4.9GB\n",
      "Epoch 1, Batch 18435, Loss: 3.7438, Time: 10968.8s, Step: 18436, GPU: 4.9GB\n",
      "Epoch 1, Batch 18440, Loss: 4.2550, Time: 10971.3s, Step: 18441, GPU: 4.9GB\n",
      "Epoch 1, Batch 18445, Loss: 4.1195, Time: 10974.7s, Step: 18446, GPU: 4.9GB\n",
      "Epoch 1, Batch 18450, Loss: 3.7363, Time: 10977.3s, Step: 18451, GPU: 4.9GB\n",
      "Epoch 1, Batch 18455, Loss: 3.7146, Time: 10980.7s, Step: 18456, GPU: 4.9GB\n",
      "Epoch 1, Batch 18460, Loss: 4.1365, Time: 10983.2s, Step: 18461, GPU: 4.9GB\n",
      "Epoch 1, Batch 18465, Loss: 3.6468, Time: 10986.6s, Step: 18466, GPU: 4.9GB\n",
      "Epoch 1, Batch 18470, Loss: 4.2339, Time: 10989.1s, Step: 18471, GPU: 4.9GB\n",
      "Epoch 1, Batch 18475, Loss: 3.8790, Time: 10992.5s, Step: 18476, GPU: 4.9GB\n",
      "Epoch 1, Batch 18480, Loss: 4.5082, Time: 10995.0s, Step: 18481, GPU: 4.9GB\n",
      "Epoch 1, Batch 18485, Loss: 4.3520, Time: 10998.5s, Step: 18486, GPU: 4.9GB\n",
      "Epoch 1, Batch 18490, Loss: 3.7529, Time: 11001.0s, Step: 18491, GPU: 4.9GB\n",
      "Epoch 1, Batch 18495, Loss: 3.3676, Time: 11004.4s, Step: 18496, GPU: 4.9GB\n",
      "Epoch 1, Batch 18500, Loss: 3.5823, Time: 11006.9s, Step: 18501, GPU: 4.9GB\n",
      "Epoch 1, Batch 18505, Loss: 4.0547, Time: 11010.4s, Step: 18506, GPU: 4.9GB\n",
      "Epoch 1, Batch 18510, Loss: 4.0218, Time: 11012.9s, Step: 18511, GPU: 4.9GB\n",
      "Epoch 1, Batch 18515, Loss: 3.8263, Time: 11016.3s, Step: 18516, GPU: 4.9GB\n",
      "Epoch 1, Batch 18520, Loss: 3.9667, Time: 11018.8s, Step: 18521, GPU: 4.9GB\n",
      "Epoch 1, Batch 18525, Loss: 3.8091, Time: 11022.2s, Step: 18526, GPU: 4.9GB\n",
      "Epoch 1, Batch 18530, Loss: 3.0756, Time: 11024.7s, Step: 18531, GPU: 4.9GB\n",
      "Epoch 1, Batch 18535, Loss: 4.0855, Time: 11028.2s, Step: 18536, GPU: 4.9GB\n",
      "Epoch 1, Batch 18540, Loss: 4.1870, Time: 11030.7s, Step: 18541, GPU: 4.9GB\n",
      "Epoch 1, Batch 18545, Loss: 4.8443, Time: 11034.1s, Step: 18546, GPU: 4.9GB\n",
      "Epoch 1, Batch 18550, Loss: 4.8475, Time: 11036.6s, Step: 18551, GPU: 4.9GB\n",
      "Epoch 1, Batch 18555, Loss: 4.6488, Time: 11040.0s, Step: 18556, GPU: 4.9GB\n",
      "Epoch 1, Batch 18560, Loss: 3.4259, Time: 11042.5s, Step: 18561, GPU: 4.9GB\n",
      "Epoch 1, Batch 18565, Loss: 4.3855, Time: 11045.9s, Step: 18566, GPU: 4.9GB\n",
      "Epoch 1, Batch 18570, Loss: 4.2254, Time: 11048.4s, Step: 18571, GPU: 4.9GB\n",
      "Epoch 1, Batch 18575, Loss: 3.8861, Time: 11051.8s, Step: 18576, GPU: 4.9GB\n",
      "Epoch 1, Batch 18580, Loss: 3.3712, Time: 11054.4s, Step: 18581, GPU: 4.9GB\n",
      "Epoch 1, Batch 18585, Loss: 4.0473, Time: 11057.8s, Step: 18586, GPU: 4.9GB\n",
      "Epoch 1, Batch 18590, Loss: 3.5870, Time: 11060.3s, Step: 18591, GPU: 4.9GB\n",
      "Epoch 1, Batch 18595, Loss: 4.1758, Time: 11063.7s, Step: 18596, GPU: 4.9GB\n",
      "Epoch 1, Batch 18600, Loss: 3.8652, Time: 11066.3s, Step: 18601, GPU: 4.9GB\n",
      "Epoch 1, Batch 18605, Loss: 3.9608, Time: 11069.7s, Step: 18606, GPU: 4.9GB\n",
      "Epoch 1, Batch 18610, Loss: 5.0500, Time: 11072.2s, Step: 18611, GPU: 4.9GB\n",
      "Epoch 1, Batch 18615, Loss: 4.2970, Time: 11075.6s, Step: 18616, GPU: 4.9GB\n",
      "Epoch 1, Batch 18620, Loss: 4.4060, Time: 11078.1s, Step: 18621, GPU: 4.9GB\n",
      "Epoch 1, Batch 18625, Loss: 3.9088, Time: 11081.5s, Step: 18626, GPU: 4.9GB\n",
      "Epoch 1, Batch 18630, Loss: 4.4198, Time: 11084.0s, Step: 18631, GPU: 4.9GB\n",
      "Epoch 1, Batch 18635, Loss: 3.4541, Time: 11087.4s, Step: 18636, GPU: 4.9GB\n",
      "Epoch 1, Batch 18640, Loss: 3.3906, Time: 11089.9s, Step: 18641, GPU: 4.9GB\n",
      "Epoch 1, Batch 18645, Loss: 4.0166, Time: 11093.3s, Step: 18646, GPU: 4.9GB\n",
      "Epoch 1, Batch 18650, Loss: 3.2050, Time: 11095.8s, Step: 18651, GPU: 4.9GB\n",
      "Epoch 1, Batch 18655, Loss: 3.8391, Time: 11099.2s, Step: 18656, GPU: 4.9GB\n",
      "Epoch 1, Batch 18660, Loss: 4.0056, Time: 11101.7s, Step: 18661, GPU: 4.9GB\n",
      "Epoch 1, Batch 18665, Loss: 4.3078, Time: 11105.1s, Step: 18666, GPU: 4.9GB\n",
      "Epoch 1, Batch 18670, Loss: 3.1984, Time: 11107.7s, Step: 18671, GPU: 4.9GB\n",
      "Epoch 1, Batch 18675, Loss: 4.0648, Time: 11111.1s, Step: 18676, GPU: 4.9GB\n",
      "Epoch 1, Batch 18680, Loss: 4.6528, Time: 11113.6s, Step: 18681, GPU: 4.9GB\n",
      "Epoch 1, Batch 18685, Loss: 3.1842, Time: 11117.0s, Step: 18686, GPU: 4.9GB\n",
      "Epoch 1, Batch 18690, Loss: 4.7259, Time: 11119.5s, Step: 18691, GPU: 4.9GB\n",
      "Epoch 1, Batch 18695, Loss: 4.1341, Time: 11123.0s, Step: 18696, GPU: 4.9GB\n",
      "Epoch 1, Batch 18700, Loss: 4.4084, Time: 11125.5s, Step: 18701, GPU: 4.9GB\n",
      "Epoch 1, Batch 18705, Loss: 3.7135, Time: 11128.9s, Step: 18706, GPU: 4.9GB\n",
      "Epoch 1, Batch 18710, Loss: 3.9623, Time: 11131.4s, Step: 18711, GPU: 4.9GB\n",
      "Epoch 1, Batch 18715, Loss: 3.4495, Time: 11134.9s, Step: 18716, GPU: 4.9GB\n",
      "Epoch 1, Batch 18720, Loss: 4.3263, Time: 11137.4s, Step: 18721, GPU: 4.9GB\n",
      "Epoch 1, Batch 18725, Loss: 4.2490, Time: 11140.8s, Step: 18726, GPU: 4.9GB\n",
      "Epoch 1, Batch 18730, Loss: 4.1890, Time: 11143.4s, Step: 18731, GPU: 4.9GB\n",
      "Epoch 1, Batch 18735, Loss: 4.0379, Time: 11146.8s, Step: 18736, GPU: 4.9GB\n",
      "Epoch 1, Batch 18740, Loss: 3.5625, Time: 11149.3s, Step: 18741, GPU: 4.9GB\n",
      "Epoch 1, Batch 18745, Loss: 4.1122, Time: 11152.7s, Step: 18746, GPU: 4.9GB\n",
      "Epoch 1, Batch 18750, Loss: 4.2452, Time: 11155.2s, Step: 18751, GPU: 4.9GB\n",
      "Epoch 1, Batch 18755, Loss: 3.5342, Time: 11158.6s, Step: 18756, GPU: 4.9GB\n",
      "Epoch 1, Batch 18760, Loss: 4.2429, Time: 11161.2s, Step: 18761, GPU: 4.9GB\n",
      "Epoch 1, Batch 18765, Loss: 3.3866, Time: 11164.6s, Step: 18766, GPU: 4.9GB\n",
      "Epoch 1, Batch 18770, Loss: 4.8191, Time: 11167.1s, Step: 18771, GPU: 4.9GB\n",
      "Epoch 1, Batch 18775, Loss: 4.1747, Time: 11170.5s, Step: 18776, GPU: 4.9GB\n",
      "Epoch 1, Batch 18780, Loss: 5.0286, Time: 11173.0s, Step: 18781, GPU: 4.9GB\n",
      "Epoch 1, Batch 18785, Loss: 3.7948, Time: 11176.4s, Step: 18786, GPU: 4.9GB\n",
      "Epoch 1, Batch 18790, Loss: 3.7762, Time: 11178.9s, Step: 18791, GPU: 4.9GB\n",
      "Epoch 1, Batch 18795, Loss: 4.5608, Time: 11182.3s, Step: 18796, GPU: 4.9GB\n",
      "Epoch 1, Batch 18800, Loss: 4.4988, Time: 11184.9s, Step: 18801, GPU: 4.9GB\n",
      "Epoch 1, Batch 18805, Loss: 3.7254, Time: 11188.3s, Step: 18806, GPU: 4.9GB\n",
      "Epoch 1, Batch 18810, Loss: 4.0476, Time: 11190.8s, Step: 18811, GPU: 4.9GB\n",
      "Epoch 1, Batch 18815, Loss: 4.5505, Time: 11194.2s, Step: 18816, GPU: 4.9GB\n",
      "Epoch 1, Batch 18820, Loss: 4.9421, Time: 11196.8s, Step: 18821, GPU: 4.9GB\n",
      "Epoch 1, Batch 18825, Loss: 3.6279, Time: 11200.2s, Step: 18826, GPU: 4.9GB\n",
      "Epoch 1, Batch 18830, Loss: 3.5367, Time: 11202.7s, Step: 18831, GPU: 4.9GB\n",
      "Epoch 1, Batch 18835, Loss: 4.3369, Time: 11206.1s, Step: 18836, GPU: 4.9GB\n",
      "Epoch 1, Batch 18840, Loss: 3.6097, Time: 11208.6s, Step: 18841, GPU: 4.9GB\n",
      "Epoch 1, Batch 18845, Loss: 4.3345, Time: 11212.1s, Step: 18846, GPU: 4.9GB\n",
      "Epoch 1, Batch 18850, Loss: 4.4252, Time: 11214.6s, Step: 18851, GPU: 4.9GB\n",
      "Epoch 1, Batch 18855, Loss: 4.8841, Time: 11218.0s, Step: 18856, GPU: 4.9GB\n",
      "Epoch 1, Batch 18860, Loss: 3.8512, Time: 11220.5s, Step: 18861, GPU: 4.9GB\n",
      "Epoch 1, Batch 18865, Loss: 4.3919, Time: 11223.9s, Step: 18866, GPU: 4.9GB\n",
      "Epoch 1, Batch 18870, Loss: 3.7572, Time: 11226.5s, Step: 18871, GPU: 4.9GB\n",
      "Epoch 1, Batch 18875, Loss: 3.2380, Time: 11229.9s, Step: 18876, GPU: 4.9GB\n",
      "Epoch 1, Batch 18880, Loss: 4.1939, Time: 11232.5s, Step: 18881, GPU: 4.9GB\n",
      "Epoch 1, Batch 18885, Loss: 3.3559, Time: 11235.9s, Step: 18886, GPU: 4.9GB\n",
      "Epoch 1, Batch 18890, Loss: 4.4854, Time: 11238.5s, Step: 18891, GPU: 4.9GB\n",
      "Epoch 1, Batch 18895, Loss: 4.1498, Time: 11241.9s, Step: 18896, GPU: 4.9GB\n",
      "Epoch 1, Batch 18900, Loss: 5.1910, Time: 11244.4s, Step: 18901, GPU: 4.9GB\n",
      "Epoch 1, Batch 18905, Loss: 3.6885, Time: 11247.8s, Step: 18906, GPU: 4.9GB\n",
      "Epoch 1, Batch 18910, Loss: 3.5039, Time: 11250.3s, Step: 18911, GPU: 4.9GB\n",
      "Epoch 1, Batch 18915, Loss: 3.7754, Time: 11253.9s, Step: 18916, GPU: 4.9GB\n",
      "Epoch 1, Batch 18920, Loss: 3.8299, Time: 11256.4s, Step: 18921, GPU: 4.9GB\n",
      "Epoch 1, Batch 18925, Loss: 4.0313, Time: 11259.9s, Step: 18926, GPU: 4.9GB\n",
      "Epoch 1, Batch 18930, Loss: 4.6857, Time: 11262.4s, Step: 18931, GPU: 4.9GB\n",
      "Epoch 1, Batch 18935, Loss: 3.9365, Time: 11265.8s, Step: 18936, GPU: 4.9GB\n",
      "Epoch 1, Batch 18940, Loss: 4.2778, Time: 11268.3s, Step: 18941, GPU: 4.9GB\n",
      "Epoch 1, Batch 18945, Loss: 4.5336, Time: 11271.7s, Step: 18946, GPU: 4.9GB\n",
      "Epoch 1, Batch 18950, Loss: 3.7734, Time: 11274.3s, Step: 18951, GPU: 4.9GB\n",
      "Epoch 1, Batch 18955, Loss: 3.6963, Time: 11277.7s, Step: 18956, GPU: 4.9GB\n",
      "Epoch 1, Batch 18960, Loss: 4.0679, Time: 11280.2s, Step: 18961, GPU: 4.9GB\n",
      "Epoch 1, Batch 18965, Loss: 3.7180, Time: 11283.7s, Step: 18966, GPU: 4.9GB\n",
      "Epoch 1, Batch 18970, Loss: 3.9726, Time: 11286.2s, Step: 18971, GPU: 4.9GB\n",
      "Epoch 1, Batch 18975, Loss: 4.7383, Time: 11289.6s, Step: 18976, GPU: 4.9GB\n",
      "Epoch 1, Batch 18980, Loss: 3.0237, Time: 11292.2s, Step: 18981, GPU: 4.9GB\n",
      "Epoch 1, Batch 18985, Loss: 3.4714, Time: 11295.6s, Step: 18986, GPU: 4.9GB\n",
      "Epoch 1, Batch 18990, Loss: 4.8548, Time: 11298.1s, Step: 18991, GPU: 4.9GB\n",
      "Epoch 1, Batch 18995, Loss: 3.6203, Time: 11301.6s, Step: 18996, GPU: 4.9GB\n",
      "Epoch 1, Batch 19000, Loss: 4.1478, Time: 11304.2s, Step: 19001, GPU: 4.9GB\n",
      "Epoch 1, Batch 19005, Loss: 4.0041, Time: 11307.7s, Step: 19006, GPU: 4.9GB\n",
      "Epoch 1, Batch 19010, Loss: 3.0607, Time: 11310.2s, Step: 19011, GPU: 4.9GB\n",
      "Epoch 1, Batch 19015, Loss: 4.6958, Time: 11313.6s, Step: 19016, GPU: 4.9GB\n",
      "Epoch 1, Batch 19020, Loss: 4.0305, Time: 11316.1s, Step: 19021, GPU: 4.9GB\n",
      "Epoch 1, Batch 19025, Loss: 3.1587, Time: 11319.6s, Step: 19026, GPU: 4.9GB\n",
      "Epoch 1, Batch 19030, Loss: 4.6646, Time: 11322.1s, Step: 19031, GPU: 4.9GB\n",
      "Epoch 1, Batch 19035, Loss: 4.5111, Time: 11325.5s, Step: 19036, GPU: 4.9GB\n",
      "Epoch 1, Batch 19040, Loss: 4.0140, Time: 11328.0s, Step: 19041, GPU: 4.9GB\n",
      "Epoch 1, Batch 19045, Loss: 4.1618, Time: 11331.4s, Step: 19046, GPU: 4.9GB\n",
      "Epoch 1, Batch 19050, Loss: 4.2335, Time: 11334.1s, Step: 19051, GPU: 4.9GB\n",
      "Epoch 1, Batch 19055, Loss: 4.9741, Time: 11337.6s, Step: 19056, GPU: 4.9GB\n",
      "Epoch 1, Batch 19060, Loss: 3.6720, Time: 11340.1s, Step: 19061, GPU: 4.9GB\n",
      "Epoch 1, Batch 19065, Loss: 4.1271, Time: 11343.5s, Step: 19066, GPU: 4.9GB\n",
      "Epoch 1, Batch 19070, Loss: 3.8701, Time: 11346.0s, Step: 19071, GPU: 4.9GB\n",
      "Epoch 1, Batch 19075, Loss: 3.6954, Time: 11349.4s, Step: 19076, GPU: 4.9GB\n",
      "Epoch 1, Batch 19080, Loss: 3.3652, Time: 11352.0s, Step: 19081, GPU: 4.9GB\n",
      "Epoch 1, Batch 19085, Loss: 5.2186, Time: 11355.4s, Step: 19086, GPU: 4.9GB\n",
      "Epoch 1, Batch 19090, Loss: 4.3892, Time: 11357.9s, Step: 19091, GPU: 4.9GB\n",
      "Epoch 1, Batch 19095, Loss: 4.4935, Time: 11361.3s, Step: 19096, GPU: 4.9GB\n",
      "Epoch 1, Batch 19100, Loss: 3.8948, Time: 11363.9s, Step: 19101, GPU: 4.9GB\n",
      "Epoch 1, Batch 19105, Loss: 4.6618, Time: 11367.3s, Step: 19106, GPU: 4.9GB\n",
      "Epoch 1, Batch 19110, Loss: 4.5450, Time: 11369.8s, Step: 19111, GPU: 4.9GB\n",
      "Epoch 1, Batch 19115, Loss: 4.2329, Time: 11373.3s, Step: 19116, GPU: 4.9GB\n",
      "Epoch 1, Batch 19120, Loss: 4.1897, Time: 11375.8s, Step: 19121, GPU: 4.9GB\n",
      "Epoch 1, Batch 19125, Loss: 3.7986, Time: 11379.2s, Step: 19126, GPU: 4.9GB\n",
      "Epoch 1, Batch 19130, Loss: 3.5549, Time: 11381.8s, Step: 19131, GPU: 4.9GB\n",
      "Epoch 1, Batch 19135, Loss: 4.1620, Time: 11385.2s, Step: 19136, GPU: 4.9GB\n",
      "Epoch 1, Batch 19140, Loss: 4.2926, Time: 11387.7s, Step: 19141, GPU: 4.9GB\n",
      "Epoch 1, Batch 19145, Loss: 3.8830, Time: 11391.1s, Step: 19146, GPU: 4.9GB\n",
      "Epoch 1, Batch 19150, Loss: 3.7883, Time: 11393.7s, Step: 19151, GPU: 4.9GB\n",
      "Epoch 1, Batch 19155, Loss: 3.5510, Time: 11397.1s, Step: 19156, GPU: 4.9GB\n",
      "Epoch 1, Batch 19160, Loss: 3.5075, Time: 11399.6s, Step: 19161, GPU: 4.9GB\n",
      "Epoch 1, Batch 19165, Loss: 3.9542, Time: 11403.0s, Step: 19166, GPU: 4.9GB\n",
      "Epoch 1, Batch 19170, Loss: 5.3636, Time: 11405.5s, Step: 19171, GPU: 4.9GB\n",
      "Epoch 1, Batch 19175, Loss: 3.8533, Time: 11409.0s, Step: 19176, GPU: 4.9GB\n",
      "Epoch 1, Batch 19180, Loss: 4.1765, Time: 11411.5s, Step: 19181, GPU: 4.9GB\n",
      "Epoch 1, Batch 19185, Loss: 5.1804, Time: 11414.9s, Step: 19186, GPU: 4.9GB\n",
      "Epoch 1, Batch 19190, Loss: 3.8966, Time: 11417.4s, Step: 19191, GPU: 4.9GB\n",
      "Epoch 1, Batch 19195, Loss: 3.9142, Time: 11420.8s, Step: 19196, GPU: 4.9GB\n",
      "Epoch 1, Batch 19200, Loss: 3.9752, Time: 11423.4s, Step: 19201, GPU: 4.9GB\n",
      "Epoch 1, Batch 19205, Loss: 4.1281, Time: 11426.9s, Step: 19206, GPU: 4.9GB\n",
      "Epoch 1, Batch 19210, Loss: 3.7099, Time: 11429.4s, Step: 19211, GPU: 4.9GB\n",
      "Epoch 1, Batch 19215, Loss: 4.8351, Time: 11432.8s, Step: 19216, GPU: 4.9GB\n",
      "Epoch 1, Batch 19220, Loss: 3.3903, Time: 11435.3s, Step: 19221, GPU: 4.9GB\n",
      "Epoch 1, Batch 19225, Loss: 3.8513, Time: 11438.8s, Step: 19226, GPU: 4.9GB\n",
      "Epoch 1, Batch 19230, Loss: 3.7250, Time: 11441.3s, Step: 19231, GPU: 4.9GB\n",
      "Epoch 1, Batch 19235, Loss: 4.4301, Time: 11444.7s, Step: 19236, GPU: 4.9GB\n",
      "Epoch 1, Batch 19240, Loss: 4.2927, Time: 11447.2s, Step: 19241, GPU: 4.9GB\n",
      "Epoch 1, Batch 19245, Loss: 4.0062, Time: 11450.6s, Step: 19246, GPU: 4.9GB\n",
      "Epoch 1, Batch 19250, Loss: 4.2642, Time: 11453.2s, Step: 19251, GPU: 4.9GB\n",
      "Epoch 1, Batch 19255, Loss: 3.9481, Time: 11456.6s, Step: 19256, GPU: 4.9GB\n",
      "Epoch 1, Batch 19260, Loss: 4.7524, Time: 11459.1s, Step: 19261, GPU: 4.9GB\n",
      "Epoch 1, Batch 19265, Loss: 4.4701, Time: 11462.5s, Step: 19266, GPU: 4.9GB\n",
      "Epoch 1, Batch 19270, Loss: 4.1556, Time: 11465.0s, Step: 19271, GPU: 4.9GB\n",
      "Epoch 1, Batch 19275, Loss: 2.9941, Time: 11468.5s, Step: 19276, GPU: 4.9GB\n",
      "Epoch 1, Batch 19280, Loss: 4.3318, Time: 11471.0s, Step: 19281, GPU: 4.9GB\n",
      "Epoch 1, Batch 19285, Loss: 3.3113, Time: 11474.4s, Step: 19286, GPU: 4.9GB\n",
      "Epoch 1, Batch 19290, Loss: 3.5312, Time: 11476.9s, Step: 19291, GPU: 4.9GB\n",
      "Epoch 1, Batch 19295, Loss: 4.1411, Time: 11480.3s, Step: 19296, GPU: 4.9GB\n",
      "Epoch 1, Batch 19300, Loss: 3.9915, Time: 11482.8s, Step: 19301, GPU: 4.9GB\n",
      "Epoch 1, Batch 19305, Loss: 3.8367, Time: 11486.2s, Step: 19306, GPU: 4.9GB\n",
      "Epoch 1, Batch 19310, Loss: 3.7604, Time: 11488.8s, Step: 19311, GPU: 4.9GB\n",
      "Epoch 1, Batch 19315, Loss: 3.7306, Time: 11492.2s, Step: 19316, GPU: 4.9GB\n",
      "Epoch 1, Batch 19320, Loss: 3.5216, Time: 11494.8s, Step: 19321, GPU: 4.9GB\n",
      "Epoch 1, Batch 19325, Loss: 3.6791, Time: 11498.2s, Step: 19326, GPU: 4.9GB\n",
      "Epoch 1, Batch 19330, Loss: 3.5990, Time: 11500.7s, Step: 19331, GPU: 4.9GB\n",
      "Epoch 1, Batch 19335, Loss: 4.2314, Time: 11504.2s, Step: 19336, GPU: 4.9GB\n",
      "Epoch 1, Batch 19340, Loss: 4.0449, Time: 11506.7s, Step: 19341, GPU: 4.9GB\n",
      "Epoch 1, Batch 19345, Loss: 3.6951, Time: 11510.1s, Step: 19346, GPU: 4.9GB\n",
      "Epoch 1, Batch 19350, Loss: 5.2741, Time: 11512.6s, Step: 19351, GPU: 4.9GB\n",
      "Epoch 1, Batch 19355, Loss: 2.5831, Time: 11516.1s, Step: 19356, GPU: 4.9GB\n",
      "Epoch 1, Batch 19360, Loss: 3.8171, Time: 11518.6s, Step: 19361, GPU: 4.9GB\n",
      "Epoch 1, Batch 19365, Loss: 3.7794, Time: 11522.0s, Step: 19366, GPU: 4.9GB\n",
      "Epoch 1, Batch 19370, Loss: 4.8563, Time: 11524.6s, Step: 19371, GPU: 4.9GB\n",
      "Epoch 1, Batch 19375, Loss: 4.3641, Time: 11528.0s, Step: 19376, GPU: 4.9GB\n",
      "Epoch 1, Batch 19380, Loss: 3.8485, Time: 11530.5s, Step: 19381, GPU: 4.9GB\n",
      "Epoch 1, Batch 19385, Loss: 2.9772, Time: 11533.9s, Step: 19386, GPU: 4.9GB\n",
      "Epoch 1, Batch 19390, Loss: 3.6974, Time: 11536.4s, Step: 19391, GPU: 4.9GB\n",
      "Epoch 1, Batch 19395, Loss: 4.3632, Time: 11539.9s, Step: 19396, GPU: 4.9GB\n",
      "Epoch 1, Batch 19400, Loss: 3.5289, Time: 11542.5s, Step: 19401, GPU: 4.9GB\n",
      "Epoch 1, Batch 19405, Loss: 4.3708, Time: 11545.9s, Step: 19406, GPU: 4.9GB\n",
      "Epoch 1, Batch 19410, Loss: 3.1248, Time: 11548.5s, Step: 19411, GPU: 4.9GB\n",
      "Epoch 1, Batch 19415, Loss: 3.0503, Time: 11551.9s, Step: 19416, GPU: 4.9GB\n",
      "Epoch 1, Batch 19420, Loss: 3.2485, Time: 11554.4s, Step: 19421, GPU: 4.9GB\n",
      "Epoch 1, Batch 19425, Loss: 4.4260, Time: 11557.8s, Step: 19426, GPU: 4.9GB\n",
      "Epoch 1, Batch 19430, Loss: 3.9787, Time: 11560.4s, Step: 19431, GPU: 4.9GB\n",
      "Epoch 1, Batch 19435, Loss: 2.8015, Time: 11563.8s, Step: 19436, GPU: 4.9GB\n",
      "Epoch 1, Batch 19440, Loss: 2.9471, Time: 11566.3s, Step: 19441, GPU: 4.9GB\n",
      "Epoch 1, Batch 19445, Loss: 4.8250, Time: 11569.7s, Step: 19446, GPU: 4.9GB\n",
      "Epoch 1, Batch 19450, Loss: 4.1089, Time: 11572.3s, Step: 19451, GPU: 4.9GB\n",
      "Epoch 1, Batch 19455, Loss: 3.5742, Time: 11575.7s, Step: 19456, GPU: 4.9GB\n",
      "Epoch 1, Batch 19460, Loss: 4.7854, Time: 11578.2s, Step: 19461, GPU: 4.9GB\n",
      "Epoch 1, Batch 19465, Loss: 4.0418, Time: 11581.6s, Step: 19466, GPU: 4.9GB\n",
      "Epoch 1, Batch 19470, Loss: 4.4644, Time: 11584.2s, Step: 19471, GPU: 4.9GB\n",
      "Epoch 1, Batch 19475, Loss: 4.4298, Time: 11587.6s, Step: 19476, GPU: 4.9GB\n",
      "Epoch 1, Batch 19480, Loss: 3.7184, Time: 11590.1s, Step: 19481, GPU: 4.9GB\n",
      "Epoch 1, Batch 19485, Loss: 4.4551, Time: 11593.6s, Step: 19486, GPU: 4.9GB\n",
      "Epoch 1, Batch 19490, Loss: 3.4267, Time: 11596.1s, Step: 19491, GPU: 4.9GB\n",
      "Epoch 1, Batch 19495, Loss: 4.1580, Time: 11599.6s, Step: 19496, GPU: 4.9GB\n",
      "Epoch 1, Batch 19500, Loss: 4.1717, Time: 11602.1s, Step: 19501, GPU: 4.9GB\n",
      "Epoch 1, Batch 19505, Loss: 5.2729, Time: 11605.5s, Step: 19506, GPU: 4.9GB\n",
      "Epoch 1, Batch 19510, Loss: 3.3583, Time: 11608.0s, Step: 19511, GPU: 4.9GB\n",
      "Epoch 1, Batch 19515, Loss: 3.0186, Time: 11611.5s, Step: 19516, GPU: 4.9GB\n",
      "Epoch 1, Batch 19520, Loss: 3.8513, Time: 11614.0s, Step: 19521, GPU: 4.9GB\n",
      "Epoch 1, Batch 19525, Loss: 3.8641, Time: 11617.4s, Step: 19526, GPU: 4.9GB\n",
      "Epoch 1, Batch 19530, Loss: 4.6219, Time: 11620.0s, Step: 19531, GPU: 4.9GB\n",
      "Epoch 1, Batch 19535, Loss: 4.7939, Time: 11623.4s, Step: 19536, GPU: 4.9GB\n",
      "Epoch 1, Batch 19540, Loss: 4.1808, Time: 11625.9s, Step: 19541, GPU: 4.9GB\n",
      "Epoch 1, Batch 19545, Loss: 4.1955, Time: 11629.3s, Step: 19546, GPU: 4.9GB\n",
      "Epoch 1, Batch 19550, Loss: 3.9968, Time: 11631.9s, Step: 19551, GPU: 4.9GB\n",
      "Epoch 1, Batch 19555, Loss: 4.0977, Time: 11635.3s, Step: 19556, GPU: 4.9GB\n",
      "Epoch 1, Batch 19560, Loss: 3.9908, Time: 11637.8s, Step: 19561, GPU: 4.9GB\n",
      "Epoch 1, Batch 19565, Loss: 3.9971, Time: 11641.2s, Step: 19566, GPU: 4.9GB\n",
      "Epoch 1, Batch 19570, Loss: 4.4747, Time: 11643.8s, Step: 19571, GPU: 4.9GB\n",
      "Epoch 1, Batch 19575, Loss: 4.0863, Time: 11647.2s, Step: 19576, GPU: 4.9GB\n",
      "Epoch 1, Batch 19580, Loss: 3.5026, Time: 11649.7s, Step: 19581, GPU: 4.9GB\n",
      "Epoch 1, Batch 19585, Loss: 4.8580, Time: 11653.1s, Step: 19586, GPU: 4.9GB\n",
      "Epoch 1, Batch 19590, Loss: 3.9858, Time: 11655.6s, Step: 19591, GPU: 4.9GB\n",
      "Epoch 1, Batch 19595, Loss: 3.8152, Time: 11659.1s, Step: 19596, GPU: 4.9GB\n",
      "Epoch 1, Batch 19600, Loss: 3.6524, Time: 11661.6s, Step: 19601, GPU: 4.9GB\n",
      "Epoch 1, Batch 19605, Loss: 3.5373, Time: 11665.0s, Step: 19606, GPU: 4.9GB\n",
      "Epoch 1, Batch 19610, Loss: 3.6100, Time: 11667.5s, Step: 19611, GPU: 4.9GB\n",
      "Epoch 1, Batch 19615, Loss: 3.8404, Time: 11671.0s, Step: 19616, GPU: 4.9GB\n",
      "Epoch 1, Batch 19620, Loss: 3.5681, Time: 11673.5s, Step: 19621, GPU: 4.9GB\n",
      "Epoch 1, Batch 19625, Loss: 4.0678, Time: 11676.9s, Step: 19626, GPU: 4.9GB\n",
      "Epoch 1, Batch 19630, Loss: 4.9002, Time: 11679.4s, Step: 19631, GPU: 4.9GB\n",
      "Epoch 1, Batch 19635, Loss: 4.1187, Time: 11682.8s, Step: 19636, GPU: 4.9GB\n",
      "Epoch 1, Batch 19640, Loss: 4.0537, Time: 11685.3s, Step: 19641, GPU: 4.9GB\n",
      "Epoch 1, Batch 19645, Loss: 3.8505, Time: 11688.7s, Step: 19646, GPU: 4.9GB\n",
      "Epoch 1, Batch 19650, Loss: 5.1980, Time: 11691.2s, Step: 19651, GPU: 4.9GB\n",
      "Epoch 1, Batch 19655, Loss: 4.1032, Time: 11694.6s, Step: 19656, GPU: 4.9GB\n",
      "Epoch 1, Batch 19660, Loss: 3.9608, Time: 11697.1s, Step: 19661, GPU: 4.9GB\n",
      "Epoch 1, Batch 19665, Loss: 4.0615, Time: 11700.5s, Step: 19666, GPU: 4.9GB\n",
      "Epoch 1, Batch 19670, Loss: 4.6526, Time: 11703.0s, Step: 19671, GPU: 4.9GB\n",
      "Epoch 1, Batch 19675, Loss: 3.1772, Time: 11706.4s, Step: 19676, GPU: 4.9GB\n",
      "Epoch 1, Batch 19680, Loss: 3.7983, Time: 11708.9s, Step: 19681, GPU: 4.9GB\n",
      "Epoch 1, Batch 19685, Loss: 4.7270, Time: 11712.3s, Step: 19686, GPU: 4.9GB\n",
      "Epoch 1, Batch 19690, Loss: 3.3913, Time: 11714.8s, Step: 19691, GPU: 4.9GB\n",
      "Epoch 1, Batch 19695, Loss: 4.3252, Time: 11718.2s, Step: 19696, GPU: 4.9GB\n",
      "Epoch 1, Batch 19700, Loss: 4.4046, Time: 11720.7s, Step: 19701, GPU: 4.9GB\n",
      "Epoch 1, Batch 19705, Loss: 3.8686, Time: 11724.2s, Step: 19706, GPU: 4.9GB\n",
      "Epoch 1, Batch 19710, Loss: 3.7485, Time: 11726.7s, Step: 19711, GPU: 4.9GB\n",
      "Epoch 1, Batch 19715, Loss: 4.1157, Time: 11730.1s, Step: 19716, GPU: 4.9GB\n",
      "Epoch 1, Batch 19720, Loss: 3.9511, Time: 11732.7s, Step: 19721, GPU: 4.9GB\n",
      "Epoch 1, Batch 19725, Loss: 3.9973, Time: 11736.1s, Step: 19726, GPU: 4.9GB\n",
      "Epoch 1, Batch 19730, Loss: 4.5930, Time: 11738.6s, Step: 19731, GPU: 4.9GB\n",
      "Epoch 1, Batch 19735, Loss: 3.2810, Time: 11742.0s, Step: 19736, GPU: 4.9GB\n",
      "Epoch 1, Batch 19740, Loss: 3.5421, Time: 11744.5s, Step: 19741, GPU: 4.9GB\n",
      "Epoch 1, Batch 19745, Loss: 3.8267, Time: 11747.9s, Step: 19746, GPU: 4.9GB\n",
      "Epoch 1, Batch 19750, Loss: 3.5260, Time: 11750.4s, Step: 19751, GPU: 4.9GB\n",
      "Epoch 1, Batch 19755, Loss: 3.3670, Time: 11753.9s, Step: 19756, GPU: 4.9GB\n",
      "Epoch 1, Batch 19760, Loss: 4.2687, Time: 11756.4s, Step: 19761, GPU: 4.9GB\n",
      "Epoch 1, Batch 19765, Loss: 4.0124, Time: 11759.8s, Step: 19766, GPU: 4.9GB\n",
      "Epoch 1, Batch 19770, Loss: 4.1894, Time: 11762.4s, Step: 19771, GPU: 4.9GB\n",
      "Epoch 1, Batch 19775, Loss: 4.1759, Time: 11765.8s, Step: 19776, GPU: 4.9GB\n",
      "Epoch 1, Batch 19780, Loss: 3.8764, Time: 11768.3s, Step: 19781, GPU: 4.9GB\n",
      "Epoch 1, Batch 19785, Loss: 3.7155, Time: 11771.8s, Step: 19786, GPU: 4.9GB\n",
      "Epoch 1, Batch 19790, Loss: 4.5878, Time: 11774.3s, Step: 19791, GPU: 4.9GB\n",
      "Epoch 1, Batch 19795, Loss: 2.6668, Time: 11777.7s, Step: 19796, GPU: 4.9GB\n",
      "Epoch 1, Batch 19800, Loss: 4.0894, Time: 11780.4s, Step: 19801, GPU: 4.9GB\n",
      "Epoch 1, Batch 19805, Loss: 4.4312, Time: 11783.8s, Step: 19806, GPU: 4.9GB\n",
      "Epoch 1, Batch 19810, Loss: 4.0908, Time: 11786.3s, Step: 19811, GPU: 4.9GB\n",
      "Epoch 1, Batch 19815, Loss: 4.1420, Time: 11789.7s, Step: 19816, GPU: 4.9GB\n",
      "Epoch 1, Batch 19820, Loss: 4.1020, Time: 11792.3s, Step: 19821, GPU: 4.9GB\n",
      "Epoch 1, Batch 19825, Loss: 4.7283, Time: 11795.7s, Step: 19826, GPU: 4.9GB\n",
      "Epoch 1, Batch 19830, Loss: 3.7521, Time: 11798.2s, Step: 19831, GPU: 4.9GB\n",
      "Epoch 1, Batch 19835, Loss: 3.9221, Time: 11801.7s, Step: 19836, GPU: 4.9GB\n",
      "Epoch 1, Batch 19840, Loss: 3.9797, Time: 11804.2s, Step: 19841, GPU: 4.9GB\n",
      "Epoch 1, Batch 19845, Loss: 4.0157, Time: 11807.6s, Step: 19846, GPU: 4.9GB\n",
      "Epoch 1, Batch 19850, Loss: 4.0486, Time: 11810.2s, Step: 19851, GPU: 4.9GB\n",
      "Epoch 1, Batch 19855, Loss: 4.2239, Time: 11813.6s, Step: 19856, GPU: 4.9GB\n",
      "Epoch 1, Batch 19860, Loss: 3.5815, Time: 11816.1s, Step: 19861, GPU: 4.9GB\n",
      "Epoch 1, Batch 19865, Loss: 3.6669, Time: 11819.6s, Step: 19866, GPU: 4.9GB\n",
      "Epoch 1, Batch 19870, Loss: 4.0383, Time: 11822.1s, Step: 19871, GPU: 4.9GB\n",
      "Epoch 1, Batch 19875, Loss: 4.1794, Time: 11825.5s, Step: 19876, GPU: 4.9GB\n",
      "Epoch 1, Batch 19880, Loss: 3.9755, Time: 11828.1s, Step: 19881, GPU: 4.9GB\n",
      "Epoch 1, Batch 19885, Loss: 3.8624, Time: 11831.5s, Step: 19886, GPU: 4.9GB\n",
      "Epoch 1, Batch 19890, Loss: 4.1745, Time: 11834.1s, Step: 19891, GPU: 4.9GB\n",
      "Epoch 1, Batch 19895, Loss: 3.6556, Time: 11837.5s, Step: 19896, GPU: 4.9GB\n",
      "Epoch 1, Batch 19900, Loss: 3.4510, Time: 11840.0s, Step: 19901, GPU: 4.9GB\n",
      "Epoch 1, Batch 19905, Loss: 3.2955, Time: 11843.4s, Step: 19906, GPU: 4.9GB\n",
      "Epoch 1, Batch 19910, Loss: 3.7422, Time: 11846.0s, Step: 19911, GPU: 4.9GB\n",
      "Epoch 1, Batch 19915, Loss: 3.9152, Time: 11849.4s, Step: 19916, GPU: 4.9GB\n",
      "Epoch 1, Batch 19920, Loss: 4.5260, Time: 11851.9s, Step: 19921, GPU: 4.9GB\n",
      "Epoch 1, Batch 19925, Loss: 4.0092, Time: 11855.3s, Step: 19926, GPU: 4.9GB\n",
      "Epoch 1, Batch 19930, Loss: 3.6250, Time: 11857.8s, Step: 19931, GPU: 4.9GB\n",
      "Epoch 1, Batch 19935, Loss: 3.6681, Time: 11861.3s, Step: 19936, GPU: 4.9GB\n",
      "Epoch 1, Batch 19940, Loss: 4.6864, Time: 11863.8s, Step: 19941, GPU: 4.9GB\n",
      "Epoch 1, Batch 19945, Loss: 4.6261, Time: 11867.2s, Step: 19946, GPU: 4.9GB\n",
      "Epoch 1, Batch 19950, Loss: 4.2215, Time: 11869.7s, Step: 19951, GPU: 4.9GB\n",
      "Epoch 1, Batch 19955, Loss: 3.4006, Time: 11873.2s, Step: 19956, GPU: 4.9GB\n",
      "Epoch 1, Batch 19960, Loss: 3.9628, Time: 11875.7s, Step: 19961, GPU: 4.9GB\n",
      "Epoch 1, Batch 19965, Loss: 3.5088, Time: 11879.1s, Step: 19966, GPU: 4.9GB\n",
      "Epoch 1, Batch 19970, Loss: 4.2513, Time: 11881.6s, Step: 19971, GPU: 4.9GB\n",
      "Epoch 1, Batch 19975, Loss: 4.3495, Time: 11885.0s, Step: 19976, GPU: 4.9GB\n",
      "Epoch 1, Batch 19980, Loss: 3.9301, Time: 11887.5s, Step: 19981, GPU: 4.9GB\n",
      "Epoch 1, Batch 19985, Loss: 3.8723, Time: 11890.9s, Step: 19986, GPU: 4.9GB\n",
      "Epoch 1, Batch 19990, Loss: 4.7836, Time: 11893.4s, Step: 19991, GPU: 4.9GB\n",
      "Epoch 1, Batch 19995, Loss: 4.1728, Time: 11896.8s, Step: 19996, GPU: 4.9GB\n",
      "Epoch 1, Batch 20000, Loss: 4.1871, Time: 11899.4s, Step: 20001, GPU: 4.9GB\n",
      "Epoch 1, Batch 20005, Loss: 4.2192, Time: 11902.8s, Step: 20006, GPU: 4.9GB\n",
      "Epoch 1, Batch 20010, Loss: 4.3378, Time: 11905.3s, Step: 20011, GPU: 4.9GB\n",
      "Epoch 1, Batch 20015, Loss: 4.3900, Time: 11908.7s, Step: 20016, GPU: 4.9GB\n",
      "Epoch 1, Batch 20020, Loss: 3.8054, Time: 11911.3s, Step: 20021, GPU: 4.9GB\n",
      "Epoch 1, Batch 20025, Loss: 4.4219, Time: 11914.7s, Step: 20026, GPU: 4.9GB\n",
      "Epoch 1, Batch 20030, Loss: 3.6239, Time: 11917.2s, Step: 20031, GPU: 4.9GB\n",
      "Epoch 1, Batch 20035, Loss: 3.5580, Time: 11920.6s, Step: 20036, GPU: 4.9GB\n",
      "Epoch 1, Batch 20040, Loss: 4.1462, Time: 11923.1s, Step: 20041, GPU: 4.9GB\n",
      "Epoch 1, Batch 20045, Loss: 3.7149, Time: 11926.5s, Step: 20046, GPU: 4.9GB\n",
      "Epoch 1, Batch 20050, Loss: 3.8503, Time: 11929.0s, Step: 20051, GPU: 4.9GB\n",
      "Epoch 1, Batch 20055, Loss: 4.8991, Time: 11932.5s, Step: 20056, GPU: 4.9GB\n",
      "Epoch 1, Batch 20060, Loss: 3.3319, Time: 11935.0s, Step: 20061, GPU: 4.9GB\n",
      "Epoch 1, Batch 20065, Loss: 3.6353, Time: 11938.4s, Step: 20066, GPU: 4.9GB\n",
      "Epoch 1, Batch 20070, Loss: 4.2276, Time: 11941.0s, Step: 20071, GPU: 4.9GB\n",
      "Epoch 1, Batch 20075, Loss: 4.0674, Time: 11944.4s, Step: 20076, GPU: 4.9GB\n",
      "Epoch 1, Batch 20080, Loss: 4.5206, Time: 11946.9s, Step: 20081, GPU: 4.9GB\n",
      "Epoch 1, Batch 20085, Loss: 4.5690, Time: 11950.3s, Step: 20086, GPU: 4.9GB\n",
      "Epoch 1, Batch 20090, Loss: 4.4088, Time: 11952.9s, Step: 20091, GPU: 4.9GB\n",
      "Epoch 1, Batch 20095, Loss: 2.8998, Time: 11956.3s, Step: 20096, GPU: 4.9GB\n",
      "Epoch 1, Batch 20100, Loss: 4.2465, Time: 11958.8s, Step: 20101, GPU: 4.9GB\n",
      "Epoch 1, Batch 20105, Loss: 3.6461, Time: 11962.2s, Step: 20106, GPU: 4.9GB\n",
      "Epoch 1, Batch 20110, Loss: 4.5330, Time: 11964.7s, Step: 20111, GPU: 4.9GB\n",
      "Epoch 1, Batch 20115, Loss: 3.4391, Time: 11968.1s, Step: 20116, GPU: 4.9GB\n",
      "Epoch 1, Batch 20120, Loss: 4.5388, Time: 11970.7s, Step: 20121, GPU: 4.9GB\n",
      "Epoch 1, Batch 20125, Loss: 4.1622, Time: 11974.1s, Step: 20126, GPU: 4.9GB\n",
      "Epoch 1, Batch 20130, Loss: 4.6956, Time: 11976.6s, Step: 20131, GPU: 4.9GB\n",
      "Epoch 1, Batch 20135, Loss: 3.8708, Time: 11980.0s, Step: 20136, GPU: 4.9GB\n",
      "Epoch 1, Batch 20140, Loss: 3.7850, Time: 11982.5s, Step: 20141, GPU: 4.9GB\n",
      "Epoch 1, Batch 20145, Loss: 4.4783, Time: 11985.9s, Step: 20146, GPU: 4.9GB\n",
      "Epoch 1, Batch 20150, Loss: 3.5964, Time: 11988.4s, Step: 20151, GPU: 4.9GB\n",
      "Epoch 1, Batch 20155, Loss: 4.1798, Time: 11991.8s, Step: 20156, GPU: 4.9GB\n",
      "Epoch 1, Batch 20160, Loss: 4.1025, Time: 11994.4s, Step: 20161, GPU: 4.9GB\n",
      "Epoch 1, Batch 20165, Loss: 3.9514, Time: 11997.8s, Step: 20166, GPU: 4.9GB\n",
      "Epoch 1, Batch 20170, Loss: 3.6317, Time: 12000.3s, Step: 20171, GPU: 4.9GB\n",
      "Epoch 1, Batch 20175, Loss: 4.4454, Time: 12003.8s, Step: 20176, GPU: 4.9GB\n",
      "Epoch 1, Batch 20180, Loss: 4.2377, Time: 12006.3s, Step: 20181, GPU: 4.9GB\n",
      "Epoch 1, Batch 20185, Loss: 4.0812, Time: 12009.7s, Step: 20186, GPU: 4.9GB\n",
      "Epoch 1, Batch 20190, Loss: 4.3871, Time: 12012.3s, Step: 20191, GPU: 4.9GB\n",
      "Epoch 1, Batch 20195, Loss: 3.7510, Time: 12015.7s, Step: 20196, GPU: 4.9GB\n",
      "Epoch 1, Batch 20200, Loss: 3.5937, Time: 12018.3s, Step: 20201, GPU: 4.9GB\n",
      "Epoch 1, Batch 20205, Loss: 3.5467, Time: 12021.7s, Step: 20206, GPU: 4.9GB\n",
      "Epoch 1, Batch 20210, Loss: 3.9128, Time: 12024.2s, Step: 20211, GPU: 4.9GB\n",
      "Epoch 1, Batch 20215, Loss: 3.5161, Time: 12027.6s, Step: 20216, GPU: 4.9GB\n",
      "Epoch 1, Batch 20220, Loss: 4.2116, Time: 12030.1s, Step: 20221, GPU: 4.9GB\n",
      "Epoch 1, Batch 20225, Loss: 4.1829, Time: 12033.6s, Step: 20226, GPU: 4.9GB\n",
      "Epoch 1, Batch 20230, Loss: 4.1113, Time: 12036.1s, Step: 20231, GPU: 4.9GB\n",
      "Epoch 1, Batch 20235, Loss: 3.4944, Time: 12039.5s, Step: 20236, GPU: 4.9GB\n",
      "Epoch 1, Batch 20240, Loss: 5.0982, Time: 12042.1s, Step: 20241, GPU: 4.9GB\n",
      "Epoch 1, Batch 20245, Loss: 4.1219, Time: 12045.5s, Step: 20246, GPU: 4.9GB\n",
      "Epoch 1, Batch 20250, Loss: 4.5216, Time: 12048.0s, Step: 20251, GPU: 4.9GB\n",
      "Epoch 1, Batch 20255, Loss: 3.6809, Time: 12051.4s, Step: 20256, GPU: 4.9GB\n",
      "Epoch 1, Batch 20260, Loss: 4.1284, Time: 12053.9s, Step: 20261, GPU: 4.9GB\n",
      "Epoch 1, Batch 20265, Loss: 4.4370, Time: 12057.3s, Step: 20266, GPU: 4.9GB\n",
      "Epoch 1, Batch 20270, Loss: 3.2707, Time: 12059.9s, Step: 20271, GPU: 4.9GB\n",
      "Epoch 1, Batch 20275, Loss: 4.3561, Time: 12063.3s, Step: 20276, GPU: 4.9GB\n",
      "Epoch 1, Batch 20280, Loss: 4.9129, Time: 12065.8s, Step: 20281, GPU: 4.9GB\n",
      "Epoch 1, Batch 20285, Loss: 4.3528, Time: 12069.2s, Step: 20286, GPU: 4.9GB\n",
      "Epoch 1, Batch 20290, Loss: 3.7596, Time: 12071.8s, Step: 20291, GPU: 4.9GB\n",
      "Epoch 1, Batch 20295, Loss: 3.5596, Time: 12075.2s, Step: 20296, GPU: 4.9GB\n",
      "Epoch 1, Batch 20300, Loss: 3.9753, Time: 12077.7s, Step: 20301, GPU: 4.9GB\n",
      "Epoch 1, Batch 20305, Loss: 3.9449, Time: 12081.2s, Step: 20306, GPU: 4.9GB\n",
      "Epoch 1, Batch 20310, Loss: 4.3668, Time: 12083.7s, Step: 20311, GPU: 4.9GB\n",
      "Epoch 1, Batch 20315, Loss: 3.7141, Time: 12087.2s, Step: 20316, GPU: 4.9GB\n",
      "Epoch 1, Batch 20320, Loss: 4.4167, Time: 12089.7s, Step: 20321, GPU: 4.9GB\n",
      "Epoch 1, Batch 20325, Loss: 5.0332, Time: 12093.1s, Step: 20326, GPU: 4.9GB\n",
      "Epoch 1, Batch 20330, Loss: 3.6986, Time: 12095.7s, Step: 20331, GPU: 4.9GB\n",
      "Epoch 1, Batch 20335, Loss: 3.4393, Time: 12099.1s, Step: 20336, GPU: 4.9GB\n",
      "Epoch 1, Batch 20340, Loss: 4.4366, Time: 12101.6s, Step: 20341, GPU: 4.9GB\n",
      "Epoch 1, Batch 20345, Loss: 2.6459, Time: 12105.0s, Step: 20346, GPU: 4.9GB\n",
      "Epoch 1, Batch 20350, Loss: 4.4215, Time: 12107.5s, Step: 20351, GPU: 4.9GB\n",
      "Epoch 1, Batch 20355, Loss: 4.9359, Time: 12110.9s, Step: 20356, GPU: 4.9GB\n",
      "Epoch 1, Batch 20360, Loss: 3.9852, Time: 12113.4s, Step: 20361, GPU: 4.9GB\n",
      "Epoch 1, Batch 20365, Loss: 4.2709, Time: 12116.9s, Step: 20366, GPU: 4.9GB\n",
      "Epoch 1, Batch 20370, Loss: 4.1669, Time: 12119.4s, Step: 20371, GPU: 4.9GB\n",
      "Epoch 1, Batch 20375, Loss: 3.8020, Time: 12122.8s, Step: 20376, GPU: 4.9GB\n",
      "Epoch 1, Batch 20380, Loss: 4.6662, Time: 12125.3s, Step: 20381, GPU: 4.9GB\n",
      "Epoch 1, Batch 20385, Loss: 3.7918, Time: 12128.7s, Step: 20386, GPU: 4.9GB\n",
      "Epoch 1, Batch 20390, Loss: 3.9711, Time: 12131.2s, Step: 20391, GPU: 4.9GB\n",
      "Epoch 1, Batch 20395, Loss: 3.6965, Time: 12134.7s, Step: 20396, GPU: 4.9GB\n",
      "Epoch 1, Batch 20400, Loss: 3.7372, Time: 12137.2s, Step: 20401, GPU: 4.9GB\n",
      "Epoch 1, Batch 20405, Loss: 4.9885, Time: 12140.6s, Step: 20406, GPU: 4.9GB\n",
      "Epoch 1, Batch 20410, Loss: 4.8326, Time: 12143.2s, Step: 20411, GPU: 4.9GB\n",
      "Epoch 1, Batch 20415, Loss: 3.9500, Time: 12146.6s, Step: 20416, GPU: 4.9GB\n",
      "Epoch 1, Batch 20420, Loss: 3.7477, Time: 12149.1s, Step: 20421, GPU: 4.9GB\n",
      "Epoch 1, Batch 20425, Loss: 4.5865, Time: 12152.5s, Step: 20426, GPU: 4.9GB\n",
      "Epoch 1, Batch 20430, Loss: 4.8178, Time: 12155.0s, Step: 20431, GPU: 4.9GB\n",
      "Epoch 1, Batch 20435, Loss: 4.8025, Time: 12158.5s, Step: 20436, GPU: 4.9GB\n",
      "Epoch 1, Batch 20440, Loss: 3.9521, Time: 12161.0s, Step: 20441, GPU: 4.9GB\n",
      "Epoch 1, Batch 20445, Loss: 4.4060, Time: 12164.4s, Step: 20446, GPU: 4.9GB\n",
      "Epoch 1, Batch 20450, Loss: 4.3714, Time: 12166.9s, Step: 20451, GPU: 4.9GB\n",
      "Epoch 1, Batch 20455, Loss: 4.4756, Time: 12170.4s, Step: 20456, GPU: 4.9GB\n",
      "Epoch 1, Batch 20460, Loss: 4.3539, Time: 12172.9s, Step: 20461, GPU: 4.9GB\n",
      "Epoch 1, Batch 20465, Loss: 5.3292, Time: 12176.3s, Step: 20466, GPU: 4.9GB\n",
      "Epoch 1, Batch 20470, Loss: 3.6554, Time: 12178.8s, Step: 20471, GPU: 4.9GB\n",
      "Epoch 1, Batch 20475, Loss: 4.1256, Time: 12182.9s, Step: 20476, GPU: 4.9GB\n",
      "Epoch 1, Batch 20480, Loss: 3.9941, Time: 12185.4s, Step: 20481, GPU: 4.9GB\n",
      "Epoch 1, Batch 20485, Loss: 3.8873, Time: 12188.9s, Step: 20486, GPU: 4.9GB\n",
      "Epoch 1, Batch 20490, Loss: 3.7828, Time: 12191.4s, Step: 20491, GPU: 4.9GB\n",
      "Epoch 1, Batch 20495, Loss: 3.3835, Time: 12194.8s, Step: 20496, GPU: 4.9GB\n",
      "Epoch 1, Batch 20500, Loss: 4.1937, Time: 12197.3s, Step: 20501, GPU: 4.9GB\n",
      "Epoch 1, Batch 20505, Loss: 3.8584, Time: 12200.8s, Step: 20506, GPU: 4.9GB\n",
      "Epoch 1, Batch 20510, Loss: 3.5545, Time: 12203.3s, Step: 20511, GPU: 4.9GB\n",
      "Epoch 1, Batch 20515, Loss: 4.6774, Time: 12206.7s, Step: 20516, GPU: 4.9GB\n",
      "Epoch 1, Batch 20520, Loss: 4.2918, Time: 12209.2s, Step: 20521, GPU: 4.9GB\n",
      "Epoch 1, Batch 20525, Loss: 3.7440, Time: 12212.7s, Step: 20526, GPU: 4.9GB\n",
      "Epoch 1, Batch 20530, Loss: 3.7078, Time: 12215.2s, Step: 20531, GPU: 4.9GB\n",
      "Epoch 1, Batch 20535, Loss: 4.0148, Time: 12218.6s, Step: 20536, GPU: 4.9GB\n",
      "Epoch 1, Batch 20540, Loss: 3.6216, Time: 12221.2s, Step: 20541, GPU: 4.9GB\n",
      "Epoch 1, Batch 20545, Loss: 4.3795, Time: 12224.6s, Step: 20546, GPU: 4.9GB\n",
      "Epoch 1, Batch 20550, Loss: 4.2078, Time: 12227.1s, Step: 20551, GPU: 4.9GB\n",
      "Epoch 1, Batch 20555, Loss: 5.1480, Time: 12230.5s, Step: 20556, GPU: 4.9GB\n",
      "Epoch 1, Batch 20560, Loss: 3.6437, Time: 12233.0s, Step: 20561, GPU: 4.9GB\n",
      "Epoch 1, Batch 20565, Loss: 4.2725, Time: 12236.5s, Step: 20566, GPU: 4.9GB\n",
      "Epoch 1, Batch 20570, Loss: 4.5201, Time: 12239.0s, Step: 20571, GPU: 4.9GB\n",
      "Epoch 1, Batch 20575, Loss: 4.4207, Time: 12242.4s, Step: 20576, GPU: 4.9GB\n",
      "Epoch 1, Batch 20580, Loss: 4.2959, Time: 12244.9s, Step: 20581, GPU: 4.9GB\n",
      "Epoch 1, Batch 20585, Loss: 4.3254, Time: 12248.3s, Step: 20586, GPU: 4.9GB\n",
      "Epoch 1, Batch 20590, Loss: 3.4821, Time: 12250.9s, Step: 20591, GPU: 4.9GB\n",
      "Epoch 1, Batch 20595, Loss: 3.6565, Time: 12254.3s, Step: 20596, GPU: 4.9GB\n",
      "Epoch 1, Batch 20600, Loss: 4.4852, Time: 12256.9s, Step: 20601, GPU: 4.9GB\n",
      "Epoch 1, Batch 20605, Loss: 3.4596, Time: 12260.3s, Step: 20606, GPU: 4.9GB\n",
      "Epoch 1, Batch 20610, Loss: 4.1615, Time: 12262.8s, Step: 20611, GPU: 4.9GB\n",
      "Epoch 1, Batch 20615, Loss: 4.6445, Time: 12266.2s, Step: 20616, GPU: 4.9GB\n",
      "Epoch 1, Batch 20620, Loss: 4.1798, Time: 12268.7s, Step: 20621, GPU: 4.9GB\n",
      "Epoch 1, Batch 20625, Loss: 3.7126, Time: 12272.2s, Step: 20626, GPU: 4.9GB\n",
      "Epoch 1, Batch 20630, Loss: 2.9895, Time: 12274.7s, Step: 20631, GPU: 4.9GB\n",
      "Epoch 1, Batch 20635, Loss: 4.6114, Time: 12278.3s, Step: 20636, GPU: 4.9GB\n",
      "Epoch 1, Batch 20640, Loss: 3.9891, Time: 12280.8s, Step: 20641, GPU: 4.9GB\n",
      "Epoch 1, Batch 20645, Loss: 5.3522, Time: 12284.2s, Step: 20646, GPU: 4.9GB\n",
      "Epoch 1, Batch 20650, Loss: 4.1668, Time: 12286.7s, Step: 20651, GPU: 4.9GB\n",
      "Epoch 1, Batch 20655, Loss: 3.8818, Time: 12290.2s, Step: 20656, GPU: 4.9GB\n",
      "Epoch 1, Batch 20660, Loss: 4.0733, Time: 12292.7s, Step: 20661, GPU: 4.9GB\n",
      "Epoch 1, Batch 20665, Loss: 3.5514, Time: 12296.1s, Step: 20666, GPU: 4.9GB\n",
      "Epoch 1, Batch 20670, Loss: 4.4154, Time: 12298.6s, Step: 20671, GPU: 4.9GB\n",
      "Epoch 1, Batch 20675, Loss: 4.7858, Time: 12302.1s, Step: 20676, GPU: 4.9GB\n",
      "Epoch 1, Batch 20680, Loss: 3.4361, Time: 12304.6s, Step: 20681, GPU: 4.9GB\n",
      "Epoch 1, Batch 20685, Loss: 4.7259, Time: 12308.0s, Step: 20686, GPU: 4.9GB\n",
      "Epoch 1, Batch 20690, Loss: 3.7218, Time: 12310.5s, Step: 20691, GPU: 4.9GB\n",
      "Epoch 1, Batch 20695, Loss: 3.8646, Time: 12313.9s, Step: 20696, GPU: 4.9GB\n",
      "Epoch 1, Batch 20700, Loss: 4.5864, Time: 12316.4s, Step: 20701, GPU: 4.9GB\n",
      "Epoch 1, Batch 20705, Loss: 4.0945, Time: 12319.9s, Step: 20706, GPU: 4.9GB\n",
      "Epoch 1, Batch 20710, Loss: 3.8050, Time: 12322.4s, Step: 20711, GPU: 4.9GB\n",
      "Epoch 1, Batch 20715, Loss: 4.9407, Time: 12325.9s, Step: 20716, GPU: 4.9GB\n",
      "Epoch 1, Batch 20720, Loss: 4.6413, Time: 12328.4s, Step: 20721, GPU: 4.9GB\n",
      "Epoch 1, Batch 20725, Loss: 4.1629, Time: 12331.8s, Step: 20726, GPU: 4.9GB\n",
      "Epoch 1, Batch 20730, Loss: 3.9904, Time: 12334.4s, Step: 20731, GPU: 4.9GB\n",
      "Epoch 1, Batch 20735, Loss: 3.8300, Time: 12337.8s, Step: 20736, GPU: 4.9GB\n",
      "Epoch 1, Batch 20740, Loss: 4.2614, Time: 12340.3s, Step: 20741, GPU: 4.9GB\n",
      "Epoch 1, Batch 20745, Loss: 3.9315, Time: 12343.8s, Step: 20746, GPU: 4.9GB\n",
      "Epoch 1, Batch 20750, Loss: 3.4796, Time: 12346.3s, Step: 20751, GPU: 4.9GB\n",
      "Epoch 1, Batch 20755, Loss: 3.8513, Time: 12349.7s, Step: 20756, GPU: 4.9GB\n",
      "Epoch 1, Batch 20760, Loss: 4.0770, Time: 12352.2s, Step: 20761, GPU: 4.9GB\n",
      "Epoch 1, Batch 20765, Loss: 3.8110, Time: 12355.6s, Step: 20766, GPU: 4.9GB\n",
      "Epoch 1, Batch 20770, Loss: 4.9363, Time: 12358.2s, Step: 20771, GPU: 4.9GB\n",
      "Epoch 1, Batch 20775, Loss: 4.2849, Time: 12361.6s, Step: 20776, GPU: 4.9GB\n",
      "Epoch 1, Batch 20780, Loss: 4.3917, Time: 12364.1s, Step: 20781, GPU: 4.9GB\n",
      "Epoch 1, Batch 20785, Loss: 3.7469, Time: 12367.5s, Step: 20786, GPU: 4.9GB\n",
      "Epoch 1, Batch 20790, Loss: 4.9148, Time: 12370.1s, Step: 20791, GPU: 4.9GB\n",
      "Epoch 1, Batch 20795, Loss: 4.3854, Time: 12373.5s, Step: 20796, GPU: 4.9GB\n",
      "Epoch 1, Batch 20800, Loss: 3.4983, Time: 12376.1s, Step: 20801, GPU: 4.9GB\n",
      "Epoch 1, Batch 20805, Loss: 4.6147, Time: 12379.6s, Step: 20806, GPU: 4.9GB\n",
      "Epoch 1, Batch 20810, Loss: 4.1275, Time: 12382.1s, Step: 20811, GPU: 4.9GB\n",
      "Epoch 1, Batch 20815, Loss: 4.3723, Time: 12385.5s, Step: 20816, GPU: 4.9GB\n",
      "Epoch 1, Batch 20820, Loss: 3.0723, Time: 12388.0s, Step: 20821, GPU: 4.9GB\n",
      "Epoch 1, Batch 20825, Loss: 3.8504, Time: 12391.4s, Step: 20826, GPU: 4.9GB\n",
      "Epoch 1, Batch 20830, Loss: 3.4402, Time: 12394.0s, Step: 20831, GPU: 4.9GB\n",
      "Epoch 1, Batch 20835, Loss: 4.0950, Time: 12397.4s, Step: 20836, GPU: 4.9GB\n",
      "Epoch 1, Batch 20840, Loss: 4.5508, Time: 12400.0s, Step: 20841, GPU: 4.9GB\n",
      "Epoch 1, Batch 20845, Loss: 3.2854, Time: 12403.4s, Step: 20846, GPU: 4.9GB\n",
      "Epoch 1, Batch 20850, Loss: 3.2325, Time: 12405.9s, Step: 20851, GPU: 4.9GB\n",
      "Epoch 1, Batch 20855, Loss: 4.8490, Time: 12409.3s, Step: 20856, GPU: 4.9GB\n",
      "Epoch 1, Batch 20860, Loss: 4.0742, Time: 12411.8s, Step: 20861, GPU: 4.9GB\n",
      "Epoch 1, Batch 20865, Loss: 4.4654, Time: 12415.3s, Step: 20866, GPU: 4.9GB\n",
      "Epoch 1, Batch 20870, Loss: 4.3025, Time: 12417.8s, Step: 20871, GPU: 4.9GB\n",
      "Epoch 1, Batch 20875, Loss: 4.8129, Time: 12421.2s, Step: 20876, GPU: 4.9GB\n",
      "Epoch 1, Batch 20880, Loss: 3.4812, Time: 12423.7s, Step: 20881, GPU: 4.9GB\n",
      "Epoch 1, Batch 20885, Loss: 3.2948, Time: 12427.1s, Step: 20886, GPU: 4.9GB\n",
      "Epoch 1, Batch 20890, Loss: 3.1791, Time: 12429.7s, Step: 20891, GPU: 4.9GB\n",
      "Epoch 1, Batch 20895, Loss: 3.5971, Time: 12433.1s, Step: 20896, GPU: 4.9GB\n",
      "Epoch 1, Batch 20900, Loss: 3.7759, Time: 12435.6s, Step: 20901, GPU: 4.9GB\n",
      "Epoch 1, Batch 20905, Loss: 3.7340, Time: 12439.0s, Step: 20906, GPU: 4.9GB\n",
      "Epoch 1, Batch 20910, Loss: 3.7983, Time: 12441.5s, Step: 20911, GPU: 4.9GB\n",
      "Epoch 1, Batch 20915, Loss: 3.6239, Time: 12444.9s, Step: 20916, GPU: 4.9GB\n",
      "Epoch 1, Batch 20920, Loss: 4.3616, Time: 12447.5s, Step: 20921, GPU: 4.9GB\n",
      "Epoch 1, Batch 20925, Loss: 4.3123, Time: 12450.9s, Step: 20926, GPU: 4.9GB\n",
      "Epoch 1, Batch 20930, Loss: 4.6234, Time: 12453.5s, Step: 20931, GPU: 4.9GB\n",
      "Epoch 1, Batch 20935, Loss: 3.9823, Time: 12456.9s, Step: 20936, GPU: 4.9GB\n",
      "Epoch 1, Batch 20940, Loss: 3.6119, Time: 12459.4s, Step: 20941, GPU: 4.9GB\n",
      "Epoch 1, Batch 20945, Loss: 4.0497, Time: 12462.8s, Step: 20946, GPU: 4.9GB\n",
      "Epoch 1, Batch 20950, Loss: 4.9339, Time: 12465.3s, Step: 20951, GPU: 4.9GB\n",
      "Epoch 1, Batch 20955, Loss: 4.5435, Time: 12468.7s, Step: 20956, GPU: 4.9GB\n",
      "Epoch 1, Batch 20960, Loss: 4.0522, Time: 12471.2s, Step: 20961, GPU: 4.9GB\n",
      "Epoch 1, Batch 20965, Loss: 3.5684, Time: 12474.6s, Step: 20966, GPU: 4.9GB\n",
      "Epoch 1, Batch 20970, Loss: 3.8933, Time: 12477.2s, Step: 20971, GPU: 4.9GB\n",
      "Epoch 1, Batch 20975, Loss: 5.1659, Time: 12480.6s, Step: 20976, GPU: 4.9GB\n",
      "Epoch 1, Batch 20980, Loss: 3.7234, Time: 12483.1s, Step: 20981, GPU: 4.9GB\n",
      "Epoch 1, Batch 20985, Loss: 3.8719, Time: 12486.5s, Step: 20986, GPU: 4.9GB\n",
      "Epoch 1, Batch 20990, Loss: 4.9606, Time: 12489.0s, Step: 20991, GPU: 4.9GB\n",
      "Epoch 1, Batch 20995, Loss: 3.9219, Time: 12492.4s, Step: 20996, GPU: 4.9GB\n",
      "Epoch 1, Batch 21000, Loss: 4.5923, Time: 12495.0s, Step: 21001, GPU: 4.9GB\n",
      "Epoch 1, Batch 21005, Loss: 4.5097, Time: 12498.4s, Step: 21006, GPU: 4.9GB\n",
      "Epoch 1, Batch 21010, Loss: 4.2571, Time: 12500.9s, Step: 21011, GPU: 4.9GB\n",
      "Epoch 1, Batch 21015, Loss: 3.6948, Time: 12504.4s, Step: 21016, GPU: 4.9GB\n",
      "Epoch 1, Batch 21020, Loss: 4.4656, Time: 12506.9s, Step: 21021, GPU: 4.9GB\n",
      "Epoch 1, Batch 21025, Loss: 4.4876, Time: 12510.3s, Step: 21026, GPU: 4.9GB\n",
      "Epoch 1, Batch 21030, Loss: 4.5586, Time: 12512.8s, Step: 21031, GPU: 4.9GB\n",
      "Epoch 1, Batch 21035, Loss: 4.3267, Time: 12516.3s, Step: 21036, GPU: 4.9GB\n",
      "Epoch 1, Batch 21040, Loss: 4.2315, Time: 12518.8s, Step: 21041, GPU: 4.9GB\n",
      "Epoch 1, Batch 21045, Loss: 5.1093, Time: 12522.2s, Step: 21046, GPU: 4.9GB\n",
      "Epoch 1, Batch 21050, Loss: 5.1934, Time: 12524.7s, Step: 21051, GPU: 4.9GB\n",
      "Epoch 1, Batch 21055, Loss: 3.6981, Time: 12528.2s, Step: 21056, GPU: 4.9GB\n",
      "Epoch 1, Batch 21060, Loss: 3.1465, Time: 12530.7s, Step: 21061, GPU: 4.9GB\n",
      "Epoch 1, Batch 21065, Loss: 3.4221, Time: 12534.1s, Step: 21066, GPU: 4.9GB\n",
      "Epoch 1, Batch 21070, Loss: 3.4847, Time: 12536.6s, Step: 21071, GPU: 4.9GB\n",
      "Epoch 1, Batch 21075, Loss: 4.0090, Time: 12540.1s, Step: 21076, GPU: 4.9GB\n",
      "Epoch 1, Batch 21080, Loss: 3.3649, Time: 12542.6s, Step: 21081, GPU: 4.9GB\n",
      "Epoch 1, Batch 21085, Loss: 4.1676, Time: 12546.0s, Step: 21086, GPU: 4.9GB\n",
      "Epoch 1, Batch 21090, Loss: 3.4535, Time: 12548.5s, Step: 21091, GPU: 4.9GB\n",
      "Epoch 1, Batch 21095, Loss: 3.8202, Time: 12552.0s, Step: 21096, GPU: 4.9GB\n",
      "Epoch 1, Batch 21100, Loss: 4.6084, Time: 12554.5s, Step: 21101, GPU: 4.9GB\n",
      "Epoch 1, Batch 21105, Loss: 3.7874, Time: 12557.9s, Step: 21106, GPU: 4.9GB\n",
      "Epoch 1, Batch 21110, Loss: 3.8323, Time: 12560.4s, Step: 21111, GPU: 4.9GB\n",
      "Epoch 1, Batch 21115, Loss: 4.4359, Time: 12563.9s, Step: 21116, GPU: 4.9GB\n",
      "Epoch 1, Batch 21120, Loss: 4.4242, Time: 12566.4s, Step: 21121, GPU: 4.9GB\n",
      "Epoch 1, Batch 21125, Loss: 2.5848, Time: 12569.8s, Step: 21126, GPU: 4.9GB\n",
      "Epoch 1, Batch 21130, Loss: 4.1539, Time: 12572.3s, Step: 21131, GPU: 4.9GB\n",
      "Epoch 1, Batch 21135, Loss: 3.8272, Time: 12575.8s, Step: 21136, GPU: 4.9GB\n",
      "Epoch 1, Batch 21140, Loss: 3.9813, Time: 12578.3s, Step: 21141, GPU: 4.9GB\n",
      "Epoch 1, Batch 21145, Loss: 4.1643, Time: 12581.7s, Step: 21146, GPU: 4.9GB\n",
      "Epoch 1, Batch 21150, Loss: 4.5489, Time: 12584.2s, Step: 21151, GPU: 4.9GB\n",
      "Epoch 1, Batch 21155, Loss: 4.2884, Time: 12587.7s, Step: 21156, GPU: 4.9GB\n",
      "Epoch 1, Batch 21160, Loss: 4.0835, Time: 12590.2s, Step: 21161, GPU: 4.9GB\n",
      "Epoch 1, Batch 21165, Loss: 4.6754, Time: 12593.6s, Step: 21166, GPU: 4.9GB\n",
      "Epoch 1, Batch 21170, Loss: 4.3243, Time: 12596.2s, Step: 21171, GPU: 4.9GB\n",
      "Epoch 1, Batch 21175, Loss: 4.0127, Time: 12599.6s, Step: 21176, GPU: 4.9GB\n",
      "Epoch 1, Batch 21180, Loss: 3.6587, Time: 12602.1s, Step: 21181, GPU: 4.9GB\n",
      "Epoch 1, Batch 21185, Loss: 4.3500, Time: 12605.5s, Step: 21186, GPU: 4.9GB\n",
      "Epoch 1, Batch 21190, Loss: 3.8956, Time: 12608.0s, Step: 21191, GPU: 4.9GB\n",
      "Epoch 1, Batch 21195, Loss: 3.9588, Time: 12612.2s, Step: 21196, GPU: 4.9GB\n",
      "Epoch 1, Batch 21200, Loss: 3.2460, Time: 12614.8s, Step: 21201, GPU: 4.9GB\n",
      "Epoch 1, Batch 21205, Loss: 3.9661, Time: 12618.2s, Step: 21206, GPU: 4.9GB\n",
      "Epoch 1, Batch 21210, Loss: 3.8959, Time: 12620.7s, Step: 21211, GPU: 4.9GB\n",
      "Epoch 1, Batch 21215, Loss: 3.8270, Time: 12624.1s, Step: 21216, GPU: 4.9GB\n",
      "Epoch 1, Batch 21220, Loss: 3.8221, Time: 12626.6s, Step: 21221, GPU: 4.9GB\n",
      "Epoch 1, Batch 21225, Loss: 3.2709, Time: 12630.0s, Step: 21226, GPU: 4.9GB\n",
      "Epoch 1, Batch 21230, Loss: 3.9734, Time: 12632.5s, Step: 21231, GPU: 4.9GB\n",
      "Epoch 1, Batch 21235, Loss: 4.1171, Time: 12635.9s, Step: 21236, GPU: 4.9GB\n",
      "Epoch 1, Batch 21240, Loss: 3.3094, Time: 12638.4s, Step: 21241, GPU: 4.9GB\n",
      "Epoch 1, Batch 21245, Loss: 3.8212, Time: 12641.9s, Step: 21246, GPU: 4.9GB\n",
      "Epoch 1, Batch 21250, Loss: 4.8264, Time: 12644.4s, Step: 21251, GPU: 4.9GB\n",
      "Epoch 1, Batch 21255, Loss: 4.4983, Time: 12647.8s, Step: 21256, GPU: 4.9GB\n",
      "Epoch 1, Batch 21260, Loss: 4.0241, Time: 12650.3s, Step: 21261, GPU: 4.9GB\n",
      "Epoch 1, Batch 21265, Loss: 4.5291, Time: 12653.7s, Step: 21266, GPU: 4.9GB\n",
      "Epoch 1, Batch 21270, Loss: 3.1898, Time: 12656.2s, Step: 21271, GPU: 4.9GB\n",
      "Epoch 1, Batch 21275, Loss: 3.9303, Time: 12659.6s, Step: 21276, GPU: 4.9GB\n",
      "Epoch 1, Batch 21280, Loss: 3.8389, Time: 12662.1s, Step: 21281, GPU: 4.9GB\n",
      "Epoch 1, Batch 21285, Loss: 4.4113, Time: 12665.5s, Step: 21286, GPU: 4.9GB\n",
      "Epoch 1, Batch 21290, Loss: 4.3634, Time: 12668.0s, Step: 21291, GPU: 4.9GB\n",
      "Epoch 1, Batch 21295, Loss: 3.7899, Time: 12671.5s, Step: 21296, GPU: 4.9GB\n",
      "Epoch 1, Batch 21300, Loss: 4.5961, Time: 12674.0s, Step: 21301, GPU: 4.9GB\n",
      "Epoch 1, Batch 21305, Loss: 3.9663, Time: 12677.4s, Step: 21306, GPU: 4.9GB\n",
      "Epoch 1, Batch 21310, Loss: 4.2124, Time: 12679.9s, Step: 21311, GPU: 4.9GB\n",
      "Epoch 1, Batch 21315, Loss: 3.4143, Time: 12683.3s, Step: 21316, GPU: 4.9GB\n",
      "Epoch 1, Batch 21320, Loss: 3.6718, Time: 12685.8s, Step: 21321, GPU: 4.9GB\n",
      "Epoch 1, Batch 21325, Loss: 3.2324, Time: 12689.2s, Step: 21326, GPU: 4.9GB\n",
      "Epoch 1, Batch 21330, Loss: 3.5410, Time: 12691.7s, Step: 21331, GPU: 4.9GB\n",
      "Epoch 1, Batch 21335, Loss: 3.7293, Time: 12695.1s, Step: 21336, GPU: 4.9GB\n",
      "Epoch 1, Batch 21340, Loss: 3.8023, Time: 12697.6s, Step: 21341, GPU: 4.9GB\n",
      "Epoch 1, Batch 21345, Loss: 3.3608, Time: 12701.0s, Step: 21346, GPU: 4.9GB\n",
      "Epoch 1, Batch 21350, Loss: 3.6688, Time: 12703.5s, Step: 21351, GPU: 4.9GB\n",
      "Epoch 1, Batch 21355, Loss: 4.3661, Time: 12706.9s, Step: 21356, GPU: 4.9GB\n",
      "Epoch 1, Batch 21360, Loss: 4.2470, Time: 12709.4s, Step: 21361, GPU: 4.9GB\n",
      "Epoch 1, Batch 21365, Loss: 3.7531, Time: 12712.8s, Step: 21366, GPU: 4.9GB\n",
      "Epoch 1, Batch 21370, Loss: 3.7047, Time: 12715.4s, Step: 21371, GPU: 4.9GB\n",
      "Epoch 1, Batch 21375, Loss: 3.5386, Time: 12718.8s, Step: 21376, GPU: 4.9GB\n",
      "Epoch 1, Batch 21380, Loss: 4.3412, Time: 12721.3s, Step: 21381, GPU: 4.9GB\n",
      "Epoch 1, Batch 21385, Loss: 4.0460, Time: 12724.7s, Step: 21386, GPU: 4.9GB\n",
      "Epoch 1, Batch 21390, Loss: 3.3440, Time: 12727.3s, Step: 21391, GPU: 4.9GB\n",
      "Epoch 1, Batch 21395, Loss: 3.6369, Time: 12730.8s, Step: 21396, GPU: 4.9GB\n",
      "Epoch 1, Batch 21400, Loss: 4.9899, Time: 12733.4s, Step: 21401, GPU: 4.9GB\n",
      "Epoch 1, Batch 21405, Loss: 3.8238, Time: 12736.8s, Step: 21406, GPU: 4.9GB\n",
      "Epoch 1, Batch 21410, Loss: 4.1160, Time: 12739.3s, Step: 21411, GPU: 4.9GB\n",
      "Epoch 1, Batch 21415, Loss: 4.4325, Time: 12742.7s, Step: 21416, GPU: 4.9GB\n",
      "Epoch 1, Batch 21420, Loss: 3.6491, Time: 12745.2s, Step: 21421, GPU: 4.9GB\n",
      "Epoch 1, Batch 21425, Loss: 3.5677, Time: 12748.7s, Step: 21426, GPU: 4.9GB\n",
      "Epoch 1, Batch 21430, Loss: 3.7108, Time: 12751.2s, Step: 21431, GPU: 4.9GB\n",
      "Epoch 1, Batch 21435, Loss: 4.1370, Time: 12754.6s, Step: 21436, GPU: 4.9GB\n",
      "Epoch 1, Batch 21440, Loss: 4.5469, Time: 12757.1s, Step: 21441, GPU: 4.9GB\n",
      "Epoch 1, Batch 21445, Loss: 3.5391, Time: 12760.6s, Step: 21446, GPU: 4.9GB\n",
      "Epoch 1, Batch 21450, Loss: 4.7297, Time: 12763.2s, Step: 21451, GPU: 4.9GB\n",
      "Epoch 1, Batch 21455, Loss: 3.3865, Time: 12766.6s, Step: 21456, GPU: 4.9GB\n",
      "Epoch 1, Batch 21460, Loss: 3.7790, Time: 12769.1s, Step: 21461, GPU: 4.9GB\n",
      "Epoch 1, Batch 21465, Loss: 3.7820, Time: 12772.5s, Step: 21466, GPU: 4.9GB\n",
      "Epoch 1, Batch 21470, Loss: 3.2134, Time: 12775.0s, Step: 21471, GPU: 4.9GB\n",
      "Epoch 1, Batch 21475, Loss: 3.2898, Time: 12778.5s, Step: 21476, GPU: 4.9GB\n",
      "Epoch 1, Batch 21480, Loss: 4.2228, Time: 12781.0s, Step: 21481, GPU: 4.9GB\n",
      "Epoch 1, Batch 21485, Loss: 4.5516, Time: 12784.5s, Step: 21486, GPU: 4.9GB\n",
      "Epoch 1, Batch 21490, Loss: 3.7069, Time: 12787.0s, Step: 21491, GPU: 4.9GB\n",
      "Epoch 1, Batch 21495, Loss: 4.0554, Time: 12790.4s, Step: 21496, GPU: 4.9GB\n",
      "Epoch 1, Batch 21500, Loss: 3.7331, Time: 12792.9s, Step: 21501, GPU: 4.9GB\n",
      "Epoch 1, Batch 21505, Loss: 3.9270, Time: 12796.3s, Step: 21506, GPU: 4.9GB\n",
      "Epoch 1, Batch 21510, Loss: 4.5623, Time: 12798.9s, Step: 21511, GPU: 4.9GB\n",
      "Epoch 1, Batch 21515, Loss: 3.6559, Time: 12802.3s, Step: 21516, GPU: 4.9GB\n",
      "Epoch 1, Batch 21520, Loss: 3.4903, Time: 12804.8s, Step: 21521, GPU: 4.9GB\n",
      "Epoch 1, Batch 21525, Loss: 3.7947, Time: 12808.2s, Step: 21526, GPU: 4.9GB\n",
      "Epoch 1, Batch 21530, Loss: 3.4727, Time: 12810.7s, Step: 21531, GPU: 4.9GB\n",
      "Epoch 1, Batch 21535, Loss: 3.5559, Time: 12814.1s, Step: 21536, GPU: 4.9GB\n",
      "Epoch 1, Batch 21540, Loss: 4.9625, Time: 12816.6s, Step: 21541, GPU: 4.9GB\n",
      "Epoch 1, Batch 21545, Loss: 2.8741, Time: 12820.1s, Step: 21546, GPU: 4.9GB\n",
      "Epoch 1, Batch 21550, Loss: 3.7287, Time: 12822.6s, Step: 21551, GPU: 4.9GB\n",
      "Epoch 1, Batch 21555, Loss: 3.8947, Time: 12826.0s, Step: 21556, GPU: 4.9GB\n",
      "Epoch 1, Batch 21560, Loss: 4.8657, Time: 12828.5s, Step: 21561, GPU: 4.9GB\n",
      "Epoch 1, Batch 21565, Loss: 3.8548, Time: 12831.9s, Step: 21566, GPU: 4.9GB\n",
      "Epoch 1, Batch 21570, Loss: 4.4767, Time: 12834.4s, Step: 21571, GPU: 4.9GB\n",
      "Epoch 1, Batch 21575, Loss: 3.3184, Time: 12837.9s, Step: 21576, GPU: 4.9GB\n",
      "Epoch 1, Batch 21580, Loss: 3.8159, Time: 12840.4s, Step: 21581, GPU: 4.9GB\n",
      "Epoch 1, Batch 21585, Loss: 4.7787, Time: 12843.8s, Step: 21586, GPU: 4.9GB\n",
      "Epoch 1, Batch 21590, Loss: 4.0054, Time: 12846.3s, Step: 21591, GPU: 4.9GB\n",
      "Epoch 1, Batch 21595, Loss: 2.3223, Time: 12849.7s, Step: 21596, GPU: 4.9GB\n",
      "Epoch 1, Batch 21600, Loss: 3.1879, Time: 12852.3s, Step: 21601, GPU: 4.9GB\n",
      "Epoch 1, Batch 21605, Loss: 4.7225, Time: 12855.7s, Step: 21606, GPU: 4.9GB\n",
      "Epoch 1, Batch 21610, Loss: 3.1683, Time: 12858.3s, Step: 21611, GPU: 4.9GB\n",
      "Epoch 1, Batch 21615, Loss: 5.6255, Time: 12861.7s, Step: 21616, GPU: 4.9GB\n",
      "Epoch 1, Batch 21620, Loss: 4.3723, Time: 12864.2s, Step: 21621, GPU: 4.9GB\n",
      "Epoch 1, Batch 21625, Loss: 3.4944, Time: 12867.6s, Step: 21626, GPU: 4.9GB\n",
      "Epoch 1, Batch 21630, Loss: 3.4870, Time: 12870.1s, Step: 21631, GPU: 4.9GB\n",
      "Epoch 1, Batch 21635, Loss: 3.7428, Time: 12873.6s, Step: 21636, GPU: 4.9GB\n",
      "Epoch 1, Batch 21640, Loss: 3.3074, Time: 12876.1s, Step: 21641, GPU: 4.9GB\n",
      "Epoch 1, Batch 21645, Loss: 4.1001, Time: 12879.5s, Step: 21646, GPU: 4.9GB\n",
      "Epoch 1, Batch 21650, Loss: 4.0958, Time: 12882.0s, Step: 21651, GPU: 4.9GB\n",
      "Epoch 1, Batch 21655, Loss: 3.5286, Time: 12885.4s, Step: 21656, GPU: 4.9GB\n",
      "Epoch 1, Batch 21660, Loss: 3.7797, Time: 12887.9s, Step: 21661, GPU: 4.9GB\n",
      "Epoch 1, Batch 21665, Loss: 3.8396, Time: 12891.3s, Step: 21666, GPU: 4.9GB\n",
      "Epoch 1, Batch 21670, Loss: 3.6167, Time: 12893.9s, Step: 21671, GPU: 4.9GB\n",
      "Epoch 1, Batch 21675, Loss: 3.2613, Time: 12897.3s, Step: 21676, GPU: 4.9GB\n",
      "Epoch 1, Batch 21680, Loss: 4.0652, Time: 12899.8s, Step: 21681, GPU: 4.9GB\n",
      "Epoch 1, Batch 21685, Loss: 4.0018, Time: 12903.2s, Step: 21686, GPU: 4.9GB\n",
      "Epoch 1, Batch 21690, Loss: 3.5827, Time: 12905.8s, Step: 21691, GPU: 4.9GB\n",
      "Epoch 1, Batch 21695, Loss: 4.1273, Time: 12909.2s, Step: 21696, GPU: 4.9GB\n",
      "Epoch 1, Batch 21700, Loss: 3.1424, Time: 12911.7s, Step: 21701, GPU: 4.9GB\n",
      "Epoch 1, Batch 21705, Loss: 3.8939, Time: 12915.1s, Step: 21706, GPU: 4.9GB\n",
      "Epoch 1, Batch 21710, Loss: 3.5406, Time: 12917.6s, Step: 21711, GPU: 4.9GB\n",
      "Epoch 1, Batch 21715, Loss: 3.2904, Time: 12921.0s, Step: 21716, GPU: 4.9GB\n",
      "Epoch 1, Batch 21720, Loss: 4.6714, Time: 12923.6s, Step: 21721, GPU: 4.9GB\n",
      "Epoch 1, Batch 21725, Loss: 4.3113, Time: 12927.0s, Step: 21726, GPU: 4.9GB\n",
      "Epoch 1, Batch 21730, Loss: 3.8846, Time: 12929.5s, Step: 21731, GPU: 4.9GB\n",
      "Epoch 1, Batch 21735, Loss: 3.7138, Time: 12932.9s, Step: 21736, GPU: 4.9GB\n",
      "Epoch 1, Batch 21740, Loss: 3.6058, Time: 12935.4s, Step: 21741, GPU: 4.9GB\n",
      "Epoch 1, Batch 21745, Loss: 4.1573, Time: 12938.8s, Step: 21746, GPU: 4.9GB\n",
      "Epoch 1, Batch 21750, Loss: 4.3521, Time: 12941.3s, Step: 21751, GPU: 4.9GB\n",
      "Epoch 1, Batch 21755, Loss: 3.5307, Time: 12944.7s, Step: 21756, GPU: 4.9GB\n",
      "Epoch 1, Batch 21760, Loss: 3.3676, Time: 12947.2s, Step: 21761, GPU: 4.9GB\n",
      "Epoch 1, Batch 21765, Loss: 3.1367, Time: 12950.7s, Step: 21766, GPU: 4.9GB\n",
      "Epoch 1, Batch 21770, Loss: 4.1135, Time: 12953.2s, Step: 21771, GPU: 4.9GB\n",
      "Epoch 1, Batch 21775, Loss: 4.2902, Time: 12956.6s, Step: 21776, GPU: 4.9GB\n",
      "Epoch 1, Batch 21780, Loss: 4.5313, Time: 12959.1s, Step: 21781, GPU: 4.9GB\n",
      "Epoch 1, Batch 21785, Loss: 3.8251, Time: 12962.5s, Step: 21786, GPU: 4.9GB\n",
      "Epoch 1, Batch 21790, Loss: 4.0958, Time: 12965.0s, Step: 21791, GPU: 4.9GB\n",
      "Epoch 1, Batch 21795, Loss: 4.0434, Time: 12968.5s, Step: 21796, GPU: 4.9GB\n",
      "Epoch 1, Batch 21800, Loss: 4.2434, Time: 12971.1s, Step: 21801, GPU: 4.9GB\n",
      "Epoch 1, Batch 21805, Loss: 3.1850, Time: 12974.5s, Step: 21806, GPU: 4.9GB\n",
      "Epoch 1, Batch 21810, Loss: 3.4757, Time: 12977.0s, Step: 21811, GPU: 4.9GB\n",
      "Epoch 1, Batch 21815, Loss: 4.3557, Time: 12980.4s, Step: 21816, GPU: 4.9GB\n",
      "Epoch 1, Batch 21820, Loss: 4.0968, Time: 12982.9s, Step: 21821, GPU: 4.9GB\n",
      "Epoch 1, Batch 21825, Loss: 4.0075, Time: 12986.3s, Step: 21826, GPU: 4.9GB\n",
      "Epoch 1, Batch 21830, Loss: 3.3930, Time: 12988.8s, Step: 21831, GPU: 4.9GB\n",
      "Epoch 1, Batch 21835, Loss: 4.1618, Time: 12992.3s, Step: 21836, GPU: 4.9GB\n",
      "Epoch 1, Batch 21840, Loss: 4.0891, Time: 12994.8s, Step: 21841, GPU: 4.9GB\n",
      "Epoch 1, Batch 21845, Loss: 3.7819, Time: 12998.2s, Step: 21846, GPU: 4.9GB\n",
      "Epoch 1, Batch 21850, Loss: 3.3653, Time: 13000.8s, Step: 21851, GPU: 4.9GB\n",
      "Epoch 1, Batch 21855, Loss: 3.3400, Time: 13004.2s, Step: 21856, GPU: 4.9GB\n",
      "Epoch 1, Batch 21860, Loss: 4.2834, Time: 13006.8s, Step: 21861, GPU: 4.9GB\n",
      "Epoch 1, Batch 21865, Loss: 3.7575, Time: 13010.2s, Step: 21866, GPU: 4.9GB\n",
      "Epoch 1, Batch 21870, Loss: 3.6054, Time: 13012.7s, Step: 21871, GPU: 4.9GB\n",
      "Epoch 1, Batch 21875, Loss: 3.8437, Time: 13016.1s, Step: 21876, GPU: 4.9GB\n",
      "Epoch 1, Batch 21880, Loss: 3.6458, Time: 13018.6s, Step: 21881, GPU: 4.9GB\n",
      "Epoch 1, Batch 21885, Loss: 4.6071, Time: 13022.1s, Step: 21886, GPU: 4.9GB\n",
      "Epoch 1, Batch 21890, Loss: 4.6173, Time: 13024.6s, Step: 21891, GPU: 4.9GB\n",
      "Epoch 1, Batch 21895, Loss: 4.9166, Time: 13028.0s, Step: 21896, GPU: 4.9GB\n",
      "Epoch 1, Batch 21900, Loss: 4.7143, Time: 13030.6s, Step: 21901, GPU: 4.9GB\n",
      "Epoch 1, Batch 21905, Loss: 4.0920, Time: 13034.0s, Step: 21906, GPU: 4.9GB\n",
      "Epoch 1, Batch 21910, Loss: 3.9529, Time: 13036.5s, Step: 21911, GPU: 4.9GB\n",
      "Epoch 1, Batch 21915, Loss: 4.4807, Time: 13039.9s, Step: 21916, GPU: 4.9GB\n",
      "Epoch 1, Batch 21920, Loss: 3.7522, Time: 13042.4s, Step: 21921, GPU: 4.9GB\n",
      "Epoch 1, Batch 21925, Loss: 3.5784, Time: 13045.8s, Step: 21926, GPU: 4.9GB\n",
      "Epoch 1, Batch 21930, Loss: 3.5168, Time: 13048.4s, Step: 21931, GPU: 4.9GB\n",
      "Epoch 1, Batch 21935, Loss: 3.1543, Time: 13051.8s, Step: 21936, GPU: 4.9GB\n",
      "Epoch 1, Batch 21940, Loss: 4.2991, Time: 13054.3s, Step: 21941, GPU: 4.9GB\n",
      "Epoch 1, Batch 21945, Loss: 3.9759, Time: 13057.7s, Step: 21946, GPU: 4.9GB\n",
      "Epoch 1, Batch 21950, Loss: 4.0780, Time: 13060.3s, Step: 21951, GPU: 4.9GB\n",
      "Epoch 1, Batch 21955, Loss: 3.5691, Time: 13063.7s, Step: 21956, GPU: 4.9GB\n",
      "Epoch 1, Batch 21960, Loss: 4.3276, Time: 13066.2s, Step: 21961, GPU: 4.9GB\n",
      "Epoch 1, Batch 21965, Loss: 3.8441, Time: 13069.6s, Step: 21966, GPU: 4.9GB\n",
      "Epoch 1, Batch 21970, Loss: 3.3952, Time: 13072.1s, Step: 21971, GPU: 4.9GB\n",
      "Epoch 1, Batch 21975, Loss: 3.3032, Time: 13075.5s, Step: 21976, GPU: 4.9GB\n",
      "Epoch 1, Batch 21980, Loss: 3.9630, Time: 13078.0s, Step: 21981, GPU: 4.9GB\n",
      "Epoch 1, Batch 21985, Loss: 3.8712, Time: 13081.4s, Step: 21986, GPU: 4.9GB\n",
      "Epoch 1, Batch 21990, Loss: 6.3565, Time: 13083.9s, Step: 21991, GPU: 4.9GB\n",
      "Epoch 1, Batch 21995, Loss: 3.7854, Time: 13087.4s, Step: 21996, GPU: 4.9GB\n",
      "Epoch 1, Batch 22000, Loss: 3.4480, Time: 13090.0s, Step: 22001, GPU: 4.9GB\n",
      "Epoch 1, Batch 22005, Loss: 2.9726, Time: 13093.4s, Step: 22006, GPU: 4.9GB\n",
      "Epoch 1, Batch 22010, Loss: 4.1532, Time: 13095.9s, Step: 22011, GPU: 4.9GB\n",
      "Epoch 1, Batch 22015, Loss: 3.6643, Time: 13099.3s, Step: 22016, GPU: 4.9GB\n",
      "Epoch 1, Batch 22020, Loss: 3.6762, Time: 13101.8s, Step: 22021, GPU: 4.9GB\n",
      "Epoch 1, Batch 22025, Loss: 3.8919, Time: 13105.2s, Step: 22026, GPU: 4.9GB\n",
      "Epoch 1, Batch 22030, Loss: 4.4165, Time: 13107.7s, Step: 22031, GPU: 4.9GB\n",
      "Epoch 1, Batch 22035, Loss: 4.4668, Time: 13111.1s, Step: 22036, GPU: 4.9GB\n",
      "Epoch 1, Batch 22040, Loss: 3.3094, Time: 13113.6s, Step: 22041, GPU: 4.9GB\n",
      "Epoch 1, Batch 22045, Loss: 3.3782, Time: 13117.0s, Step: 22046, GPU: 4.9GB\n",
      "Epoch 1, Batch 22050, Loss: 3.1691, Time: 13119.6s, Step: 22051, GPU: 4.9GB\n",
      "Epoch 1, Batch 22055, Loss: 4.0228, Time: 13123.0s, Step: 22056, GPU: 4.9GB\n",
      "Epoch 1, Batch 22060, Loss: 3.1716, Time: 13125.5s, Step: 22061, GPU: 4.9GB\n",
      "Epoch 1, Batch 22065, Loss: 4.9117, Time: 13128.9s, Step: 22066, GPU: 4.9GB\n",
      "Epoch 1, Batch 22070, Loss: 3.3831, Time: 13131.4s, Step: 22071, GPU: 4.9GB\n",
      "Epoch 1, Batch 22075, Loss: 3.0043, Time: 13134.8s, Step: 22076, GPU: 4.9GB\n",
      "Epoch 1, Batch 22080, Loss: 4.6809, Time: 13137.3s, Step: 22081, GPU: 4.9GB\n",
      "Epoch 1, Batch 22085, Loss: 3.9263, Time: 13140.6s, Step: 22086, GPU: 4.9GB\n",
      "Epoch 1, Batch 22090, Loss: 4.0177, Time: 13143.2s, Step: 22091, GPU: 4.9GB\n",
      "Epoch 1, Batch 22095, Loss: 2.9711, Time: 13146.6s, Step: 22096, GPU: 4.9GB\n",
      "Epoch 1, Batch 22100, Loss: 3.0639, Time: 13149.1s, Step: 22101, GPU: 4.9GB\n",
      "Epoch 1, Batch 22105, Loss: 4.1154, Time: 13152.5s, Step: 22106, GPU: 4.9GB\n",
      "Epoch 1, Batch 22110, Loss: 5.0058, Time: 13155.0s, Step: 22111, GPU: 4.9GB\n",
      "Epoch 1, Batch 22115, Loss: 4.1119, Time: 13158.4s, Step: 22116, GPU: 4.9GB\n",
      "Epoch 1, Batch 22120, Loss: 4.5349, Time: 13160.9s, Step: 22121, GPU: 4.9GB\n",
      "Epoch 1, Batch 22125, Loss: 4.1611, Time: 13164.3s, Step: 22126, GPU: 4.9GB\n",
      "Epoch 1, Batch 22130, Loss: 3.1314, Time: 13166.8s, Step: 22131, GPU: 4.9GB\n",
      "Epoch 1, Batch 22135, Loss: 3.7607, Time: 13170.2s, Step: 22136, GPU: 4.9GB\n",
      "Epoch 1, Batch 22140, Loss: 4.2564, Time: 13172.7s, Step: 22141, GPU: 4.9GB\n",
      "Epoch 1, Batch 22145, Loss: 2.9510, Time: 13176.1s, Step: 22146, GPU: 4.9GB\n",
      "Epoch 1, Batch 22150, Loss: 3.4421, Time: 13178.6s, Step: 22151, GPU: 4.9GB\n",
      "Epoch 1, Batch 22155, Loss: 3.9831, Time: 13182.0s, Step: 22156, GPU: 4.9GB\n",
      "Epoch 1, Batch 22160, Loss: 5.1663, Time: 13184.5s, Step: 22161, GPU: 4.9GB\n",
      "Epoch 1, Batch 22165, Loss: 3.5905, Time: 13188.0s, Step: 22166, GPU: 4.9GB\n",
      "Epoch 1, Batch 22170, Loss: 4.2482, Time: 13190.5s, Step: 22171, GPU: 4.9GB\n",
      "Epoch 1, Batch 22175, Loss: 3.5245, Time: 13193.9s, Step: 22176, GPU: 4.9GB\n",
      "Epoch 1, Batch 22180, Loss: 3.3942, Time: 13196.5s, Step: 22181, GPU: 4.9GB\n",
      "Epoch 1, Batch 22185, Loss: 4.9849, Time: 13199.9s, Step: 22186, GPU: 4.9GB\n",
      "Epoch 1, Batch 22190, Loss: 3.7050, Time: 13202.4s, Step: 22191, GPU: 4.9GB\n",
      "Epoch 1, Batch 22195, Loss: 4.2220, Time: 13205.9s, Step: 22196, GPU: 4.9GB\n",
      "Epoch 1, Batch 22200, Loss: 4.0994, Time: 13208.5s, Step: 22201, GPU: 4.9GB\n",
      "Epoch 1, Batch 22205, Loss: 3.3796, Time: 13211.9s, Step: 22206, GPU: 4.9GB\n",
      "Epoch 1, Batch 22210, Loss: 4.3020, Time: 13214.4s, Step: 22211, GPU: 4.9GB\n",
      "Epoch 1, Batch 22215, Loss: 2.8282, Time: 13217.8s, Step: 22216, GPU: 4.9GB\n",
      "Epoch 1, Batch 22220, Loss: 3.4891, Time: 13220.4s, Step: 22221, GPU: 4.9GB\n",
      "Epoch 1, Batch 22225, Loss: 4.4979, Time: 13223.8s, Step: 22226, GPU: 4.9GB\n",
      "Epoch 1, Batch 22230, Loss: 4.3197, Time: 13226.4s, Step: 22231, GPU: 4.9GB\n",
      "Epoch 1, Batch 22235, Loss: 4.2623, Time: 13229.8s, Step: 22236, GPU: 4.9GB\n",
      "Epoch 1, Batch 22240, Loss: 3.4429, Time: 13232.3s, Step: 22241, GPU: 4.9GB\n",
      "Epoch 1, Batch 22245, Loss: 3.7778, Time: 13235.7s, Step: 22246, GPU: 4.9GB\n",
      "Epoch 1, Batch 22250, Loss: 3.6899, Time: 13238.3s, Step: 22251, GPU: 4.9GB\n",
      "Epoch 1, Batch 22255, Loss: 3.6774, Time: 13241.7s, Step: 22256, GPU: 4.9GB\n",
      "Epoch 1, Batch 22260, Loss: 3.6494, Time: 13244.2s, Step: 22261, GPU: 4.9GB\n",
      "Epoch 1, Batch 22265, Loss: 3.2770, Time: 13247.6s, Step: 22266, GPU: 4.9GB\n",
      "Epoch 1, Batch 22270, Loss: 3.6977, Time: 13250.1s, Step: 22271, GPU: 4.9GB\n",
      "Epoch 1, Batch 22275, Loss: 3.5173, Time: 13253.6s, Step: 22276, GPU: 4.9GB\n",
      "Epoch 1, Batch 22280, Loss: 3.7172, Time: 13256.1s, Step: 22281, GPU: 4.9GB\n",
      "Epoch 1, Batch 22285, Loss: 4.1252, Time: 13259.5s, Step: 22286, GPU: 4.9GB\n",
      "Epoch 1, Batch 22290, Loss: 3.9019, Time: 13262.0s, Step: 22291, GPU: 4.9GB\n",
      "Epoch 1, Batch 22295, Loss: 3.3929, Time: 13265.5s, Step: 22296, GPU: 4.9GB\n",
      "Epoch 1, Batch 22300, Loss: 4.7039, Time: 13268.0s, Step: 22301, GPU: 4.9GB\n",
      "Epoch 1, Batch 22305, Loss: 3.6353, Time: 13271.4s, Step: 22306, GPU: 4.9GB\n",
      "Epoch 1, Batch 22310, Loss: 4.2300, Time: 13274.0s, Step: 22311, GPU: 4.9GB\n",
      "Epoch 1, Batch 22315, Loss: 3.5991, Time: 13277.4s, Step: 22316, GPU: 4.9GB\n",
      "Epoch 1, Batch 22320, Loss: 3.6784, Time: 13280.0s, Step: 22321, GPU: 4.9GB\n",
      "Epoch 1, Batch 22325, Loss: 4.2334, Time: 13283.4s, Step: 22326, GPU: 4.9GB\n",
      "Epoch 1, Batch 22330, Loss: 4.2794, Time: 13285.9s, Step: 22331, GPU: 4.9GB\n",
      "Epoch 1, Batch 22335, Loss: 4.2233, Time: 13289.3s, Step: 22336, GPU: 4.9GB\n",
      "Epoch 1, Batch 22340, Loss: 3.9083, Time: 13291.9s, Step: 22341, GPU: 4.9GB\n",
      "Epoch 1, Batch 22345, Loss: 3.8362, Time: 13295.3s, Step: 22346, GPU: 4.9GB\n",
      "Epoch 1, Batch 22350, Loss: 4.4336, Time: 13297.8s, Step: 22351, GPU: 4.9GB\n",
      "Epoch 1, Batch 22355, Loss: 3.6444, Time: 13301.3s, Step: 22356, GPU: 4.9GB\n",
      "Epoch 1, Batch 22360, Loss: 3.4740, Time: 13303.8s, Step: 22361, GPU: 4.9GB\n",
      "Epoch 1, Batch 22365, Loss: 4.0565, Time: 13307.2s, Step: 22366, GPU: 4.9GB\n",
      "Epoch 1, Batch 22370, Loss: 3.3524, Time: 13309.8s, Step: 22371, GPU: 4.9GB\n",
      "Epoch 1, Batch 22375, Loss: 4.7610, Time: 13313.2s, Step: 22376, GPU: 4.9GB\n",
      "Epoch 1, Batch 22380, Loss: 3.9658, Time: 13315.7s, Step: 22381, GPU: 4.9GB\n",
      "Epoch 1, Batch 22385, Loss: 2.6789, Time: 13319.2s, Step: 22386, GPU: 4.9GB\n",
      "Epoch 1, Batch 22390, Loss: 3.4856, Time: 13321.7s, Step: 22391, GPU: 4.9GB\n",
      "Epoch 1, Batch 22395, Loss: 4.5505, Time: 13325.3s, Step: 22396, GPU: 4.9GB\n",
      "Epoch 1, Batch 22400, Loss: 3.9495, Time: 13327.9s, Step: 22401, GPU: 4.9GB\n",
      "Epoch 1, Batch 22405, Loss: 3.4213, Time: 13331.4s, Step: 22406, GPU: 4.9GB\n",
      "Epoch 1, Batch 22410, Loss: 3.7950, Time: 13333.9s, Step: 22411, GPU: 4.9GB\n",
      "Epoch 1, Batch 22415, Loss: 3.0073, Time: 13337.3s, Step: 22416, GPU: 4.9GB\n",
      "Epoch 1, Batch 22420, Loss: 3.8822, Time: 13339.8s, Step: 22421, GPU: 4.9GB\n",
      "Epoch 1, Batch 22425, Loss: 3.8736, Time: 13343.3s, Step: 22426, GPU: 4.9GB\n",
      "Epoch 1, Batch 22430, Loss: 3.8356, Time: 13345.8s, Step: 22431, GPU: 4.9GB\n",
      "Epoch 1, Batch 22435, Loss: 4.8089, Time: 13349.2s, Step: 22436, GPU: 4.9GB\n",
      "Epoch 1, Batch 22440, Loss: 3.8516, Time: 13351.8s, Step: 22441, GPU: 4.9GB\n",
      "Epoch 1, Batch 22445, Loss: 3.4355, Time: 13355.2s, Step: 22446, GPU: 4.9GB\n",
      "Epoch 1, Batch 22450, Loss: 3.6588, Time: 13357.7s, Step: 22451, GPU: 4.9GB\n",
      "Epoch 1, Batch 22455, Loss: 3.5769, Time: 13361.2s, Step: 22456, GPU: 4.9GB\n",
      "Epoch 1, Batch 22460, Loss: 3.9386, Time: 13363.7s, Step: 22461, GPU: 4.9GB\n",
      "Epoch 1, Batch 22465, Loss: 3.7357, Time: 13367.1s, Step: 22466, GPU: 4.9GB\n",
      "Epoch 1, Batch 22470, Loss: 4.4841, Time: 13369.6s, Step: 22471, GPU: 4.9GB\n",
      "Epoch 1, Batch 22475, Loss: 4.1036, Time: 13373.1s, Step: 22476, GPU: 4.9GB\n",
      "Epoch 1, Batch 22480, Loss: 3.5580, Time: 13375.6s, Step: 22481, GPU: 4.9GB\n",
      "Epoch 1, Batch 22485, Loss: 2.7744, Time: 13379.0s, Step: 22486, GPU: 4.9GB\n",
      "Epoch 1, Batch 22490, Loss: 3.5109, Time: 13381.5s, Step: 22491, GPU: 4.9GB\n",
      "Epoch 1, Batch 22495, Loss: 4.0046, Time: 13385.0s, Step: 22496, GPU: 4.9GB\n",
      "Epoch 1, Batch 22500, Loss: 3.6567, Time: 13387.5s, Step: 22501, GPU: 4.9GB\n",
      "Epoch 1, Batch 22505, Loss: 3.9616, Time: 13390.9s, Step: 22506, GPU: 4.9GB\n",
      "Epoch 1, Batch 22510, Loss: 4.2294, Time: 13393.4s, Step: 22511, GPU: 4.9GB\n",
      "Epoch 1, Batch 22515, Loss: 4.3991, Time: 13396.9s, Step: 22516, GPU: 4.9GB\n",
      "Epoch 1, Batch 22520, Loss: 3.7847, Time: 13399.4s, Step: 22521, GPU: 4.9GB\n",
      "Epoch 1, Batch 22525, Loss: 3.7732, Time: 13402.8s, Step: 22526, GPU: 4.9GB\n",
      "Epoch 1, Batch 22530, Loss: 3.3995, Time: 13405.3s, Step: 22531, GPU: 4.9GB\n",
      "Epoch 1, Batch 22535, Loss: 4.8319, Time: 13408.7s, Step: 22536, GPU: 4.9GB\n",
      "Epoch 1, Batch 22540, Loss: 3.0991, Time: 13411.3s, Step: 22541, GPU: 4.9GB\n",
      "Epoch 1, Batch 22545, Loss: 3.7034, Time: 13414.7s, Step: 22546, GPU: 4.9GB\n",
      "Epoch 1, Batch 22550, Loss: 3.7382, Time: 13417.2s, Step: 22551, GPU: 4.9GB\n",
      "Epoch 1, Batch 22555, Loss: 3.1953, Time: 13420.6s, Step: 22556, GPU: 4.9GB\n",
      "Epoch 1, Batch 22560, Loss: 3.1066, Time: 13423.2s, Step: 22561, GPU: 4.9GB\n",
      "Epoch 1, Batch 22565, Loss: 3.8918, Time: 13426.6s, Step: 22566, GPU: 4.9GB\n",
      "Epoch 1, Batch 22570, Loss: 3.7175, Time: 13429.1s, Step: 22571, GPU: 4.9GB\n",
      "Epoch 1, Batch 22575, Loss: 3.7073, Time: 13432.5s, Step: 22576, GPU: 4.9GB\n",
      "Epoch 1, Batch 22580, Loss: 3.7932, Time: 13435.0s, Step: 22581, GPU: 4.9GB\n",
      "Epoch 1, Batch 22585, Loss: 3.8667, Time: 13438.4s, Step: 22586, GPU: 4.9GB\n",
      "Epoch 1, Batch 22590, Loss: 3.3913, Time: 13441.0s, Step: 22591, GPU: 4.9GB\n",
      "Epoch 1, Batch 22595, Loss: 3.0422, Time: 13444.4s, Step: 22596, GPU: 4.9GB\n",
      "Epoch 1, Batch 22600, Loss: 2.7136, Time: 13447.0s, Step: 22601, GPU: 4.9GB\n",
      "Epoch 1, Batch 22605, Loss: 3.7427, Time: 13450.4s, Step: 22606, GPU: 4.9GB\n",
      "Epoch 1, Batch 22610, Loss: 4.0500, Time: 13452.9s, Step: 22611, GPU: 4.9GB\n",
      "Epoch 1, Batch 22615, Loss: 4.1716, Time: 13456.4s, Step: 22616, GPU: 4.9GB\n",
      "Epoch 1, Batch 22620, Loss: 4.3058, Time: 13458.9s, Step: 22621, GPU: 4.9GB\n",
      "Epoch 1, Batch 22625, Loss: 3.9945, Time: 13462.3s, Step: 22626, GPU: 4.9GB\n",
      "Epoch 1, Batch 22630, Loss: 3.8673, Time: 13464.8s, Step: 22631, GPU: 4.9GB\n",
      "Epoch 1, Batch 22635, Loss: 3.5762, Time: 13468.2s, Step: 22636, GPU: 4.9GB\n",
      "Epoch 1, Batch 22640, Loss: 4.2857, Time: 13470.8s, Step: 22641, GPU: 4.9GB\n",
      "Epoch 1, Batch 22645, Loss: 3.9696, Time: 13474.2s, Step: 22646, GPU: 4.9GB\n",
      "Epoch 1, Batch 22650, Loss: 4.3241, Time: 13476.7s, Step: 22651, GPU: 4.9GB\n",
      "Epoch 1, Batch 22655, Loss: 4.3614, Time: 13480.1s, Step: 22656, GPU: 4.9GB\n",
      "Epoch 1, Batch 22660, Loss: 3.9164, Time: 13482.6s, Step: 22661, GPU: 4.9GB\n",
      "Epoch 1, Batch 22665, Loss: 4.6289, Time: 13486.1s, Step: 22666, GPU: 4.9GB\n",
      "Epoch 1, Batch 22670, Loss: 3.6484, Time: 13488.6s, Step: 22671, GPU: 4.9GB\n",
      "Epoch 1, Batch 22675, Loss: 4.4851, Time: 13492.0s, Step: 22676, GPU: 4.9GB\n",
      "Epoch 1, Batch 22680, Loss: 3.1451, Time: 13494.5s, Step: 22681, GPU: 4.9GB\n",
      "Epoch 1, Batch 22685, Loss: 3.9714, Time: 13498.0s, Step: 22686, GPU: 4.9GB\n",
      "Epoch 1, Batch 22690, Loss: 3.8306, Time: 13500.5s, Step: 22691, GPU: 4.9GB\n",
      "Epoch 1, Batch 22695, Loss: 3.6968, Time: 13504.0s, Step: 22696, GPU: 4.9GB\n",
      "Epoch 1, Batch 22700, Loss: 3.6757, Time: 13506.5s, Step: 22701, GPU: 4.9GB\n",
      "Epoch 1, Batch 22705, Loss: 4.7521, Time: 13509.9s, Step: 22706, GPU: 4.9GB\n",
      "Epoch 1, Batch 22710, Loss: 4.1282, Time: 13512.4s, Step: 22711, GPU: 4.9GB\n",
      "Epoch 1, Batch 22715, Loss: 3.5773, Time: 13515.9s, Step: 22716, GPU: 4.9GB\n",
      "Epoch 1, Batch 22720, Loss: 3.5252, Time: 13518.4s, Step: 22721, GPU: 4.9GB\n",
      "Epoch 1, Batch 22725, Loss: 3.6214, Time: 13521.8s, Step: 22726, GPU: 4.9GB\n",
      "Epoch 1, Batch 22730, Loss: 4.0304, Time: 13524.3s, Step: 22731, GPU: 4.9GB\n",
      "Epoch 1, Batch 22735, Loss: 4.6312, Time: 13527.7s, Step: 22736, GPU: 4.9GB\n",
      "Epoch 1, Batch 22740, Loss: 3.8402, Time: 13530.3s, Step: 22741, GPU: 4.9GB\n",
      "Epoch 1, Batch 22745, Loss: 3.8386, Time: 13533.7s, Step: 22746, GPU: 4.9GB\n",
      "Epoch 1, Batch 22750, Loss: 4.3242, Time: 13536.2s, Step: 22751, GPU: 4.9GB\n",
      "Epoch 1, Batch 22755, Loss: 3.8166, Time: 13539.6s, Step: 22756, GPU: 4.9GB\n",
      "Epoch 1, Batch 22760, Loss: 3.9866, Time: 13542.1s, Step: 22761, GPU: 4.9GB\n",
      "Epoch 1, Batch 22765, Loss: 4.0531, Time: 13545.6s, Step: 22766, GPU: 4.9GB\n",
      "Epoch 1, Batch 22770, Loss: 3.5968, Time: 13548.1s, Step: 22771, GPU: 4.9GB\n",
      "Epoch 1, Batch 22775, Loss: 3.6429, Time: 13551.5s, Step: 22776, GPU: 4.9GB\n",
      "Epoch 1, Batch 22780, Loss: 2.6707, Time: 13554.0s, Step: 22781, GPU: 4.9GB\n",
      "Epoch 1, Batch 22785, Loss: 3.8367, Time: 13557.4s, Step: 22786, GPU: 4.9GB\n",
      "Epoch 1, Batch 22790, Loss: 3.4864, Time: 13559.9s, Step: 22791, GPU: 4.9GB\n",
      "Epoch 1, Batch 22795, Loss: 3.1766, Time: 13563.4s, Step: 22796, GPU: 4.9GB\n",
      "Epoch 1, Batch 22800, Loss: 3.9940, Time: 13566.0s, Step: 22801, GPU: 4.9GB\n",
      "Epoch 1, Batch 22805, Loss: 3.9576, Time: 13569.4s, Step: 22806, GPU: 4.9GB\n",
      "Epoch 1, Batch 22810, Loss: 3.5008, Time: 13572.0s, Step: 22811, GPU: 4.9GB\n",
      "Epoch 1, Batch 22815, Loss: 3.9983, Time: 13575.4s, Step: 22816, GPU: 4.9GB\n",
      "Epoch 1, Batch 22820, Loss: 4.2627, Time: 13577.9s, Step: 22821, GPU: 4.9GB\n",
      "Epoch 1, Batch 22825, Loss: 3.8734, Time: 13581.3s, Step: 22826, GPU: 4.9GB\n",
      "Epoch 1, Batch 22830, Loss: 4.0901, Time: 13583.8s, Step: 22831, GPU: 4.9GB\n",
      "Epoch 1, Batch 22835, Loss: 3.3481, Time: 13587.3s, Step: 22836, GPU: 4.9GB\n",
      "Epoch 1, Batch 22840, Loss: 3.8600, Time: 13589.8s, Step: 22841, GPU: 4.9GB\n",
      "Epoch 1, Batch 22845, Loss: 4.1241, Time: 13593.2s, Step: 22846, GPU: 4.9GB\n",
      "Epoch 1, Batch 22850, Loss: 3.9777, Time: 13595.7s, Step: 22851, GPU: 4.9GB\n",
      "Epoch 1, Batch 22855, Loss: 4.3910, Time: 13599.1s, Step: 22856, GPU: 4.9GB\n",
      "Epoch 1, Batch 22860, Loss: 3.7012, Time: 13601.7s, Step: 22861, GPU: 4.9GB\n",
      "Epoch 1, Batch 22865, Loss: 4.3457, Time: 13605.1s, Step: 22866, GPU: 4.9GB\n",
      "Epoch 1, Batch 22870, Loss: 3.3716, Time: 13607.6s, Step: 22871, GPU: 4.9GB\n",
      "Epoch 1, Batch 22875, Loss: 3.7807, Time: 13611.1s, Step: 22876, GPU: 4.9GB\n",
      "Epoch 1, Batch 22880, Loss: 3.2263, Time: 13613.6s, Step: 22881, GPU: 4.9GB\n",
      "Epoch 1, Batch 22885, Loss: 3.9048, Time: 13617.0s, Step: 22886, GPU: 4.9GB\n",
      "Epoch 1, Batch 22890, Loss: 3.6975, Time: 13619.6s, Step: 22891, GPU: 4.9GB\n",
      "Epoch 1, Batch 22895, Loss: 3.7479, Time: 13623.0s, Step: 22896, GPU: 4.9GB\n",
      "Epoch 1, Batch 22900, Loss: 4.3562, Time: 13625.5s, Step: 22901, GPU: 4.9GB\n",
      "Epoch 1, Batch 22905, Loss: 3.9920, Time: 13629.1s, Step: 22906, GPU: 4.9GB\n",
      "Epoch 1, Batch 22910, Loss: 4.2881, Time: 13631.6s, Step: 22911, GPU: 4.9GB\n",
      "Epoch 1, Batch 22915, Loss: 4.2351, Time: 13635.0s, Step: 22916, GPU: 4.9GB\n",
      "Epoch 1, Batch 22920, Loss: 3.9860, Time: 13637.5s, Step: 22921, GPU: 4.9GB\n",
      "Epoch 1, Batch 22925, Loss: 4.9361, Time: 13640.9s, Step: 22926, GPU: 4.9GB\n",
      "Epoch 1, Batch 22930, Loss: 3.7531, Time: 13643.5s, Step: 22931, GPU: 4.9GB\n",
      "Epoch 1, Batch 22935, Loss: 4.0014, Time: 13646.9s, Step: 22936, GPU: 4.9GB\n",
      "Epoch 1, Batch 22940, Loss: 4.5054, Time: 13649.4s, Step: 22941, GPU: 4.9GB\n",
      "Epoch 1, Batch 22945, Loss: 3.1066, Time: 13652.9s, Step: 22946, GPU: 4.9GB\n",
      "Epoch 1, Batch 22950, Loss: 3.8100, Time: 13655.4s, Step: 22951, GPU: 4.9GB\n",
      "Epoch 1, Batch 22955, Loss: 3.9514, Time: 13658.8s, Step: 22956, GPU: 4.9GB\n",
      "Epoch 1, Batch 22960, Loss: 3.0632, Time: 13661.3s, Step: 22961, GPU: 4.9GB\n",
      "Epoch 1, Batch 22965, Loss: 3.6083, Time: 13664.7s, Step: 22966, GPU: 4.9GB\n",
      "Epoch 1, Batch 22970, Loss: 3.3822, Time: 13667.2s, Step: 22971, GPU: 4.9GB\n",
      "Epoch 1, Batch 22975, Loss: 4.1712, Time: 13670.6s, Step: 22976, GPU: 4.9GB\n",
      "Epoch 1, Batch 22980, Loss: 3.8125, Time: 13673.1s, Step: 22981, GPU: 4.9GB\n",
      "Epoch 1, Batch 22985, Loss: 3.9004, Time: 13676.5s, Step: 22986, GPU: 4.9GB\n",
      "Epoch 1, Batch 22990, Loss: 4.7575, Time: 13679.0s, Step: 22991, GPU: 4.9GB\n",
      "Epoch 1, Batch 22995, Loss: 4.3287, Time: 13682.4s, Step: 22996, GPU: 4.9GB\n",
      "Epoch 1, Batch 23000, Loss: 3.6513, Time: 13685.0s, Step: 23001, GPU: 4.9GB\n",
      "Epoch 1, Batch 23005, Loss: 4.5361, Time: 13688.4s, Step: 23006, GPU: 4.9GB\n",
      "Epoch 1, Batch 23010, Loss: 4.0776, Time: 13690.9s, Step: 23011, GPU: 4.9GB\n",
      "Epoch 1, Batch 23015, Loss: 4.2357, Time: 13694.3s, Step: 23016, GPU: 4.9GB\n",
      "Epoch 1, Batch 23020, Loss: 3.3244, Time: 13696.8s, Step: 23021, GPU: 4.9GB\n",
      "Epoch 1, Batch 23025, Loss: 4.8298, Time: 13700.2s, Step: 23026, GPU: 4.9GB\n",
      "Epoch 1, Batch 23030, Loss: 3.5355, Time: 13702.8s, Step: 23031, GPU: 4.9GB\n",
      "Epoch 1, Batch 23035, Loss: 3.7893, Time: 13706.2s, Step: 23036, GPU: 4.9GB\n",
      "Epoch 1, Batch 23040, Loss: 4.1249, Time: 13708.7s, Step: 23041, GPU: 4.9GB\n",
      "Epoch 1, Batch 23045, Loss: 3.4555, Time: 13712.1s, Step: 23046, GPU: 4.9GB\n",
      "Epoch 1, Batch 23050, Loss: 4.2063, Time: 13714.6s, Step: 23051, GPU: 4.9GB\n",
      "Epoch 1, Batch 23055, Loss: 2.9699, Time: 13718.0s, Step: 23056, GPU: 4.9GB\n",
      "Epoch 1, Batch 23060, Loss: 2.7651, Time: 13720.6s, Step: 23061, GPU: 4.9GB\n",
      "Epoch 1, Batch 23065, Loss: 3.4645, Time: 13724.0s, Step: 23066, GPU: 4.9GB\n",
      "Epoch 1, Batch 23070, Loss: 4.5580, Time: 13726.5s, Step: 23071, GPU: 4.9GB\n",
      "Epoch 1, Batch 23075, Loss: 4.2726, Time: 13729.9s, Step: 23076, GPU: 4.9GB\n",
      "Epoch 1, Batch 23080, Loss: 4.3853, Time: 13732.4s, Step: 23081, GPU: 4.9GB\n",
      "Epoch 1, Batch 23085, Loss: 3.8908, Time: 13735.8s, Step: 23086, GPU: 4.9GB\n",
      "Epoch 1, Batch 23090, Loss: 3.9869, Time: 13738.3s, Step: 23091, GPU: 4.9GB\n",
      "Epoch 1, Batch 23095, Loss: 4.9294, Time: 13741.7s, Step: 23096, GPU: 4.9GB\n",
      "Epoch 1, Batch 23100, Loss: 4.0744, Time: 13744.2s, Step: 23101, GPU: 4.9GB\n",
      "Epoch 1, Batch 23105, Loss: 3.6401, Time: 13747.6s, Step: 23106, GPU: 4.9GB\n",
      "Epoch 1, Batch 23110, Loss: 3.8679, Time: 13750.2s, Step: 23111, GPU: 4.9GB\n",
      "Epoch 1, Batch 23115, Loss: 3.3551, Time: 13753.6s, Step: 23116, GPU: 4.9GB\n",
      "Epoch 1, Batch 23120, Loss: 4.1085, Time: 13756.1s, Step: 23121, GPU: 4.9GB\n",
      "Epoch 1, Batch 23125, Loss: 3.5305, Time: 13759.5s, Step: 23126, GPU: 4.9GB\n",
      "Epoch 1, Batch 23130, Loss: 3.4149, Time: 13762.1s, Step: 23131, GPU: 4.9GB\n",
      "Epoch 1, Batch 23135, Loss: 4.6596, Time: 13765.5s, Step: 23136, GPU: 4.9GB\n",
      "Epoch 1, Batch 23140, Loss: 3.2973, Time: 13768.0s, Step: 23141, GPU: 4.9GB\n",
      "Epoch 1, Batch 23145, Loss: 3.4930, Time: 13771.4s, Step: 23146, GPU: 4.9GB\n",
      "Epoch 1, Batch 23150, Loss: 3.7994, Time: 13773.9s, Step: 23151, GPU: 4.9GB\n",
      "Epoch 1, Batch 23155, Loss: 4.2583, Time: 13777.3s, Step: 23156, GPU: 4.9GB\n",
      "Epoch 1, Batch 23160, Loss: 4.1054, Time: 13779.8s, Step: 23161, GPU: 4.9GB\n",
      "Epoch 1, Batch 23165, Loss: 3.8144, Time: 13783.2s, Step: 23166, GPU: 4.9GB\n",
      "Epoch 1, Batch 23170, Loss: 3.7353, Time: 13785.8s, Step: 23171, GPU: 4.9GB\n",
      "Epoch 1, Batch 23175, Loss: 3.8407, Time: 13789.2s, Step: 23176, GPU: 4.9GB\n",
      "Epoch 1, Batch 23180, Loss: 3.7970, Time: 13791.7s, Step: 23181, GPU: 4.9GB\n",
      "Epoch 1, Batch 23185, Loss: 3.5314, Time: 13795.1s, Step: 23186, GPU: 4.9GB\n",
      "Epoch 1, Batch 23190, Loss: 4.3344, Time: 13797.7s, Step: 23191, GPU: 4.9GB\n",
      "Epoch 1, Batch 23195, Loss: 4.9577, Time: 13801.1s, Step: 23196, GPU: 4.9GB\n",
      "Epoch 1, Batch 23200, Loss: 4.0751, Time: 13803.7s, Step: 23201, GPU: 4.9GB\n",
      "Epoch 1, Batch 23205, Loss: 3.6092, Time: 13807.1s, Step: 23206, GPU: 4.9GB\n",
      "Epoch 1, Batch 23210, Loss: 4.4462, Time: 13809.6s, Step: 23211, GPU: 4.9GB\n",
      "Epoch 1, Batch 23215, Loss: 4.0508, Time: 13813.0s, Step: 23216, GPU: 4.9GB\n",
      "Epoch 1, Batch 23220, Loss: 3.5436, Time: 13815.5s, Step: 23221, GPU: 4.9GB\n",
      "Epoch 1, Batch 23225, Loss: 3.4668, Time: 13818.9s, Step: 23226, GPU: 4.9GB\n",
      "Epoch 1, Batch 23230, Loss: 4.2869, Time: 13821.4s, Step: 23231, GPU: 4.9GB\n",
      "Epoch 1, Batch 23235, Loss: 3.4750, Time: 13824.9s, Step: 23236, GPU: 4.9GB\n",
      "Epoch 1, Batch 23240, Loss: 4.3298, Time: 13827.4s, Step: 23241, GPU: 4.9GB\n",
      "Epoch 1, Batch 23245, Loss: 3.7434, Time: 13830.8s, Step: 23246, GPU: 4.9GB\n",
      "Epoch 1, Batch 23250, Loss: 3.4671, Time: 13833.4s, Step: 23251, GPU: 4.9GB\n",
      "Epoch 1, Batch 23255, Loss: 4.3689, Time: 13836.8s, Step: 23256, GPU: 4.9GB\n",
      "Epoch 1, Batch 23260, Loss: 4.1629, Time: 13839.4s, Step: 23261, GPU: 4.9GB\n",
      "Epoch 1, Batch 23265, Loss: 4.9689, Time: 13842.9s, Step: 23266, GPU: 4.9GB\n",
      "Epoch 1, Batch 23270, Loss: 3.5859, Time: 13845.4s, Step: 23271, GPU: 4.9GB\n",
      "Epoch 1, Batch 23275, Loss: 3.5609, Time: 13848.9s, Step: 23276, GPU: 4.9GB\n",
      "Epoch 1, Batch 23280, Loss: 3.8695, Time: 13851.4s, Step: 23281, GPU: 4.9GB\n",
      "Epoch 1, Batch 23285, Loss: 4.0591, Time: 13854.8s, Step: 23286, GPU: 4.9GB\n",
      "Epoch 1, Batch 23290, Loss: 4.0512, Time: 13857.3s, Step: 23291, GPU: 4.9GB\n",
      "Epoch 1, Batch 23295, Loss: 3.0207, Time: 13860.7s, Step: 23296, GPU: 4.9GB\n",
      "Epoch 1, Batch 23300, Loss: 3.0956, Time: 13863.2s, Step: 23301, GPU: 4.9GB\n",
      "Epoch 1, Batch 23305, Loss: 4.0688, Time: 13866.7s, Step: 23306, GPU: 4.9GB\n",
      "Epoch 1, Batch 23310, Loss: 3.9874, Time: 13869.2s, Step: 23311, GPU: 4.9GB\n",
      "Epoch 1, Batch 23315, Loss: 4.2559, Time: 13872.6s, Step: 23316, GPU: 4.9GB\n",
      "Epoch 1, Batch 23320, Loss: 4.3504, Time: 13875.1s, Step: 23321, GPU: 4.9GB\n",
      "Epoch 1, Batch 23325, Loss: 4.4874, Time: 13878.5s, Step: 23326, GPU: 4.9GB\n",
      "Epoch 1, Batch 23330, Loss: 3.8309, Time: 13881.1s, Step: 23331, GPU: 4.9GB\n",
      "Epoch 1, Batch 23335, Loss: 3.7228, Time: 13884.5s, Step: 23336, GPU: 4.9GB\n",
      "Epoch 1, Batch 23340, Loss: 3.8293, Time: 13887.0s, Step: 23341, GPU: 4.9GB\n",
      "Epoch 1, Batch 23345, Loss: 3.3829, Time: 13890.5s, Step: 23346, GPU: 4.9GB\n",
      "Epoch 1, Batch 23350, Loss: 3.8698, Time: 13893.0s, Step: 23351, GPU: 4.9GB\n",
      "Epoch 1, Batch 23355, Loss: 3.9989, Time: 13896.4s, Step: 23356, GPU: 4.9GB\n",
      "Epoch 1, Batch 23360, Loss: 4.0416, Time: 13899.0s, Step: 23361, GPU: 4.9GB\n",
      "Epoch 1, Batch 23365, Loss: 3.7848, Time: 13902.4s, Step: 23366, GPU: 4.9GB\n",
      "Epoch 1, Batch 23370, Loss: 3.8556, Time: 13904.9s, Step: 23371, GPU: 4.9GB\n",
      "Epoch 1, Batch 23375, Loss: 3.4372, Time: 13908.4s, Step: 23376, GPU: 4.9GB\n",
      "Epoch 1, Batch 23380, Loss: 3.3884, Time: 13910.9s, Step: 23381, GPU: 4.9GB\n",
      "Epoch 1, Batch 23385, Loss: 4.2420, Time: 13914.3s, Step: 23386, GPU: 4.9GB\n",
      "Epoch 1, Batch 23390, Loss: 3.8462, Time: 13916.9s, Step: 23391, GPU: 4.9GB\n",
      "Epoch 1, Batch 23395, Loss: 4.6748, Time: 13920.3s, Step: 23396, GPU: 4.9GB\n",
      "Epoch 1, Batch 23400, Loss: 4.8666, Time: 13922.9s, Step: 23401, GPU: 4.9GB\n",
      "Epoch 1, Batch 23405, Loss: 4.0224, Time: 13926.3s, Step: 23406, GPU: 4.9GB\n",
      "Epoch 1, Batch 23410, Loss: 3.9410, Time: 13928.8s, Step: 23411, GPU: 4.9GB\n",
      "Epoch 1, Batch 23415, Loss: 4.9786, Time: 13932.3s, Step: 23416, GPU: 4.9GB\n",
      "Epoch 1, Batch 23420, Loss: 4.0539, Time: 13934.8s, Step: 23421, GPU: 4.9GB\n",
      "Epoch 1, Batch 23425, Loss: 4.2700, Time: 13938.2s, Step: 23426, GPU: 4.9GB\n",
      "Epoch 1, Batch 23430, Loss: 4.0846, Time: 13940.7s, Step: 23431, GPU: 4.9GB\n",
      "Epoch 1, Batch 23435, Loss: 3.0025, Time: 13944.1s, Step: 23436, GPU: 4.9GB\n",
      "Epoch 1, Batch 23440, Loss: 4.9665, Time: 13946.6s, Step: 23441, GPU: 4.9GB\n",
      "Epoch 1, Batch 23445, Loss: 3.4971, Time: 13950.0s, Step: 23446, GPU: 4.9GB\n",
      "Epoch 1, Batch 23450, Loss: 4.6501, Time: 13952.5s, Step: 23451, GPU: 4.9GB\n",
      "Epoch 1, Batch 23455, Loss: 4.0959, Time: 13955.9s, Step: 23456, GPU: 4.9GB\n",
      "Epoch 1, Batch 23460, Loss: 3.7925, Time: 13958.4s, Step: 23461, GPU: 4.9GB\n",
      "Epoch 1, Batch 23465, Loss: 3.8498, Time: 13961.8s, Step: 23466, GPU: 4.9GB\n",
      "Epoch 1, Batch 23470, Loss: 3.4723, Time: 13964.4s, Step: 23471, GPU: 4.9GB\n",
      "Epoch 1, Batch 23475, Loss: 3.6374, Time: 13967.7s, Step: 23476, GPU: 4.9GB\n",
      "Epoch 1, Batch 23480, Loss: 5.1617, Time: 13970.3s, Step: 23481, GPU: 4.9GB\n",
      "Epoch 1, Batch 23485, Loss: 3.7641, Time: 13973.7s, Step: 23486, GPU: 4.9GB\n",
      "Epoch 1, Batch 23490, Loss: 4.3975, Time: 13976.2s, Step: 23491, GPU: 4.9GB\n",
      "Epoch 1, Batch 23495, Loss: 3.9999, Time: 13979.6s, Step: 23496, GPU: 4.9GB\n",
      "Epoch 1, Batch 23500, Loss: 3.8544, Time: 13982.1s, Step: 23501, GPU: 4.9GB\n",
      "Epoch 1, Batch 23505, Loss: 2.7218, Time: 13985.5s, Step: 23506, GPU: 4.9GB\n",
      "Epoch 1, Batch 23510, Loss: 3.5196, Time: 13988.0s, Step: 23511, GPU: 4.9GB\n",
      "Epoch 1, Batch 23515, Loss: 3.7598, Time: 13991.4s, Step: 23516, GPU: 4.9GB\n",
      "Epoch 1, Batch 23520, Loss: 4.0373, Time: 13993.9s, Step: 23521, GPU: 4.9GB\n",
      "Epoch 1, Batch 23525, Loss: 4.1875, Time: 13997.4s, Step: 23526, GPU: 4.9GB\n",
      "Epoch 1, Batch 23530, Loss: 3.3046, Time: 13999.9s, Step: 23531, GPU: 4.9GB\n",
      "Epoch 1, Batch 23535, Loss: 3.6828, Time: 14003.3s, Step: 23536, GPU: 4.9GB\n",
      "Epoch 1, Batch 23540, Loss: 3.7175, Time: 14005.8s, Step: 23541, GPU: 4.9GB\n",
      "Epoch 1, Batch 23545, Loss: 4.0268, Time: 14009.2s, Step: 23546, GPU: 4.9GB\n",
      "Epoch 1, Batch 23550, Loss: 3.0167, Time: 14011.7s, Step: 23551, GPU: 4.9GB\n",
      "Epoch 1, Batch 23555, Loss: 3.2846, Time: 14015.2s, Step: 23556, GPU: 4.9GB\n",
      "Epoch 1, Batch 23560, Loss: 2.5911, Time: 14017.7s, Step: 23561, GPU: 4.9GB\n",
      "Epoch 1, Batch 23565, Loss: 4.2180, Time: 14021.1s, Step: 23566, GPU: 4.9GB\n",
      "Epoch 1, Batch 23570, Loss: 3.8451, Time: 14023.6s, Step: 23571, GPU: 4.9GB\n",
      "Epoch 1, Batch 23575, Loss: 4.2259, Time: 14027.1s, Step: 23576, GPU: 4.9GB\n",
      "Epoch 1, Batch 23580, Loss: 4.0410, Time: 14029.6s, Step: 23581, GPU: 4.9GB\n",
      "Epoch 1, Batch 23585, Loss: 2.8263, Time: 14033.0s, Step: 23586, GPU: 4.9GB\n",
      "Epoch 1, Batch 23590, Loss: 3.6113, Time: 14035.5s, Step: 23591, GPU: 4.9GB\n",
      "Epoch 1, Batch 23595, Loss: 3.8906, Time: 14038.9s, Step: 23596, GPU: 4.9GB\n",
      "Epoch 1, Batch 23600, Loss: 4.1977, Time: 14041.5s, Step: 23601, GPU: 4.9GB\n",
      "Epoch 1, Batch 23605, Loss: 3.4526, Time: 14044.9s, Step: 23606, GPU: 4.9GB\n",
      "Epoch 1, Batch 23610, Loss: 3.4532, Time: 14047.4s, Step: 23611, GPU: 4.9GB\n",
      "Epoch 1, Batch 23615, Loss: 3.8484, Time: 14050.9s, Step: 23616, GPU: 4.9GB\n",
      "Epoch 1, Batch 23620, Loss: 4.3055, Time: 14053.4s, Step: 23621, GPU: 4.9GB\n",
      "Epoch 1, Batch 23625, Loss: 3.6020, Time: 14056.8s, Step: 23626, GPU: 4.9GB\n",
      "Epoch 1, Batch 23630, Loss: 3.5281, Time: 14059.3s, Step: 23631, GPU: 4.9GB\n",
      "Epoch 1, Batch 23635, Loss: 3.3500, Time: 14062.7s, Step: 23636, GPU: 4.9GB\n",
      "Epoch 1, Batch 23640, Loss: 3.8265, Time: 14065.3s, Step: 23641, GPU: 4.9GB\n",
      "Epoch 1, Batch 23645, Loss: 4.2849, Time: 14068.7s, Step: 23646, GPU: 4.9GB\n",
      "Epoch 1, Batch 23650, Loss: 3.8951, Time: 14071.2s, Step: 23651, GPU: 4.9GB\n",
      "Epoch 1, Batch 23655, Loss: 3.6683, Time: 14074.6s, Step: 23656, GPU: 4.9GB\n",
      "Epoch 1, Batch 23660, Loss: 3.6233, Time: 14077.1s, Step: 23661, GPU: 4.9GB\n",
      "Epoch 1, Batch 23665, Loss: 3.8415, Time: 14080.5s, Step: 23666, GPU: 4.9GB\n",
      "Epoch 1, Batch 23670, Loss: 3.8931, Time: 14083.0s, Step: 23671, GPU: 4.9GB\n",
      "Epoch 1, Batch 23675, Loss: 3.4564, Time: 14086.4s, Step: 23676, GPU: 4.9GB\n",
      "Epoch 1, Batch 23680, Loss: 3.8182, Time: 14089.0s, Step: 23681, GPU: 4.9GB\n",
      "Epoch 1, Batch 23685, Loss: 4.4219, Time: 14092.4s, Step: 23686, GPU: 4.9GB\n",
      "Epoch 1, Batch 23690, Loss: 3.3638, Time: 14094.9s, Step: 23691, GPU: 4.9GB\n",
      "Epoch 1, Batch 23695, Loss: 4.8909, Time: 14098.3s, Step: 23696, GPU: 4.9GB\n",
      "Epoch 1, Batch 23700, Loss: 3.6394, Time: 14100.9s, Step: 23701, GPU: 4.9GB\n",
      "Epoch 1, Batch 23705, Loss: 3.3944, Time: 14104.3s, Step: 23706, GPU: 4.9GB\n",
      "Epoch 1, Batch 23710, Loss: 3.7826, Time: 14106.8s, Step: 23711, GPU: 4.9GB\n",
      "Epoch 1, Batch 23715, Loss: 3.8751, Time: 14110.2s, Step: 23716, GPU: 4.9GB\n",
      "Epoch 1, Batch 23720, Loss: 3.3005, Time: 14112.7s, Step: 23721, GPU: 4.9GB\n",
      "Epoch 1, Batch 23725, Loss: 3.4451, Time: 14116.1s, Step: 23726, GPU: 4.9GB\n",
      "Epoch 1, Batch 23730, Loss: 3.6573, Time: 14118.7s, Step: 23731, GPU: 4.9GB\n",
      "Epoch 1, Batch 23735, Loss: 2.9353, Time: 14122.1s, Step: 23736, GPU: 4.9GB\n",
      "Epoch 1, Batch 23740, Loss: 4.4828, Time: 14124.6s, Step: 23741, GPU: 4.9GB\n",
      "Epoch 1, Batch 23745, Loss: 3.4067, Time: 14128.0s, Step: 23746, GPU: 4.9GB\n",
      "Epoch 1, Batch 23750, Loss: 4.3882, Time: 14130.5s, Step: 23751, GPU: 4.9GB\n",
      "Epoch 1, Batch 23755, Loss: 3.6445, Time: 14134.0s, Step: 23756, GPU: 4.9GB\n",
      "Epoch 1, Batch 23760, Loss: 4.2202, Time: 14136.5s, Step: 23761, GPU: 4.9GB\n",
      "Epoch 1, Batch 23765, Loss: 3.9485, Time: 14139.9s, Step: 23766, GPU: 4.9GB\n",
      "Epoch 1, Batch 23770, Loss: 3.7033, Time: 14142.4s, Step: 23771, GPU: 4.9GB\n",
      "Epoch 1, Batch 23775, Loss: 3.9107, Time: 14145.8s, Step: 23776, GPU: 4.9GB\n",
      "Epoch 1, Batch 23780, Loss: 4.4269, Time: 14148.3s, Step: 23781, GPU: 4.9GB\n",
      "Epoch 1, Batch 23785, Loss: 4.1761, Time: 14151.8s, Step: 23786, GPU: 4.9GB\n",
      "Epoch 1, Batch 23790, Loss: 3.8782, Time: 14154.3s, Step: 23791, GPU: 4.9GB\n",
      "Epoch 1, Batch 23795, Loss: 3.9769, Time: 14157.7s, Step: 23796, GPU: 4.9GB\n",
      "Epoch 1, Batch 23800, Loss: 3.6331, Time: 14160.3s, Step: 23801, GPU: 4.9GB\n",
      "Epoch 1, Batch 23805, Loss: 4.1519, Time: 14163.7s, Step: 23806, GPU: 4.9GB\n",
      "Epoch 1, Batch 23810, Loss: 3.5483, Time: 14166.2s, Step: 23811, GPU: 4.9GB\n",
      "Epoch 1, Batch 23815, Loss: 4.1437, Time: 14169.7s, Step: 23816, GPU: 4.9GB\n",
      "Epoch 1, Batch 23820, Loss: 4.1757, Time: 14172.2s, Step: 23821, GPU: 4.9GB\n",
      "Epoch 1, Batch 23825, Loss: 3.9487, Time: 14175.6s, Step: 23826, GPU: 4.9GB\n",
      "Epoch 1, Batch 23830, Loss: 4.8356, Time: 14178.2s, Step: 23831, GPU: 4.9GB\n",
      "Epoch 1, Batch 23835, Loss: 3.8269, Time: 14181.6s, Step: 23836, GPU: 4.9GB\n",
      "Epoch 1, Batch 23840, Loss: 4.1203, Time: 14184.1s, Step: 23841, GPU: 4.9GB\n",
      "Epoch 1, Batch 23845, Loss: 3.1190, Time: 14187.5s, Step: 23846, GPU: 4.9GB\n",
      "Epoch 1, Batch 23850, Loss: 5.0700, Time: 14190.0s, Step: 23851, GPU: 4.9GB\n",
      "Epoch 1, Batch 23855, Loss: 3.8200, Time: 14193.5s, Step: 23856, GPU: 4.9GB\n",
      "Epoch 1, Batch 23860, Loss: 4.2120, Time: 14196.0s, Step: 23861, GPU: 4.9GB\n",
      "Epoch 1, Batch 23865, Loss: 4.4621, Time: 14199.4s, Step: 23866, GPU: 4.9GB\n",
      "Epoch 1, Batch 23870, Loss: 3.0310, Time: 14201.9s, Step: 23871, GPU: 4.9GB\n",
      "Epoch 1, Batch 23875, Loss: 3.7705, Time: 14205.4s, Step: 23876, GPU: 4.9GB\n",
      "Epoch 1, Batch 23880, Loss: 4.0702, Time: 14207.9s, Step: 23881, GPU: 4.9GB\n",
      "Epoch 1, Batch 23885, Loss: 3.8530, Time: 14211.3s, Step: 23886, GPU: 4.9GB\n",
      "Epoch 1, Batch 23890, Loss: 3.6230, Time: 14213.8s, Step: 23891, GPU: 4.9GB\n",
      "Epoch 1, Batch 23895, Loss: 3.9207, Time: 14217.3s, Step: 23896, GPU: 4.9GB\n",
      "Epoch 1, Batch 23900, Loss: 3.3546, Time: 14220.9s, Step: 23901, GPU: 4.9GB\n",
      "Epoch 1, Batch 23905, Loss: 3.9797, Time: 14224.3s, Step: 23906, GPU: 4.9GB\n",
      "Epoch 1, Batch 23910, Loss: 2.9526, Time: 14226.8s, Step: 23911, GPU: 4.9GB\n",
      "Epoch 1, Batch 23915, Loss: 3.5742, Time: 14230.2s, Step: 23916, GPU: 4.9GB\n",
      "Epoch 1, Batch 23920, Loss: 4.1186, Time: 14232.8s, Step: 23921, GPU: 4.9GB\n",
      "Epoch 1, Batch 23925, Loss: 4.2319, Time: 14236.2s, Step: 23926, GPU: 4.9GB\n",
      "Epoch 1, Batch 23930, Loss: 4.3942, Time: 14238.7s, Step: 23931, GPU: 4.9GB\n",
      "Epoch 1, Batch 23935, Loss: 3.7120, Time: 14242.1s, Step: 23936, GPU: 4.9GB\n",
      "Epoch 1, Batch 23940, Loss: 3.9638, Time: 14244.6s, Step: 23941, GPU: 4.9GB\n",
      "Epoch 1, Batch 23945, Loss: 3.3245, Time: 14248.0s, Step: 23946, GPU: 4.9GB\n",
      "Epoch 1, Batch 23950, Loss: 3.9765, Time: 14250.5s, Step: 23951, GPU: 4.9GB\n",
      "Epoch 1, Batch 23955, Loss: 4.6909, Time: 14254.0s, Step: 23956, GPU: 4.9GB\n",
      "Epoch 1, Batch 23960, Loss: 4.0232, Time: 14256.5s, Step: 23961, GPU: 4.9GB\n",
      "Epoch 1, Batch 23965, Loss: 4.3832, Time: 14259.9s, Step: 23966, GPU: 4.9GB\n",
      "Epoch 1, Batch 23970, Loss: 4.0789, Time: 14262.5s, Step: 23971, GPU: 4.9GB\n",
      "Epoch 1, Batch 23975, Loss: 4.2623, Time: 14265.9s, Step: 23976, GPU: 4.9GB\n",
      "Epoch 1, Batch 23980, Loss: 4.9744, Time: 14268.4s, Step: 23981, GPU: 4.9GB\n",
      "Epoch 1, Batch 23985, Loss: 4.0191, Time: 14271.9s, Step: 23986, GPU: 4.9GB\n",
      "Epoch 1, Batch 23990, Loss: 3.7812, Time: 14274.5s, Step: 23991, GPU: 4.9GB\n",
      "Epoch 1, Batch 23995, Loss: 3.9369, Time: 14277.9s, Step: 23996, GPU: 4.9GB\n",
      "Epoch 1, Batch 24000, Loss: 3.5036, Time: 14280.5s, Step: 24001, GPU: 4.9GB\n",
      "Epoch 1, Batch 24005, Loss: 3.6326, Time: 14283.9s, Step: 24006, GPU: 4.9GB\n",
      "Epoch 1, Batch 24010, Loss: 3.6640, Time: 14286.4s, Step: 24011, GPU: 4.9GB\n",
      "Epoch 1, Batch 24015, Loss: 3.2013, Time: 14289.8s, Step: 24016, GPU: 4.9GB\n",
      "Epoch 1, Batch 24020, Loss: 3.3706, Time: 14292.3s, Step: 24021, GPU: 4.9GB\n",
      "Epoch 1, Batch 24025, Loss: 2.9911, Time: 14295.8s, Step: 24026, GPU: 4.9GB\n",
      "Epoch 1, Batch 24030, Loss: 3.8895, Time: 14298.3s, Step: 24031, GPU: 4.9GB\n",
      "Epoch 1, Batch 24035, Loss: 2.3900, Time: 14301.7s, Step: 24036, GPU: 4.9GB\n",
      "Epoch 1, Batch 24040, Loss: 3.9624, Time: 14304.2s, Step: 24041, GPU: 4.9GB\n",
      "Epoch 1, Batch 24045, Loss: 3.4386, Time: 14307.7s, Step: 24046, GPU: 4.9GB\n",
      "Epoch 1, Batch 24050, Loss: 4.0409, Time: 14310.2s, Step: 24051, GPU: 4.9GB\n",
      "Epoch 1, Batch 24055, Loss: 3.6827, Time: 14313.6s, Step: 24056, GPU: 4.9GB\n",
      "Epoch 1, Batch 24060, Loss: 3.2942, Time: 14316.2s, Step: 24061, GPU: 4.9GB\n",
      "Epoch 1, Batch 24065, Loss: 3.2677, Time: 14319.6s, Step: 24066, GPU: 4.9GB\n",
      "Epoch 1, Batch 24070, Loss: 3.7376, Time: 14322.1s, Step: 24071, GPU: 4.9GB\n",
      "Epoch 1, Batch 24075, Loss: 3.8281, Time: 14325.5s, Step: 24076, GPU: 4.9GB\n",
      "Epoch 1, Batch 24080, Loss: 3.7927, Time: 14328.0s, Step: 24081, GPU: 4.9GB\n",
      "Epoch 1, Batch 24085, Loss: 3.4818, Time: 14331.4s, Step: 24086, GPU: 4.9GB\n",
      "Epoch 1, Batch 24090, Loss: 3.9834, Time: 14333.9s, Step: 24091, GPU: 4.9GB\n",
      "Epoch 1, Batch 24095, Loss: 3.3781, Time: 14337.3s, Step: 24096, GPU: 4.9GB\n",
      "Epoch 1, Batch 24100, Loss: 3.1806, Time: 14339.8s, Step: 24101, GPU: 4.9GB\n",
      "Epoch 1, Batch 24105, Loss: 3.3097, Time: 14343.3s, Step: 24106, GPU: 4.9GB\n",
      "Epoch 1, Batch 24110, Loss: 3.7332, Time: 14345.8s, Step: 24111, GPU: 4.9GB\n",
      "Epoch 1, Batch 24115, Loss: 3.7424, Time: 14349.2s, Step: 24116, GPU: 4.9GB\n",
      "Epoch 1, Batch 24120, Loss: 3.8526, Time: 14351.7s, Step: 24121, GPU: 4.9GB\n",
      "Epoch 1, Batch 24125, Loss: 4.2747, Time: 14355.1s, Step: 24126, GPU: 4.9GB\n",
      "Epoch 1, Batch 24130, Loss: 3.3951, Time: 14357.6s, Step: 24131, GPU: 4.9GB\n",
      "Epoch 1, Batch 24135, Loss: 3.6591, Time: 14361.0s, Step: 24136, GPU: 4.9GB\n",
      "Epoch 1, Batch 24140, Loss: 4.2056, Time: 14363.5s, Step: 24141, GPU: 4.9GB\n",
      "Epoch 1, Batch 24145, Loss: 3.2757, Time: 14367.0s, Step: 24146, GPU: 4.9GB\n",
      "Epoch 1, Batch 24150, Loss: 4.3572, Time: 14369.5s, Step: 24151, GPU: 4.9GB\n",
      "Epoch 1, Batch 24155, Loss: 4.3559, Time: 14372.9s, Step: 24156, GPU: 4.9GB\n",
      "Epoch 1, Batch 24160, Loss: 3.5987, Time: 14375.4s, Step: 24161, GPU: 4.9GB\n",
      "Epoch 1, Batch 24165, Loss: 2.8933, Time: 14378.8s, Step: 24166, GPU: 4.9GB\n",
      "Epoch 1, Batch 24170, Loss: 3.9400, Time: 14381.3s, Step: 24171, GPU: 4.9GB\n",
      "Epoch 1, Batch 24175, Loss: 3.9666, Time: 14384.8s, Step: 24176, GPU: 4.9GB\n",
      "Epoch 1, Batch 24180, Loss: 3.8232, Time: 14387.3s, Step: 24181, GPU: 4.9GB\n",
      "Epoch 1, Batch 24185, Loss: 3.7673, Time: 14390.7s, Step: 24186, GPU: 4.9GB\n",
      "Epoch 1, Batch 24190, Loss: 4.5119, Time: 14393.2s, Step: 24191, GPU: 4.9GB\n",
      "Epoch 1, Batch 24195, Loss: 3.2994, Time: 14396.7s, Step: 24196, GPU: 4.9GB\n",
      "Epoch 1, Batch 24200, Loss: 3.7003, Time: 14399.3s, Step: 24201, GPU: 4.9GB\n",
      "\n",
      "🔄 Auto-saving checkpoint at epoch 1, batch 24202...\n",
      "💾 Checkpoint saved to: ./my_model_checkpoints/auto_checkpoint_epoch_1_step_24202.pt\n",
      "✅ Checkpoint saved successfully!\n",
      "\n",
      "Epoch 1, Batch 24205, Loss: 4.2616, Time: 14404.4s, Step: 24206, GPU: 4.9GB\n",
      "Epoch 1, Batch 24210, Loss: 3.7271, Time: 14406.9s, Step: 24211, GPU: 4.9GB\n",
      "Epoch 1, Batch 24215, Loss: 4.1897, Time: 14410.3s, Step: 24216, GPU: 4.9GB\n",
      "Epoch 1, Batch 24220, Loss: 4.0752, Time: 14412.8s, Step: 24221, GPU: 4.9GB\n",
      "Epoch 1, Batch 24225, Loss: 3.8970, Time: 14416.2s, Step: 24226, GPU: 4.9GB\n",
      "Epoch 1, Batch 24230, Loss: 4.0583, Time: 14418.7s, Step: 24231, GPU: 4.9GB\n",
      "Epoch 1, Batch 24235, Loss: 3.5981, Time: 14422.1s, Step: 24236, GPU: 4.9GB\n",
      "Epoch 1, Batch 24240, Loss: 4.1600, Time: 14424.6s, Step: 24241, GPU: 4.9GB\n",
      "Epoch 1, Batch 24245, Loss: 4.0369, Time: 14428.1s, Step: 24246, GPU: 4.9GB\n",
      "Epoch 1, Batch 24250, Loss: 3.6849, Time: 14430.6s, Step: 24251, GPU: 4.9GB\n",
      "Epoch 1, Batch 24255, Loss: 3.4561, Time: 14434.0s, Step: 24256, GPU: 4.9GB\n",
      "Epoch 1, Batch 24260, Loss: 3.7480, Time: 14436.5s, Step: 24261, GPU: 4.9GB\n",
      "Epoch 1, Batch 24265, Loss: 3.5068, Time: 14439.9s, Step: 24266, GPU: 4.9GB\n",
      "Epoch 1, Batch 24270, Loss: 3.5114, Time: 14442.5s, Step: 24271, GPU: 4.9GB\n",
      "Epoch 1, Batch 24275, Loss: 4.0990, Time: 14445.9s, Step: 24276, GPU: 4.9GB\n",
      "Epoch 1, Batch 24280, Loss: 4.0508, Time: 14448.4s, Step: 24281, GPU: 4.9GB\n",
      "Epoch 1, Batch 24285, Loss: 3.5975, Time: 14451.8s, Step: 24286, GPU: 4.9GB\n",
      "Epoch 1, Batch 24290, Loss: 5.0175, Time: 14454.3s, Step: 24291, GPU: 4.9GB\n",
      "Epoch 1, Batch 24295, Loss: 3.8330, Time: 14457.7s, Step: 24296, GPU: 4.9GB\n",
      "Epoch 1, Batch 24300, Loss: 3.7856, Time: 14460.2s, Step: 24301, GPU: 4.9GB\n",
      "Epoch 1, Batch 24305, Loss: 3.5065, Time: 14463.6s, Step: 24306, GPU: 4.9GB\n",
      "Epoch 1, Batch 24310, Loss: 3.5773, Time: 14466.1s, Step: 24311, GPU: 4.9GB\n",
      "Epoch 1, Batch 24315, Loss: 4.3963, Time: 14469.5s, Step: 24316, GPU: 4.9GB\n",
      "Epoch 1, Batch 24320, Loss: 4.2179, Time: 14472.0s, Step: 24321, GPU: 4.9GB\n",
      "Epoch 1, Batch 24325, Loss: 4.3102, Time: 14475.4s, Step: 24326, GPU: 4.9GB\n",
      "Epoch 1, Batch 24330, Loss: 3.8952, Time: 14478.0s, Step: 24331, GPU: 4.9GB\n",
      "Epoch 1, Batch 24335, Loss: 4.3214, Time: 14481.4s, Step: 24336, GPU: 4.9GB\n",
      "Epoch 1, Batch 24340, Loss: 3.7755, Time: 14483.9s, Step: 24341, GPU: 4.9GB\n",
      "Epoch 1, Batch 24345, Loss: 3.5115, Time: 14487.3s, Step: 24346, GPU: 4.9GB\n",
      "Epoch 1, Batch 24350, Loss: 2.9247, Time: 14489.8s, Step: 24351, GPU: 4.9GB\n",
      "Epoch 1, Batch 24355, Loss: 3.2183, Time: 14493.2s, Step: 24356, GPU: 4.9GB\n",
      "Epoch 1, Batch 24360, Loss: 4.3144, Time: 14495.7s, Step: 24361, GPU: 4.9GB\n",
      "Epoch 1, Batch 24365, Loss: 3.9183, Time: 14499.1s, Step: 24366, GPU: 4.9GB\n",
      "Epoch 1, Batch 24370, Loss: 3.4339, Time: 14501.7s, Step: 24371, GPU: 4.9GB\n",
      "Epoch 1, Batch 24375, Loss: 3.3295, Time: 14505.1s, Step: 24376, GPU: 4.9GB\n",
      "Epoch 1, Batch 24380, Loss: 3.1141, Time: 14507.6s, Step: 24381, GPU: 4.9GB\n",
      "Epoch 1, Batch 24385, Loss: 4.0552, Time: 14511.0s, Step: 24386, GPU: 4.9GB\n",
      "Epoch 1, Batch 24390, Loss: 4.0538, Time: 14513.5s, Step: 24391, GPU: 4.9GB\n",
      "Epoch 1, Batch 24395, Loss: 4.5512, Time: 14516.9s, Step: 24396, GPU: 4.9GB\n",
      "Epoch 1, Batch 24400, Loss: 3.9605, Time: 14519.5s, Step: 24401, GPU: 4.9GB\n",
      "Epoch 1, Batch 24405, Loss: 3.6021, Time: 14522.9s, Step: 24406, GPU: 4.9GB\n",
      "Epoch 1, Batch 24410, Loss: 3.2452, Time: 14525.4s, Step: 24411, GPU: 4.9GB\n",
      "Epoch 1, Batch 24415, Loss: 3.1157, Time: 14528.8s, Step: 24416, GPU: 4.9GB\n",
      "Epoch 1, Batch 24420, Loss: 4.4730, Time: 14531.3s, Step: 24421, GPU: 4.9GB\n",
      "Epoch 1, Batch 24425, Loss: 4.1619, Time: 14534.8s, Step: 24426, GPU: 4.9GB\n",
      "Epoch 1, Batch 24430, Loss: 3.3246, Time: 14537.3s, Step: 24431, GPU: 4.9GB\n",
      "Epoch 1, Batch 24435, Loss: 4.2998, Time: 14540.7s, Step: 24436, GPU: 4.9GB\n",
      "Epoch 1, Batch 24440, Loss: 3.7105, Time: 14543.2s, Step: 24441, GPU: 4.9GB\n",
      "Epoch 1, Batch 24445, Loss: 4.5936, Time: 14546.7s, Step: 24446, GPU: 4.9GB\n",
      "Epoch 1, Batch 24450, Loss: 4.0647, Time: 14549.2s, Step: 24451, GPU: 4.9GB\n",
      "Epoch 1, Batch 24455, Loss: 4.0465, Time: 14552.6s, Step: 24456, GPU: 4.9GB\n",
      "Epoch 1, Batch 24460, Loss: 3.0543, Time: 14555.1s, Step: 24461, GPU: 4.9GB\n",
      "Epoch 1, Batch 24465, Loss: 4.2115, Time: 14558.5s, Step: 24466, GPU: 4.9GB\n",
      "Epoch 1, Batch 24470, Loss: 3.7759, Time: 14561.1s, Step: 24471, GPU: 4.9GB\n",
      "Epoch 1, Batch 24475, Loss: 3.4876, Time: 14564.5s, Step: 24476, GPU: 4.9GB\n",
      "Epoch 1, Batch 24480, Loss: 3.7292, Time: 14567.0s, Step: 24481, GPU: 4.9GB\n",
      "Epoch 1, Batch 24485, Loss: 3.4923, Time: 14570.4s, Step: 24486, GPU: 4.9GB\n",
      "Epoch 1, Batch 24490, Loss: 4.1447, Time: 14572.9s, Step: 24491, GPU: 4.9GB\n",
      "Epoch 1, Batch 24495, Loss: 2.7154, Time: 14576.3s, Step: 24496, GPU: 4.9GB\n",
      "Epoch 1, Batch 24500, Loss: 3.8998, Time: 14578.8s, Step: 24501, GPU: 4.9GB\n",
      "Epoch 1, Batch 24505, Loss: 2.9615, Time: 14582.2s, Step: 24506, GPU: 4.9GB\n",
      "Epoch 1, Batch 24510, Loss: 3.9869, Time: 14584.7s, Step: 24511, GPU: 4.9GB\n",
      "Epoch 1, Batch 24515, Loss: 3.3491, Time: 14588.1s, Step: 24516, GPU: 4.9GB\n",
      "Epoch 1, Batch 24520, Loss: 4.2616, Time: 14590.6s, Step: 24521, GPU: 4.9GB\n",
      "Epoch 1, Batch 24525, Loss: 3.9950, Time: 14594.1s, Step: 24526, GPU: 4.9GB\n",
      "Epoch 1, Batch 24530, Loss: 2.9639, Time: 14596.6s, Step: 24531, GPU: 4.9GB\n",
      "Epoch 1, Batch 24535, Loss: 3.2289, Time: 14600.0s, Step: 24536, GPU: 4.9GB\n",
      "Epoch 1, Batch 24540, Loss: 4.0678, Time: 14602.5s, Step: 24541, GPU: 4.9GB\n",
      "Epoch 1, Batch 24545, Loss: 3.5542, Time: 14606.0s, Step: 24546, GPU: 4.9GB\n",
      "Epoch 1, Batch 24550, Loss: 3.4353, Time: 14608.5s, Step: 24551, GPU: 4.9GB\n",
      "Epoch 1, Batch 24555, Loss: 3.3488, Time: 14611.9s, Step: 24556, GPU: 4.9GB\n",
      "Epoch 1, Batch 24560, Loss: 3.7558, Time: 14614.4s, Step: 24561, GPU: 4.9GB\n",
      "Epoch 1, Batch 24565, Loss: 3.2668, Time: 14617.9s, Step: 24566, GPU: 4.9GB\n",
      "Epoch 1, Batch 24570, Loss: 3.7036, Time: 14620.4s, Step: 24571, GPU: 4.9GB\n",
      "Epoch 1, Batch 24575, Loss: 3.6118, Time: 14623.8s, Step: 24576, GPU: 4.9GB\n",
      "Epoch 1, Batch 24580, Loss: 3.7589, Time: 14626.3s, Step: 24581, GPU: 4.9GB\n",
      "Epoch 1, Batch 24585, Loss: 3.7938, Time: 14629.7s, Step: 24586, GPU: 4.9GB\n",
      "Epoch 1, Batch 24590, Loss: 3.5407, Time: 14632.3s, Step: 24591, GPU: 4.9GB\n",
      "Epoch 1, Batch 24595, Loss: 3.6644, Time: 14635.7s, Step: 24596, GPU: 4.9GB\n",
      "Epoch 1, Batch 24600, Loss: 2.9704, Time: 14638.3s, Step: 24601, GPU: 4.9GB\n",
      "Epoch 1, Batch 24605, Loss: 3.8520, Time: 14641.7s, Step: 24606, GPU: 4.9GB\n",
      "Epoch 1, Batch 24610, Loss: 3.4180, Time: 14644.2s, Step: 24611, GPU: 4.9GB\n",
      "Epoch 1, Batch 24615, Loss: 3.8121, Time: 14647.6s, Step: 24616, GPU: 4.9GB\n",
      "Epoch 1, Batch 24620, Loss: 4.4063, Time: 14650.1s, Step: 24621, GPU: 4.9GB\n",
      "Epoch 1, Batch 24625, Loss: 3.6836, Time: 14653.5s, Step: 24626, GPU: 4.9GB\n",
      "Epoch 1, Batch 24630, Loss: 3.7555, Time: 14656.1s, Step: 24631, GPU: 4.9GB\n",
      "Epoch 1, Batch 24635, Loss: 4.3706, Time: 14659.5s, Step: 24636, GPU: 4.9GB\n",
      "Epoch 1, Batch 24640, Loss: 4.2044, Time: 14662.0s, Step: 24641, GPU: 4.9GB\n",
      "Epoch 1, Batch 24645, Loss: 3.1504, Time: 14665.4s, Step: 24646, GPU: 4.9GB\n",
      "Epoch 1, Batch 24650, Loss: 3.7004, Time: 14668.0s, Step: 24651, GPU: 4.9GB\n",
      "Epoch 1, Batch 24655, Loss: 3.7619, Time: 14671.4s, Step: 24656, GPU: 4.9GB\n",
      "Epoch 1, Batch 24660, Loss: 4.2918, Time: 14673.9s, Step: 24661, GPU: 4.9GB\n",
      "Epoch 1, Batch 24665, Loss: 3.8032, Time: 14677.3s, Step: 24666, GPU: 4.9GB\n",
      "Epoch 1, Batch 24670, Loss: 3.3671, Time: 14679.8s, Step: 24671, GPU: 4.9GB\n",
      "Epoch 1, Batch 24675, Loss: 4.1011, Time: 14683.2s, Step: 24676, GPU: 4.9GB\n",
      "Epoch 1, Batch 24680, Loss: 3.1161, Time: 14685.8s, Step: 24681, GPU: 4.9GB\n",
      "Epoch 1, Batch 24685, Loss: 4.2201, Time: 14689.2s, Step: 24686, GPU: 4.9GB\n",
      "Epoch 1, Batch 24690, Loss: 3.4275, Time: 14691.7s, Step: 24691, GPU: 4.9GB\n",
      "Epoch 1, Batch 24695, Loss: 4.3055, Time: 14695.1s, Step: 24696, GPU: 4.9GB\n",
      "Epoch 1, Batch 24700, Loss: 3.3466, Time: 14697.6s, Step: 24701, GPU: 4.9GB\n",
      "Epoch 1, Batch 24705, Loss: 3.9010, Time: 14701.0s, Step: 24706, GPU: 4.9GB\n",
      "Epoch 1, Batch 24710, Loss: 4.7961, Time: 14703.5s, Step: 24711, GPU: 4.9GB\n",
      "Epoch 1, Batch 24715, Loss: 4.0230, Time: 14706.9s, Step: 24716, GPU: 4.9GB\n",
      "Epoch 1, Batch 24720, Loss: 4.1887, Time: 14709.5s, Step: 24721, GPU: 4.9GB\n",
      "Epoch 1, Batch 24725, Loss: 4.9914, Time: 14712.9s, Step: 24726, GPU: 4.9GB\n",
      "Epoch 1, Batch 24730, Loss: 4.6828, Time: 14715.4s, Step: 24731, GPU: 4.9GB\n",
      "Epoch 1, Batch 24735, Loss: 3.3050, Time: 14718.8s, Step: 24736, GPU: 4.9GB\n",
      "Epoch 1, Batch 24740, Loss: 3.8536, Time: 14721.4s, Step: 24741, GPU: 4.9GB\n",
      "Epoch 1, Batch 24745, Loss: 3.2809, Time: 14724.8s, Step: 24746, GPU: 4.9GB\n",
      "Epoch 1, Batch 24750, Loss: 3.6134, Time: 14727.3s, Step: 24751, GPU: 4.9GB\n",
      "Epoch 1, Batch 24755, Loss: 3.6040, Time: 14730.7s, Step: 24756, GPU: 4.9GB\n",
      "Epoch 1, Batch 24760, Loss: 4.6146, Time: 14733.3s, Step: 24761, GPU: 4.9GB\n",
      "Epoch 1, Batch 24765, Loss: 4.0029, Time: 14736.7s, Step: 24766, GPU: 4.9GB\n",
      "Epoch 1, Batch 24770, Loss: 4.6608, Time: 14739.2s, Step: 24771, GPU: 4.9GB\n",
      "Epoch 1, Batch 24775, Loss: 3.6676, Time: 14742.6s, Step: 24776, GPU: 4.9GB\n",
      "Epoch 1, Batch 24780, Loss: 2.8679, Time: 14745.2s, Step: 24781, GPU: 4.9GB\n",
      "Epoch 1, Batch 24785, Loss: 3.8860, Time: 14748.6s, Step: 24786, GPU: 4.9GB\n",
      "Epoch 1, Batch 24790, Loss: 5.3850, Time: 14751.2s, Step: 24791, GPU: 4.9GB\n",
      "Epoch 1, Batch 24795, Loss: 3.4770, Time: 14754.6s, Step: 24796, GPU: 4.9GB\n",
      "Epoch 1, Batch 24800, Loss: 3.3965, Time: 14757.2s, Step: 24801, GPU: 4.9GB\n",
      "Epoch 1, Batch 24805, Loss: 3.7361, Time: 14760.7s, Step: 24806, GPU: 4.9GB\n",
      "Epoch 1, Batch 24810, Loss: 3.3917, Time: 14763.2s, Step: 24811, GPU: 4.9GB\n",
      "Epoch 1, Batch 24815, Loss: 3.8217, Time: 14766.6s, Step: 24816, GPU: 4.9GB\n",
      "Epoch 1, Batch 24820, Loss: 3.8087, Time: 14769.2s, Step: 24821, GPU: 4.9GB\n",
      "Epoch 1, Batch 24825, Loss: 3.3170, Time: 14772.6s, Step: 24826, GPU: 4.9GB\n",
      "Epoch 1, Batch 24830, Loss: 3.5858, Time: 14775.1s, Step: 24831, GPU: 4.9GB\n",
      "Epoch 1, Batch 24835, Loss: 3.7269, Time: 14778.6s, Step: 24836, GPU: 4.9GB\n",
      "Epoch 1, Batch 24840, Loss: 4.1054, Time: 14781.1s, Step: 24841, GPU: 4.9GB\n",
      "Epoch 1, Batch 24845, Loss: 3.8764, Time: 14784.5s, Step: 24846, GPU: 4.9GB\n",
      "Epoch 1, Batch 24850, Loss: 3.7359, Time: 14787.1s, Step: 24851, GPU: 4.9GB\n",
      "Epoch 1, Batch 24855, Loss: 3.4877, Time: 14790.5s, Step: 24856, GPU: 4.9GB\n",
      "Epoch 1, Batch 24860, Loss: 3.2674, Time: 14793.0s, Step: 24861, GPU: 4.9GB\n",
      "Epoch 1, Batch 24865, Loss: 3.7571, Time: 14796.4s, Step: 24866, GPU: 4.9GB\n",
      "Epoch 1, Batch 24870, Loss: 3.7106, Time: 14798.9s, Step: 24871, GPU: 4.9GB\n",
      "Epoch 1, Batch 24875, Loss: 4.1423, Time: 14802.3s, Step: 24876, GPU: 4.9GB\n",
      "Epoch 1, Batch 24880, Loss: 3.6017, Time: 14804.9s, Step: 24881, GPU: 4.9GB\n",
      "Epoch 1, Batch 24885, Loss: 3.7363, Time: 14808.3s, Step: 24886, GPU: 4.9GB\n",
      "Epoch 1, Batch 24890, Loss: 3.9926, Time: 14810.8s, Step: 24891, GPU: 4.9GB\n",
      "Epoch 1, Batch 24895, Loss: 3.8730, Time: 14814.3s, Step: 24896, GPU: 4.9GB\n",
      "Epoch 1, Batch 24900, Loss: 3.7273, Time: 14816.8s, Step: 24901, GPU: 4.9GB\n",
      "Epoch 1, Batch 24905, Loss: 3.4280, Time: 14820.2s, Step: 24906, GPU: 4.9GB\n",
      "Epoch 1, Batch 24910, Loss: 4.7052, Time: 14822.7s, Step: 24911, GPU: 4.9GB\n",
      "Epoch 1, Batch 24915, Loss: 3.6755, Time: 14826.1s, Step: 24916, GPU: 4.9GB\n",
      "Epoch 1, Batch 24920, Loss: 3.1057, Time: 14828.6s, Step: 24921, GPU: 4.9GB\n",
      "Epoch 1, Batch 24925, Loss: 3.6515, Time: 14832.1s, Step: 24926, GPU: 4.9GB\n",
      "Epoch 1, Batch 24930, Loss: 3.9020, Time: 14834.6s, Step: 24931, GPU: 4.9GB\n",
      "Epoch 1, Batch 24935, Loss: 3.4044, Time: 14838.0s, Step: 24936, GPU: 4.9GB\n",
      "Epoch 1, Batch 24940, Loss: 3.7887, Time: 14840.5s, Step: 24941, GPU: 4.9GB\n",
      "Epoch 1, Batch 24945, Loss: 4.4999, Time: 14843.9s, Step: 24946, GPU: 4.9GB\n",
      "Epoch 1, Batch 24950, Loss: 3.5929, Time: 14846.4s, Step: 24951, GPU: 4.9GB\n",
      "Epoch 1, Batch 24955, Loss: 3.9635, Time: 14849.8s, Step: 24956, GPU: 4.9GB\n",
      "Epoch 1, Batch 24960, Loss: 4.8270, Time: 14852.4s, Step: 24961, GPU: 4.9GB\n",
      "Epoch 1, Batch 24965, Loss: 3.0053, Time: 14855.8s, Step: 24966, GPU: 4.9GB\n",
      "Epoch 1, Batch 24970, Loss: 3.6410, Time: 14858.3s, Step: 24971, GPU: 4.9GB\n",
      "Epoch 1, Batch 24975, Loss: 4.0738, Time: 14861.7s, Step: 24976, GPU: 4.9GB\n",
      "Epoch 1, Batch 24980, Loss: 4.2476, Time: 14864.2s, Step: 24981, GPU: 4.9GB\n",
      "Epoch 1, Batch 24985, Loss: 4.2242, Time: 14867.6s, Step: 24986, GPU: 4.9GB\n",
      "Epoch 1, Batch 24990, Loss: 3.5720, Time: 14870.1s, Step: 24991, GPU: 4.9GB\n",
      "Epoch 1, Batch 24995, Loss: 3.4018, Time: 14873.5s, Step: 24996, GPU: 4.9GB\n",
      "Epoch 1, Batch 25000, Loss: 3.7964, Time: 14876.1s, Step: 25001, GPU: 4.9GB\n",
      "Epoch 1, Batch 25005, Loss: 4.7974, Time: 14879.5s, Step: 25006, GPU: 4.9GB\n",
      "Epoch 1, Batch 25010, Loss: 4.6226, Time: 14882.0s, Step: 25011, GPU: 4.9GB\n",
      "Epoch 1, Batch 25015, Loss: 3.5892, Time: 14885.4s, Step: 25016, GPU: 4.9GB\n",
      "Epoch 1, Batch 25020, Loss: 3.8441, Time: 14887.9s, Step: 25021, GPU: 4.9GB\n",
      "Epoch 1, Batch 25025, Loss: 3.6430, Time: 14891.3s, Step: 25026, GPU: 4.9GB\n",
      "Epoch 1, Batch 25030, Loss: 3.3698, Time: 14893.9s, Step: 25031, GPU: 4.9GB\n",
      "Epoch 1, Batch 25035, Loss: 4.5080, Time: 14897.3s, Step: 25036, GPU: 4.9GB\n",
      "Epoch 1, Batch 25040, Loss: 3.3502, Time: 14899.8s, Step: 25041, GPU: 4.9GB\n",
      "Epoch 1, Batch 25045, Loss: 4.3168, Time: 14903.2s, Step: 25046, GPU: 4.9GB\n",
      "Epoch 1, Batch 25050, Loss: 3.4147, Time: 14905.7s, Step: 25051, GPU: 4.9GB\n",
      "Epoch 1, Batch 25055, Loss: 3.8820, Time: 14909.1s, Step: 25056, GPU: 4.9GB\n",
      "Epoch 1, Batch 25060, Loss: 4.0657, Time: 14911.7s, Step: 25061, GPU: 4.9GB\n",
      "Epoch 1, Batch 25065, Loss: 2.5514, Time: 14915.1s, Step: 25066, GPU: 4.9GB\n",
      "Epoch 1, Batch 25070, Loss: 3.5967, Time: 14917.6s, Step: 25071, GPU: 4.9GB\n",
      "Epoch 1, Batch 25075, Loss: 3.2393, Time: 14921.0s, Step: 25076, GPU: 4.9GB\n",
      "Epoch 1, Batch 25080, Loss: 3.0856, Time: 14923.5s, Step: 25081, GPU: 4.9GB\n",
      "Epoch 1, Batch 25085, Loss: 4.1082, Time: 14926.9s, Step: 25086, GPU: 4.9GB\n",
      "Epoch 1, Batch 25090, Loss: 3.7794, Time: 14929.4s, Step: 25091, GPU: 4.9GB\n",
      "Epoch 1, Batch 25095, Loss: 4.2261, Time: 14932.9s, Step: 25096, GPU: 4.9GB\n",
      "Epoch 1, Batch 25100, Loss: 3.7440, Time: 14935.4s, Step: 25101, GPU: 4.9GB\n",
      "Epoch 1, Batch 25105, Loss: 3.1917, Time: 14938.8s, Step: 25106, GPU: 4.9GB\n",
      "Epoch 1, Batch 25110, Loss: 3.1482, Time: 14941.3s, Step: 25111, GPU: 4.9GB\n",
      "Epoch 1, Batch 25115, Loss: 4.3499, Time: 14944.8s, Step: 25116, GPU: 4.9GB\n",
      "Epoch 1, Batch 25120, Loss: 3.8627, Time: 14947.3s, Step: 25121, GPU: 4.9GB\n",
      "Epoch 1, Batch 25125, Loss: 3.6522, Time: 14950.7s, Step: 25126, GPU: 4.9GB\n",
      "Epoch 1, Batch 25130, Loss: 4.0839, Time: 14953.3s, Step: 25131, GPU: 4.9GB\n",
      "Epoch 1, Batch 25135, Loss: 3.9171, Time: 14956.7s, Step: 25136, GPU: 4.9GB\n",
      "Epoch 1, Batch 25140, Loss: 3.6159, Time: 14959.2s, Step: 25141, GPU: 4.9GB\n",
      "Epoch 1, Batch 25145, Loss: 5.3405, Time: 14962.7s, Step: 25146, GPU: 4.9GB\n",
      "Epoch 1, Batch 25150, Loss: 4.5480, Time: 14965.2s, Step: 25151, GPU: 4.9GB\n",
      "Epoch 1, Batch 25155, Loss: 4.6454, Time: 14968.6s, Step: 25156, GPU: 4.9GB\n",
      "Epoch 1, Batch 25160, Loss: 4.0530, Time: 14971.1s, Step: 25161, GPU: 4.9GB\n",
      "Epoch 1, Batch 25165, Loss: 4.5429, Time: 14974.5s, Step: 25166, GPU: 4.9GB\n",
      "Epoch 1, Batch 25170, Loss: 2.9531, Time: 14977.0s, Step: 25171, GPU: 4.9GB\n",
      "Epoch 1, Batch 25175, Loss: 3.6610, Time: 14980.4s, Step: 25176, GPU: 4.9GB\n",
      "Epoch 1, Batch 25180, Loss: 3.3520, Time: 14982.9s, Step: 25181, GPU: 4.9GB\n",
      "Epoch 1, Batch 25185, Loss: 3.8238, Time: 14986.3s, Step: 25186, GPU: 4.9GB\n",
      "Epoch 1, Batch 25190, Loss: 3.9182, Time: 14988.8s, Step: 25191, GPU: 4.9GB\n",
      "Epoch 1, Batch 25195, Loss: 3.5597, Time: 14992.3s, Step: 25196, GPU: 4.9GB\n",
      "Epoch 1, Batch 25200, Loss: 4.0767, Time: 14994.9s, Step: 25201, GPU: 4.9GB\n",
      "Epoch 1, Batch 25205, Loss: 4.3329, Time: 14998.3s, Step: 25206, GPU: 4.9GB\n",
      "Epoch 1, Batch 25210, Loss: 3.8567, Time: 15000.8s, Step: 25211, GPU: 4.9GB\n",
      "Epoch 1, Batch 25215, Loss: 3.5401, Time: 15004.2s, Step: 25216, GPU: 4.9GB\n",
      "Epoch 1, Batch 25220, Loss: 3.5385, Time: 15006.7s, Step: 25221, GPU: 4.9GB\n",
      "Epoch 1, Batch 25225, Loss: 3.5421, Time: 15010.2s, Step: 25226, GPU: 4.9GB\n",
      "Epoch 1, Batch 25230, Loss: 4.1768, Time: 15012.7s, Step: 25231, GPU: 4.9GB\n",
      "Epoch 1, Batch 25235, Loss: 3.6920, Time: 15016.2s, Step: 25236, GPU: 4.9GB\n",
      "Epoch 1, Batch 25240, Loss: 3.7316, Time: 15018.7s, Step: 25241, GPU: 4.9GB\n",
      "Epoch 1, Batch 25245, Loss: 4.1636, Time: 15022.1s, Step: 25246, GPU: 4.9GB\n",
      "Epoch 1, Batch 25250, Loss: 3.0003, Time: 15024.6s, Step: 25251, GPU: 4.9GB\n",
      "Epoch 1, Batch 25255, Loss: 3.4485, Time: 15028.1s, Step: 25256, GPU: 4.9GB\n",
      "Epoch 1, Batch 25260, Loss: 3.6352, Time: 15030.6s, Step: 25261, GPU: 4.9GB\n",
      "Epoch 1, Batch 25265, Loss: 3.8958, Time: 15034.0s, Step: 25266, GPU: 4.9GB\n",
      "Epoch 1, Batch 25270, Loss: 3.3757, Time: 15036.5s, Step: 25271, GPU: 4.9GB\n",
      "Epoch 1, Batch 25275, Loss: 3.4382, Time: 15040.0s, Step: 25276, GPU: 4.9GB\n",
      "Epoch 1, Batch 25280, Loss: 4.4966, Time: 15042.5s, Step: 25281, GPU: 4.9GB\n",
      "Epoch 1, Batch 25285, Loss: 2.5933, Time: 15045.9s, Step: 25286, GPU: 4.9GB\n",
      "Epoch 1, Batch 25290, Loss: 3.9256, Time: 15048.4s, Step: 25291, GPU: 4.9GB\n",
      "Epoch 1, Batch 25295, Loss: 3.5898, Time: 15051.8s, Step: 25296, GPU: 4.9GB\n",
      "Epoch 1, Batch 25300, Loss: 3.5839, Time: 15054.3s, Step: 25301, GPU: 4.9GB\n",
      "Epoch 1, Batch 25305, Loss: 3.1441, Time: 15057.7s, Step: 25306, GPU: 4.9GB\n",
      "Epoch 1, Batch 25310, Loss: 3.4937, Time: 15060.2s, Step: 25311, GPU: 4.9GB\n",
      "Epoch 1, Batch 25315, Loss: 3.7965, Time: 15063.6s, Step: 25316, GPU: 4.9GB\n",
      "Epoch 1, Batch 25320, Loss: 3.5819, Time: 15066.2s, Step: 25321, GPU: 4.9GB\n",
      "Epoch 1, Batch 25325, Loss: 3.4743, Time: 15069.6s, Step: 25326, GPU: 4.9GB\n",
      "Epoch 1, Batch 25330, Loss: 4.8686, Time: 15072.1s, Step: 25331, GPU: 4.9GB\n",
      "Epoch 1, Batch 25335, Loss: 3.3356, Time: 15075.5s, Step: 25336, GPU: 4.9GB\n",
      "Epoch 1, Batch 25340, Loss: 4.0048, Time: 15078.0s, Step: 25341, GPU: 4.9GB\n",
      "Epoch 1, Batch 25345, Loss: 3.2373, Time: 15081.5s, Step: 25346, GPU: 4.9GB\n",
      "Epoch 1, Batch 25350, Loss: 3.6064, Time: 15084.0s, Step: 25351, GPU: 4.9GB\n",
      "Epoch 1, Batch 25355, Loss: 3.6687, Time: 15087.4s, Step: 25356, GPU: 4.9GB\n",
      "Epoch 1, Batch 25360, Loss: 4.3729, Time: 15089.9s, Step: 25361, GPU: 4.9GB\n",
      "Epoch 1, Batch 25365, Loss: 3.5114, Time: 15093.4s, Step: 25366, GPU: 4.9GB\n",
      "Epoch 1, Batch 25370, Loss: 3.6500, Time: 15095.9s, Step: 25371, GPU: 4.9GB\n",
      "Epoch 1, Batch 25375, Loss: 4.0633, Time: 15099.3s, Step: 25376, GPU: 4.9GB\n",
      "Epoch 1, Batch 25380, Loss: 3.7578, Time: 15101.9s, Step: 25381, GPU: 4.9GB\n",
      "Epoch 1, Batch 25385, Loss: 3.1487, Time: 15105.3s, Step: 25386, GPU: 4.9GB\n",
      "Epoch 1, Batch 25390, Loss: 3.8128, Time: 15107.8s, Step: 25391, GPU: 4.9GB\n",
      "Epoch 1, Batch 25395, Loss: 4.0030, Time: 15111.3s, Step: 25396, GPU: 4.9GB\n",
      "Epoch 1, Batch 25400, Loss: 3.7066, Time: 15113.9s, Step: 25401, GPU: 4.9GB\n",
      "Epoch 1, Batch 25405, Loss: 3.7811, Time: 15117.3s, Step: 25406, GPU: 4.9GB\n",
      "Epoch 1, Batch 25410, Loss: 3.2998, Time: 15119.8s, Step: 25411, GPU: 4.9GB\n",
      "Epoch 1, Batch 25415, Loss: 4.1385, Time: 15123.3s, Step: 25416, GPU: 4.9GB\n",
      "Epoch 1, Batch 25420, Loss: 4.4458, Time: 15125.8s, Step: 25421, GPU: 4.9GB\n",
      "Epoch 1, Batch 25425, Loss: 3.2347, Time: 15129.2s, Step: 25426, GPU: 4.9GB\n",
      "Epoch 1, Batch 25430, Loss: 3.0917, Time: 15131.7s, Step: 25431, GPU: 4.9GB\n",
      "Epoch 1, Batch 25435, Loss: 3.6875, Time: 15135.1s, Step: 25436, GPU: 4.9GB\n",
      "Epoch 1, Batch 25440, Loss: 3.0718, Time: 15137.7s, Step: 25441, GPU: 4.9GB\n",
      "Epoch 1, Batch 25445, Loss: 3.5827, Time: 15141.1s, Step: 25446, GPU: 4.9GB\n",
      "Epoch 1, Batch 25450, Loss: 4.5381, Time: 15143.6s, Step: 25451, GPU: 4.9GB\n",
      "Epoch 1, Batch 25455, Loss: 3.4829, Time: 15147.0s, Step: 25456, GPU: 4.9GB\n",
      "Epoch 1, Batch 25460, Loss: 3.6648, Time: 15149.5s, Step: 25461, GPU: 4.9GB\n",
      "Epoch 1, Batch 25465, Loss: 3.0074, Time: 15152.9s, Step: 25466, GPU: 4.9GB\n",
      "Epoch 1, Batch 25470, Loss: 3.3876, Time: 15155.5s, Step: 25471, GPU: 4.9GB\n",
      "Epoch 1, Batch 25475, Loss: 3.2092, Time: 15158.8s, Step: 25476, GPU: 4.9GB\n",
      "Epoch 1, Batch 25480, Loss: 3.7756, Time: 15161.4s, Step: 25481, GPU: 4.9GB\n",
      "Epoch 1, Batch 25485, Loss: 4.1678, Time: 15164.8s, Step: 25486, GPU: 4.9GB\n",
      "Epoch 1, Batch 25490, Loss: 3.4675, Time: 15167.3s, Step: 25491, GPU: 4.9GB\n",
      "Epoch 1, Batch 25495, Loss: 3.9605, Time: 15170.7s, Step: 25496, GPU: 4.9GB\n",
      "Epoch 1, Batch 25500, Loss: 3.8247, Time: 15173.2s, Step: 25501, GPU: 4.9GB\n",
      "Epoch 1, Batch 25505, Loss: 3.6016, Time: 15176.7s, Step: 25506, GPU: 4.9GB\n",
      "Epoch 1, Batch 25510, Loss: 3.1520, Time: 15179.2s, Step: 25511, GPU: 4.9GB\n",
      "Epoch 1, Batch 25515, Loss: 3.7899, Time: 15182.6s, Step: 25516, GPU: 4.9GB\n",
      "Epoch 1, Batch 25520, Loss: 3.3803, Time: 15185.2s, Step: 25521, GPU: 4.9GB\n",
      "Epoch 1, Batch 25525, Loss: 3.4233, Time: 15188.6s, Step: 25526, GPU: 4.9GB\n",
      "Epoch 1, Batch 25530, Loss: 4.1651, Time: 15191.1s, Step: 25531, GPU: 4.9GB\n",
      "Epoch 1, Batch 25535, Loss: 4.3373, Time: 15194.5s, Step: 25536, GPU: 4.9GB\n",
      "Epoch 1, Batch 25540, Loss: 3.3168, Time: 15197.0s, Step: 25541, GPU: 4.9GB\n",
      "Epoch 1, Batch 25545, Loss: 3.5784, Time: 15200.5s, Step: 25546, GPU: 4.9GB\n",
      "Epoch 1, Batch 25550, Loss: 3.6303, Time: 15203.0s, Step: 25551, GPU: 4.9GB\n",
      "Epoch 1, Batch 25555, Loss: 4.0194, Time: 15206.4s, Step: 25556, GPU: 4.9GB\n",
      "Epoch 1, Batch 25560, Loss: 5.2667, Time: 15208.9s, Step: 25561, GPU: 4.9GB\n",
      "Epoch 1, Batch 25565, Loss: 3.2292, Time: 15212.3s, Step: 25566, GPU: 4.9GB\n",
      "Epoch 1, Batch 25570, Loss: 4.3727, Time: 15214.8s, Step: 25571, GPU: 4.9GB\n",
      "Epoch 1, Batch 25575, Loss: 3.2362, Time: 15218.3s, Step: 25576, GPU: 4.9GB\n",
      "Epoch 1, Batch 25580, Loss: 3.9173, Time: 15220.8s, Step: 25581, GPU: 4.9GB\n",
      "Epoch 1, Batch 25585, Loss: 3.7221, Time: 15224.2s, Step: 25586, GPU: 4.9GB\n",
      "Epoch 1, Batch 25590, Loss: 4.3427, Time: 15226.7s, Step: 25591, GPU: 4.9GB\n",
      "Epoch 1, Batch 25595, Loss: 4.6118, Time: 15230.2s, Step: 25596, GPU: 4.9GB\n",
      "Epoch 1, Batch 25600, Loss: 2.7598, Time: 15232.8s, Step: 25601, GPU: 4.9GB\n",
      "Epoch 1, Batch 25605, Loss: 3.9832, Time: 15236.2s, Step: 25606, GPU: 4.9GB\n",
      "Epoch 1, Batch 25610, Loss: 2.8229, Time: 15238.7s, Step: 25611, GPU: 4.9GB\n",
      "Epoch 1, Batch 25615, Loss: 4.3112, Time: 15242.1s, Step: 25616, GPU: 4.9GB\n",
      "Epoch 1, Batch 25620, Loss: 3.4453, Time: 15244.7s, Step: 25621, GPU: 4.9GB\n",
      "Epoch 1, Batch 25625, Loss: 3.5978, Time: 15248.1s, Step: 25626, GPU: 4.9GB\n",
      "Epoch 1, Batch 25630, Loss: 4.1469, Time: 15250.6s, Step: 25631, GPU: 4.9GB\n",
      "Epoch 1, Batch 25635, Loss: 3.4824, Time: 15254.0s, Step: 25636, GPU: 4.9GB\n",
      "Epoch 1, Batch 25640, Loss: 4.1344, Time: 15256.6s, Step: 25641, GPU: 4.9GB\n",
      "Epoch 1, Batch 25645, Loss: 3.0148, Time: 15260.0s, Step: 25646, GPU: 4.9GB\n",
      "Epoch 1, Batch 25650, Loss: 3.9522, Time: 15262.5s, Step: 25651, GPU: 4.9GB\n",
      "Epoch 1, Batch 25655, Loss: 3.7671, Time: 15265.9s, Step: 25656, GPU: 4.9GB\n",
      "Epoch 1, Batch 25660, Loss: 3.4311, Time: 15268.4s, Step: 25661, GPU: 4.9GB\n",
      "Epoch 1, Batch 25665, Loss: 3.1927, Time: 15271.9s, Step: 25666, GPU: 4.9GB\n",
      "Epoch 1, Batch 25670, Loss: 3.8842, Time: 15274.4s, Step: 25671, GPU: 4.9GB\n",
      "Epoch 1, Batch 25675, Loss: 3.4284, Time: 15277.8s, Step: 25676, GPU: 4.9GB\n",
      "Epoch 1, Batch 25680, Loss: 3.4143, Time: 15280.3s, Step: 25681, GPU: 4.9GB\n",
      "Epoch 1, Batch 25685, Loss: 3.7253, Time: 15283.8s, Step: 25686, GPU: 4.9GB\n",
      "Epoch 1, Batch 25690, Loss: 4.2304, Time: 15286.3s, Step: 25691, GPU: 4.9GB\n",
      "Epoch 1, Batch 25695, Loss: 3.7690, Time: 15289.8s, Step: 25696, GPU: 4.9GB\n",
      "Epoch 1, Batch 25700, Loss: 4.5256, Time: 15292.3s, Step: 25701, GPU: 4.9GB\n",
      "Epoch 1, Batch 25705, Loss: 4.3697, Time: 15295.7s, Step: 25706, GPU: 4.9GB\n",
      "Epoch 1, Batch 25710, Loss: 4.8236, Time: 15298.3s, Step: 25711, GPU: 4.9GB\n",
      "Epoch 1, Batch 25715, Loss: 3.4418, Time: 15301.7s, Step: 25716, GPU: 4.9GB\n",
      "Epoch 1, Batch 25720, Loss: 3.2807, Time: 15304.3s, Step: 25721, GPU: 4.9GB\n",
      "Epoch 1, Batch 25725, Loss: 3.5310, Time: 15307.7s, Step: 25726, GPU: 4.9GB\n",
      "Epoch 1, Batch 25730, Loss: 3.0416, Time: 15310.2s, Step: 25731, GPU: 4.9GB\n",
      "Epoch 1, Batch 25735, Loss: 3.5326, Time: 15313.6s, Step: 25736, GPU: 4.9GB\n",
      "Epoch 1, Batch 25740, Loss: 3.3194, Time: 15316.1s, Step: 25741, GPU: 4.9GB\n",
      "Epoch 1, Batch 25745, Loss: 4.0978, Time: 15319.5s, Step: 25746, GPU: 4.9GB\n",
      "Epoch 1, Batch 25750, Loss: 5.0014, Time: 15322.0s, Step: 25751, GPU: 4.9GB\n",
      "Epoch 1, Batch 25755, Loss: 3.9300, Time: 15325.5s, Step: 25756, GPU: 4.9GB\n",
      "Epoch 1, Batch 25760, Loss: 3.4167, Time: 15328.0s, Step: 25761, GPU: 4.9GB\n",
      "Epoch 1, Batch 25765, Loss: 3.2942, Time: 15331.4s, Step: 25766, GPU: 4.9GB\n",
      "Epoch 1, Batch 25770, Loss: 3.4369, Time: 15333.9s, Step: 25771, GPU: 4.9GB\n",
      "Epoch 1, Batch 25775, Loss: 4.4437, Time: 15337.4s, Step: 25776, GPU: 4.9GB\n",
      "Epoch 1, Batch 25780, Loss: 3.5684, Time: 15339.9s, Step: 25781, GPU: 4.9GB\n",
      "Epoch 1, Batch 25785, Loss: 3.7434, Time: 15343.3s, Step: 25786, GPU: 4.9GB\n",
      "Epoch 1, Batch 25790, Loss: 3.1797, Time: 15345.8s, Step: 25791, GPU: 4.9GB\n",
      "Epoch 1, Batch 25795, Loss: 3.8923, Time: 15349.2s, Step: 25796, GPU: 4.9GB\n",
      "Epoch 1, Batch 25800, Loss: 3.9414, Time: 15351.8s, Step: 25801, GPU: 4.9GB\n",
      "Epoch 1, Batch 25805, Loss: 3.2735, Time: 15355.2s, Step: 25806, GPU: 4.9GB\n",
      "Epoch 1, Batch 25810, Loss: 4.3073, Time: 15357.8s, Step: 25811, GPU: 4.9GB\n",
      "Epoch 1, Batch 25815, Loss: 4.2801, Time: 15361.2s, Step: 25816, GPU: 4.9GB\n",
      "Epoch 1, Batch 25820, Loss: 3.7696, Time: 15363.7s, Step: 25821, GPU: 4.9GB\n",
      "Epoch 1, Batch 25825, Loss: 3.9532, Time: 15367.1s, Step: 25826, GPU: 4.9GB\n",
      "Epoch 1, Batch 25830, Loss: 3.6477, Time: 15369.6s, Step: 25831, GPU: 4.9GB\n",
      "Epoch 1, Batch 25835, Loss: 3.5944, Time: 15373.0s, Step: 25836, GPU: 4.9GB\n",
      "Epoch 1, Batch 25840, Loss: 3.4416, Time: 15375.6s, Step: 25841, GPU: 4.9GB\n",
      "Epoch 1, Batch 25845, Loss: 3.5915, Time: 15379.0s, Step: 25846, GPU: 4.9GB\n",
      "Epoch 1, Batch 25850, Loss: 4.0215, Time: 15381.6s, Step: 25851, GPU: 4.9GB\n",
      "Epoch 1, Batch 25855, Loss: 3.6679, Time: 15385.0s, Step: 25856, GPU: 4.9GB\n",
      "Epoch 1, Batch 25860, Loss: 3.1510, Time: 15387.5s, Step: 25861, GPU: 4.9GB\n",
      "Epoch 1, Batch 25865, Loss: 5.0897, Time: 15390.9s, Step: 25866, GPU: 4.9GB\n",
      "Epoch 1, Batch 25870, Loss: 3.3330, Time: 15393.4s, Step: 25871, GPU: 4.9GB\n",
      "Epoch 1, Batch 25875, Loss: 3.5893, Time: 15396.8s, Step: 25876, GPU: 4.9GB\n",
      "Epoch 1, Batch 25880, Loss: 3.6050, Time: 15399.4s, Step: 25881, GPU: 4.9GB\n",
      "Epoch 1, Batch 25885, Loss: 3.0116, Time: 15402.7s, Step: 25886, GPU: 4.9GB\n",
      "Epoch 1, Batch 25890, Loss: 3.5808, Time: 15405.2s, Step: 25891, GPU: 4.9GB\n",
      "Epoch 1, Batch 25895, Loss: 3.8208, Time: 15408.6s, Step: 25896, GPU: 4.9GB\n",
      "Epoch 1, Batch 25900, Loss: 3.8754, Time: 15411.2s, Step: 25901, GPU: 4.9GB\n",
      "Epoch 1, Batch 25905, Loss: 3.8096, Time: 15414.6s, Step: 25906, GPU: 4.9GB\n",
      "Epoch 1, Batch 25910, Loss: 3.6806, Time: 15417.1s, Step: 25911, GPU: 4.9GB\n",
      "Epoch 1, Batch 25915, Loss: 3.5710, Time: 15420.5s, Step: 25916, GPU: 4.9GB\n",
      "Epoch 1, Batch 25920, Loss: 4.6792, Time: 15423.0s, Step: 25921, GPU: 4.9GB\n",
      "Epoch 1, Batch 25925, Loss: 4.1557, Time: 15426.4s, Step: 25926, GPU: 4.9GB\n",
      "Epoch 1, Batch 25930, Loss: 3.0167, Time: 15428.9s, Step: 25931, GPU: 4.9GB\n",
      "Epoch 1, Batch 25935, Loss: 3.4937, Time: 15432.3s, Step: 25936, GPU: 4.9GB\n",
      "Epoch 1, Batch 25940, Loss: 3.4691, Time: 15434.8s, Step: 25941, GPU: 4.9GB\n",
      "Epoch 1, Batch 25945, Loss: 3.2728, Time: 15438.2s, Step: 25946, GPU: 4.9GB\n",
      "Epoch 1, Batch 25950, Loss: 3.2838, Time: 15440.7s, Step: 25951, GPU: 4.9GB\n",
      "Epoch 1, Batch 25955, Loss: 3.8652, Time: 15444.2s, Step: 25956, GPU: 4.9GB\n",
      "Epoch 1, Batch 25960, Loss: 3.1432, Time: 15446.7s, Step: 25961, GPU: 4.9GB\n",
      "Epoch 1, Batch 25965, Loss: 4.3339, Time: 15450.1s, Step: 25966, GPU: 4.9GB\n",
      "Epoch 1, Batch 25970, Loss: 2.6414, Time: 15452.6s, Step: 25971, GPU: 4.9GB\n",
      "Epoch 1, Batch 25975, Loss: 4.4413, Time: 15456.0s, Step: 25976, GPU: 4.9GB\n",
      "Epoch 1, Batch 25980, Loss: 3.0936, Time: 15458.5s, Step: 25981, GPU: 4.9GB\n",
      "Epoch 1, Batch 25985, Loss: 3.6902, Time: 15462.0s, Step: 25986, GPU: 4.9GB\n",
      "Epoch 1, Batch 25990, Loss: 4.0329, Time: 15464.5s, Step: 25991, GPU: 4.9GB\n",
      "Epoch 1, Batch 25995, Loss: 3.4941, Time: 15467.9s, Step: 25996, GPU: 4.9GB\n",
      "Epoch 1, Batch 26000, Loss: 4.1940, Time: 15470.6s, Step: 26001, GPU: 4.9GB\n",
      "Epoch 1, Batch 26005, Loss: 4.7308, Time: 15474.0s, Step: 26006, GPU: 4.9GB\n",
      "Epoch 1, Batch 26010, Loss: 4.1256, Time: 15476.5s, Step: 26011, GPU: 4.9GB\n",
      "Epoch 1, Batch 26015, Loss: 3.0024, Time: 15479.9s, Step: 26016, GPU: 4.9GB\n",
      "Epoch 1, Batch 26020, Loss: 3.5607, Time: 15482.4s, Step: 26021, GPU: 4.9GB\n",
      "Epoch 1, Batch 26025, Loss: 4.0183, Time: 15485.8s, Step: 26026, GPU: 4.9GB\n",
      "Epoch 1, Batch 26030, Loss: 4.2836, Time: 15488.3s, Step: 26031, GPU: 4.9GB\n",
      "Epoch 1, Batch 26035, Loss: 4.7244, Time: 15491.7s, Step: 26036, GPU: 4.9GB\n",
      "Epoch 1, Batch 26040, Loss: 4.2892, Time: 15494.2s, Step: 26041, GPU: 4.9GB\n",
      "Epoch 1, Batch 26045, Loss: 3.3229, Time: 15497.6s, Step: 26046, GPU: 4.9GB\n",
      "Epoch 1, Batch 26050, Loss: 3.6361, Time: 15500.1s, Step: 26051, GPU: 4.9GB\n",
      "Epoch 1, Batch 26055, Loss: 3.5719, Time: 15503.6s, Step: 26056, GPU: 4.9GB\n",
      "Epoch 1, Batch 26060, Loss: 2.9671, Time: 15506.1s, Step: 26061, GPU: 4.9GB\n",
      "Epoch 1, Batch 26065, Loss: 3.2622, Time: 15509.5s, Step: 26066, GPU: 4.9GB\n",
      "Epoch 1, Batch 26070, Loss: 3.4313, Time: 15512.0s, Step: 26071, GPU: 4.9GB\n",
      "Epoch 1, Batch 26075, Loss: 4.8534, Time: 15515.4s, Step: 26076, GPU: 4.9GB\n",
      "Epoch 1, Batch 26080, Loss: 3.0224, Time: 15517.9s, Step: 26081, GPU: 4.9GB\n",
      "Epoch 1, Batch 26085, Loss: 5.0520, Time: 15521.3s, Step: 26086, GPU: 4.9GB\n",
      "Epoch 1, Batch 26090, Loss: 3.4642, Time: 15523.8s, Step: 26091, GPU: 4.9GB\n",
      "Epoch 1, Batch 26095, Loss: 3.3442, Time: 15527.2s, Step: 26096, GPU: 4.9GB\n",
      "Epoch 1, Batch 26100, Loss: 3.3462, Time: 15529.7s, Step: 26101, GPU: 4.9GB\n",
      "Epoch 1, Batch 26105, Loss: 4.0894, Time: 15533.1s, Step: 26106, GPU: 4.9GB\n",
      "Epoch 1, Batch 26110, Loss: 3.9143, Time: 15535.7s, Step: 26111, GPU: 4.9GB\n",
      "Epoch 1, Batch 26115, Loss: 3.3456, Time: 15539.1s, Step: 26116, GPU: 4.9GB\n",
      "Epoch 1, Batch 26120, Loss: 4.0949, Time: 15541.6s, Step: 26121, GPU: 4.9GB\n",
      "Epoch 1, Batch 26125, Loss: 3.9495, Time: 15545.0s, Step: 26126, GPU: 4.9GB\n",
      "Epoch 1, Batch 26130, Loss: 3.1985, Time: 15547.5s, Step: 26131, GPU: 4.9GB\n",
      "Epoch 1, Batch 26135, Loss: 3.7782, Time: 15550.9s, Step: 26136, GPU: 4.9GB\n",
      "Epoch 1, Batch 26140, Loss: 4.1427, Time: 15553.5s, Step: 26141, GPU: 4.9GB\n",
      "Epoch 1, Batch 26145, Loss: 3.6732, Time: 15556.9s, Step: 26146, GPU: 4.9GB\n",
      "Epoch 1, Batch 26150, Loss: 3.1582, Time: 15559.4s, Step: 26151, GPU: 4.9GB\n",
      "Epoch 1, Batch 26155, Loss: 3.4826, Time: 15562.8s, Step: 26156, GPU: 4.9GB\n",
      "Epoch 1, Batch 26160, Loss: 4.3128, Time: 15565.3s, Step: 26161, GPU: 4.9GB\n",
      "Epoch 1, Batch 26165, Loss: 3.1046, Time: 15568.7s, Step: 26166, GPU: 4.9GB\n",
      "Epoch 1, Batch 26170, Loss: 3.7907, Time: 15571.2s, Step: 26171, GPU: 4.9GB\n",
      "Epoch 1, Batch 26175, Loss: 3.1890, Time: 15574.6s, Step: 26176, GPU: 4.9GB\n",
      "Epoch 1, Batch 26180, Loss: 4.2990, Time: 15577.2s, Step: 26181, GPU: 4.9GB\n",
      "Epoch 1, Batch 26185, Loss: 3.2424, Time: 15580.5s, Step: 26186, GPU: 4.9GB\n",
      "Epoch 1, Batch 26190, Loss: 3.7632, Time: 15583.1s, Step: 26191, GPU: 4.9GB\n",
      "Epoch 1, Batch 26195, Loss: 2.8998, Time: 15586.5s, Step: 26196, GPU: 4.9GB\n",
      "Epoch 1, Batch 26200, Loss: 3.4558, Time: 15589.0s, Step: 26201, GPU: 4.9GB\n",
      "Epoch 1, Batch 26205, Loss: 4.8402, Time: 15592.5s, Step: 26206, GPU: 4.9GB\n",
      "Epoch 1, Batch 26210, Loss: 3.5255, Time: 15595.0s, Step: 26211, GPU: 4.9GB\n",
      "Epoch 1, Batch 26215, Loss: 4.1619, Time: 15598.4s, Step: 26216, GPU: 4.9GB\n",
      "Epoch 1, Batch 26220, Loss: 2.8482, Time: 15601.0s, Step: 26221, GPU: 4.9GB\n",
      "Epoch 1, Batch 26225, Loss: 3.6671, Time: 15604.4s, Step: 26226, GPU: 4.9GB\n",
      "Epoch 1, Batch 26230, Loss: 4.0122, Time: 15606.9s, Step: 26231, GPU: 4.9GB\n",
      "Epoch 1, Batch 26235, Loss: 4.0468, Time: 15610.3s, Step: 26236, GPU: 4.9GB\n",
      "Epoch 1, Batch 26240, Loss: 3.8880, Time: 15612.8s, Step: 26241, GPU: 4.9GB\n",
      "Epoch 1, Batch 26245, Loss: 4.0514, Time: 15616.2s, Step: 26246, GPU: 4.9GB\n",
      "Epoch 1, Batch 26250, Loss: 3.8637, Time: 15618.7s, Step: 26251, GPU: 4.9GB\n",
      "Epoch 1, Batch 26255, Loss: 4.1575, Time: 15622.1s, Step: 26256, GPU: 4.9GB\n",
      "Epoch 1, Batch 26260, Loss: 4.2317, Time: 15624.6s, Step: 26261, GPU: 4.9GB\n",
      "Epoch 1, Batch 26265, Loss: 3.7548, Time: 15628.0s, Step: 26266, GPU: 4.9GB\n",
      "Epoch 1, Batch 26270, Loss: 4.3746, Time: 15630.5s, Step: 26271, GPU: 4.9GB\n",
      "Epoch 1, Batch 26275, Loss: 3.5937, Time: 15633.9s, Step: 26276, GPU: 4.9GB\n",
      "Epoch 1, Batch 26280, Loss: 3.5540, Time: 15636.4s, Step: 26281, GPU: 4.9GB\n",
      "Epoch 1, Batch 26285, Loss: 3.8095, Time: 15639.8s, Step: 26286, GPU: 4.9GB\n",
      "Epoch 1, Batch 26290, Loss: 4.7670, Time: 15642.3s, Step: 26291, GPU: 4.9GB\n",
      "Epoch 1, Batch 26295, Loss: 4.0351, Time: 15645.8s, Step: 26296, GPU: 4.9GB\n",
      "Epoch 1, Batch 26300, Loss: 4.3344, Time: 15648.3s, Step: 26301, GPU: 4.9GB\n",
      "Epoch 1, Batch 26305, Loss: 3.4628, Time: 15651.7s, Step: 26306, GPU: 4.9GB\n",
      "Epoch 1, Batch 26310, Loss: 3.1446, Time: 15654.2s, Step: 26311, GPU: 4.9GB\n",
      "Epoch 1, Batch 26315, Loss: 3.1817, Time: 15657.6s, Step: 26316, GPU: 4.9GB\n",
      "Epoch 1, Batch 26320, Loss: 4.5565, Time: 15660.1s, Step: 26321, GPU: 4.9GB\n",
      "Epoch 1, Batch 26325, Loss: 4.1258, Time: 15663.5s, Step: 26326, GPU: 4.9GB\n",
      "Epoch 1, Batch 26330, Loss: 4.9568, Time: 15666.0s, Step: 26331, GPU: 4.9GB\n",
      "Epoch 1, Batch 26335, Loss: 3.8382, Time: 15669.4s, Step: 26336, GPU: 4.9GB\n",
      "Epoch 1, Batch 26340, Loss: 3.8487, Time: 15671.9s, Step: 26341, GPU: 4.9GB\n",
      "Epoch 1, Batch 26345, Loss: 4.0163, Time: 15675.3s, Step: 26346, GPU: 4.9GB\n",
      "Epoch 1, Batch 26350, Loss: 3.2737, Time: 15677.9s, Step: 26351, GPU: 4.9GB\n",
      "Epoch 1, Batch 26355, Loss: 3.2707, Time: 15681.3s, Step: 26356, GPU: 4.9GB\n",
      "Epoch 1, Batch 26360, Loss: 3.0944, Time: 15683.8s, Step: 26361, GPU: 4.9GB\n",
      "Epoch 1, Batch 26365, Loss: 2.7838, Time: 15687.2s, Step: 26366, GPU: 4.9GB\n",
      "Epoch 1, Batch 26370, Loss: 3.8775, Time: 15689.8s, Step: 26371, GPU: 4.9GB\n",
      "Epoch 1, Batch 26375, Loss: 3.8678, Time: 15693.1s, Step: 26376, GPU: 4.9GB\n",
      "Epoch 1, Batch 26380, Loss: 3.5665, Time: 15695.7s, Step: 26381, GPU: 4.9GB\n",
      "Epoch 1, Batch 26385, Loss: 4.6017, Time: 15699.1s, Step: 26386, GPU: 4.9GB\n",
      "Epoch 1, Batch 26390, Loss: 3.8276, Time: 15701.6s, Step: 26391, GPU: 4.9GB\n",
      "Epoch 1, Batch 26395, Loss: 3.7068, Time: 15705.0s, Step: 26396, GPU: 4.9GB\n",
      "Epoch 1, Batch 26400, Loss: 3.8608, Time: 15707.6s, Step: 26401, GPU: 4.9GB\n",
      "Epoch 1, Batch 26405, Loss: 3.6623, Time: 15711.0s, Step: 26406, GPU: 4.9GB\n",
      "Epoch 1, Batch 26410, Loss: 2.8954, Time: 15713.6s, Step: 26411, GPU: 4.9GB\n",
      "Epoch 1, Batch 26415, Loss: 2.3406, Time: 15717.0s, Step: 26416, GPU: 4.9GB\n",
      "Epoch 1, Batch 26420, Loss: 3.7512, Time: 15719.6s, Step: 26421, GPU: 4.9GB\n",
      "Epoch 1, Batch 26425, Loss: 3.1343, Time: 15723.0s, Step: 26426, GPU: 4.9GB\n",
      "Epoch 1, Batch 26430, Loss: 3.9230, Time: 15725.5s, Step: 26431, GPU: 4.9GB\n",
      "Epoch 1, Batch 26435, Loss: 2.9473, Time: 15728.9s, Step: 26436, GPU: 4.9GB\n",
      "Epoch 1, Batch 26440, Loss: 3.3323, Time: 15731.5s, Step: 26441, GPU: 4.9GB\n",
      "Epoch 1, Batch 26445, Loss: 3.5384, Time: 15734.9s, Step: 26446, GPU: 4.9GB\n",
      "Epoch 1, Batch 26450, Loss: 4.3742, Time: 15737.4s, Step: 26451, GPU: 4.9GB\n",
      "Epoch 1, Batch 26455, Loss: 4.0993, Time: 15740.8s, Step: 26456, GPU: 4.9GB\n",
      "Epoch 1, Batch 26460, Loss: 3.9137, Time: 15743.3s, Step: 26461, GPU: 4.9GB\n",
      "Epoch 1, Batch 26465, Loss: 3.9155, Time: 15746.8s, Step: 26466, GPU: 4.9GB\n",
      "Epoch 1, Batch 26470, Loss: 2.7173, Time: 15749.3s, Step: 26471, GPU: 4.9GB\n",
      "Epoch 1, Batch 26475, Loss: 3.4145, Time: 15752.7s, Step: 26476, GPU: 4.9GB\n",
      "Epoch 1, Batch 26480, Loss: 3.6243, Time: 15755.2s, Step: 26481, GPU: 4.9GB\n",
      "Epoch 1, Batch 26485, Loss: 3.6796, Time: 15758.6s, Step: 26486, GPU: 4.9GB\n",
      "Epoch 1, Batch 26490, Loss: 4.3102, Time: 15761.1s, Step: 26491, GPU: 4.9GB\n",
      "Epoch 1, Batch 26495, Loss: 3.7491, Time: 15764.5s, Step: 26496, GPU: 4.9GB\n",
      "Epoch 1, Batch 26500, Loss: 4.5207, Time: 15767.0s, Step: 26501, GPU: 4.9GB\n",
      "Epoch 1, Batch 26505, Loss: 3.8363, Time: 15770.4s, Step: 26506, GPU: 4.9GB\n",
      "Epoch 1, Batch 26510, Loss: 3.8397, Time: 15772.9s, Step: 26511, GPU: 4.9GB\n",
      "Epoch 1, Batch 26515, Loss: 3.1637, Time: 15776.3s, Step: 26516, GPU: 4.9GB\n",
      "Epoch 1, Batch 26520, Loss: 4.2734, Time: 15778.9s, Step: 26521, GPU: 4.9GB\n",
      "Epoch 1, Batch 26525, Loss: 4.3099, Time: 15782.3s, Step: 26526, GPU: 4.9GB\n",
      "Epoch 1, Batch 26530, Loss: 2.9791, Time: 15784.8s, Step: 26531, GPU: 4.9GB\n",
      "Epoch 1, Batch 26535, Loss: 3.8972, Time: 15788.2s, Step: 26536, GPU: 4.9GB\n",
      "Epoch 1, Batch 26540, Loss: 3.9929, Time: 15790.8s, Step: 26541, GPU: 4.9GB\n",
      "Epoch 1, Batch 26545, Loss: 2.5950, Time: 15794.3s, Step: 26546, GPU: 4.9GB\n",
      "Epoch 1, Batch 26550, Loss: 4.1620, Time: 15796.8s, Step: 26551, GPU: 4.9GB\n",
      "Epoch 1, Batch 26555, Loss: 3.3116, Time: 15800.2s, Step: 26556, GPU: 4.9GB\n",
      "Epoch 1, Batch 26560, Loss: 3.3389, Time: 15802.7s, Step: 26561, GPU: 4.9GB\n",
      "Epoch 1, Batch 26565, Loss: 3.4336, Time: 15806.2s, Step: 26566, GPU: 4.9GB\n",
      "Epoch 1, Batch 26570, Loss: 3.8423, Time: 15808.7s, Step: 26571, GPU: 4.9GB\n",
      "Epoch 1, Batch 26575, Loss: 3.2946, Time: 15812.1s, Step: 26576, GPU: 4.9GB\n",
      "Epoch 1, Batch 26580, Loss: 2.8891, Time: 15814.6s, Step: 26581, GPU: 4.9GB\n",
      "Epoch 1, Batch 26585, Loss: 4.0818, Time: 15818.1s, Step: 26586, GPU: 4.9GB\n",
      "Epoch 1, Batch 26590, Loss: 4.0509, Time: 15820.6s, Step: 26591, GPU: 4.9GB\n",
      "Epoch 1, Batch 26595, Loss: 3.8590, Time: 15824.0s, Step: 26596, GPU: 4.9GB\n",
      "Epoch 1, Batch 26600, Loss: 2.9152, Time: 15826.6s, Step: 26601, GPU: 4.9GB\n",
      "Epoch 1, Batch 26605, Loss: 3.3396, Time: 15830.0s, Step: 26606, GPU: 4.9GB\n",
      "Epoch 1, Batch 26610, Loss: 3.9777, Time: 15832.6s, Step: 26611, GPU: 4.9GB\n",
      "Epoch 1, Batch 26615, Loss: 4.2971, Time: 15836.0s, Step: 26616, GPU: 4.9GB\n",
      "Epoch 1, Batch 26620, Loss: 3.7988, Time: 15838.5s, Step: 26621, GPU: 4.9GB\n",
      "Epoch 1, Batch 26625, Loss: 3.8954, Time: 15841.9s, Step: 26626, GPU: 4.9GB\n",
      "Epoch 1, Batch 26630, Loss: 3.5184, Time: 15844.4s, Step: 26631, GPU: 4.9GB\n",
      "Epoch 1, Batch 26635, Loss: 3.3566, Time: 15847.8s, Step: 26636, GPU: 4.9GB\n",
      "Epoch 1, Batch 26640, Loss: 4.1796, Time: 15850.3s, Step: 26641, GPU: 4.9GB\n",
      "Epoch 1, Batch 26645, Loss: 3.2964, Time: 15853.8s, Step: 26646, GPU: 4.9GB\n",
      "Epoch 1, Batch 26650, Loss: 3.5147, Time: 15856.3s, Step: 26651, GPU: 4.9GB\n",
      "Epoch 1, Batch 26655, Loss: 4.0398, Time: 15859.7s, Step: 26656, GPU: 4.9GB\n",
      "Epoch 1, Batch 26660, Loss: 4.6078, Time: 15862.2s, Step: 26661, GPU: 4.9GB\n",
      "Epoch 1, Batch 26665, Loss: 4.2742, Time: 15865.6s, Step: 26666, GPU: 4.9GB\n",
      "Epoch 1, Batch 26670, Loss: 3.8460, Time: 15868.1s, Step: 26671, GPU: 4.9GB\n",
      "Epoch 1, Batch 26675, Loss: 3.4309, Time: 15871.5s, Step: 26676, GPU: 4.9GB\n",
      "Epoch 1, Batch 26680, Loss: 3.6768, Time: 15874.0s, Step: 26681, GPU: 4.9GB\n",
      "Epoch 1, Batch 26685, Loss: 4.9148, Time: 15877.5s, Step: 26686, GPU: 4.9GB\n",
      "Epoch 1, Batch 26690, Loss: 4.1976, Time: 15880.0s, Step: 26691, GPU: 4.9GB\n",
      "Epoch 1, Batch 26695, Loss: 4.2277, Time: 15883.4s, Step: 26696, GPU: 4.9GB\n",
      "Epoch 1, Batch 26700, Loss: 4.5007, Time: 15885.9s, Step: 26701, GPU: 4.9GB\n",
      "Epoch 1, Batch 26705, Loss: 3.7493, Time: 15889.3s, Step: 26706, GPU: 4.9GB\n",
      "Epoch 1, Batch 26710, Loss: 3.1509, Time: 15891.8s, Step: 26711, GPU: 4.9GB\n",
      "Epoch 1, Batch 26715, Loss: 4.1004, Time: 15895.2s, Step: 26716, GPU: 4.9GB\n",
      "Epoch 1, Batch 26720, Loss: 2.8381, Time: 15897.7s, Step: 26721, GPU: 4.9GB\n",
      "Epoch 1, Batch 26725, Loss: 3.4330, Time: 15901.1s, Step: 26726, GPU: 4.9GB\n",
      "Epoch 1, Batch 26730, Loss: 3.2430, Time: 15903.6s, Step: 26731, GPU: 4.9GB\n",
      "Epoch 1, Batch 26735, Loss: 3.8085, Time: 15907.1s, Step: 26736, GPU: 4.9GB\n",
      "Epoch 1, Batch 26740, Loss: 3.1892, Time: 15909.6s, Step: 26741, GPU: 4.9GB\n",
      "Epoch 1, Batch 26745, Loss: 4.2437, Time: 15913.0s, Step: 26746, GPU: 4.9GB\n",
      "Epoch 1, Batch 26750, Loss: 3.5196, Time: 15915.5s, Step: 26751, GPU: 4.9GB\n",
      "Epoch 1, Batch 26755, Loss: 3.0931, Time: 15918.9s, Step: 26756, GPU: 4.9GB\n",
      "Epoch 1, Batch 26760, Loss: 3.5004, Time: 15921.5s, Step: 26761, GPU: 4.9GB\n",
      "Epoch 1, Batch 26765, Loss: 3.5661, Time: 15924.9s, Step: 26766, GPU: 4.9GB\n",
      "Epoch 1, Batch 26770, Loss: 4.1206, Time: 15927.4s, Step: 26771, GPU: 4.9GB\n",
      "Epoch 1, Batch 26775, Loss: 3.7386, Time: 15930.8s, Step: 26776, GPU: 4.9GB\n",
      "Epoch 1, Batch 26780, Loss: 3.2268, Time: 15933.3s, Step: 26781, GPU: 4.9GB\n",
      "Epoch 1, Batch 26785, Loss: 3.6563, Time: 15936.8s, Step: 26786, GPU: 4.9GB\n",
      "Epoch 1, Batch 26790, Loss: 4.0286, Time: 15939.3s, Step: 26791, GPU: 4.9GB\n",
      "Epoch 1, Batch 26795, Loss: 3.0958, Time: 15942.7s, Step: 26796, GPU: 4.9GB\n",
      "Epoch 1, Batch 26800, Loss: 4.5250, Time: 15945.3s, Step: 26801, GPU: 4.9GB\n",
      "Epoch 1, Batch 26805, Loss: 3.7150, Time: 15948.7s, Step: 26806, GPU: 4.9GB\n",
      "Epoch 1, Batch 26810, Loss: 3.5939, Time: 15951.2s, Step: 26811, GPU: 4.9GB\n",
      "Epoch 1, Batch 26815, Loss: 4.3238, Time: 15954.7s, Step: 26816, GPU: 4.9GB\n",
      "Epoch 1, Batch 26820, Loss: 3.7296, Time: 15957.2s, Step: 26821, GPU: 4.9GB\n",
      "Epoch 1, Batch 26825, Loss: 3.6012, Time: 15960.6s, Step: 26826, GPU: 4.9GB\n",
      "Epoch 1, Batch 26830, Loss: 3.8251, Time: 15963.5s, Step: 26831, GPU: 4.9GB\n",
      "Epoch 1, Batch 26835, Loss: 3.5206, Time: 15967.0s, Step: 26836, GPU: 4.9GB\n",
      "Epoch 1, Batch 26840, Loss: 2.8702, Time: 15969.5s, Step: 26841, GPU: 4.9GB\n",
      "Epoch 1, Batch 26845, Loss: 3.4504, Time: 15972.9s, Step: 26846, GPU: 4.9GB\n",
      "Epoch 1, Batch 26850, Loss: 3.5731, Time: 15975.4s, Step: 26851, GPU: 4.9GB\n",
      "Epoch 1, Batch 26855, Loss: 4.4170, Time: 15978.8s, Step: 26856, GPU: 4.9GB\n",
      "Epoch 1, Batch 26860, Loss: 4.3752, Time: 15981.3s, Step: 26861, GPU: 4.9GB\n",
      "Epoch 1, Batch 26865, Loss: 3.4621, Time: 15984.7s, Step: 26866, GPU: 4.9GB\n",
      "Epoch 1, Batch 26870, Loss: 3.7578, Time: 15987.2s, Step: 26871, GPU: 4.9GB\n",
      "Epoch 1, Batch 26875, Loss: 2.7512, Time: 15990.6s, Step: 26876, GPU: 4.9GB\n",
      "Epoch 1, Batch 26880, Loss: 3.2972, Time: 15993.1s, Step: 26881, GPU: 4.9GB\n",
      "Epoch 1, Batch 26885, Loss: 4.1302, Time: 15996.6s, Step: 26886, GPU: 4.9GB\n",
      "Epoch 1, Batch 26890, Loss: 3.5637, Time: 15999.1s, Step: 26891, GPU: 4.9GB\n",
      "Epoch 1, Batch 26895, Loss: 3.8856, Time: 16002.5s, Step: 26896, GPU: 4.9GB\n",
      "Epoch 1, Batch 26900, Loss: 3.9817, Time: 16005.0s, Step: 26901, GPU: 4.9GB\n",
      "Epoch 1, Batch 26905, Loss: 4.4351, Time: 16008.4s, Step: 26906, GPU: 4.9GB\n",
      "Epoch 1, Batch 26910, Loss: 4.1624, Time: 16010.9s, Step: 26911, GPU: 4.9GB\n",
      "Epoch 1, Batch 26915, Loss: 4.2516, Time: 16014.3s, Step: 26916, GPU: 4.9GB\n",
      "Epoch 1, Batch 26920, Loss: 3.5245, Time: 16016.8s, Step: 26921, GPU: 4.9GB\n",
      "Epoch 1, Batch 26925, Loss: 4.2813, Time: 16020.2s, Step: 26926, GPU: 4.9GB\n",
      "Epoch 1, Batch 26930, Loss: 3.7820, Time: 16022.7s, Step: 26931, GPU: 4.9GB\n",
      "Epoch 1, Batch 26935, Loss: 3.5045, Time: 16026.2s, Step: 26936, GPU: 4.9GB\n",
      "Epoch 1, Batch 26940, Loss: 3.7384, Time: 16028.7s, Step: 26941, GPU: 4.9GB\n",
      "Epoch 1, Batch 26945, Loss: 4.1176, Time: 16032.2s, Step: 26946, GPU: 4.9GB\n",
      "Epoch 1, Batch 26950, Loss: 2.8940, Time: 16034.8s, Step: 26951, GPU: 4.9GB\n",
      "Epoch 1, Batch 26955, Loss: 3.5395, Time: 16038.2s, Step: 26956, GPU: 4.9GB\n",
      "Epoch 1, Batch 26960, Loss: 3.7423, Time: 16040.7s, Step: 26961, GPU: 4.9GB\n",
      "Epoch 1, Batch 26965, Loss: 3.6811, Time: 16044.1s, Step: 26966, GPU: 4.9GB\n",
      "Epoch 1, Batch 26970, Loss: 3.8964, Time: 16046.7s, Step: 26971, GPU: 4.9GB\n",
      "Epoch 1, Batch 26975, Loss: 3.7730, Time: 16050.1s, Step: 26976, GPU: 4.9GB\n",
      "Epoch 1, Batch 26980, Loss: 3.5851, Time: 16052.7s, Step: 26981, GPU: 4.9GB\n",
      "Epoch 1, Batch 26985, Loss: 2.9388, Time: 16056.1s, Step: 26986, GPU: 4.9GB\n",
      "Epoch 1, Batch 26990, Loss: 3.0535, Time: 16058.6s, Step: 26991, GPU: 4.9GB\n",
      "Epoch 1, Batch 26995, Loss: 2.8298, Time: 16062.0s, Step: 26996, GPU: 4.9GB\n",
      "Epoch 1, Batch 27000, Loss: 5.0165, Time: 16064.6s, Step: 27001, GPU: 4.9GB\n",
      "Epoch 1, Batch 27005, Loss: 3.1843, Time: 16068.0s, Step: 27006, GPU: 4.9GB\n",
      "Epoch 1, Batch 27010, Loss: 3.8633, Time: 16070.6s, Step: 27011, GPU: 4.9GB\n",
      "Epoch 1, Batch 27015, Loss: 3.5895, Time: 16074.0s, Step: 27016, GPU: 4.9GB\n",
      "Epoch 1, Batch 27020, Loss: 3.7982, Time: 16076.5s, Step: 27021, GPU: 4.9GB\n",
      "Epoch 1, Batch 27025, Loss: 2.7765, Time: 16079.9s, Step: 27026, GPU: 4.9GB\n",
      "Epoch 1, Batch 27030, Loss: 3.6635, Time: 16082.4s, Step: 27031, GPU: 4.9GB\n",
      "Epoch 1, Batch 27035, Loss: 3.9907, Time: 16085.8s, Step: 27036, GPU: 4.9GB\n",
      "Epoch 1, Batch 27040, Loss: 4.4614, Time: 16088.3s, Step: 27041, GPU: 4.9GB\n",
      "Epoch 1, Batch 27045, Loss: 3.3427, Time: 16091.7s, Step: 27046, GPU: 4.9GB\n",
      "Epoch 1, Batch 27050, Loss: 3.7822, Time: 16094.3s, Step: 27051, GPU: 4.9GB\n",
      "Epoch 1, Batch 27055, Loss: 3.7735, Time: 16097.7s, Step: 27056, GPU: 4.9GB\n",
      "Epoch 1, Batch 27060, Loss: 3.9520, Time: 16100.2s, Step: 27061, GPU: 4.9GB\n",
      "Epoch 1, Batch 27065, Loss: 3.7703, Time: 16103.6s, Step: 27066, GPU: 4.9GB\n",
      "Epoch 1, Batch 27070, Loss: 2.8793, Time: 16106.1s, Step: 27071, GPU: 4.9GB\n",
      "Epoch 1, Batch 27075, Loss: 3.6086, Time: 16109.5s, Step: 27076, GPU: 4.9GB\n",
      "Epoch 1, Batch 27080, Loss: 4.3452, Time: 16112.0s, Step: 27081, GPU: 4.9GB\n",
      "Epoch 1, Batch 27085, Loss: 3.8728, Time: 16115.4s, Step: 27086, GPU: 4.9GB\n",
      "Epoch 1, Batch 27090, Loss: 3.7756, Time: 16117.9s, Step: 27091, GPU: 4.9GB\n",
      "Epoch 1, Batch 27095, Loss: 3.4601, Time: 16121.3s, Step: 27096, GPU: 4.9GB\n",
      "Epoch 1, Batch 27100, Loss: 3.8165, Time: 16123.8s, Step: 27101, GPU: 4.9GB\n",
      "Epoch 1, Batch 27105, Loss: 3.4588, Time: 16127.3s, Step: 27106, GPU: 4.9GB\n",
      "Epoch 1, Batch 27110, Loss: 4.1812, Time: 16129.8s, Step: 27111, GPU: 4.9GB\n",
      "Epoch 1, Batch 27115, Loss: 3.4662, Time: 16133.2s, Step: 27116, GPU: 4.9GB\n",
      "Epoch 1, Batch 27120, Loss: 3.7789, Time: 16135.7s, Step: 27121, GPU: 4.9GB\n",
      "Epoch 1, Batch 27125, Loss: 3.4723, Time: 16139.1s, Step: 27126, GPU: 4.9GB\n",
      "Epoch 1, Batch 27130, Loss: 4.2772, Time: 16141.6s, Step: 27131, GPU: 4.9GB\n",
      "Epoch 1, Batch 27135, Loss: 3.4727, Time: 16145.0s, Step: 27136, GPU: 4.9GB\n",
      "Epoch 1, Batch 27140, Loss: 3.1213, Time: 16147.6s, Step: 27141, GPU: 4.9GB\n",
      "Epoch 1, Batch 27145, Loss: 3.9487, Time: 16151.0s, Step: 27146, GPU: 4.9GB\n",
      "Epoch 1, Batch 27150, Loss: 4.1781, Time: 16153.5s, Step: 27151, GPU: 4.9GB\n",
      "Epoch 1, Batch 27155, Loss: 3.2958, Time: 16156.9s, Step: 27156, GPU: 4.9GB\n",
      "Epoch 1, Batch 27160, Loss: 3.7326, Time: 16159.5s, Step: 27161, GPU: 4.9GB\n",
      "Epoch 1, Batch 27165, Loss: 3.1802, Time: 16162.9s, Step: 27166, GPU: 4.9GB\n",
      "Epoch 1, Batch 27170, Loss: 4.0991, Time: 16165.4s, Step: 27171, GPU: 4.9GB\n",
      "Epoch 1, Batch 27175, Loss: 4.2626, Time: 16168.8s, Step: 27176, GPU: 4.9GB\n",
      "Epoch 1, Batch 27180, Loss: 3.8272, Time: 16171.3s, Step: 27181, GPU: 4.9GB\n",
      "Epoch 1, Batch 27185, Loss: 3.5369, Time: 16174.7s, Step: 27186, GPU: 4.9GB\n",
      "Epoch 1, Batch 27190, Loss: 3.6786, Time: 16177.3s, Step: 27191, GPU: 4.9GB\n",
      "Epoch 1, Batch 27195, Loss: 3.2645, Time: 16180.7s, Step: 27196, GPU: 4.9GB\n",
      "Epoch 1, Batch 27200, Loss: 3.8093, Time: 16183.2s, Step: 27201, GPU: 4.9GB\n",
      "Epoch 1, Batch 27205, Loss: 2.6810, Time: 16186.6s, Step: 27206, GPU: 4.9GB\n",
      "Epoch 1, Batch 27210, Loss: 3.4607, Time: 16189.1s, Step: 27211, GPU: 4.9GB\n",
      "Epoch 1, Batch 27215, Loss: 3.4910, Time: 16192.5s, Step: 27216, GPU: 4.9GB\n",
      "Epoch 1, Batch 27220, Loss: 3.3975, Time: 16195.0s, Step: 27221, GPU: 4.9GB\n",
      "Epoch 1, Batch 27225, Loss: 4.3616, Time: 16198.4s, Step: 27226, GPU: 4.9GB\n",
      "Epoch 1, Batch 27230, Loss: 3.5695, Time: 16200.9s, Step: 27231, GPU: 4.9GB\n",
      "Epoch 1, Batch 27235, Loss: 3.6004, Time: 16204.3s, Step: 27236, GPU: 4.9GB\n",
      "Epoch 1, Batch 27240, Loss: 3.0511, Time: 16206.9s, Step: 27241, GPU: 4.9GB\n",
      "Epoch 1, Batch 27245, Loss: 3.0896, Time: 16210.3s, Step: 27246, GPU: 4.9GB\n",
      "Epoch 1, Batch 27250, Loss: 4.3465, Time: 16212.8s, Step: 27251, GPU: 4.9GB\n",
      "Epoch 1, Batch 27255, Loss: 3.9770, Time: 16216.2s, Step: 27256, GPU: 4.9GB\n",
      "Epoch 1, Batch 27260, Loss: 4.0471, Time: 16218.7s, Step: 27261, GPU: 4.9GB\n",
      "Epoch 1, Batch 27265, Loss: 3.6704, Time: 16222.1s, Step: 27266, GPU: 4.9GB\n",
      "Epoch 1, Batch 27270, Loss: 2.5723, Time: 16224.6s, Step: 27271, GPU: 4.9GB\n",
      "Epoch 1, Batch 27275, Loss: 3.8864, Time: 16228.0s, Step: 27276, GPU: 4.9GB\n",
      "Epoch 1, Batch 27280, Loss: 3.7701, Time: 16230.5s, Step: 27281, GPU: 4.9GB\n",
      "Epoch 1, Batch 27285, Loss: 4.7636, Time: 16233.9s, Step: 27286, GPU: 4.9GB\n",
      "Epoch 1, Batch 27290, Loss: 3.7306, Time: 16236.4s, Step: 27291, GPU: 4.9GB\n",
      "Epoch 1, Batch 27295, Loss: 3.6223, Time: 16239.9s, Step: 27296, GPU: 4.9GB\n",
      "Epoch 1, Batch 27300, Loss: 3.4376, Time: 16242.4s, Step: 27301, GPU: 4.9GB\n",
      "Epoch 1, Batch 27305, Loss: 3.3605, Time: 16245.8s, Step: 27306, GPU: 4.9GB\n",
      "Epoch 1, Batch 27310, Loss: 2.8333, Time: 16248.3s, Step: 27311, GPU: 4.9GB\n",
      "Epoch 1, Batch 27315, Loss: 3.7441, Time: 16251.7s, Step: 27316, GPU: 4.9GB\n",
      "Epoch 1, Batch 27320, Loss: 4.4952, Time: 16254.2s, Step: 27321, GPU: 4.9GB\n",
      "Epoch 1, Batch 27325, Loss: 3.4069, Time: 16257.6s, Step: 27326, GPU: 4.9GB\n",
      "Epoch 1, Batch 27330, Loss: 3.8068, Time: 16260.1s, Step: 27331, GPU: 4.9GB\n",
      "Epoch 1, Batch 27335, Loss: 3.0431, Time: 16263.5s, Step: 27336, GPU: 4.9GB\n",
      "Epoch 1, Batch 27340, Loss: 3.7562, Time: 16266.0s, Step: 27341, GPU: 4.9GB\n",
      "Epoch 1, Batch 27345, Loss: 3.5074, Time: 16269.4s, Step: 27346, GPU: 4.9GB\n",
      "Epoch 1, Batch 27350, Loss: 3.1949, Time: 16271.9s, Step: 27351, GPU: 4.9GB\n",
      "Epoch 1, Batch 27355, Loss: 3.1955, Time: 16275.3s, Step: 27356, GPU: 4.9GB\n",
      "Epoch 1, Batch 27360, Loss: 3.0612, Time: 16277.8s, Step: 27361, GPU: 4.9GB\n",
      "Epoch 1, Batch 27365, Loss: 3.9225, Time: 16281.3s, Step: 27366, GPU: 4.9GB\n",
      "Epoch 1, Batch 27370, Loss: 3.3523, Time: 16283.8s, Step: 27371, GPU: 4.9GB\n",
      "Epoch 1, Batch 27375, Loss: 3.6485, Time: 16287.2s, Step: 27376, GPU: 4.9GB\n",
      "Epoch 1, Batch 27380, Loss: 3.1970, Time: 16289.7s, Step: 27381, GPU: 4.9GB\n",
      "Epoch 1, Batch 27385, Loss: 3.5646, Time: 16293.1s, Step: 27386, GPU: 4.9GB\n",
      "Epoch 1, Batch 27390, Loss: 3.5898, Time: 16295.6s, Step: 27391, GPU: 4.9GB\n",
      "Epoch 1, Batch 27395, Loss: 2.5147, Time: 16299.0s, Step: 27396, GPU: 4.9GB\n",
      "Epoch 1, Batch 27400, Loss: 3.3982, Time: 16301.6s, Step: 27401, GPU: 4.9GB\n",
      "Epoch 1, Batch 27405, Loss: 5.1096, Time: 16305.0s, Step: 27406, GPU: 4.9GB\n",
      "Epoch 1, Batch 27410, Loss: 2.3579, Time: 16307.5s, Step: 27411, GPU: 4.9GB\n",
      "Epoch 1, Batch 27415, Loss: 3.0472, Time: 16310.9s, Step: 27416, GPU: 4.9GB\n",
      "Epoch 1, Batch 27420, Loss: 3.4678, Time: 16313.5s, Step: 27421, GPU: 4.9GB\n",
      "Epoch 1, Batch 27425, Loss: 4.3026, Time: 16317.0s, Step: 27426, GPU: 4.9GB\n",
      "Epoch 1, Batch 27430, Loss: 3.9345, Time: 16319.5s, Step: 27431, GPU: 4.9GB\n",
      "Epoch 1, Batch 27435, Loss: 2.9394, Time: 16322.8s, Step: 27436, GPU: 4.9GB\n",
      "Epoch 1, Batch 27440, Loss: 3.8234, Time: 16325.4s, Step: 27441, GPU: 4.9GB\n",
      "Epoch 1, Batch 27445, Loss: 4.4489, Time: 16328.8s, Step: 27446, GPU: 4.9GB\n",
      "Epoch 1, Batch 27450, Loss: 3.7526, Time: 16331.3s, Step: 27451, GPU: 4.9GB\n",
      "Epoch 1, Batch 27455, Loss: 3.9206, Time: 16334.7s, Step: 27456, GPU: 4.9GB\n",
      "Epoch 1, Batch 27460, Loss: 3.2421, Time: 16337.2s, Step: 27461, GPU: 4.9GB\n",
      "Epoch 1, Batch 27465, Loss: 3.9330, Time: 16340.7s, Step: 27466, GPU: 4.9GB\n",
      "Epoch 1, Batch 27470, Loss: 3.6931, Time: 16343.2s, Step: 27471, GPU: 4.9GB\n",
      "Epoch 1, Batch 27475, Loss: 3.4870, Time: 16346.6s, Step: 27476, GPU: 4.9GB\n",
      "Epoch 1, Batch 27480, Loss: 3.7142, Time: 16349.1s, Step: 27481, GPU: 4.9GB\n",
      "Epoch 1, Batch 27485, Loss: 2.6070, Time: 16352.5s, Step: 27486, GPU: 4.9GB\n",
      "Epoch 1, Batch 27490, Loss: 2.9284, Time: 16355.0s, Step: 27491, GPU: 4.9GB\n",
      "Epoch 1, Batch 27495, Loss: 3.3925, Time: 16358.5s, Step: 27496, GPU: 4.9GB\n",
      "Epoch 1, Batch 27500, Loss: 2.7788, Time: 16361.0s, Step: 27501, GPU: 4.9GB\n",
      "Epoch 1, Batch 27505, Loss: 3.0927, Time: 16364.4s, Step: 27506, GPU: 4.9GB\n",
      "Epoch 1, Batch 27510, Loss: 3.8102, Time: 16366.9s, Step: 27511, GPU: 4.9GB\n",
      "Epoch 1, Batch 27515, Loss: 3.4239, Time: 16370.4s, Step: 27516, GPU: 4.9GB\n",
      "Epoch 1, Batch 27520, Loss: 3.2399, Time: 16372.9s, Step: 27521, GPU: 4.9GB\n",
      "Epoch 1, Batch 27525, Loss: 3.4248, Time: 16376.3s, Step: 27526, GPU: 4.9GB\n",
      "Epoch 1, Batch 27530, Loss: 3.3215, Time: 16378.9s, Step: 27531, GPU: 4.9GB\n",
      "Epoch 1, Batch 27535, Loss: 3.1791, Time: 16382.3s, Step: 27536, GPU: 4.9GB\n",
      "Epoch 1, Batch 27540, Loss: 4.6344, Time: 16384.8s, Step: 27541, GPU: 4.9GB\n",
      "Epoch 1, Batch 27545, Loss: 3.9321, Time: 16388.3s, Step: 27546, GPU: 4.9GB\n",
      "Epoch 1, Batch 27550, Loss: 4.0522, Time: 16390.8s, Step: 27551, GPU: 4.9GB\n",
      "Epoch 1, Batch 27555, Loss: 2.8602, Time: 16394.3s, Step: 27556, GPU: 4.9GB\n",
      "Epoch 1, Batch 27560, Loss: 3.8657, Time: 16396.8s, Step: 27561, GPU: 4.9GB\n",
      "Epoch 1, Batch 27565, Loss: 4.7791, Time: 16400.2s, Step: 27566, GPU: 4.9GB\n",
      "Epoch 1, Batch 27570, Loss: 3.8422, Time: 16403.2s, Step: 27571, GPU: 4.9GB\n",
      "Epoch 1, Batch 27575, Loss: 2.5611, Time: 16406.6s, Step: 27576, GPU: 4.9GB\n",
      "Epoch 1, Batch 27580, Loss: 3.5660, Time: 16409.1s, Step: 27581, GPU: 4.9GB\n",
      "Epoch 1, Batch 27585, Loss: 4.2953, Time: 16412.6s, Step: 27586, GPU: 4.9GB\n",
      "Epoch 1, Batch 27590, Loss: 4.0475, Time: 16415.1s, Step: 27591, GPU: 4.9GB\n",
      "Epoch 1, Batch 27595, Loss: 3.3622, Time: 16418.5s, Step: 27596, GPU: 4.9GB\n",
      "Epoch 1, Batch 27600, Loss: 3.4983, Time: 16421.1s, Step: 27601, GPU: 4.9GB\n",
      "Epoch 1, Batch 27605, Loss: 3.5260, Time: 16424.5s, Step: 27606, GPU: 4.9GB\n",
      "Epoch 1, Batch 27610, Loss: 2.6429, Time: 16427.1s, Step: 27611, GPU: 4.9GB\n",
      "Epoch 1, Batch 27615, Loss: 3.5166, Time: 16430.5s, Step: 27616, GPU: 4.9GB\n",
      "Epoch 1, Batch 27620, Loss: 3.5150, Time: 16433.0s, Step: 27621, GPU: 4.9GB\n",
      "Epoch 1, Batch 27625, Loss: 3.5227, Time: 16436.8s, Step: 27626, GPU: 4.9GB\n",
      "Epoch 1, Batch 27630, Loss: 2.8253, Time: 16439.3s, Step: 27631, GPU: 4.9GB\n",
      "Epoch 1, Batch 27635, Loss: 3.8524, Time: 16442.8s, Step: 27636, GPU: 4.9GB\n",
      "Epoch 1, Batch 27640, Loss: 4.0630, Time: 16445.4s, Step: 27641, GPU: 4.9GB\n",
      "Epoch 1, Batch 27645, Loss: 4.0191, Time: 16448.9s, Step: 27646, GPU: 4.9GB\n",
      "Epoch 1, Batch 27650, Loss: 3.9489, Time: 16451.4s, Step: 27651, GPU: 4.9GB\n",
      "Epoch 1, Batch 27655, Loss: 3.7713, Time: 16454.8s, Step: 27656, GPU: 4.9GB\n",
      "Epoch 1, Batch 27660, Loss: 2.4275, Time: 16457.4s, Step: 27661, GPU: 4.9GB\n",
      "Epoch 1, Batch 27665, Loss: 4.2901, Time: 16460.8s, Step: 27666, GPU: 4.9GB\n",
      "Epoch 1, Batch 27670, Loss: 3.4321, Time: 16463.3s, Step: 27671, GPU: 4.9GB\n",
      "Epoch 1, Batch 27675, Loss: 3.8152, Time: 16466.8s, Step: 27676, GPU: 4.9GB\n",
      "Epoch 1, Batch 27680, Loss: 3.5718, Time: 16469.3s, Step: 27681, GPU: 4.9GB\n",
      "Epoch 1, Batch 27685, Loss: 3.9005, Time: 16472.7s, Step: 27686, GPU: 4.9GB\n",
      "Epoch 1, Batch 27690, Loss: 4.4485, Time: 16475.2s, Step: 27691, GPU: 4.9GB\n",
      "Epoch 1, Batch 27695, Loss: 3.2540, Time: 16478.6s, Step: 27696, GPU: 4.9GB\n",
      "Epoch 1, Batch 27700, Loss: 3.6195, Time: 16481.1s, Step: 27701, GPU: 4.9GB\n",
      "Epoch 1, Batch 27705, Loss: 3.9034, Time: 16484.5s, Step: 27706, GPU: 4.9GB\n",
      "Epoch 1, Batch 27710, Loss: 3.9585, Time: 16487.0s, Step: 27711, GPU: 4.9GB\n",
      "Epoch 1, Batch 27715, Loss: 3.9044, Time: 16490.4s, Step: 27716, GPU: 4.9GB\n",
      "Epoch 1, Batch 27720, Loss: 3.8119, Time: 16493.0s, Step: 27721, GPU: 4.9GB\n",
      "Epoch 1, Batch 27725, Loss: 3.1563, Time: 16496.4s, Step: 27726, GPU: 4.9GB\n",
      "Epoch 1, Batch 27730, Loss: 3.6595, Time: 16498.9s, Step: 27731, GPU: 4.9GB\n",
      "Epoch 1, Batch 27735, Loss: 3.4482, Time: 16502.3s, Step: 27736, GPU: 4.9GB\n",
      "Epoch 1, Batch 27740, Loss: 3.6314, Time: 16504.8s, Step: 27741, GPU: 4.9GB\n",
      "Epoch 1, Batch 27745, Loss: 3.7638, Time: 16508.2s, Step: 27746, GPU: 4.9GB\n",
      "Epoch 1, Batch 27750, Loss: 3.1780, Time: 16510.8s, Step: 27751, GPU: 4.9GB\n",
      "Epoch 1, Batch 27755, Loss: 3.9708, Time: 16514.2s, Step: 27756, GPU: 4.9GB\n",
      "Epoch 1, Batch 27760, Loss: 3.7070, Time: 16516.7s, Step: 27761, GPU: 4.9GB\n",
      "Epoch 1, Batch 27765, Loss: 3.9911, Time: 16520.1s, Step: 27766, GPU: 4.9GB\n",
      "Epoch 1, Batch 27770, Loss: 4.3840, Time: 16522.6s, Step: 27771, GPU: 4.9GB\n",
      "Epoch 1, Batch 27775, Loss: 3.7729, Time: 16526.0s, Step: 27776, GPU: 4.9GB\n",
      "Epoch 1, Batch 27780, Loss: 3.0448, Time: 16528.6s, Step: 27781, GPU: 4.9GB\n",
      "Epoch 1, Batch 27785, Loss: 3.4624, Time: 16532.0s, Step: 27786, GPU: 4.9GB\n",
      "Epoch 1, Batch 27790, Loss: 3.5548, Time: 16534.5s, Step: 27791, GPU: 4.9GB\n",
      "Epoch 1, Batch 27795, Loss: 3.2895, Time: 16537.9s, Step: 27796, GPU: 4.9GB\n",
      "Epoch 1, Batch 27800, Loss: 3.6587, Time: 16540.5s, Step: 27801, GPU: 4.9GB\n",
      "Epoch 1, Batch 27805, Loss: 4.7608, Time: 16543.9s, Step: 27806, GPU: 4.9GB\n",
      "Epoch 1, Batch 27810, Loss: 3.2738, Time: 16546.4s, Step: 27811, GPU: 4.9GB\n",
      "Epoch 1, Batch 27815, Loss: 2.9572, Time: 16549.8s, Step: 27816, GPU: 4.9GB\n",
      "Epoch 1, Batch 27820, Loss: 3.7704, Time: 16552.3s, Step: 27821, GPU: 4.9GB\n",
      "Epoch 1, Batch 27825, Loss: 3.7783, Time: 16555.7s, Step: 27826, GPU: 4.9GB\n",
      "Epoch 1, Batch 27830, Loss: 3.5109, Time: 16558.3s, Step: 27831, GPU: 4.9GB\n",
      "Epoch 1, Batch 27835, Loss: 3.3174, Time: 16561.7s, Step: 27836, GPU: 4.9GB\n",
      "Epoch 1, Batch 27840, Loss: 3.1359, Time: 16564.2s, Step: 27841, GPU: 4.9GB\n",
      "Epoch 1, Batch 27845, Loss: 3.9603, Time: 16567.6s, Step: 27846, GPU: 4.9GB\n",
      "Epoch 1, Batch 27850, Loss: 3.9366, Time: 16570.1s, Step: 27851, GPU: 4.9GB\n",
      "Epoch 1, Batch 27855, Loss: 3.8374, Time: 16573.5s, Step: 27856, GPU: 4.9GB\n",
      "Epoch 1, Batch 27860, Loss: 3.3908, Time: 16576.0s, Step: 27861, GPU: 4.9GB\n",
      "Epoch 1, Batch 27865, Loss: 4.0852, Time: 16579.4s, Step: 27866, GPU: 4.9GB\n",
      "Epoch 1, Batch 27870, Loss: 3.7156, Time: 16582.0s, Step: 27871, GPU: 4.9GB\n",
      "Epoch 1, Batch 27875, Loss: 3.9453, Time: 16585.4s, Step: 27876, GPU: 4.9GB\n",
      "Epoch 1, Batch 27880, Loss: 3.1994, Time: 16587.9s, Step: 27881, GPU: 4.9GB\n",
      "Epoch 1, Batch 27885, Loss: 3.3225, Time: 16591.3s, Step: 27886, GPU: 4.9GB\n",
      "Epoch 1, Batch 27890, Loss: 3.6561, Time: 16593.9s, Step: 27891, GPU: 4.9GB\n",
      "Epoch 1, Batch 27895, Loss: 3.0437, Time: 16597.3s, Step: 27896, GPU: 4.9GB\n",
      "Epoch 1, Batch 27900, Loss: 3.9572, Time: 16599.8s, Step: 27901, GPU: 4.9GB\n",
      "Epoch 1, Batch 27905, Loss: 3.6571, Time: 16603.2s, Step: 27906, GPU: 4.9GB\n",
      "Epoch 1, Batch 27910, Loss: 3.4600, Time: 16605.8s, Step: 27911, GPU: 4.9GB\n",
      "Epoch 1, Batch 27915, Loss: 4.0559, Time: 16609.2s, Step: 27916, GPU: 4.9GB\n",
      "Epoch 1, Batch 27920, Loss: 3.0211, Time: 16611.7s, Step: 27921, GPU: 4.9GB\n",
      "Epoch 1, Batch 27925, Loss: 3.6137, Time: 16615.1s, Step: 27926, GPU: 4.9GB\n",
      "Epoch 1, Batch 27930, Loss: 3.4909, Time: 16617.6s, Step: 27931, GPU: 4.9GB\n",
      "Epoch 1, Batch 27935, Loss: 4.1644, Time: 16621.0s, Step: 27936, GPU: 4.9GB\n",
      "Epoch 1, Batch 27940, Loss: 3.6559, Time: 16623.5s, Step: 27941, GPU: 4.9GB\n",
      "Epoch 1, Batch 27945, Loss: 3.6526, Time: 16626.9s, Step: 27946, GPU: 4.9GB\n",
      "Epoch 1, Batch 27950, Loss: 3.5763, Time: 16629.4s, Step: 27951, GPU: 4.9GB\n",
      "Epoch 1, Batch 27955, Loss: 3.1402, Time: 16632.8s, Step: 27956, GPU: 4.9GB\n",
      "Epoch 1, Batch 27960, Loss: 3.5249, Time: 16635.3s, Step: 27961, GPU: 4.9GB\n",
      "Epoch 1, Batch 27965, Loss: 4.1228, Time: 16638.7s, Step: 27966, GPU: 4.9GB\n",
      "Epoch 1, Batch 27970, Loss: 3.5712, Time: 16641.2s, Step: 27971, GPU: 4.9GB\n",
      "Epoch 1, Batch 27975, Loss: 3.7652, Time: 16644.6s, Step: 27976, GPU: 4.9GB\n",
      "Epoch 1, Batch 27980, Loss: 3.9922, Time: 16647.2s, Step: 27981, GPU: 4.9GB\n",
      "Epoch 1, Batch 27985, Loss: 3.2236, Time: 16650.6s, Step: 27986, GPU: 4.9GB\n",
      "Epoch 1, Batch 27990, Loss: 3.7765, Time: 16653.1s, Step: 27991, GPU: 4.9GB\n",
      "Epoch 1, Batch 27995, Loss: 3.5230, Time: 16656.5s, Step: 27996, GPU: 4.9GB\n",
      "Epoch 1, Batch 28000, Loss: 3.8281, Time: 16659.1s, Step: 28001, GPU: 4.9GB\n",
      "Epoch 1, Batch 28005, Loss: 3.4882, Time: 16662.5s, Step: 28006, GPU: 4.9GB\n",
      "Epoch 1, Batch 28010, Loss: 3.2004, Time: 16665.0s, Step: 28011, GPU: 4.9GB\n",
      "Epoch 1, Batch 28015, Loss: 4.3264, Time: 16668.4s, Step: 28016, GPU: 4.9GB\n",
      "Epoch 1, Batch 28020, Loss: 4.0875, Time: 16670.9s, Step: 28021, GPU: 4.9GB\n",
      "Epoch 1, Batch 28025, Loss: 3.3750, Time: 16674.3s, Step: 28026, GPU: 4.9GB\n",
      "Epoch 1, Batch 28030, Loss: 4.4818, Time: 16676.8s, Step: 28031, GPU: 4.9GB\n",
      "Epoch 1, Batch 28035, Loss: 3.7186, Time: 16680.2s, Step: 28036, GPU: 4.9GB\n",
      "Epoch 1, Batch 28040, Loss: 4.5280, Time: 16682.8s, Step: 28041, GPU: 4.9GB\n",
      "Epoch 1, Batch 28045, Loss: 3.3253, Time: 16686.1s, Step: 28046, GPU: 4.9GB\n",
      "Epoch 1, Batch 28050, Loss: 2.9804, Time: 16688.7s, Step: 28051, GPU: 4.9GB\n",
      "Epoch 1, Batch 28055, Loss: 4.6755, Time: 16692.1s, Step: 28056, GPU: 4.9GB\n",
      "Epoch 1, Batch 28060, Loss: 4.6005, Time: 16694.6s, Step: 28061, GPU: 4.9GB\n",
      "Epoch 1, Batch 28065, Loss: 3.5646, Time: 16698.0s, Step: 28066, GPU: 4.9GB\n",
      "Epoch 1, Batch 28070, Loss: 3.8617, Time: 16700.5s, Step: 28071, GPU: 4.9GB\n",
      "Epoch 1, Batch 28075, Loss: 3.5346, Time: 16704.0s, Step: 28076, GPU: 4.9GB\n",
      "Epoch 1, Batch 28080, Loss: 3.3385, Time: 16706.6s, Step: 28081, GPU: 4.9GB\n",
      "Epoch 1, Batch 28085, Loss: 3.5248, Time: 16710.0s, Step: 28086, GPU: 4.9GB\n",
      "Epoch 1, Batch 28090, Loss: 3.3764, Time: 16712.5s, Step: 28091, GPU: 4.9GB\n",
      "Epoch 1, Batch 28095, Loss: 4.2439, Time: 16716.0s, Step: 28096, GPU: 4.9GB\n",
      "Epoch 1, Batch 28100, Loss: 1.5459, Time: 16718.5s, Step: 28101, GPU: 4.9GB\n",
      "Epoch 1, Batch 28105, Loss: 3.8829, Time: 16721.9s, Step: 28106, GPU: 4.9GB\n",
      "Epoch 1, Batch 28110, Loss: 3.3506, Time: 16724.5s, Step: 28111, GPU: 4.9GB\n",
      "Epoch 1, Batch 28115, Loss: 4.1332, Time: 16727.9s, Step: 28116, GPU: 4.9GB\n",
      "Epoch 1, Batch 28120, Loss: 3.1612, Time: 16730.4s, Step: 28121, GPU: 4.9GB\n",
      "Epoch 1, Batch 28125, Loss: 3.7822, Time: 16733.8s, Step: 28126, GPU: 4.9GB\n",
      "Epoch 1, Batch 28130, Loss: 3.1157, Time: 16736.3s, Step: 28131, GPU: 4.9GB\n",
      "Epoch 1, Batch 28135, Loss: 2.4660, Time: 16739.8s, Step: 28136, GPU: 4.9GB\n",
      "Epoch 1, Batch 28140, Loss: 3.4907, Time: 16742.3s, Step: 28141, GPU: 4.9GB\n",
      "Epoch 1, Batch 28145, Loss: 2.9400, Time: 16745.7s, Step: 28146, GPU: 4.9GB\n",
      "Epoch 1, Batch 28150, Loss: 3.5589, Time: 16748.2s, Step: 28151, GPU: 4.9GB\n",
      "Epoch 1, Batch 28155, Loss: 3.5439, Time: 16751.7s, Step: 28156, GPU: 4.9GB\n",
      "Epoch 1, Batch 28160, Loss: 2.9805, Time: 16754.2s, Step: 28161, GPU: 4.9GB\n",
      "Epoch 1, Batch 28165, Loss: 3.8031, Time: 16757.6s, Step: 28166, GPU: 4.9GB\n",
      "Epoch 1, Batch 28170, Loss: 3.8387, Time: 16760.1s, Step: 28171, GPU: 4.9GB\n",
      "Epoch 1, Batch 28175, Loss: 3.2619, Time: 16763.5s, Step: 28176, GPU: 4.9GB\n",
      "Epoch 1, Batch 28180, Loss: 3.5565, Time: 16766.0s, Step: 28181, GPU: 4.9GB\n",
      "Epoch 1, Batch 28185, Loss: 3.4203, Time: 16769.4s, Step: 28186, GPU: 4.9GB\n",
      "Epoch 1, Batch 28190, Loss: 3.6078, Time: 16771.9s, Step: 28191, GPU: 4.9GB\n",
      "Epoch 1, Batch 28195, Loss: 3.7421, Time: 16775.3s, Step: 28196, GPU: 4.9GB\n",
      "Epoch 1, Batch 28200, Loss: 3.8010, Time: 16777.9s, Step: 28201, GPU: 4.9GB\n",
      "Epoch 1, Batch 28205, Loss: 2.1758, Time: 16781.3s, Step: 28206, GPU: 4.9GB\n",
      "Epoch 1, Batch 28210, Loss: 2.9027, Time: 16783.9s, Step: 28211, GPU: 4.9GB\n",
      "Epoch 1, Batch 28215, Loss: 3.3570, Time: 16787.3s, Step: 28216, GPU: 4.9GB\n",
      "Epoch 1, Batch 28220, Loss: 3.3935, Time: 16789.8s, Step: 28221, GPU: 4.9GB\n",
      "Epoch 1, Batch 28225, Loss: 3.3219, Time: 16793.2s, Step: 28226, GPU: 4.9GB\n",
      "Epoch 1, Batch 28230, Loss: 3.7183, Time: 16795.7s, Step: 28231, GPU: 4.9GB\n",
      "Epoch 1, Batch 28235, Loss: 4.0520, Time: 16799.1s, Step: 28236, GPU: 4.9GB\n",
      "Epoch 1, Batch 28240, Loss: 3.1046, Time: 16801.7s, Step: 28241, GPU: 4.9GB\n",
      "Epoch 1, Batch 28245, Loss: 3.9061, Time: 16805.1s, Step: 28246, GPU: 4.9GB\n",
      "Epoch 1, Batch 28250, Loss: 4.2278, Time: 16807.6s, Step: 28251, GPU: 4.9GB\n",
      "Epoch 1, Batch 28255, Loss: 2.9483, Time: 16811.1s, Step: 28256, GPU: 4.9GB\n",
      "Epoch 1, Batch 28260, Loss: 2.6775, Time: 16813.6s, Step: 28261, GPU: 4.9GB\n",
      "Epoch 1, Batch 28265, Loss: 2.9581, Time: 16817.0s, Step: 28266, GPU: 4.9GB\n",
      "Epoch 1, Batch 28270, Loss: 3.9359, Time: 16819.5s, Step: 28271, GPU: 4.9GB\n",
      "Epoch 1, Batch 28275, Loss: 3.5407, Time: 16822.9s, Step: 28276, GPU: 4.9GB\n",
      "Epoch 1, Batch 28280, Loss: 3.5791, Time: 16825.4s, Step: 28281, GPU: 4.9GB\n",
      "Epoch 1, Batch 28285, Loss: 2.7586, Time: 16828.8s, Step: 28286, GPU: 4.9GB\n",
      "Epoch 1, Batch 28290, Loss: 3.7885, Time: 16831.4s, Step: 28291, GPU: 4.9GB\n",
      "Epoch 1, Batch 28295, Loss: 3.3043, Time: 16835.0s, Step: 28296, GPU: 4.9GB\n",
      "Epoch 1, Batch 28300, Loss: 4.2939, Time: 16837.5s, Step: 28301, GPU: 4.9GB\n",
      "Epoch 1, Batch 28305, Loss: 2.9218, Time: 16840.9s, Step: 28306, GPU: 4.9GB\n",
      "Epoch 1, Batch 28310, Loss: 3.5592, Time: 16843.4s, Step: 28311, GPU: 4.9GB\n",
      "Epoch 1, Batch 28315, Loss: 3.1747, Time: 16846.8s, Step: 28316, GPU: 4.9GB\n",
      "Epoch 1, Batch 28320, Loss: 3.9454, Time: 16849.4s, Step: 28321, GPU: 4.9GB\n",
      "Epoch 1, Batch 28325, Loss: 3.5655, Time: 16852.8s, Step: 28326, GPU: 4.9GB\n",
      "Epoch 1, Batch 28330, Loss: 3.5270, Time: 16855.3s, Step: 28331, GPU: 4.9GB\n",
      "Epoch 1, Batch 28335, Loss: 3.6003, Time: 16858.8s, Step: 28336, GPU: 4.9GB\n",
      "Epoch 1, Batch 28340, Loss: 3.4912, Time: 16861.3s, Step: 28341, GPU: 4.9GB\n",
      "Epoch 1, Batch 28345, Loss: 3.6523, Time: 16864.7s, Step: 28346, GPU: 4.9GB\n",
      "Epoch 1, Batch 28350, Loss: 2.4684, Time: 16867.2s, Step: 28351, GPU: 4.9GB\n",
      "Epoch 1, Batch 28355, Loss: 3.6470, Time: 16870.6s, Step: 28356, GPU: 4.9GB\n",
      "Epoch 1, Batch 28360, Loss: 3.1466, Time: 16873.2s, Step: 28361, GPU: 4.9GB\n",
      "Epoch 1, Batch 28365, Loss: 4.6032, Time: 16876.6s, Step: 28366, GPU: 4.9GB\n",
      "Epoch 1, Batch 28370, Loss: 3.9861, Time: 16879.1s, Step: 28371, GPU: 4.9GB\n",
      "Epoch 1, Batch 28375, Loss: 3.7922, Time: 16882.5s, Step: 28376, GPU: 4.9GB\n",
      "Epoch 1, Batch 28380, Loss: 3.4770, Time: 16885.0s, Step: 28381, GPU: 4.9GB\n",
      "Epoch 1, Batch 28385, Loss: 2.9992, Time: 16888.4s, Step: 28386, GPU: 4.9GB\n",
      "Epoch 1, Batch 28390, Loss: 3.4764, Time: 16890.9s, Step: 28391, GPU: 4.9GB\n",
      "Epoch 1, Batch 28395, Loss: 2.8661, Time: 16894.4s, Step: 28396, GPU: 4.9GB\n",
      "Epoch 1, Batch 28400, Loss: 4.2760, Time: 16897.0s, Step: 28401, GPU: 4.9GB\n",
      "Epoch 1, Batch 28405, Loss: 4.0557, Time: 16900.4s, Step: 28406, GPU: 4.9GB\n",
      "Epoch 1, Batch 28410, Loss: 3.5090, Time: 16902.9s, Step: 28411, GPU: 4.9GB\n",
      "Epoch 1, Batch 28415, Loss: 3.4814, Time: 16906.3s, Step: 28416, GPU: 4.9GB\n",
      "Epoch 1, Batch 28420, Loss: 4.2495, Time: 16908.8s, Step: 28421, GPU: 4.9GB\n",
      "Epoch 1, Batch 28425, Loss: 3.6186, Time: 16912.2s, Step: 28426, GPU: 4.9GB\n",
      "Epoch 1, Batch 28430, Loss: 3.7803, Time: 16914.8s, Step: 28431, GPU: 4.9GB\n",
      "Epoch 1, Batch 28435, Loss: 3.3045, Time: 16918.2s, Step: 28436, GPU: 4.9GB\n",
      "Epoch 1, Batch 28440, Loss: 3.1308, Time: 16920.7s, Step: 28441, GPU: 4.9GB\n",
      "Epoch 1, Batch 28445, Loss: 4.2697, Time: 16924.1s, Step: 28446, GPU: 4.9GB\n",
      "Epoch 1, Batch 28450, Loss: 3.7103, Time: 16926.6s, Step: 28451, GPU: 4.9GB\n",
      "Epoch 1, Batch 28455, Loss: 4.6885, Time: 16930.1s, Step: 28456, GPU: 4.9GB\n",
      "Epoch 1, Batch 28460, Loss: 3.1726, Time: 16932.6s, Step: 28461, GPU: 4.9GB\n",
      "Epoch 1, Batch 28465, Loss: 3.3387, Time: 16936.0s, Step: 28466, GPU: 4.9GB\n",
      "Epoch 1, Batch 28470, Loss: 4.0434, Time: 16938.5s, Step: 28471, GPU: 4.9GB\n",
      "Epoch 1, Batch 28475, Loss: 2.9024, Time: 16941.9s, Step: 28476, GPU: 4.9GB\n",
      "Epoch 1, Batch 28480, Loss: 3.8616, Time: 16944.4s, Step: 28481, GPU: 4.9GB\n",
      "Epoch 1, Batch 28485, Loss: 3.2135, Time: 16947.8s, Step: 28486, GPU: 4.9GB\n",
      "Epoch 1, Batch 28490, Loss: 3.4496, Time: 16950.3s, Step: 28491, GPU: 4.9GB\n",
      "Epoch 1, Batch 28495, Loss: 3.7213, Time: 16953.7s, Step: 28496, GPU: 4.9GB\n",
      "Epoch 1, Batch 28500, Loss: 3.9397, Time: 16956.2s, Step: 28501, GPU: 4.9GB\n",
      "Epoch 1, Batch 28505, Loss: 3.5347, Time: 16959.6s, Step: 28506, GPU: 4.9GB\n",
      "Epoch 1, Batch 28510, Loss: 3.7947, Time: 16962.1s, Step: 28511, GPU: 4.9GB\n",
      "Epoch 1, Batch 28515, Loss: 3.1019, Time: 16965.5s, Step: 28516, GPU: 4.9GB\n",
      "Epoch 1, Batch 28520, Loss: 3.4799, Time: 16968.0s, Step: 28521, GPU: 4.9GB\n",
      "Epoch 1, Batch 28525, Loss: 3.6716, Time: 16971.4s, Step: 28526, GPU: 4.9GB\n",
      "Epoch 1, Batch 28530, Loss: 3.0354, Time: 16973.9s, Step: 28531, GPU: 4.9GB\n",
      "Epoch 1, Batch 28535, Loss: 3.8016, Time: 16977.4s, Step: 28536, GPU: 4.9GB\n",
      "Epoch 1, Batch 28540, Loss: 2.9601, Time: 16979.9s, Step: 28541, GPU: 4.9GB\n",
      "Epoch 1, Batch 28545, Loss: 3.9541, Time: 16983.3s, Step: 28546, GPU: 4.9GB\n",
      "Epoch 1, Batch 28550, Loss: 3.3725, Time: 16985.8s, Step: 28551, GPU: 4.9GB\n",
      "Epoch 1, Batch 28555, Loss: 4.6079, Time: 16989.2s, Step: 28556, GPU: 4.9GB\n",
      "Epoch 1, Batch 28560, Loss: 3.1900, Time: 16991.8s, Step: 28561, GPU: 4.9GB\n",
      "Epoch 1, Batch 28565, Loss: 4.1817, Time: 16995.2s, Step: 28566, GPU: 4.9GB\n",
      "Epoch 1, Batch 28570, Loss: 3.2742, Time: 16997.7s, Step: 28571, GPU: 4.9GB\n",
      "Epoch 1, Batch 28575, Loss: 4.2970, Time: 17001.0s, Step: 28576, GPU: 4.9GB\n",
      "Epoch 1, Batch 28580, Loss: 2.8628, Time: 17003.5s, Step: 28581, GPU: 4.9GB\n",
      "Epoch 1, Batch 28585, Loss: 2.5735, Time: 17007.0s, Step: 28586, GPU: 4.9GB\n",
      "Epoch 1, Batch 28590, Loss: 3.0378, Time: 17009.5s, Step: 28591, GPU: 4.9GB\n",
      "Epoch 1, Batch 28595, Loss: 3.0516, Time: 17012.9s, Step: 28596, GPU: 4.9GB\n",
      "Epoch 1, Batch 28600, Loss: 3.6195, Time: 17015.5s, Step: 28601, GPU: 4.9GB\n",
      "Epoch 1, Batch 28605, Loss: 3.7872, Time: 17018.9s, Step: 28606, GPU: 4.9GB\n",
      "Epoch 1, Batch 28610, Loss: 3.7370, Time: 17021.5s, Step: 28611, GPU: 4.9GB\n",
      "Epoch 1, Batch 28615, Loss: 3.6610, Time: 17024.9s, Step: 28616, GPU: 4.9GB\n",
      "Epoch 1, Batch 28620, Loss: 3.7681, Time: 17027.4s, Step: 28621, GPU: 4.9GB\n",
      "Epoch 1, Batch 28625, Loss: 3.5260, Time: 17030.8s, Step: 28626, GPU: 4.9GB\n",
      "Epoch 1, Batch 28630, Loss: 3.4369, Time: 17033.4s, Step: 28631, GPU: 4.9GB\n",
      "Epoch 1, Batch 28635, Loss: 3.6004, Time: 17036.8s, Step: 28636, GPU: 4.9GB\n",
      "Epoch 1, Batch 28640, Loss: 3.5716, Time: 17039.3s, Step: 28641, GPU: 4.9GB\n",
      "Epoch 1, Batch 28645, Loss: 3.4793, Time: 17042.8s, Step: 28646, GPU: 4.9GB\n",
      "Epoch 1, Batch 28650, Loss: 3.8259, Time: 17045.3s, Step: 28651, GPU: 4.9GB\n",
      "Epoch 1, Batch 28655, Loss: 3.9653, Time: 17048.8s, Step: 28656, GPU: 4.9GB\n",
      "Epoch 1, Batch 28660, Loss: 3.6147, Time: 17051.3s, Step: 28661, GPU: 4.9GB\n",
      "Epoch 1, Batch 28665, Loss: 4.0935, Time: 17054.7s, Step: 28666, GPU: 4.9GB\n",
      "Epoch 1, Batch 28670, Loss: 3.0970, Time: 17057.2s, Step: 28671, GPU: 4.9GB\n",
      "Epoch 1, Batch 28675, Loss: 4.0694, Time: 17060.6s, Step: 28676, GPU: 4.9GB\n",
      "Epoch 1, Batch 28680, Loss: 3.5463, Time: 17063.2s, Step: 28681, GPU: 4.9GB\n",
      "Epoch 1, Batch 28685, Loss: 3.3779, Time: 17066.6s, Step: 28686, GPU: 4.9GB\n",
      "Epoch 1, Batch 28690, Loss: 4.1309, Time: 17069.1s, Step: 28691, GPU: 4.9GB\n",
      "Epoch 1, Batch 28695, Loss: 4.2250, Time: 17072.5s, Step: 28696, GPU: 4.9GB\n",
      "Epoch 1, Batch 28700, Loss: 2.8335, Time: 17075.1s, Step: 28701, GPU: 4.9GB\n",
      "Epoch 1, Batch 28705, Loss: 4.0267, Time: 17078.5s, Step: 28706, GPU: 4.9GB\n",
      "Epoch 1, Batch 28710, Loss: 3.9900, Time: 17081.0s, Step: 28711, GPU: 4.9GB\n",
      "Epoch 1, Batch 28715, Loss: 3.2630, Time: 17084.4s, Step: 28716, GPU: 4.9GB\n",
      "Epoch 1, Batch 28720, Loss: 3.3022, Time: 17086.9s, Step: 28721, GPU: 4.9GB\n",
      "Epoch 1, Batch 28725, Loss: 3.7361, Time: 17090.4s, Step: 28726, GPU: 4.9GB\n",
      "Epoch 1, Batch 28730, Loss: 3.8347, Time: 17092.9s, Step: 28731, GPU: 4.9GB\n",
      "Epoch 1, Batch 28735, Loss: 3.4490, Time: 17097.3s, Step: 28736, GPU: 4.9GB\n",
      "Epoch 1, Batch 28740, Loss: 3.2196, Time: 17099.9s, Step: 28741, GPU: 4.9GB\n",
      "Epoch 1, Batch 28745, Loss: 3.6562, Time: 17103.3s, Step: 28746, GPU: 4.9GB\n",
      "Epoch 1, Batch 28750, Loss: 2.8331, Time: 17105.8s, Step: 28751, GPU: 4.9GB\n",
      "Epoch 1, Batch 28755, Loss: 3.5374, Time: 17109.2s, Step: 28756, GPU: 4.9GB\n",
      "Epoch 1, Batch 28760, Loss: 4.1280, Time: 17111.8s, Step: 28761, GPU: 4.9GB\n",
      "Epoch 1, Batch 28765, Loss: 3.5505, Time: 17115.2s, Step: 28766, GPU: 4.9GB\n",
      "Epoch 1, Batch 28770, Loss: 3.8731, Time: 17117.7s, Step: 28771, GPU: 4.9GB\n",
      "Epoch 1, Batch 28775, Loss: 3.4482, Time: 17121.1s, Step: 28776, GPU: 4.9GB\n",
      "Epoch 1, Batch 28780, Loss: 3.1221, Time: 17123.6s, Step: 28781, GPU: 4.9GB\n",
      "Epoch 1, Batch 28785, Loss: 3.4243, Time: 17127.1s, Step: 28786, GPU: 4.9GB\n",
      "Epoch 1, Batch 28790, Loss: 3.2450, Time: 17129.6s, Step: 28791, GPU: 4.9GB\n",
      "Epoch 1, Batch 28795, Loss: 2.7695, Time: 17133.0s, Step: 28796, GPU: 4.9GB\n",
      "Epoch 1, Batch 28800, Loss: 3.8547, Time: 17135.6s, Step: 28801, GPU: 4.9GB\n",
      "Epoch 1, Batch 28805, Loss: 2.6858, Time: 17139.0s, Step: 28806, GPU: 4.9GB\n",
      "Epoch 1, Batch 28810, Loss: 3.7217, Time: 17141.5s, Step: 28811, GPU: 4.9GB\n",
      "Epoch 1, Batch 28815, Loss: 3.2902, Time: 17144.9s, Step: 28816, GPU: 4.9GB\n",
      "Epoch 1, Batch 28820, Loss: 4.4976, Time: 17147.4s, Step: 28821, GPU: 4.9GB\n",
      "Epoch 1, Batch 28825, Loss: 4.6877, Time: 17150.8s, Step: 28826, GPU: 4.9GB\n",
      "Epoch 1, Batch 28830, Loss: 3.8030, Time: 17153.4s, Step: 28831, GPU: 4.9GB\n",
      "Epoch 1, Batch 28835, Loss: 3.6185, Time: 17156.8s, Step: 28836, GPU: 4.9GB\n",
      "Epoch 1, Batch 28840, Loss: 3.3526, Time: 17159.3s, Step: 28841, GPU: 4.9GB\n",
      "Epoch 1, Batch 28845, Loss: 3.6591, Time: 17162.7s, Step: 28846, GPU: 4.9GB\n",
      "Epoch 1, Batch 28850, Loss: 2.8562, Time: 17165.3s, Step: 28851, GPU: 4.9GB\n",
      "Epoch 1, Batch 28855, Loss: 3.2172, Time: 17168.7s, Step: 28856, GPU: 4.9GB\n",
      "Epoch 1, Batch 28860, Loss: 4.0708, Time: 17171.2s, Step: 28861, GPU: 4.9GB\n",
      "Epoch 1, Batch 28865, Loss: 3.5838, Time: 17174.6s, Step: 28866, GPU: 4.9GB\n",
      "Epoch 1, Batch 28870, Loss: 3.1123, Time: 17177.1s, Step: 28871, GPU: 4.9GB\n",
      "Epoch 1, Batch 28875, Loss: 3.9547, Time: 17180.6s, Step: 28876, GPU: 4.9GB\n",
      "Epoch 1, Batch 28880, Loss: 4.3318, Time: 17183.1s, Step: 28881, GPU: 4.9GB\n",
      "Epoch 1, Batch 28885, Loss: 3.4781, Time: 17186.5s, Step: 28886, GPU: 4.9GB\n",
      "Epoch 1, Batch 28890, Loss: 3.3292, Time: 17189.0s, Step: 28891, GPU: 4.9GB\n",
      "Epoch 1, Batch 28895, Loss: 2.5635, Time: 17192.4s, Step: 28896, GPU: 4.9GB\n",
      "Epoch 1, Batch 28900, Loss: 3.6506, Time: 17194.9s, Step: 28901, GPU: 4.9GB\n",
      "Epoch 1, Batch 28905, Loss: 3.7472, Time: 17198.3s, Step: 28906, GPU: 4.9GB\n",
      "Epoch 1, Batch 28910, Loss: 3.5017, Time: 17200.8s, Step: 28911, GPU: 4.9GB\n",
      "Epoch 1, Batch 28915, Loss: 3.5702, Time: 17204.2s, Step: 28916, GPU: 4.9GB\n",
      "Epoch 1, Batch 28920, Loss: 3.6276, Time: 17206.7s, Step: 28921, GPU: 4.9GB\n",
      "Epoch 1, Batch 28925, Loss: 3.0070, Time: 17210.1s, Step: 28926, GPU: 4.9GB\n",
      "Epoch 1, Batch 28930, Loss: 2.7822, Time: 17212.6s, Step: 28931, GPU: 4.9GB\n",
      "Epoch 1, Batch 28935, Loss: 3.1474, Time: 17216.0s, Step: 28936, GPU: 4.9GB\n",
      "Epoch 1, Batch 28940, Loss: 4.2722, Time: 17218.6s, Step: 28941, GPU: 4.9GB\n",
      "Epoch 1, Batch 28945, Loss: 3.3769, Time: 17222.0s, Step: 28946, GPU: 4.9GB\n",
      "Epoch 1, Batch 28950, Loss: 3.4481, Time: 17224.5s, Step: 28951, GPU: 4.9GB\n",
      "Epoch 1, Batch 28955, Loss: 3.7248, Time: 17227.9s, Step: 28956, GPU: 4.9GB\n",
      "Epoch 1, Batch 28960, Loss: 3.2171, Time: 17230.4s, Step: 28961, GPU: 4.9GB\n",
      "Epoch 1, Batch 28965, Loss: 3.7810, Time: 17233.8s, Step: 28966, GPU: 4.9GB\n",
      "Epoch 1, Batch 28970, Loss: 3.2790, Time: 17236.3s, Step: 28971, GPU: 4.9GB\n",
      "Epoch 1, Batch 28975, Loss: 3.9005, Time: 17239.8s, Step: 28976, GPU: 4.9GB\n",
      "Epoch 1, Batch 28980, Loss: 3.3167, Time: 17242.3s, Step: 28981, GPU: 4.9GB\n",
      "Epoch 1, Batch 28985, Loss: 3.5050, Time: 17245.7s, Step: 28986, GPU: 4.9GB\n",
      "Epoch 1, Batch 28990, Loss: 2.4262, Time: 17248.2s, Step: 28991, GPU: 4.9GB\n",
      "Epoch 1, Batch 28995, Loss: 3.7698, Time: 17251.7s, Step: 28996, GPU: 4.9GB\n",
      "Epoch 1, Batch 29000, Loss: 2.3875, Time: 17254.3s, Step: 29001, GPU: 4.9GB\n",
      "Epoch 1, Batch 29005, Loss: 3.3701, Time: 17257.7s, Step: 29006, GPU: 4.9GB\n",
      "Epoch 1, Batch 29010, Loss: 4.2497, Time: 17260.2s, Step: 29011, GPU: 4.9GB\n",
      "Epoch 1, Batch 29015, Loss: 3.8134, Time: 17263.6s, Step: 29016, GPU: 4.9GB\n",
      "Epoch 1, Batch 29020, Loss: 3.4722, Time: 17266.2s, Step: 29021, GPU: 4.9GB\n",
      "Epoch 1, Batch 29025, Loss: 3.5126, Time: 17269.6s, Step: 29026, GPU: 4.9GB\n",
      "Epoch 1, Batch 29030, Loss: 3.7122, Time: 17272.1s, Step: 29031, GPU: 4.9GB\n",
      "Epoch 1, Batch 29035, Loss: 3.2157, Time: 17275.5s, Step: 29036, GPU: 4.9GB\n",
      "Epoch 1, Batch 29040, Loss: 3.5155, Time: 17278.1s, Step: 29041, GPU: 4.9GB\n",
      "Epoch 1, Batch 29045, Loss: 4.0004, Time: 17281.5s, Step: 29046, GPU: 4.9GB\n",
      "Epoch 1, Batch 29050, Loss: 2.8520, Time: 17284.0s, Step: 29051, GPU: 4.9GB\n",
      "Epoch 1, Batch 29055, Loss: 3.7102, Time: 17287.4s, Step: 29056, GPU: 4.9GB\n",
      "Epoch 1, Batch 29060, Loss: 3.1660, Time: 17289.9s, Step: 29061, GPU: 4.9GB\n",
      "Epoch 1, Batch 29065, Loss: 3.3861, Time: 17293.4s, Step: 29066, GPU: 4.9GB\n",
      "Epoch 1, Batch 29070, Loss: 4.2608, Time: 17295.9s, Step: 29071, GPU: 4.9GB\n",
      "Epoch 1, Batch 29075, Loss: 3.1200, Time: 17299.3s, Step: 29076, GPU: 4.9GB\n",
      "Epoch 1, Batch 29080, Loss: 3.9782, Time: 17301.8s, Step: 29081, GPU: 4.9GB\n",
      "Epoch 1, Batch 29085, Loss: 4.3048, Time: 17305.2s, Step: 29086, GPU: 4.9GB\n",
      "Epoch 1, Batch 29090, Loss: 3.7517, Time: 17307.8s, Step: 29091, GPU: 4.9GB\n",
      "Epoch 1, Batch 29095, Loss: 3.1999, Time: 17311.2s, Step: 29096, GPU: 4.9GB\n",
      "Epoch 1, Batch 29100, Loss: 3.8579, Time: 17313.7s, Step: 29101, GPU: 4.9GB\n",
      "Epoch 1, Batch 29105, Loss: 3.6686, Time: 17317.2s, Step: 29106, GPU: 4.9GB\n",
      "Epoch 1, Batch 29110, Loss: 3.8289, Time: 17319.7s, Step: 29111, GPU: 4.9GB\n",
      "Epoch 1, Batch 29115, Loss: 3.6262, Time: 17323.1s, Step: 29116, GPU: 4.9GB\n",
      "Epoch 1, Batch 29120, Loss: 3.2596, Time: 17325.6s, Step: 29121, GPU: 4.9GB\n",
      "Epoch 1, Batch 29125, Loss: 2.9492, Time: 17329.1s, Step: 29126, GPU: 4.9GB\n",
      "Epoch 1, Batch 29130, Loss: 3.3872, Time: 17331.6s, Step: 29131, GPU: 4.9GB\n",
      "Epoch 1, Batch 29135, Loss: 3.4257, Time: 17335.0s, Step: 29136, GPU: 4.9GB\n",
      "Epoch 1, Batch 29140, Loss: 4.7997, Time: 17337.6s, Step: 29141, GPU: 4.9GB\n",
      "Epoch 1, Batch 29145, Loss: 2.5756, Time: 17341.0s, Step: 29146, GPU: 4.9GB\n",
      "Epoch 1, Batch 29150, Loss: 3.1127, Time: 17343.5s, Step: 29151, GPU: 4.9GB\n",
      "Epoch 1, Batch 29155, Loss: 3.6432, Time: 17347.0s, Step: 29156, GPU: 4.9GB\n",
      "Epoch 1, Batch 29160, Loss: 3.5144, Time: 17349.5s, Step: 29161, GPU: 4.9GB\n",
      "Epoch 1, Batch 29165, Loss: 3.5231, Time: 17353.0s, Step: 29166, GPU: 4.9GB\n",
      "Epoch 1, Batch 29170, Loss: 3.9567, Time: 17355.5s, Step: 29171, GPU: 4.9GB\n",
      "Epoch 1, Batch 29175, Loss: 4.0722, Time: 17358.9s, Step: 29176, GPU: 4.9GB\n",
      "Epoch 1, Batch 29180, Loss: 3.1261, Time: 17361.5s, Step: 29181, GPU: 4.9GB\n",
      "Epoch 1, Batch 29185, Loss: 3.2942, Time: 17364.9s, Step: 29186, GPU: 4.9GB\n",
      "Epoch 1, Batch 29190, Loss: 3.6913, Time: 17367.4s, Step: 29191, GPU: 4.9GB\n",
      "Epoch 1, Batch 29195, Loss: 4.0784, Time: 17370.8s, Step: 29196, GPU: 4.9GB\n",
      "Epoch 1, Batch 29200, Loss: 4.5246, Time: 17373.4s, Step: 29201, GPU: 4.9GB\n",
      "Epoch 1, Batch 29205, Loss: 3.3495, Time: 17376.8s, Step: 29206, GPU: 4.9GB\n",
      "Epoch 1, Batch 29210, Loss: 4.1781, Time: 17379.3s, Step: 29211, GPU: 4.9GB\n",
      "Epoch 1, Batch 29215, Loss: 2.9120, Time: 17382.7s, Step: 29216, GPU: 4.9GB\n",
      "Epoch 1, Batch 29220, Loss: 3.0882, Time: 17385.2s, Step: 29221, GPU: 4.9GB\n",
      "Epoch 1, Batch 29225, Loss: 2.9686, Time: 17388.6s, Step: 29226, GPU: 4.9GB\n",
      "Epoch 1, Batch 29230, Loss: 4.1188, Time: 17391.1s, Step: 29231, GPU: 4.9GB\n",
      "Epoch 1, Batch 29235, Loss: 3.5001, Time: 17394.5s, Step: 29236, GPU: 4.9GB\n",
      "Epoch 1, Batch 29240, Loss: 3.4682, Time: 17397.0s, Step: 29241, GPU: 4.9GB\n",
      "Epoch 1, Batch 29245, Loss: 3.8783, Time: 17400.4s, Step: 29246, GPU: 4.9GB\n",
      "Epoch 1, Batch 29250, Loss: 2.8994, Time: 17402.9s, Step: 29251, GPU: 4.9GB\n",
      "Epoch 1, Batch 29255, Loss: 4.0636, Time: 17406.3s, Step: 29256, GPU: 4.9GB\n",
      "Epoch 1, Batch 29260, Loss: 3.0190, Time: 17408.9s, Step: 29261, GPU: 4.9GB\n",
      "Epoch 1, Batch 29265, Loss: 3.7985, Time: 17412.3s, Step: 29266, GPU: 4.9GB\n",
      "Epoch 1, Batch 29270, Loss: 3.6326, Time: 17414.8s, Step: 29271, GPU: 4.9GB\n",
      "Epoch 1, Batch 29275, Loss: 3.7422, Time: 17418.2s, Step: 29276, GPU: 4.9GB\n",
      "Epoch 1, Batch 29280, Loss: 3.2346, Time: 17420.7s, Step: 29281, GPU: 4.9GB\n",
      "Epoch 1, Batch 29285, Loss: 4.1369, Time: 17424.1s, Step: 29286, GPU: 4.9GB\n",
      "Epoch 1, Batch 29290, Loss: 3.7434, Time: 17426.6s, Step: 29291, GPU: 4.9GB\n",
      "Epoch 1, Batch 29295, Loss: 3.2996, Time: 17430.1s, Step: 29296, GPU: 4.9GB\n",
      "Epoch 1, Batch 29300, Loss: 3.8120, Time: 17432.6s, Step: 29301, GPU: 4.9GB\n",
      "Epoch 1, Batch 29305, Loss: 3.3354, Time: 17436.0s, Step: 29306, GPU: 4.9GB\n",
      "Epoch 1, Batch 29310, Loss: 4.6111, Time: 17438.5s, Step: 29311, GPU: 4.9GB\n",
      "Epoch 1, Batch 29315, Loss: 3.5790, Time: 17441.9s, Step: 29316, GPU: 4.9GB\n",
      "Epoch 1, Batch 29320, Loss: 4.1440, Time: 17444.4s, Step: 29321, GPU: 4.9GB\n",
      "Epoch 1, Batch 29325, Loss: 3.9158, Time: 17447.8s, Step: 29326, GPU: 4.9GB\n",
      "Epoch 1, Batch 29330, Loss: 3.6008, Time: 17450.3s, Step: 29331, GPU: 4.9GB\n",
      "Epoch 1, Batch 29335, Loss: 3.6425, Time: 17453.7s, Step: 29336, GPU: 4.9GB\n",
      "Epoch 1, Batch 29340, Loss: 3.2356, Time: 17456.2s, Step: 29341, GPU: 4.9GB\n",
      "Epoch 1, Batch 29345, Loss: 3.4936, Time: 17459.6s, Step: 29346, GPU: 4.9GB\n",
      "Epoch 1, Batch 29350, Loss: 3.5348, Time: 17462.2s, Step: 29351, GPU: 4.9GB\n",
      "Epoch 1, Batch 29355, Loss: 3.8350, Time: 17465.6s, Step: 29356, GPU: 4.9GB\n",
      "Epoch 1, Batch 29360, Loss: 3.9672, Time: 17468.1s, Step: 29361, GPU: 4.9GB\n",
      "Epoch 1, Batch 29365, Loss: 3.7386, Time: 17471.5s, Step: 29366, GPU: 4.9GB\n",
      "Epoch 1, Batch 29370, Loss: 3.2672, Time: 17474.0s, Step: 29371, GPU: 4.9GB\n",
      "Epoch 1, Batch 29375, Loss: 3.7204, Time: 17477.4s, Step: 29376, GPU: 4.9GB\n",
      "Epoch 1, Batch 29380, Loss: 3.9866, Time: 17479.9s, Step: 29381, GPU: 4.9GB\n",
      "Epoch 1, Batch 29385, Loss: 3.4986, Time: 17483.4s, Step: 29386, GPU: 4.9GB\n",
      "Epoch 1, Batch 29390, Loss: 2.2798, Time: 17485.9s, Step: 29391, GPU: 4.9GB\n",
      "Epoch 1, Batch 29395, Loss: 2.9245, Time: 17489.3s, Step: 29396, GPU: 4.9GB\n",
      "Epoch 1, Batch 29400, Loss: 3.2142, Time: 17491.9s, Step: 29401, GPU: 4.9GB\n",
      "Epoch 1, Batch 29405, Loss: 4.0842, Time: 17495.4s, Step: 29406, GPU: 4.9GB\n",
      "Epoch 1, Batch 29410, Loss: 3.1511, Time: 17497.9s, Step: 29411, GPU: 4.9GB\n",
      "Epoch 1, Batch 29415, Loss: 2.7025, Time: 17501.3s, Step: 29416, GPU: 4.9GB\n",
      "Epoch 1, Batch 29420, Loss: 3.8838, Time: 17503.9s, Step: 29421, GPU: 4.9GB\n",
      "Epoch 1, Batch 29425, Loss: 3.4818, Time: 17507.3s, Step: 29426, GPU: 4.9GB\n",
      "Epoch 1, Batch 29430, Loss: 3.7605, Time: 17509.8s, Step: 29431, GPU: 4.9GB\n",
      "Epoch 1, Batch 29435, Loss: 3.0861, Time: 17513.2s, Step: 29436, GPU: 4.9GB\n",
      "Epoch 1, Batch 29440, Loss: 3.1557, Time: 17515.8s, Step: 29441, GPU: 4.9GB\n",
      "Epoch 1, Batch 29445, Loss: 3.5803, Time: 17519.2s, Step: 29446, GPU: 4.9GB\n",
      "Epoch 1, Batch 29450, Loss: 3.9189, Time: 17521.8s, Step: 29451, GPU: 4.9GB\n",
      "Epoch 1, Batch 29455, Loss: 3.0403, Time: 17525.2s, Step: 29456, GPU: 4.9GB\n",
      "Epoch 1, Batch 29460, Loss: 3.8690, Time: 17527.8s, Step: 29461, GPU: 4.9GB\n",
      "Epoch 1, Batch 29465, Loss: 3.9655, Time: 17531.2s, Step: 29466, GPU: 4.9GB\n",
      "Epoch 1, Batch 29470, Loss: 3.6470, Time: 17533.7s, Step: 29471, GPU: 4.9GB\n",
      "Epoch 1, Batch 29475, Loss: 3.3727, Time: 17537.1s, Step: 29476, GPU: 4.9GB\n",
      "Epoch 1, Batch 29480, Loss: 3.4957, Time: 17539.6s, Step: 29481, GPU: 4.9GB\n",
      "Epoch 1, Batch 29485, Loss: 4.0201, Time: 17543.0s, Step: 29486, GPU: 4.9GB\n",
      "Epoch 1, Batch 29490, Loss: 3.2208, Time: 17545.5s, Step: 29491, GPU: 4.9GB\n",
      "Epoch 1, Batch 29495, Loss: 3.9526, Time: 17548.9s, Step: 29496, GPU: 4.9GB\n",
      "Epoch 1, Batch 29500, Loss: 4.6731, Time: 17551.4s, Step: 29501, GPU: 4.9GB\n",
      "Epoch 1, Batch 29505, Loss: 3.5079, Time: 17554.8s, Step: 29506, GPU: 4.9GB\n",
      "Epoch 1, Batch 29510, Loss: 3.2232, Time: 17557.3s, Step: 29511, GPU: 4.9GB\n",
      "Epoch 1, Batch 29515, Loss: 3.6588, Time: 17560.7s, Step: 29516, GPU: 4.9GB\n",
      "Epoch 1, Batch 29520, Loss: 3.3567, Time: 17563.3s, Step: 29521, GPU: 4.9GB\n",
      "Epoch 1, Batch 29525, Loss: 3.6145, Time: 17566.7s, Step: 29526, GPU: 4.9GB\n",
      "Epoch 1, Batch 29530, Loss: 3.1111, Time: 17569.2s, Step: 29531, GPU: 4.9GB\n",
      "Epoch 1, Batch 29535, Loss: 3.4853, Time: 17572.6s, Step: 29536, GPU: 4.9GB\n",
      "Epoch 1, Batch 29540, Loss: 3.4721, Time: 17575.1s, Step: 29541, GPU: 4.9GB\n",
      "Epoch 1, Batch 29545, Loss: 3.9693, Time: 17578.5s, Step: 29546, GPU: 4.9GB\n",
      "Epoch 1, Batch 29550, Loss: 2.6619, Time: 17581.0s, Step: 29551, GPU: 4.9GB\n",
      "Epoch 1, Batch 29555, Loss: 2.4560, Time: 17584.4s, Step: 29556, GPU: 4.9GB\n",
      "Epoch 1, Batch 29560, Loss: 3.9718, Time: 17587.0s, Step: 29561, GPU: 4.9GB\n",
      "Epoch 1, Batch 29565, Loss: 3.5763, Time: 17590.4s, Step: 29566, GPU: 4.9GB\n",
      "Epoch 1, Batch 29570, Loss: 3.8147, Time: 17593.0s, Step: 29571, GPU: 4.9GB\n",
      "Epoch 1, Batch 29575, Loss: 3.4657, Time: 17596.4s, Step: 29576, GPU: 4.9GB\n",
      "Epoch 1, Batch 29580, Loss: 3.3564, Time: 17598.9s, Step: 29581, GPU: 4.9GB\n",
      "Epoch 1, Batch 29585, Loss: 3.6843, Time: 17602.3s, Step: 29586, GPU: 4.9GB\n",
      "Epoch 1, Batch 29590, Loss: 3.9078, Time: 17604.9s, Step: 29591, GPU: 4.9GB\n",
      "Epoch 1, Batch 29595, Loss: 3.6406, Time: 17608.2s, Step: 29596, GPU: 4.9GB\n",
      "Epoch 1, Batch 29600, Loss: 3.2082, Time: 17610.9s, Step: 29601, GPU: 4.9GB\n",
      "Epoch 1, Batch 29605, Loss: 3.5754, Time: 17614.3s, Step: 29606, GPU: 4.9GB\n",
      "Epoch 1, Batch 29610, Loss: 3.0766, Time: 17616.8s, Step: 29611, GPU: 4.9GB\n",
      "Epoch 1, Batch 29615, Loss: 3.2626, Time: 17620.3s, Step: 29616, GPU: 4.9GB\n",
      "Epoch 1, Batch 29620, Loss: 3.6201, Time: 17622.8s, Step: 29621, GPU: 4.9GB\n",
      "Epoch 1, Batch 29625, Loss: 3.1846, Time: 17626.2s, Step: 29626, GPU: 4.9GB\n",
      "Epoch 1, Batch 29630, Loss: 3.9682, Time: 17628.7s, Step: 29631, GPU: 4.9GB\n",
      "Epoch 1, Batch 29635, Loss: 3.2222, Time: 17632.1s, Step: 29636, GPU: 4.9GB\n",
      "Epoch 1, Batch 29640, Loss: 3.4206, Time: 17634.8s, Step: 29641, GPU: 4.9GB\n",
      "Epoch 1, Batch 29645, Loss: 3.4830, Time: 17638.2s, Step: 29646, GPU: 4.9GB\n",
      "Epoch 1, Batch 29650, Loss: 4.1667, Time: 17640.8s, Step: 29651, GPU: 4.9GB\n",
      "Epoch 1, Batch 29655, Loss: 3.5643, Time: 17644.2s, Step: 29656, GPU: 4.9GB\n",
      "Epoch 1, Batch 29660, Loss: 3.8316, Time: 17646.7s, Step: 29661, GPU: 4.9GB\n",
      "Epoch 1, Batch 29665, Loss: 2.9050, Time: 17650.1s, Step: 29666, GPU: 4.9GB\n",
      "Epoch 1, Batch 29670, Loss: 3.0557, Time: 17652.7s, Step: 29671, GPU: 4.9GB\n",
      "Epoch 1, Batch 29675, Loss: 3.2693, Time: 17656.1s, Step: 29676, GPU: 4.9GB\n",
      "Epoch 1, Batch 29680, Loss: 2.9914, Time: 17658.6s, Step: 29681, GPU: 4.9GB\n",
      "Epoch 1, Batch 29685, Loss: 3.8796, Time: 17662.0s, Step: 29686, GPU: 4.9GB\n",
      "Epoch 1, Batch 29690, Loss: 4.7693, Time: 17664.5s, Step: 29691, GPU: 4.9GB\n",
      "Epoch 1, Batch 29695, Loss: 3.2659, Time: 17667.9s, Step: 29696, GPU: 4.9GB\n",
      "Epoch 1, Batch 29700, Loss: 3.0560, Time: 17670.5s, Step: 29701, GPU: 4.9GB\n",
      "Epoch 1, Batch 29705, Loss: 3.3431, Time: 17673.9s, Step: 29706, GPU: 4.9GB\n",
      "Epoch 1, Batch 29710, Loss: 3.0725, Time: 17676.4s, Step: 29711, GPU: 4.9GB\n",
      "Epoch 1, Batch 29715, Loss: 2.9165, Time: 17679.9s, Step: 29716, GPU: 4.9GB\n",
      "Epoch 1, Batch 29720, Loss: 3.5388, Time: 17682.4s, Step: 29721, GPU: 4.9GB\n",
      "Epoch 1, Batch 29725, Loss: 4.1690, Time: 17685.8s, Step: 29726, GPU: 4.9GB\n",
      "Epoch 1, Batch 29730, Loss: 4.9027, Time: 17688.3s, Step: 29731, GPU: 4.9GB\n",
      "Epoch 1, Batch 29735, Loss: 4.0031, Time: 17691.7s, Step: 29736, GPU: 4.9GB\n",
      "Epoch 1, Batch 29740, Loss: 3.1630, Time: 17694.2s, Step: 29741, GPU: 4.9GB\n",
      "Epoch 1, Batch 29745, Loss: 3.6193, Time: 17697.7s, Step: 29746, GPU: 4.9GB\n",
      "Epoch 1, Batch 29750, Loss: 3.8570, Time: 17700.2s, Step: 29751, GPU: 4.9GB\n",
      "Epoch 1, Batch 29755, Loss: 3.4649, Time: 17703.6s, Step: 29756, GPU: 4.9GB\n",
      "Epoch 1, Batch 29760, Loss: 3.5832, Time: 17706.2s, Step: 29761, GPU: 4.9GB\n",
      "Epoch 1, Batch 29765, Loss: 2.6654, Time: 17709.6s, Step: 29766, GPU: 4.9GB\n",
      "Epoch 1, Batch 29770, Loss: 3.6116, Time: 17712.1s, Step: 29771, GPU: 4.9GB\n",
      "Epoch 1, Batch 29775, Loss: 3.4337, Time: 17715.5s, Step: 29776, GPU: 4.9GB\n",
      "Epoch 1, Batch 29780, Loss: 3.2437, Time: 17718.1s, Step: 29781, GPU: 4.9GB\n",
      "Epoch 1, Batch 29785, Loss: 3.2256, Time: 17721.5s, Step: 29786, GPU: 4.9GB\n",
      "Epoch 1, Batch 29790, Loss: 3.1416, Time: 17724.0s, Step: 29791, GPU: 4.9GB\n",
      "Epoch 1, Batch 29795, Loss: 3.3145, Time: 17727.5s, Step: 29796, GPU: 4.9GB\n",
      "Epoch 1, Batch 29800, Loss: 3.7436, Time: 17730.1s, Step: 29801, GPU: 4.9GB\n",
      "Epoch 1, Batch 29805, Loss: 4.0812, Time: 17733.5s, Step: 29806, GPU: 4.9GB\n",
      "Epoch 1, Batch 29810, Loss: 3.9050, Time: 17736.0s, Step: 29811, GPU: 4.9GB\n",
      "Epoch 1, Batch 29815, Loss: 3.3650, Time: 17739.5s, Step: 29816, GPU: 4.9GB\n",
      "Epoch 1, Batch 29820, Loss: 3.3367, Time: 17742.0s, Step: 29821, GPU: 4.9GB\n",
      "Epoch 1, Batch 29825, Loss: 3.5675, Time: 17745.4s, Step: 29826, GPU: 4.9GB\n",
      "Epoch 1, Batch 29830, Loss: 3.9294, Time: 17747.9s, Step: 29831, GPU: 4.9GB\n",
      "Epoch 1, Batch 29835, Loss: 4.0714, Time: 17751.3s, Step: 29836, GPU: 4.9GB\n",
      "Epoch 1, Batch 29840, Loss: 3.6255, Time: 17753.8s, Step: 29841, GPU: 4.9GB\n",
      "Epoch 1, Batch 29845, Loss: 3.8464, Time: 17757.3s, Step: 29846, GPU: 4.9GB\n",
      "Epoch 1, Batch 29850, Loss: 3.0714, Time: 17759.8s, Step: 29851, GPU: 4.9GB\n",
      "Epoch 1, Batch 29855, Loss: 2.6832, Time: 17763.2s, Step: 29856, GPU: 4.9GB\n",
      "Epoch 1, Batch 29860, Loss: 3.7974, Time: 17765.7s, Step: 29861, GPU: 4.9GB\n",
      "Epoch 1, Batch 29865, Loss: 3.2189, Time: 17769.2s, Step: 29866, GPU: 4.9GB\n",
      "Epoch 1, Batch 29870, Loss: 4.4877, Time: 17771.7s, Step: 29871, GPU: 4.9GB\n",
      "Epoch 1, Batch 29875, Loss: 3.6651, Time: 17775.1s, Step: 29876, GPU: 4.9GB\n",
      "Epoch 1, Batch 29880, Loss: 3.2678, Time: 17777.6s, Step: 29881, GPU: 4.9GB\n",
      "Epoch 1, Batch 29885, Loss: 3.4557, Time: 17781.0s, Step: 29886, GPU: 4.9GB\n",
      "Epoch 1, Batch 29890, Loss: 3.8078, Time: 17783.5s, Step: 29891, GPU: 4.9GB\n",
      "Epoch 1, Batch 29895, Loss: 4.0185, Time: 17786.9s, Step: 29896, GPU: 4.9GB\n",
      "Epoch 1, Batch 29900, Loss: 4.2006, Time: 17789.4s, Step: 29901, GPU: 4.9GB\n",
      "Epoch 1, Batch 29905, Loss: 3.7569, Time: 17792.9s, Step: 29906, GPU: 4.9GB\n",
      "Epoch 1, Batch 29910, Loss: 4.2501, Time: 17795.4s, Step: 29911, GPU: 4.9GB\n",
      "Epoch 1, Batch 29915, Loss: 3.3297, Time: 17798.8s, Step: 29916, GPU: 4.9GB\n",
      "Epoch 1, Batch 29920, Loss: 3.0876, Time: 17801.3s, Step: 29921, GPU: 4.9GB\n",
      "Epoch 1, Batch 29925, Loss: 4.0641, Time: 17804.7s, Step: 29926, GPU: 4.9GB\n",
      "Epoch 1, Batch 29930, Loss: 3.6519, Time: 17807.2s, Step: 29931, GPU: 4.9GB\n",
      "Epoch 1, Batch 29935, Loss: 3.6797, Time: 17810.6s, Step: 29936, GPU: 4.9GB\n",
      "Epoch 1, Batch 29940, Loss: 3.3293, Time: 17813.1s, Step: 29941, GPU: 4.9GB\n",
      "Epoch 1, Batch 29945, Loss: 3.4232, Time: 17816.6s, Step: 29946, GPU: 4.9GB\n",
      "Epoch 1, Batch 29950, Loss: 3.0483, Time: 17819.1s, Step: 29951, GPU: 4.9GB\n",
      "Epoch 1, Batch 29955, Loss: 4.0536, Time: 17822.6s, Step: 29956, GPU: 4.9GB\n",
      "Epoch 1, Batch 29960, Loss: 4.7837, Time: 17825.1s, Step: 29961, GPU: 4.9GB\n",
      "Epoch 1, Batch 29965, Loss: 3.5301, Time: 17828.6s, Step: 29966, GPU: 4.9GB\n",
      "Epoch 1, Batch 29970, Loss: 3.8454, Time: 17831.1s, Step: 29971, GPU: 4.9GB\n",
      "Epoch 1, Batch 29975, Loss: 3.3955, Time: 17834.5s, Step: 29976, GPU: 4.9GB\n",
      "Epoch 1, Batch 29980, Loss: 4.0162, Time: 17837.0s, Step: 29981, GPU: 4.9GB\n",
      "Epoch 1, Batch 29985, Loss: 3.0076, Time: 17840.5s, Step: 29986, GPU: 4.9GB\n",
      "Epoch 1, Batch 29990, Loss: 4.0937, Time: 17843.0s, Step: 29991, GPU: 4.9GB\n",
      "Epoch 1, Batch 29995, Loss: 3.3759, Time: 17846.4s, Step: 29996, GPU: 4.9GB\n",
      "Epoch 1, Batch 30000, Loss: 3.9950, Time: 17849.0s, Step: 30001, GPU: 4.9GB\n",
      "Epoch 1, Batch 30005, Loss: 3.4780, Time: 17852.4s, Step: 30006, GPU: 4.9GB\n",
      "Epoch 1, Batch 30010, Loss: 3.7375, Time: 17854.9s, Step: 30011, GPU: 4.9GB\n",
      "Epoch 1, Batch 30015, Loss: 4.0208, Time: 17858.4s, Step: 30016, GPU: 4.9GB\n",
      "Epoch 1, Batch 30020, Loss: 3.3368, Time: 17860.9s, Step: 30021, GPU: 4.9GB\n",
      "Epoch 1, Batch 30025, Loss: 3.8687, Time: 17864.3s, Step: 30026, GPU: 4.9GB\n",
      "Epoch 1, Batch 30030, Loss: 2.8981, Time: 17866.9s, Step: 30031, GPU: 4.9GB\n",
      "Epoch 1, Batch 30035, Loss: 3.9665, Time: 17870.3s, Step: 30036, GPU: 4.9GB\n",
      "Epoch 1, Batch 30040, Loss: 3.5580, Time: 17872.8s, Step: 30041, GPU: 4.9GB\n",
      "Epoch 1, Batch 30045, Loss: 3.8329, Time: 17876.2s, Step: 30046, GPU: 4.9GB\n",
      "Epoch 1, Batch 30050, Loss: 3.1820, Time: 17878.8s, Step: 30051, GPU: 4.9GB\n",
      "Epoch 1, Batch 30055, Loss: 2.1346, Time: 17882.2s, Step: 30056, GPU: 4.9GB\n",
      "Epoch 1, Batch 30060, Loss: 4.5443, Time: 17884.7s, Step: 30061, GPU: 4.9GB\n",
      "Epoch 1, Batch 30065, Loss: 3.2048, Time: 17888.1s, Step: 30066, GPU: 4.9GB\n",
      "Epoch 1, Batch 30070, Loss: 3.6389, Time: 17890.7s, Step: 30071, GPU: 4.9GB\n",
      "Epoch 1, Batch 30075, Loss: 3.5414, Time: 17894.1s, Step: 30076, GPU: 4.9GB\n",
      "Epoch 1, Batch 30080, Loss: 4.3351, Time: 17896.6s, Step: 30081, GPU: 4.9GB\n",
      "Epoch 1, Batch 30085, Loss: 3.7835, Time: 17900.0s, Step: 30086, GPU: 4.9GB\n",
      "Epoch 1, Batch 30090, Loss: 3.3924, Time: 17902.6s, Step: 30091, GPU: 4.9GB\n",
      "Epoch 1, Batch 30095, Loss: 2.9051, Time: 17906.0s, Step: 30096, GPU: 4.9GB\n",
      "Epoch 1, Batch 30100, Loss: 3.9858, Time: 17908.5s, Step: 30101, GPU: 4.9GB\n",
      "Epoch 1, Batch 30105, Loss: 4.0237, Time: 17911.9s, Step: 30106, GPU: 4.9GB\n",
      "Epoch 1, Batch 30110, Loss: 4.0831, Time: 17914.4s, Step: 30111, GPU: 4.9GB\n",
      "Epoch 1, Batch 30115, Loss: 3.3767, Time: 17917.9s, Step: 30116, GPU: 4.9GB\n",
      "Epoch 1, Batch 30120, Loss: 3.7404, Time: 17920.4s, Step: 30121, GPU: 4.9GB\n",
      "Epoch 1, Batch 30125, Loss: 3.9476, Time: 17923.8s, Step: 30126, GPU: 4.9GB\n",
      "Epoch 1, Batch 30130, Loss: 3.8798, Time: 17926.3s, Step: 30131, GPU: 4.9GB\n",
      "Epoch 1, Batch 30135, Loss: 3.6259, Time: 17929.7s, Step: 30136, GPU: 4.9GB\n",
      "Epoch 1, Batch 30140, Loss: 3.2813, Time: 17932.2s, Step: 30141, GPU: 4.9GB\n",
      "Epoch 1, Batch 30145, Loss: 3.9883, Time: 17935.7s, Step: 30146, GPU: 4.9GB\n",
      "Epoch 1, Batch 30150, Loss: 2.8777, Time: 17938.2s, Step: 30151, GPU: 4.9GB\n",
      "Epoch 1, Batch 30155, Loss: 3.6074, Time: 17941.6s, Step: 30156, GPU: 4.9GB\n",
      "Epoch 1, Batch 30160, Loss: 4.0298, Time: 17944.1s, Step: 30161, GPU: 4.9GB\n",
      "Epoch 1, Batch 30165, Loss: 3.7307, Time: 17947.6s, Step: 30166, GPU: 4.9GB\n",
      "Epoch 1, Batch 30170, Loss: 2.9718, Time: 17950.1s, Step: 30171, GPU: 4.9GB\n",
      "Epoch 1, Batch 30175, Loss: 3.4428, Time: 17953.5s, Step: 30176, GPU: 4.9GB\n",
      "Epoch 1, Batch 30180, Loss: 3.3012, Time: 17956.0s, Step: 30181, GPU: 4.9GB\n",
      "Epoch 1, Batch 30185, Loss: 3.8457, Time: 17959.5s, Step: 30186, GPU: 4.9GB\n",
      "Epoch 1, Batch 30190, Loss: 3.4013, Time: 17962.0s, Step: 30191, GPU: 4.9GB\n",
      "Epoch 1, Batch 30195, Loss: 4.2578, Time: 17965.4s, Step: 30196, GPU: 4.9GB\n",
      "Epoch 1, Batch 30200, Loss: 2.5502, Time: 17968.0s, Step: 30201, GPU: 4.9GB\n",
      "Epoch 1, Batch 30205, Loss: 3.7782, Time: 17971.4s, Step: 30206, GPU: 4.9GB\n",
      "Epoch 1, Batch 30210, Loss: 2.5113, Time: 17973.9s, Step: 30211, GPU: 4.9GB\n",
      "Epoch 1, Batch 30215, Loss: 3.7120, Time: 17977.4s, Step: 30216, GPU: 4.9GB\n",
      "Epoch 1, Batch 30220, Loss: 3.4626, Time: 17979.9s, Step: 30221, GPU: 4.9GB\n",
      "Epoch 1, Batch 30225, Loss: 3.9709, Time: 17983.3s, Step: 30226, GPU: 4.9GB\n",
      "Epoch 1, Batch 30230, Loss: 3.5056, Time: 17985.8s, Step: 30231, GPU: 4.9GB\n",
      "Epoch 1, Batch 30235, Loss: 3.7819, Time: 17989.3s, Step: 30236, GPU: 4.9GB\n",
      "Epoch 1, Batch 30240, Loss: 3.3064, Time: 17991.8s, Step: 30241, GPU: 4.9GB\n",
      "Epoch 1, Batch 30245, Loss: 3.3713, Time: 17995.2s, Step: 30246, GPU: 4.9GB\n",
      "Epoch 1, Batch 30250, Loss: 3.7731, Time: 17997.8s, Step: 30251, GPU: 4.9GB\n",
      "Epoch 1, Batch 30255, Loss: 3.6406, Time: 18001.2s, Step: 30256, GPU: 4.9GB\n",
      "Epoch 1, Batch 30260, Loss: 3.1740, Time: 18003.7s, Step: 30261, GPU: 4.9GB\n",
      "Epoch 1, Batch 30265, Loss: 3.9771, Time: 18007.1s, Step: 30266, GPU: 4.9GB\n",
      "Epoch 1, Batch 30270, Loss: 3.7826, Time: 18009.7s, Step: 30271, GPU: 4.9GB\n",
      "Epoch 1, Batch 30275, Loss: 2.6300, Time: 18013.1s, Step: 30276, GPU: 4.9GB\n",
      "Epoch 1, Batch 30280, Loss: 3.6302, Time: 18015.6s, Step: 30281, GPU: 4.9GB\n",
      "Epoch 1, Batch 30285, Loss: 2.9310, Time: 18019.0s, Step: 30286, GPU: 4.9GB\n",
      "Epoch 1, Batch 30290, Loss: 3.4004, Time: 18021.6s, Step: 30291, GPU: 4.9GB\n",
      "Epoch 1, Batch 30295, Loss: 3.3467, Time: 18024.9s, Step: 30296, GPU: 4.9GB\n",
      "Epoch 1, Batch 30300, Loss: 3.6429, Time: 18027.5s, Step: 30301, GPU: 4.9GB\n",
      "Epoch 1, Batch 30305, Loss: 3.6184, Time: 18030.9s, Step: 30306, GPU: 4.9GB\n",
      "Epoch 1, Batch 30310, Loss: 3.3830, Time: 18033.4s, Step: 30311, GPU: 4.9GB\n",
      "Epoch 1, Batch 30315, Loss: 4.4522, Time: 18036.8s, Step: 30316, GPU: 4.9GB\n",
      "Epoch 1, Batch 30320, Loss: 3.3340, Time: 18039.3s, Step: 30321, GPU: 4.9GB\n",
      "Epoch 1, Batch 30325, Loss: 3.4595, Time: 18042.7s, Step: 30326, GPU: 4.9GB\n",
      "Epoch 1, Batch 30330, Loss: 3.5730, Time: 18045.2s, Step: 30331, GPU: 4.9GB\n",
      "Epoch 1, Batch 30335, Loss: 2.9463, Time: 18048.6s, Step: 30336, GPU: 4.9GB\n",
      "Epoch 1, Batch 30340, Loss: 3.1801, Time: 18051.2s, Step: 30341, GPU: 4.9GB\n",
      "Epoch 1, Batch 30345, Loss: 3.9605, Time: 18054.6s, Step: 30346, GPU: 4.9GB\n",
      "Epoch 1, Batch 30350, Loss: 3.2157, Time: 18057.1s, Step: 30351, GPU: 4.9GB\n",
      "Epoch 1, Batch 30355, Loss: 4.2624, Time: 18060.5s, Step: 30356, GPU: 4.9GB\n",
      "Epoch 1, Batch 30360, Loss: 3.8557, Time: 18063.0s, Step: 30361, GPU: 4.9GB\n",
      "Epoch 1, Batch 30365, Loss: 3.2374, Time: 18066.5s, Step: 30366, GPU: 4.9GB\n",
      "Epoch 1, Batch 30370, Loss: 2.3866, Time: 18069.0s, Step: 30371, GPU: 4.9GB\n",
      "Epoch 1, Batch 30375, Loss: 3.4888, Time: 18072.4s, Step: 30376, GPU: 4.9GB\n",
      "Epoch 1, Batch 30380, Loss: 3.6656, Time: 18075.0s, Step: 30381, GPU: 4.9GB\n",
      "Epoch 1, Batch 30385, Loss: 3.0480, Time: 18078.4s, Step: 30386, GPU: 4.9GB\n",
      "Epoch 1, Batch 30390, Loss: 4.0847, Time: 18080.9s, Step: 30391, GPU: 4.9GB\n",
      "Epoch 1, Batch 30395, Loss: 3.1799, Time: 18084.4s, Step: 30396, GPU: 4.9GB\n",
      "Epoch 1, Batch 30400, Loss: 3.6121, Time: 18086.9s, Step: 30401, GPU: 4.9GB\n",
      "Epoch 1, Batch 30405, Loss: 3.9331, Time: 18090.4s, Step: 30406, GPU: 4.9GB\n",
      "Epoch 1, Batch 30410, Loss: 3.6184, Time: 18092.9s, Step: 30411, GPU: 4.9GB\n",
      "Epoch 1, Batch 30415, Loss: 2.8781, Time: 18096.3s, Step: 30416, GPU: 4.9GB\n",
      "Epoch 1, Batch 30420, Loss: 4.5773, Time: 18098.8s, Step: 30421, GPU: 4.9GB\n",
      "Epoch 1, Batch 30425, Loss: 3.2337, Time: 18102.3s, Step: 30426, GPU: 4.9GB\n",
      "Epoch 1, Batch 30430, Loss: 3.2138, Time: 18104.8s, Step: 30431, GPU: 4.9GB\n",
      "Epoch 1, Batch 30435, Loss: 2.9092, Time: 18108.2s, Step: 30436, GPU: 4.9GB\n",
      "Epoch 1, Batch 30440, Loss: 3.9859, Time: 18110.8s, Step: 30441, GPU: 4.9GB\n",
      "Epoch 1, Batch 30445, Loss: 3.7459, Time: 18114.2s, Step: 30446, GPU: 4.9GB\n",
      "Epoch 1, Batch 30450, Loss: 3.2557, Time: 18116.7s, Step: 30451, GPU: 4.9GB\n",
      "Epoch 1, Batch 30455, Loss: 3.0827, Time: 18120.2s, Step: 30456, GPU: 4.9GB\n",
      "Epoch 1, Batch 30460, Loss: 3.1057, Time: 18122.7s, Step: 30461, GPU: 4.9GB\n",
      "Epoch 1, Batch 30465, Loss: 2.9167, Time: 18126.1s, Step: 30466, GPU: 4.9GB\n",
      "Epoch 1, Batch 30470, Loss: 3.6605, Time: 18128.7s, Step: 30471, GPU: 4.9GB\n",
      "Epoch 1, Batch 30475, Loss: 2.9363, Time: 18132.1s, Step: 30476, GPU: 4.9GB\n",
      "Epoch 1, Batch 30480, Loss: 2.8981, Time: 18134.6s, Step: 30481, GPU: 4.9GB\n",
      "Epoch 1, Batch 30485, Loss: 4.5684, Time: 18138.0s, Step: 30486, GPU: 4.9GB\n",
      "Epoch 1, Batch 30490, Loss: 2.8835, Time: 18140.6s, Step: 30491, GPU: 4.9GB\n",
      "Epoch 1, Batch 30495, Loss: 3.3969, Time: 18144.0s, Step: 30496, GPU: 4.9GB\n",
      "Epoch 1, Batch 30500, Loss: 3.3471, Time: 18146.5s, Step: 30501, GPU: 4.9GB\n",
      "Epoch 1, Batch 30505, Loss: 3.0165, Time: 18149.9s, Step: 30506, GPU: 4.9GB\n",
      "Epoch 1, Batch 30510, Loss: 2.8232, Time: 18152.4s, Step: 30511, GPU: 4.9GB\n",
      "Epoch 1, Batch 30515, Loss: 3.2764, Time: 18155.8s, Step: 30516, GPU: 4.9GB\n",
      "Epoch 1, Batch 30520, Loss: 3.5840, Time: 18158.3s, Step: 30521, GPU: 4.9GB\n",
      "Epoch 1, Batch 30525, Loss: 4.2909, Time: 18161.7s, Step: 30526, GPU: 4.9GB\n",
      "Epoch 1, Batch 30530, Loss: 2.0809, Time: 18164.2s, Step: 30531, GPU: 4.9GB\n",
      "Epoch 1, Batch 30535, Loss: 3.1717, Time: 18167.6s, Step: 30536, GPU: 4.9GB\n",
      "Epoch 1, Batch 30540, Loss: 3.2354, Time: 18170.2s, Step: 30541, GPU: 4.9GB\n",
      "Epoch 1, Batch 30545, Loss: 2.8623, Time: 18173.6s, Step: 30546, GPU: 4.9GB\n",
      "Epoch 1, Batch 30550, Loss: 3.6247, Time: 18176.1s, Step: 30551, GPU: 4.9GB\n",
      "Epoch 1, Batch 30555, Loss: 3.2676, Time: 18179.5s, Step: 30556, GPU: 4.9GB\n",
      "Epoch 1, Batch 30560, Loss: 3.7345, Time: 18182.0s, Step: 30561, GPU: 4.9GB\n",
      "Epoch 1, Batch 30565, Loss: 3.0673, Time: 18185.5s, Step: 30566, GPU: 4.9GB\n",
      "Epoch 1, Batch 30570, Loss: 3.1512, Time: 18188.0s, Step: 30571, GPU: 4.9GB\n",
      "Epoch 1, Batch 30575, Loss: 3.9663, Time: 18191.4s, Step: 30576, GPU: 4.9GB\n",
      "Epoch 1, Batch 30580, Loss: 3.1503, Time: 18193.9s, Step: 30581, GPU: 4.9GB\n",
      "Epoch 1, Batch 30585, Loss: 3.9331, Time: 18197.3s, Step: 30586, GPU: 4.9GB\n",
      "Epoch 1, Batch 30590, Loss: 3.4561, Time: 18199.9s, Step: 30591, GPU: 4.9GB\n",
      "Epoch 1, Batch 30595, Loss: 4.0177, Time: 18203.3s, Step: 30596, GPU: 4.9GB\n",
      "Epoch 1, Batch 30600, Loss: 4.0444, Time: 18205.9s, Step: 30601, GPU: 4.9GB\n",
      "Epoch 1, Batch 30605, Loss: 2.3553, Time: 18209.3s, Step: 30606, GPU: 4.9GB\n",
      "Epoch 1, Batch 30610, Loss: 3.7831, Time: 18211.8s, Step: 30611, GPU: 4.9GB\n",
      "Epoch 1, Batch 30615, Loss: 4.0714, Time: 18215.3s, Step: 30616, GPU: 4.9GB\n",
      "Epoch 1, Batch 30620, Loss: 4.0226, Time: 18217.8s, Step: 30621, GPU: 4.9GB\n",
      "Epoch 1, Batch 30625, Loss: 3.6767, Time: 18221.2s, Step: 30626, GPU: 4.9GB\n",
      "Epoch 1, Batch 30630, Loss: 3.8653, Time: 18223.7s, Step: 30631, GPU: 4.9GB\n",
      "Epoch 1, Batch 30635, Loss: 3.9493, Time: 18227.1s, Step: 30636, GPU: 4.9GB\n",
      "Epoch 1, Batch 30640, Loss: 4.0459, Time: 18229.6s, Step: 30641, GPU: 4.9GB\n",
      "Epoch 1, Batch 30645, Loss: 3.8724, Time: 18233.0s, Step: 30646, GPU: 4.9GB\n",
      "Epoch 1, Batch 30650, Loss: 2.6874, Time: 18235.5s, Step: 30651, GPU: 4.9GB\n",
      "Epoch 1, Batch 30655, Loss: 3.2151, Time: 18238.9s, Step: 30656, GPU: 4.9GB\n",
      "Epoch 1, Batch 30660, Loss: 3.4054, Time: 18241.5s, Step: 30661, GPU: 4.9GB\n",
      "Epoch 1, Batch 30665, Loss: 3.0049, Time: 18244.9s, Step: 30666, GPU: 4.9GB\n",
      "Epoch 1, Batch 30670, Loss: 3.6709, Time: 18247.4s, Step: 30671, GPU: 4.9GB\n",
      "Epoch 1, Batch 30675, Loss: 3.7746, Time: 18250.9s, Step: 30676, GPU: 4.9GB\n",
      "Epoch 1, Batch 30680, Loss: 3.1702, Time: 18253.4s, Step: 30681, GPU: 4.9GB\n",
      "Epoch 1, Batch 30685, Loss: 3.5727, Time: 18256.8s, Step: 30686, GPU: 4.9GB\n",
      "Epoch 1, Batch 30690, Loss: 3.1869, Time: 18259.4s, Step: 30691, GPU: 4.9GB\n",
      "Epoch 1, Batch 30695, Loss: 3.1447, Time: 18262.8s, Step: 30696, GPU: 4.9GB\n",
      "Epoch 1, Batch 30700, Loss: 4.5022, Time: 18265.3s, Step: 30701, GPU: 4.9GB\n",
      "Epoch 1, Batch 30705, Loss: 2.7664, Time: 18268.7s, Step: 30706, GPU: 4.9GB\n",
      "Epoch 1, Batch 30710, Loss: 4.0233, Time: 18271.3s, Step: 30711, GPU: 4.9GB\n",
      "Epoch 1, Batch 30715, Loss: 3.8035, Time: 18274.7s, Step: 30716, GPU: 4.9GB\n",
      "Epoch 1, Batch 30720, Loss: 3.4037, Time: 18277.2s, Step: 30721, GPU: 4.9GB\n",
      "Epoch 1, Batch 30725, Loss: 3.8645, Time: 18280.6s, Step: 30726, GPU: 4.9GB\n",
      "Epoch 1, Batch 30730, Loss: 3.9781, Time: 18283.2s, Step: 30731, GPU: 4.9GB\n",
      "Epoch 1, Batch 30735, Loss: 3.4721, Time: 18286.6s, Step: 30736, GPU: 4.9GB\n",
      "Epoch 1, Batch 30740, Loss: 3.9499, Time: 18289.1s, Step: 30741, GPU: 4.9GB\n",
      "Epoch 1, Batch 30745, Loss: 3.6541, Time: 18292.5s, Step: 30746, GPU: 4.9GB\n",
      "Epoch 1, Batch 30750, Loss: 3.7873, Time: 18295.1s, Step: 30751, GPU: 4.9GB\n",
      "Epoch 1, Batch 30755, Loss: 2.6764, Time: 18298.5s, Step: 30756, GPU: 4.9GB\n",
      "Epoch 1, Batch 30760, Loss: 3.4213, Time: 18301.0s, Step: 30761, GPU: 4.9GB\n",
      "Epoch 1, Batch 30765, Loss: 3.1580, Time: 18304.4s, Step: 30766, GPU: 4.9GB\n",
      "Epoch 1, Batch 30770, Loss: 3.7916, Time: 18307.0s, Step: 30771, GPU: 4.9GB\n",
      "Epoch 1, Batch 30775, Loss: 2.7768, Time: 18310.4s, Step: 30776, GPU: 4.9GB\n",
      "Epoch 1, Batch 30780, Loss: 3.4285, Time: 18312.9s, Step: 30781, GPU: 4.9GB\n",
      "Epoch 1, Batch 30785, Loss: 3.3517, Time: 18316.4s, Step: 30786, GPU: 4.9GB\n",
      "Epoch 1, Batch 30790, Loss: 3.7773, Time: 18318.9s, Step: 30791, GPU: 4.9GB\n",
      "Epoch 1, Batch 30795, Loss: 3.8547, Time: 18322.3s, Step: 30796, GPU: 4.9GB\n",
      "Epoch 1, Batch 30800, Loss: 3.0042, Time: 18324.9s, Step: 30801, GPU: 4.9GB\n",
      "Epoch 1, Batch 30805, Loss: 3.4839, Time: 18328.3s, Step: 30806, GPU: 4.9GB\n",
      "Epoch 1, Batch 30810, Loss: 2.9844, Time: 18330.8s, Step: 30811, GPU: 4.9GB\n",
      "Epoch 1, Batch 30815, Loss: 2.9217, Time: 18334.3s, Step: 30816, GPU: 4.9GB\n",
      "Epoch 1, Batch 30820, Loss: 4.3099, Time: 18336.8s, Step: 30821, GPU: 4.9GB\n",
      "Epoch 1, Batch 30825, Loss: 4.2897, Time: 18340.2s, Step: 30826, GPU: 4.9GB\n",
      "Epoch 1, Batch 30830, Loss: 3.1539, Time: 18342.7s, Step: 30831, GPU: 4.9GB\n",
      "Epoch 1, Batch 30835, Loss: 3.9489, Time: 18346.1s, Step: 30836, GPU: 4.9GB\n",
      "Epoch 1, Batch 30840, Loss: 4.4004, Time: 18348.7s, Step: 30841, GPU: 4.9GB\n",
      "Epoch 1, Batch 30845, Loss: 4.6159, Time: 18352.1s, Step: 30846, GPU: 4.9GB\n",
      "Epoch 1, Batch 30850, Loss: 3.3326, Time: 18354.6s, Step: 30851, GPU: 4.9GB\n",
      "Epoch 1, Batch 30855, Loss: 3.5836, Time: 18358.0s, Step: 30856, GPU: 4.9GB\n",
      "Epoch 1, Batch 30860, Loss: 3.3356, Time: 18360.6s, Step: 30861, GPU: 4.9GB\n",
      "Epoch 1, Batch 30865, Loss: 3.6101, Time: 18364.0s, Step: 30866, GPU: 4.9GB\n",
      "Epoch 1, Batch 30870, Loss: 3.4512, Time: 18366.5s, Step: 30871, GPU: 4.9GB\n",
      "Epoch 1, Batch 30875, Loss: 3.9528, Time: 18369.9s, Step: 30876, GPU: 4.9GB\n",
      "Epoch 1, Batch 30880, Loss: 3.8705, Time: 18372.5s, Step: 30881, GPU: 4.9GB\n",
      "Epoch 1, Batch 30885, Loss: 3.3839, Time: 18375.9s, Step: 30886, GPU: 4.9GB\n",
      "Epoch 1, Batch 30890, Loss: 3.8298, Time: 18378.4s, Step: 30891, GPU: 4.9GB\n",
      "Epoch 1, Batch 30895, Loss: 3.0539, Time: 18381.8s, Step: 30896, GPU: 4.9GB\n",
      "Epoch 1, Batch 30900, Loss: 4.1336, Time: 18384.4s, Step: 30901, GPU: 4.9GB\n",
      "Epoch 1, Batch 30905, Loss: 3.5717, Time: 18387.8s, Step: 30906, GPU: 4.9GB\n",
      "Epoch 1, Batch 30910, Loss: 3.7847, Time: 18390.3s, Step: 30911, GPU: 4.9GB\n",
      "Epoch 1, Batch 30915, Loss: 2.8281, Time: 18393.7s, Step: 30916, GPU: 4.9GB\n",
      "Epoch 1, Batch 30920, Loss: 3.1631, Time: 18396.2s, Step: 30921, GPU: 4.9GB\n",
      "Epoch 1, Batch 30925, Loss: 3.4810, Time: 18399.6s, Step: 30926, GPU: 4.9GB\n",
      "Epoch 1, Batch 30930, Loss: 3.3691, Time: 18402.1s, Step: 30931, GPU: 4.9GB\n",
      "Epoch 1, Batch 30935, Loss: 3.5764, Time: 18405.5s, Step: 30936, GPU: 4.9GB\n",
      "Epoch 1, Batch 30940, Loss: 3.8162, Time: 18408.0s, Step: 30941, GPU: 4.9GB\n",
      "Epoch 1, Batch 30945, Loss: 2.7992, Time: 18411.5s, Step: 30946, GPU: 4.9GB\n",
      "Epoch 1, Batch 30950, Loss: 3.8693, Time: 18414.0s, Step: 30951, GPU: 4.9GB\n",
      "Epoch 1, Batch 30955, Loss: 3.3845, Time: 18417.4s, Step: 30956, GPU: 4.9GB\n",
      "Epoch 1, Batch 30960, Loss: 3.8868, Time: 18419.9s, Step: 30961, GPU: 4.9GB\n",
      "Epoch 1, Batch 30965, Loss: 4.0252, Time: 18423.3s, Step: 30966, GPU: 4.9GB\n",
      "Epoch 1, Batch 30970, Loss: 2.5413, Time: 18425.9s, Step: 30971, GPU: 4.9GB\n",
      "Epoch 1, Batch 30975, Loss: 3.5467, Time: 18429.3s, Step: 30976, GPU: 4.9GB\n",
      "Epoch 1, Batch 30980, Loss: 3.5974, Time: 18431.8s, Step: 30981, GPU: 4.9GB\n",
      "Epoch 1, Batch 30985, Loss: 3.7452, Time: 18435.2s, Step: 30986, GPU: 4.9GB\n",
      "Epoch 1, Batch 30990, Loss: 3.4214, Time: 18437.7s, Step: 30991, GPU: 4.9GB\n",
      "Epoch 1, Batch 30995, Loss: 3.1294, Time: 18441.2s, Step: 30996, GPU: 4.9GB\n",
      "Epoch 1, Batch 31000, Loss: 3.7023, Time: 18443.8s, Step: 31001, GPU: 4.9GB\n",
      "Epoch 1, Batch 31005, Loss: 2.3190, Time: 18447.2s, Step: 31006, GPU: 4.9GB\n",
      "Epoch 1, Batch 31010, Loss: 4.2282, Time: 18449.7s, Step: 31011, GPU: 4.9GB\n",
      "Epoch 1, Batch 31015, Loss: 2.3428, Time: 18453.1s, Step: 31016, GPU: 4.9GB\n",
      "Epoch 1, Batch 31020, Loss: 2.8054, Time: 18455.7s, Step: 31021, GPU: 4.9GB\n",
      "Epoch 1, Batch 31025, Loss: 3.5162, Time: 18459.1s, Step: 31026, GPU: 4.9GB\n",
      "Epoch 1, Batch 31030, Loss: 3.8566, Time: 18461.6s, Step: 31031, GPU: 4.9GB\n",
      "Epoch 1, Batch 31035, Loss: 3.0794, Time: 18465.0s, Step: 31036, GPU: 4.9GB\n",
      "Epoch 1, Batch 31040, Loss: 3.7067, Time: 18467.5s, Step: 31041, GPU: 4.9GB\n",
      "Epoch 1, Batch 31045, Loss: 4.0982, Time: 18470.9s, Step: 31046, GPU: 4.9GB\n",
      "Epoch 1, Batch 31050, Loss: 4.0998, Time: 18473.4s, Step: 31051, GPU: 4.9GB\n",
      "Epoch 1, Batch 31055, Loss: 3.3310, Time: 18476.9s, Step: 31056, GPU: 4.9GB\n",
      "Epoch 1, Batch 31060, Loss: 3.1554, Time: 18479.4s, Step: 31061, GPU: 4.9GB\n",
      "Epoch 1, Batch 31065, Loss: 3.5075, Time: 18482.8s, Step: 31066, GPU: 4.9GB\n",
      "Epoch 1, Batch 31070, Loss: 3.6028, Time: 18485.3s, Step: 31071, GPU: 4.9GB\n",
      "Epoch 1, Batch 31075, Loss: 4.1662, Time: 18488.7s, Step: 31076, GPU: 4.9GB\n",
      "Epoch 1, Batch 31080, Loss: 3.5144, Time: 18491.2s, Step: 31081, GPU: 4.9GB\n",
      "Epoch 1, Batch 31085, Loss: 4.1852, Time: 18494.7s, Step: 31086, GPU: 4.9GB\n",
      "Epoch 1, Batch 31090, Loss: 3.0879, Time: 18497.2s, Step: 31091, GPU: 4.9GB\n",
      "Epoch 1, Batch 31095, Loss: 3.2774, Time: 18500.6s, Step: 31096, GPU: 4.9GB\n",
      "Epoch 1, Batch 31100, Loss: 3.3243, Time: 18503.1s, Step: 31101, GPU: 4.9GB\n",
      "Epoch 1, Batch 31105, Loss: 3.8234, Time: 18506.5s, Step: 31106, GPU: 4.9GB\n",
      "Epoch 1, Batch 31110, Loss: 3.0772, Time: 18509.0s, Step: 31111, GPU: 4.9GB\n",
      "Epoch 1, Batch 31115, Loss: 3.2086, Time: 18512.4s, Step: 31116, GPU: 4.9GB\n",
      "Epoch 1, Batch 31120, Loss: 3.6007, Time: 18514.9s, Step: 31121, GPU: 4.9GB\n",
      "Epoch 1, Batch 31125, Loss: 3.6428, Time: 18518.3s, Step: 31126, GPU: 4.9GB\n",
      "Epoch 1, Batch 31130, Loss: 3.0995, Time: 18520.8s, Step: 31131, GPU: 4.9GB\n",
      "Epoch 1, Batch 31135, Loss: 3.6957, Time: 18524.3s, Step: 31136, GPU: 4.9GB\n",
      "Epoch 1, Batch 31140, Loss: 2.6013, Time: 18526.8s, Step: 31141, GPU: 4.9GB\n",
      "Epoch 1, Batch 31145, Loss: 3.0155, Time: 18530.2s, Step: 31146, GPU: 4.9GB\n",
      "Epoch 1, Batch 31150, Loss: 3.0168, Time: 18532.7s, Step: 31151, GPU: 4.9GB\n",
      "Epoch 1, Batch 31155, Loss: 3.7029, Time: 18536.1s, Step: 31156, GPU: 4.9GB\n",
      "Epoch 1, Batch 31160, Loss: 3.1265, Time: 18538.6s, Step: 31161, GPU: 4.9GB\n",
      "Epoch 1, Batch 31165, Loss: 3.2689, Time: 18542.0s, Step: 31166, GPU: 4.9GB\n",
      "Epoch 1, Batch 31170, Loss: 3.0717, Time: 18544.5s, Step: 31171, GPU: 4.9GB\n",
      "Epoch 1, Batch 31175, Loss: 2.8701, Time: 18547.9s, Step: 31176, GPU: 4.9GB\n",
      "Epoch 1, Batch 31180, Loss: 4.1553, Time: 18550.4s, Step: 31181, GPU: 4.9GB\n",
      "Epoch 1, Batch 31185, Loss: 3.8751, Time: 18553.8s, Step: 31186, GPU: 4.9GB\n",
      "Epoch 1, Batch 31190, Loss: 2.6770, Time: 18556.3s, Step: 31191, GPU: 4.9GB\n",
      "Epoch 1, Batch 31195, Loss: 2.5754, Time: 18559.7s, Step: 31196, GPU: 4.9GB\n",
      "Epoch 1, Batch 31200, Loss: 3.8096, Time: 18562.3s, Step: 31201, GPU: 4.9GB\n",
      "Epoch 1, Batch 31205, Loss: 3.5607, Time: 18565.7s, Step: 31206, GPU: 4.9GB\n",
      "Epoch 1, Batch 31210, Loss: 3.5057, Time: 18568.2s, Step: 31211, GPU: 4.9GB\n",
      "Epoch 1, Batch 31215, Loss: 3.0993, Time: 18571.7s, Step: 31216, GPU: 4.9GB\n",
      "Epoch 1, Batch 31220, Loss: 3.4390, Time: 18574.2s, Step: 31221, GPU: 4.9GB\n",
      "Epoch 1, Batch 31225, Loss: 4.0537, Time: 18577.6s, Step: 31226, GPU: 4.9GB\n",
      "Epoch 1, Batch 31230, Loss: 2.6713, Time: 18580.1s, Step: 31231, GPU: 4.9GB\n",
      "Epoch 1, Batch 31235, Loss: 3.6056, Time: 18583.5s, Step: 31236, GPU: 4.9GB\n",
      "Epoch 1, Batch 31240, Loss: 4.0738, Time: 18586.0s, Step: 31241, GPU: 4.9GB\n",
      "Epoch 1, Batch 31245, Loss: 3.6018, Time: 18589.4s, Step: 31246, GPU: 4.9GB\n",
      "Epoch 1, Batch 31250, Loss: 3.6127, Time: 18591.9s, Step: 31251, GPU: 4.9GB\n",
      "Epoch 1, Batch 31255, Loss: 3.1049, Time: 18595.3s, Step: 31256, GPU: 4.9GB\n",
      "Epoch 1, Batch 31260, Loss: 3.9131, Time: 18597.8s, Step: 31261, GPU: 4.9GB\n",
      "Epoch 1, Batch 31265, Loss: 3.5782, Time: 18601.2s, Step: 31266, GPU: 4.9GB\n",
      "Epoch 1, Batch 31270, Loss: 3.5153, Time: 18603.7s, Step: 31271, GPU: 4.9GB\n",
      "Epoch 1, Batch 31275, Loss: 2.7466, Time: 18607.2s, Step: 31276, GPU: 4.9GB\n",
      "Epoch 1, Batch 31280, Loss: 3.3669, Time: 18609.7s, Step: 31281, GPU: 4.9GB\n",
      "Epoch 1, Batch 31285, Loss: 3.3192, Time: 18613.1s, Step: 31286, GPU: 4.9GB\n",
      "Epoch 1, Batch 31290, Loss: 3.4158, Time: 18615.7s, Step: 31291, GPU: 4.9GB\n",
      "Epoch 1, Batch 31295, Loss: 3.5398, Time: 18619.1s, Step: 31296, GPU: 4.9GB\n",
      "Epoch 1, Batch 31300, Loss: 4.2185, Time: 18621.7s, Step: 31301, GPU: 4.9GB\n",
      "Epoch 1, Batch 31305, Loss: 3.4647, Time: 18625.1s, Step: 31306, GPU: 4.9GB\n",
      "Epoch 1, Batch 31310, Loss: 3.4915, Time: 18627.6s, Step: 31311, GPU: 4.9GB\n",
      "Epoch 1, Batch 31315, Loss: 3.2782, Time: 18631.2s, Step: 31316, GPU: 4.9GB\n",
      "Epoch 1, Batch 31320, Loss: 3.9769, Time: 18633.7s, Step: 31321, GPU: 4.9GB\n",
      "Epoch 1, Batch 31325, Loss: 3.2963, Time: 18637.1s, Step: 31326, GPU: 4.9GB\n",
      "Epoch 1, Batch 31330, Loss: 3.1456, Time: 18639.6s, Step: 31331, GPU: 4.9GB\n",
      "Epoch 1, Batch 31335, Loss: 2.9599, Time: 18643.0s, Step: 31336, GPU: 4.9GB\n",
      "Epoch 1, Batch 31340, Loss: 3.3688, Time: 18645.5s, Step: 31341, GPU: 4.9GB\n",
      "Epoch 1, Batch 31345, Loss: 3.3335, Time: 18649.0s, Step: 31346, GPU: 4.9GB\n",
      "Epoch 1, Batch 31350, Loss: 3.5339, Time: 18651.5s, Step: 31351, GPU: 4.9GB\n",
      "Epoch 1, Batch 31355, Loss: 3.7563, Time: 18654.9s, Step: 31356, GPU: 4.9GB\n",
      "Epoch 1, Batch 31360, Loss: 4.0472, Time: 18657.4s, Step: 31361, GPU: 4.9GB\n",
      "Epoch 1, Batch 31365, Loss: 3.7130, Time: 18660.8s, Step: 31366, GPU: 4.9GB\n",
      "Epoch 1, Batch 31370, Loss: 3.6888, Time: 18663.4s, Step: 31371, GPU: 4.9GB\n",
      "Epoch 1, Batch 31375, Loss: 4.0503, Time: 18666.8s, Step: 31376, GPU: 4.9GB\n",
      "Epoch 1, Batch 31380, Loss: 3.3596, Time: 18669.3s, Step: 31381, GPU: 4.9GB\n",
      "Epoch 1, Batch 31385, Loss: 3.7902, Time: 18672.7s, Step: 31386, GPU: 4.9GB\n",
      "Epoch 1, Batch 31390, Loss: 3.1094, Time: 18675.3s, Step: 31391, GPU: 4.9GB\n",
      "Epoch 1, Batch 31395, Loss: 3.8237, Time: 18678.7s, Step: 31396, GPU: 4.9GB\n",
      "Epoch 1, Batch 31400, Loss: 4.0478, Time: 18681.3s, Step: 31401, GPU: 4.9GB\n",
      "Epoch 1, Batch 31405, Loss: 3.1974, Time: 18684.6s, Step: 31406, GPU: 4.9GB\n",
      "Epoch 1, Batch 31410, Loss: 3.1401, Time: 18687.2s, Step: 31411, GPU: 4.9GB\n",
      "Epoch 1, Batch 31415, Loss: 3.7041, Time: 18690.6s, Step: 31416, GPU: 4.9GB\n",
      "Epoch 1, Batch 31420, Loss: 3.7799, Time: 18693.1s, Step: 31421, GPU: 4.9GB\n",
      "Epoch 1, Batch 31425, Loss: 3.9484, Time: 18696.5s, Step: 31426, GPU: 4.9GB\n",
      "Epoch 1, Batch 31430, Loss: 4.1998, Time: 18699.0s, Step: 31431, GPU: 4.9GB\n",
      "Epoch 1, Batch 31435, Loss: 3.0880, Time: 18702.4s, Step: 31436, GPU: 4.9GB\n",
      "Epoch 1, Batch 31440, Loss: 3.5731, Time: 18704.9s, Step: 31441, GPU: 4.9GB\n",
      "Epoch 1, Batch 31445, Loss: 3.8452, Time: 18708.3s, Step: 31446, GPU: 4.9GB\n",
      "Epoch 1, Batch 31450, Loss: 3.4645, Time: 18710.8s, Step: 31451, GPU: 4.9GB\n",
      "Epoch 1, Batch 31455, Loss: 3.9926, Time: 18714.2s, Step: 31456, GPU: 4.9GB\n",
      "Epoch 1, Batch 31460, Loss: 3.4048, Time: 18716.7s, Step: 31461, GPU: 4.9GB\n",
      "Epoch 1, Batch 31465, Loss: 4.2276, Time: 18720.2s, Step: 31466, GPU: 4.9GB\n",
      "Epoch 1, Batch 31470, Loss: 3.2853, Time: 18722.7s, Step: 31471, GPU: 4.9GB\n",
      "Epoch 1, Batch 31475, Loss: 3.5570, Time: 18726.1s, Step: 31476, GPU: 4.9GB\n",
      "Epoch 1, Batch 31480, Loss: 3.1283, Time: 18728.6s, Step: 31481, GPU: 4.9GB\n",
      "Epoch 1, Batch 31485, Loss: 3.1693, Time: 18732.0s, Step: 31486, GPU: 4.9GB\n",
      "Epoch 1, Batch 31490, Loss: 2.7316, Time: 18734.5s, Step: 31491, GPU: 4.9GB\n",
      "Epoch 1, Batch 31495, Loss: 2.9219, Time: 18737.9s, Step: 31496, GPU: 4.9GB\n",
      "Epoch 1, Batch 31500, Loss: 2.7271, Time: 18740.5s, Step: 31501, GPU: 4.9GB\n",
      "Epoch 1, Batch 31505, Loss: 3.2399, Time: 18743.9s, Step: 31506, GPU: 4.9GB\n",
      "Epoch 1, Batch 31510, Loss: 3.3381, Time: 18746.4s, Step: 31511, GPU: 4.9GB\n",
      "Epoch 1, Batch 31515, Loss: 3.6029, Time: 18749.8s, Step: 31516, GPU: 4.9GB\n",
      "Epoch 1, Batch 31520, Loss: 3.5253, Time: 18752.4s, Step: 31521, GPU: 4.9GB\n",
      "Epoch 1, Batch 31525, Loss: 3.9828, Time: 18755.8s, Step: 31526, GPU: 4.9GB\n",
      "Epoch 1, Batch 31530, Loss: 3.9868, Time: 18758.3s, Step: 31531, GPU: 4.9GB\n",
      "Epoch 1, Batch 31535, Loss: 3.2199, Time: 18761.8s, Step: 31536, GPU: 4.9GB\n",
      "Epoch 1, Batch 31540, Loss: 3.7799, Time: 18764.3s, Step: 31541, GPU: 4.9GB\n",
      "Epoch 1, Batch 31545, Loss: 3.3221, Time: 18767.7s, Step: 31546, GPU: 4.9GB\n",
      "Epoch 1, Batch 31550, Loss: 4.0120, Time: 18770.2s, Step: 31551, GPU: 4.9GB\n",
      "Epoch 1, Batch 31555, Loss: 4.1190, Time: 18773.6s, Step: 31556, GPU: 4.9GB\n",
      "Epoch 1, Batch 31560, Loss: 4.0950, Time: 18776.2s, Step: 31561, GPU: 4.9GB\n",
      "Epoch 1, Batch 31565, Loss: 3.0844, Time: 18779.6s, Step: 31566, GPU: 4.9GB\n",
      "Epoch 1, Batch 31570, Loss: 4.4894, Time: 18782.1s, Step: 31571, GPU: 4.9GB\n",
      "Epoch 1, Batch 31575, Loss: 3.7284, Time: 18785.6s, Step: 31576, GPU: 4.9GB\n",
      "Epoch 1, Batch 31580, Loss: 3.2892, Time: 18788.1s, Step: 31581, GPU: 4.9GB\n",
      "Epoch 1, Batch 31585, Loss: 2.5674, Time: 18791.5s, Step: 31586, GPU: 4.9GB\n",
      "Epoch 1, Batch 31590, Loss: 3.7034, Time: 18794.0s, Step: 31591, GPU: 4.9GB\n",
      "Epoch 1, Batch 31595, Loss: 2.8995, Time: 18797.4s, Step: 31596, GPU: 4.9GB\n",
      "Epoch 1, Batch 31600, Loss: 2.4243, Time: 18800.1s, Step: 31601, GPU: 4.9GB\n",
      "Epoch 1, Batch 31605, Loss: 3.3721, Time: 18803.5s, Step: 31606, GPU: 4.9GB\n",
      "Epoch 1, Batch 31610, Loss: 3.1306, Time: 18806.0s, Step: 31611, GPU: 4.9GB\n",
      "Epoch 1, Batch 31615, Loss: 3.0566, Time: 18809.5s, Step: 31616, GPU: 4.9GB\n",
      "Epoch 1, Batch 31620, Loss: 3.9505, Time: 18812.0s, Step: 31621, GPU: 4.9GB\n",
      "Epoch 1, Batch 31625, Loss: 3.0605, Time: 18815.4s, Step: 31626, GPU: 4.9GB\n",
      "Epoch 1, Batch 31630, Loss: 3.6311, Time: 18817.9s, Step: 31631, GPU: 4.9GB\n",
      "Epoch 1, Batch 31635, Loss: 3.6747, Time: 18821.3s, Step: 31636, GPU: 4.9GB\n",
      "Epoch 1, Batch 31640, Loss: 3.9841, Time: 18823.8s, Step: 31641, GPU: 4.9GB\n",
      "Epoch 1, Batch 31645, Loss: 3.5937, Time: 18827.2s, Step: 31646, GPU: 4.9GB\n",
      "Epoch 1, Batch 31650, Loss: 3.9201, Time: 18829.8s, Step: 31651, GPU: 4.9GB\n",
      "Epoch 1, Batch 31655, Loss: 3.1532, Time: 18833.2s, Step: 31656, GPU: 4.9GB\n",
      "Epoch 1, Batch 31660, Loss: 3.3350, Time: 18835.7s, Step: 31661, GPU: 4.9GB\n",
      "Epoch 1, Batch 31665, Loss: 4.1303, Time: 18839.2s, Step: 31666, GPU: 4.9GB\n",
      "Epoch 1, Batch 31670, Loss: 3.3076, Time: 18841.7s, Step: 31671, GPU: 4.9GB\n",
      "Epoch 1, Batch 31675, Loss: 2.5610, Time: 18845.1s, Step: 31676, GPU: 4.9GB\n",
      "Epoch 1, Batch 31680, Loss: 3.0462, Time: 18847.6s, Step: 31681, GPU: 4.9GB\n",
      "Epoch 1, Batch 31685, Loss: 3.2170, Time: 18851.0s, Step: 31686, GPU: 4.9GB\n",
      "Epoch 1, Batch 31690, Loss: 3.6977, Time: 18853.6s, Step: 31691, GPU: 4.9GB\n",
      "Epoch 1, Batch 31695, Loss: 2.3109, Time: 18857.0s, Step: 31696, GPU: 4.9GB\n",
      "Epoch 1, Batch 31700, Loss: 3.7560, Time: 18859.5s, Step: 31701, GPU: 4.9GB\n",
      "Epoch 1, Batch 31705, Loss: 3.5039, Time: 18863.0s, Step: 31706, GPU: 4.9GB\n",
      "Epoch 1, Batch 31710, Loss: 2.9360, Time: 18865.5s, Step: 31711, GPU: 4.9GB\n",
      "Epoch 1, Batch 31715, Loss: 3.7138, Time: 18868.9s, Step: 31716, GPU: 4.9GB\n",
      "Epoch 1, Batch 31720, Loss: 3.6232, Time: 18871.4s, Step: 31721, GPU: 4.9GB\n",
      "Epoch 1, Batch 31725, Loss: 3.3507, Time: 18874.8s, Step: 31726, GPU: 4.9GB\n",
      "Epoch 1, Batch 31730, Loss: 3.3155, Time: 18877.4s, Step: 31731, GPU: 4.9GB\n",
      "Epoch 1, Batch 31735, Loss: 3.8070, Time: 18880.8s, Step: 31736, GPU: 4.9GB\n",
      "Epoch 1, Batch 31740, Loss: 3.9968, Time: 18883.3s, Step: 31741, GPU: 4.9GB\n",
      "Epoch 1, Batch 31745, Loss: 3.8356, Time: 18886.7s, Step: 31746, GPU: 4.9GB\n",
      "Epoch 1, Batch 31750, Loss: 3.4684, Time: 18889.2s, Step: 31751, GPU: 4.9GB\n",
      "Epoch 1, Batch 31755, Loss: 3.5255, Time: 18892.7s, Step: 31756, GPU: 4.9GB\n",
      "Epoch 1, Batch 31760, Loss: 3.4126, Time: 18895.2s, Step: 31761, GPU: 4.9GB\n",
      "Epoch 1, Batch 31765, Loss: 3.6557, Time: 18898.6s, Step: 31766, GPU: 4.9GB\n",
      "Epoch 1, Batch 31770, Loss: 2.4939, Time: 18901.1s, Step: 31771, GPU: 4.9GB\n",
      "Epoch 1, Batch 31775, Loss: 3.6713, Time: 18904.5s, Step: 31776, GPU: 4.9GB\n",
      "Epoch 1, Batch 31780, Loss: 4.0922, Time: 18907.0s, Step: 31781, GPU: 4.9GB\n",
      "Epoch 1, Batch 31785, Loss: 3.3145, Time: 18910.5s, Step: 31786, GPU: 4.9GB\n",
      "Epoch 1, Batch 31790, Loss: 3.4485, Time: 18913.0s, Step: 31791, GPU: 4.9GB\n",
      "Epoch 1, Batch 31795, Loss: 4.0327, Time: 18916.5s, Step: 31796, GPU: 4.9GB\n",
      "Epoch 1, Batch 31800, Loss: 4.0418, Time: 18919.1s, Step: 31801, GPU: 4.9GB\n",
      "Epoch 1, Batch 31805, Loss: 3.2853, Time: 18922.5s, Step: 31806, GPU: 4.9GB\n",
      "Epoch 1, Batch 31810, Loss: 3.0918, Time: 18925.1s, Step: 31811, GPU: 4.9GB\n",
      "Epoch 1, Batch 31815, Loss: 3.8773, Time: 18928.5s, Step: 31816, GPU: 4.9GB\n",
      "Epoch 1, Batch 31820, Loss: 4.1296, Time: 18931.0s, Step: 31821, GPU: 4.9GB\n",
      "Epoch 1, Batch 31825, Loss: 3.4634, Time: 18934.5s, Step: 31826, GPU: 4.9GB\n",
      "Epoch 1, Batch 31830, Loss: 3.5087, Time: 18937.0s, Step: 31831, GPU: 4.9GB\n",
      "Epoch 1, Batch 31835, Loss: 3.8813, Time: 18940.4s, Step: 31836, GPU: 4.9GB\n",
      "Epoch 1, Batch 31840, Loss: 3.0905, Time: 18942.9s, Step: 31841, GPU: 4.9GB\n",
      "Epoch 1, Batch 31845, Loss: 2.9952, Time: 18946.4s, Step: 31846, GPU: 4.9GB\n",
      "Epoch 1, Batch 31850, Loss: 4.2210, Time: 18948.9s, Step: 31851, GPU: 4.9GB\n",
      "Epoch 1, Batch 31855, Loss: 3.3640, Time: 18952.3s, Step: 31856, GPU: 4.9GB\n",
      "Epoch 1, Batch 31860, Loss: 3.4525, Time: 18954.8s, Step: 31861, GPU: 4.9GB\n",
      "Epoch 1, Batch 31865, Loss: 3.3669, Time: 18958.3s, Step: 31866, GPU: 4.9GB\n",
      "Epoch 1, Batch 31870, Loss: 3.6022, Time: 18960.8s, Step: 31871, GPU: 4.9GB\n",
      "Epoch 1, Batch 31875, Loss: 2.8856, Time: 18964.2s, Step: 31876, GPU: 4.9GB\n",
      "Epoch 1, Batch 31880, Loss: 4.0118, Time: 18966.7s, Step: 31881, GPU: 4.9GB\n",
      "Epoch 1, Batch 31885, Loss: 3.6115, Time: 18970.1s, Step: 31886, GPU: 4.9GB\n",
      "Epoch 1, Batch 31890, Loss: 3.5826, Time: 18972.7s, Step: 31891, GPU: 4.9GB\n",
      "Epoch 1, Batch 31895, Loss: 3.3937, Time: 18976.1s, Step: 31896, GPU: 4.9GB\n",
      "Epoch 1, Batch 31900, Loss: 3.4978, Time: 18978.6s, Step: 31901, GPU: 4.9GB\n",
      "Epoch 1, Batch 31905, Loss: 3.4751, Time: 18982.0s, Step: 31906, GPU: 4.9GB\n",
      "Epoch 1, Batch 31910, Loss: 3.2292, Time: 18984.5s, Step: 31911, GPU: 4.9GB\n",
      "Epoch 1, Batch 31915, Loss: 3.0777, Time: 18988.0s, Step: 31916, GPU: 4.9GB\n",
      "Epoch 1, Batch 31920, Loss: 3.7592, Time: 18990.5s, Step: 31921, GPU: 4.9GB\n",
      "Epoch 1, Batch 31925, Loss: 3.4609, Time: 18993.8s, Step: 31926, GPU: 4.9GB\n",
      "Epoch 1, Batch 31930, Loss: 3.5948, Time: 18996.3s, Step: 31931, GPU: 4.9GB\n",
      "Epoch 1, Batch 31935, Loss: 3.0483, Time: 18999.8s, Step: 31936, GPU: 4.9GB\n",
      "Epoch 1, Batch 31940, Loss: 3.2361, Time: 19002.3s, Step: 31941, GPU: 4.9GB\n",
      "Epoch 1, Batch 31945, Loss: 3.0375, Time: 19005.7s, Step: 31946, GPU: 4.9GB\n",
      "Epoch 1, Batch 31950, Loss: 3.6546, Time: 19008.2s, Step: 31951, GPU: 4.9GB\n",
      "Epoch 1, Batch 31955, Loss: 3.1000, Time: 19011.6s, Step: 31956, GPU: 4.9GB\n",
      "Epoch 1, Batch 31960, Loss: 3.9348, Time: 19014.1s, Step: 31961, GPU: 4.9GB\n",
      "Epoch 1, Batch 31965, Loss: 4.3417, Time: 19017.5s, Step: 31966, GPU: 4.9GB\n",
      "Epoch 1, Batch 31970, Loss: 4.0897, Time: 19020.0s, Step: 31971, GPU: 4.9GB\n",
      "Epoch 1, Batch 31975, Loss: 3.7066, Time: 19023.4s, Step: 31976, GPU: 4.9GB\n",
      "Epoch 1, Batch 31980, Loss: 3.6235, Time: 19025.9s, Step: 31981, GPU: 4.9GB\n",
      "Epoch 1, Batch 31985, Loss: 3.2903, Time: 19029.4s, Step: 31986, GPU: 4.9GB\n",
      "Epoch 1, Batch 31990, Loss: 3.5121, Time: 19031.9s, Step: 31991, GPU: 4.9GB\n",
      "Epoch 1, Batch 31995, Loss: 3.0478, Time: 19035.3s, Step: 31996, GPU: 4.9GB\n",
      "Epoch 1, Batch 32000, Loss: 2.8055, Time: 19037.9s, Step: 32001, GPU: 4.9GB\n",
      "Epoch 1, Batch 32005, Loss: 3.5086, Time: 19041.3s, Step: 32006, GPU: 4.9GB\n",
      "Epoch 1, Batch 32010, Loss: 4.0565, Time: 19043.8s, Step: 32011, GPU: 4.9GB\n",
      "Epoch 1, Batch 32015, Loss: 3.9786, Time: 19047.3s, Step: 32016, GPU: 4.9GB\n",
      "Epoch 1, Batch 32020, Loss: 3.9789, Time: 19049.8s, Step: 32021, GPU: 4.9GB\n",
      "Epoch 1, Batch 32025, Loss: 3.6682, Time: 19053.2s, Step: 32026, GPU: 4.9GB\n",
      "Epoch 1, Batch 32030, Loss: 1.9834, Time: 19055.7s, Step: 32031, GPU: 4.9GB\n",
      "Epoch 1, Batch 32035, Loss: 4.2151, Time: 19059.1s, Step: 32036, GPU: 4.9GB\n",
      "Epoch 1, Batch 32040, Loss: 3.2536, Time: 19061.7s, Step: 32041, GPU: 4.9GB\n",
      "Epoch 1, Batch 32045, Loss: 3.6899, Time: 19065.1s, Step: 32046, GPU: 4.9GB\n",
      "Epoch 1, Batch 32050, Loss: 4.2016, Time: 19067.6s, Step: 32051, GPU: 4.9GB\n",
      "Epoch 1, Batch 32055, Loss: 3.8912, Time: 19071.0s, Step: 32056, GPU: 4.9GB\n",
      "Epoch 1, Batch 32060, Loss: 3.4979, Time: 19073.5s, Step: 32061, GPU: 4.9GB\n",
      "Epoch 1, Batch 32065, Loss: 3.6904, Time: 19077.0s, Step: 32066, GPU: 4.9GB\n",
      "Epoch 1, Batch 32070, Loss: 3.9099, Time: 19079.5s, Step: 32071, GPU: 4.9GB\n",
      "Epoch 1, Batch 32075, Loss: 4.2720, Time: 19082.9s, Step: 32076, GPU: 4.9GB\n",
      "Epoch 1, Batch 32080, Loss: 3.7302, Time: 19085.4s, Step: 32081, GPU: 4.9GB\n",
      "Epoch 1, Batch 32085, Loss: 4.0436, Time: 19088.8s, Step: 32086, GPU: 4.9GB\n",
      "Epoch 1, Batch 32090, Loss: 3.7318, Time: 19091.3s, Step: 32091, GPU: 4.9GB\n",
      "Epoch 1, Batch 32095, Loss: 3.5934, Time: 19094.8s, Step: 32096, GPU: 4.9GB\n",
      "Epoch 1, Batch 32100, Loss: 3.7799, Time: 19097.3s, Step: 32101, GPU: 4.9GB\n",
      "Epoch 1, Batch 32105, Loss: 3.2922, Time: 19100.7s, Step: 32106, GPU: 4.9GB\n",
      "Epoch 1, Batch 32110, Loss: 2.9986, Time: 19103.2s, Step: 32111, GPU: 4.9GB\n",
      "Epoch 1, Batch 32115, Loss: 4.5209, Time: 19106.7s, Step: 32116, GPU: 4.9GB\n",
      "Epoch 1, Batch 32120, Loss: 3.3062, Time: 19109.2s, Step: 32121, GPU: 4.9GB\n",
      "Epoch 1, Batch 32125, Loss: 3.6509, Time: 19112.7s, Step: 32126, GPU: 4.9GB\n",
      "Epoch 1, Batch 32130, Loss: 3.5902, Time: 19115.3s, Step: 32131, GPU: 4.9GB\n",
      "Epoch 1, Batch 32135, Loss: 3.4822, Time: 19118.7s, Step: 32136, GPU: 4.9GB\n",
      "Epoch 1, Batch 32140, Loss: 3.6662, Time: 19121.2s, Step: 32141, GPU: 4.9GB\n",
      "Epoch 1, Batch 32145, Loss: 2.4884, Time: 19124.6s, Step: 32146, GPU: 4.9GB\n",
      "Epoch 1, Batch 32150, Loss: 3.1375, Time: 19127.1s, Step: 32151, GPU: 4.9GB\n",
      "Epoch 1, Batch 32155, Loss: 3.5757, Time: 19130.6s, Step: 32156, GPU: 4.9GB\n",
      "Epoch 1, Batch 32160, Loss: 3.5290, Time: 19133.1s, Step: 32161, GPU: 4.9GB\n",
      "Epoch 1, Batch 32165, Loss: 2.9971, Time: 19136.5s, Step: 32166, GPU: 4.9GB\n",
      "Epoch 1, Batch 32170, Loss: 3.1158, Time: 19139.0s, Step: 32171, GPU: 4.9GB\n",
      "Epoch 1, Batch 32175, Loss: 2.9021, Time: 19142.4s, Step: 32176, GPU: 4.9GB\n",
      "Epoch 1, Batch 32180, Loss: 3.5419, Time: 19144.9s, Step: 32181, GPU: 4.9GB\n",
      "Epoch 1, Batch 32185, Loss: 3.7486, Time: 19148.4s, Step: 32186, GPU: 4.9GB\n",
      "Epoch 1, Batch 32190, Loss: 3.7907, Time: 19150.9s, Step: 32191, GPU: 4.9GB\n",
      "Epoch 1, Batch 32195, Loss: 3.4358, Time: 19154.4s, Step: 32196, GPU: 4.9GB\n",
      "Epoch 1, Batch 32200, Loss: 3.5653, Time: 19157.0s, Step: 32201, GPU: 4.9GB\n",
      "Epoch 1, Batch 32205, Loss: 2.9400, Time: 19160.4s, Step: 32206, GPU: 4.9GB\n",
      "Epoch 1, Batch 32210, Loss: 3.3360, Time: 19162.9s, Step: 32211, GPU: 4.9GB\n",
      "Epoch 1, Batch 32215, Loss: 4.5128, Time: 19166.4s, Step: 32216, GPU: 4.9GB\n",
      "Epoch 1, Batch 32220, Loss: 3.7057, Time: 19168.9s, Step: 32221, GPU: 4.9GB\n",
      "Epoch 1, Batch 32225, Loss: 3.0620, Time: 19172.4s, Step: 32226, GPU: 4.9GB\n",
      "Epoch 1, Batch 32230, Loss: 4.3264, Time: 19174.9s, Step: 32231, GPU: 4.9GB\n",
      "Epoch 1, Batch 32235, Loss: 3.9132, Time: 19178.9s, Step: 32236, GPU: 4.9GB\n",
      "Epoch 1, Batch 32240, Loss: 4.4421, Time: 19181.4s, Step: 32241, GPU: 4.9GB\n",
      "Epoch 1, Batch 32245, Loss: 2.9572, Time: 19184.8s, Step: 32246, GPU: 4.9GB\n",
      "Epoch 1, Batch 32250, Loss: 3.0680, Time: 19187.3s, Step: 32251, GPU: 4.9GB\n",
      "Epoch 1, Batch 32255, Loss: 3.7181, Time: 19190.8s, Step: 32256, GPU: 4.9GB\n",
      "Epoch 1, Batch 32260, Loss: 3.8154, Time: 19193.3s, Step: 32261, GPU: 4.9GB\n",
      "Epoch 1, Batch 32265, Loss: 3.5176, Time: 19196.7s, Step: 32266, GPU: 4.9GB\n",
      "Epoch 1, Batch 32270, Loss: 3.2635, Time: 19199.3s, Step: 32271, GPU: 4.9GB\n",
      "Epoch 1, Batch 32275, Loss: 4.0732, Time: 19202.7s, Step: 32276, GPU: 4.9GB\n",
      "Epoch 1, Batch 32280, Loss: 3.0787, Time: 19205.2s, Step: 32281, GPU: 4.9GB\n",
      "Epoch 1, Batch 32285, Loss: 2.9256, Time: 19208.6s, Step: 32286, GPU: 4.9GB\n",
      "Epoch 1, Batch 32290, Loss: 3.7684, Time: 19211.1s, Step: 32291, GPU: 4.9GB\n",
      "Epoch 1, Batch 32295, Loss: 3.6147, Time: 19214.6s, Step: 32296, GPU: 4.9GB\n",
      "Epoch 1, Batch 32300, Loss: 2.8688, Time: 19217.1s, Step: 32301, GPU: 4.9GB\n",
      "Epoch 1, Batch 32305, Loss: 3.3223, Time: 19220.5s, Step: 32306, GPU: 4.9GB\n",
      "Epoch 1, Batch 32310, Loss: 3.0968, Time: 19223.0s, Step: 32311, GPU: 4.9GB\n",
      "Epoch 1, Batch 32315, Loss: 4.1778, Time: 19226.4s, Step: 32316, GPU: 4.9GB\n",
      "Epoch 1, Batch 32320, Loss: 3.8182, Time: 19229.0s, Step: 32321, GPU: 4.9GB\n",
      "Epoch 1, Batch 32325, Loss: 3.4648, Time: 19232.4s, Step: 32326, GPU: 4.9GB\n",
      "Epoch 1, Batch 32330, Loss: 3.3020, Time: 19234.9s, Step: 32331, GPU: 4.9GB\n",
      "Epoch 1, Batch 32335, Loss: 2.9136, Time: 19238.3s, Step: 32336, GPU: 4.9GB\n",
      "Epoch 1, Batch 32340, Loss: 3.6064, Time: 19240.9s, Step: 32341, GPU: 4.9GB\n",
      "Epoch 1, Batch 32345, Loss: 4.2211, Time: 19244.3s, Step: 32346, GPU: 4.9GB\n",
      "Epoch 1, Batch 32350, Loss: 3.5497, Time: 19246.8s, Step: 32351, GPU: 4.9GB\n",
      "Epoch 1, Batch 32355, Loss: 2.8534, Time: 19250.2s, Step: 32356, GPU: 4.9GB\n",
      "Epoch 1, Batch 32360, Loss: 3.2821, Time: 19252.7s, Step: 32361, GPU: 4.9GB\n",
      "Epoch 1, Batch 32365, Loss: 3.7005, Time: 19256.2s, Step: 32366, GPU: 4.9GB\n",
      "Epoch 1, Batch 32370, Loss: 3.6047, Time: 19258.7s, Step: 32371, GPU: 4.9GB\n",
      "Epoch 1, Batch 32375, Loss: 3.3159, Time: 19262.1s, Step: 32376, GPU: 4.9GB\n",
      "Epoch 1, Batch 32380, Loss: 2.9770, Time: 19264.6s, Step: 32381, GPU: 4.9GB\n",
      "Epoch 1, Batch 32385, Loss: 3.6247, Time: 19268.0s, Step: 32386, GPU: 4.9GB\n",
      "Epoch 1, Batch 32390, Loss: 4.1638, Time: 19270.6s, Step: 32391, GPU: 4.9GB\n",
      "Epoch 1, Batch 32395, Loss: 3.7346, Time: 19274.0s, Step: 32396, GPU: 4.9GB\n",
      "Epoch 1, Batch 32400, Loss: 3.0457, Time: 19276.6s, Step: 32401, GPU: 4.9GB\n",
      "Epoch 1, Batch 32405, Loss: 4.0749, Time: 19280.0s, Step: 32406, GPU: 4.9GB\n",
      "Epoch 1, Batch 32410, Loss: 2.8431, Time: 19282.5s, Step: 32411, GPU: 4.9GB\n",
      "Epoch 1, Batch 32415, Loss: 3.1697, Time: 19285.9s, Step: 32416, GPU: 4.9GB\n",
      "Epoch 1, Batch 32420, Loss: 3.1240, Time: 19288.5s, Step: 32421, GPU: 4.9GB\n",
      "Epoch 1, Batch 32425, Loss: 4.0429, Time: 19291.9s, Step: 32426, GPU: 4.9GB\n",
      "Epoch 1, Batch 32430, Loss: 3.5347, Time: 19294.5s, Step: 32431, GPU: 4.9GB\n",
      "Epoch 1, Batch 32435, Loss: 3.6129, Time: 19297.9s, Step: 32436, GPU: 4.9GB\n",
      "Epoch 1, Batch 32440, Loss: 3.1049, Time: 19300.4s, Step: 32441, GPU: 4.9GB\n",
      "Epoch 1, Batch 32445, Loss: 3.2468, Time: 19303.8s, Step: 32446, GPU: 4.9GB\n",
      "Epoch 1, Batch 32450, Loss: 2.9288, Time: 19306.4s, Step: 32451, GPU: 4.9GB\n",
      "Epoch 1, Batch 32455, Loss: 3.5824, Time: 19309.8s, Step: 32456, GPU: 4.9GB\n",
      "Epoch 1, Batch 32460, Loss: 3.1656, Time: 19312.3s, Step: 32461, GPU: 4.9GB\n",
      "Epoch 1, Batch 32465, Loss: 2.9590, Time: 19315.7s, Step: 32466, GPU: 4.9GB\n",
      "Epoch 1, Batch 32470, Loss: 2.9799, Time: 19318.2s, Step: 32471, GPU: 4.9GB\n",
      "Epoch 1, Batch 32475, Loss: 4.0292, Time: 19321.6s, Step: 32476, GPU: 4.9GB\n",
      "Epoch 1, Batch 32480, Loss: 3.5523, Time: 19324.2s, Step: 32481, GPU: 4.9GB\n",
      "Epoch 1, Batch 32485, Loss: 2.7003, Time: 19327.6s, Step: 32486, GPU: 4.9GB\n",
      "Epoch 1, Batch 32490, Loss: 3.6616, Time: 19330.1s, Step: 32491, GPU: 4.9GB\n",
      "Epoch 1, Batch 32495, Loss: 3.4148, Time: 19333.5s, Step: 32496, GPU: 4.9GB\n",
      "Epoch 1, Batch 32500, Loss: 4.6644, Time: 19336.0s, Step: 32501, GPU: 4.9GB\n",
      "Epoch 1, Batch 32505, Loss: 3.4006, Time: 19339.4s, Step: 32506, GPU: 4.9GB\n",
      "Epoch 1, Batch 32510, Loss: 3.0575, Time: 19341.9s, Step: 32511, GPU: 4.9GB\n",
      "Epoch 1, Batch 32515, Loss: 2.9957, Time: 19345.3s, Step: 32516, GPU: 4.9GB\n",
      "Epoch 1, Batch 32520, Loss: 3.0894, Time: 19347.8s, Step: 32521, GPU: 4.9GB\n",
      "Epoch 1, Batch 32525, Loss: 3.4343, Time: 19351.3s, Step: 32526, GPU: 4.9GB\n",
      "Epoch 1, Batch 32530, Loss: 4.0440, Time: 19353.8s, Step: 32531, GPU: 4.9GB\n",
      "Epoch 1, Batch 32535, Loss: 3.3751, Time: 19357.2s, Step: 32536, GPU: 4.9GB\n",
      "Epoch 1, Batch 32540, Loss: 2.9253, Time: 19359.7s, Step: 32541, GPU: 4.9GB\n",
      "Epoch 1, Batch 32545, Loss: 3.9869, Time: 19363.2s, Step: 32546, GPU: 4.9GB\n",
      "Epoch 1, Batch 32550, Loss: 2.6225, Time: 19365.7s, Step: 32551, GPU: 4.9GB\n",
      "Epoch 1, Batch 32555, Loss: 4.0325, Time: 19369.1s, Step: 32556, GPU: 4.9GB\n",
      "Epoch 1, Batch 32560, Loss: 3.6448, Time: 19371.6s, Step: 32561, GPU: 4.9GB\n",
      "Epoch 1, Batch 32565, Loss: 3.3183, Time: 19375.0s, Step: 32566, GPU: 4.9GB\n",
      "Epoch 1, Batch 32570, Loss: 3.3588, Time: 19377.5s, Step: 32571, GPU: 4.9GB\n",
      "Epoch 1, Batch 32575, Loss: 4.3005, Time: 19381.0s, Step: 32576, GPU: 4.9GB\n",
      "Epoch 1, Batch 32580, Loss: 3.8555, Time: 19383.5s, Step: 32581, GPU: 4.9GB\n",
      "Epoch 1, Batch 32585, Loss: 3.9400, Time: 19386.9s, Step: 32586, GPU: 4.9GB\n",
      "Epoch 1, Batch 32590, Loss: 3.0365, Time: 19389.4s, Step: 32591, GPU: 4.9GB\n",
      "Epoch 1, Batch 32595, Loss: 4.5100, Time: 19392.9s, Step: 32596, GPU: 4.9GB\n",
      "Epoch 1, Batch 32600, Loss: 2.9707, Time: 19395.5s, Step: 32601, GPU: 4.9GB\n",
      "Epoch 1, Batch 32605, Loss: 2.9702, Time: 19398.9s, Step: 32606, GPU: 4.9GB\n",
      "Epoch 1, Batch 32610, Loss: 3.4550, Time: 19401.4s, Step: 32611, GPU: 4.9GB\n",
      "Epoch 1, Batch 32615, Loss: 4.1931, Time: 19404.8s, Step: 32616, GPU: 4.9GB\n",
      "Epoch 1, Batch 32620, Loss: 3.1711, Time: 19407.3s, Step: 32621, GPU: 4.9GB\n",
      "Epoch 1, Batch 32625, Loss: 4.1534, Time: 19410.7s, Step: 32626, GPU: 4.9GB\n",
      "Epoch 1, Batch 32630, Loss: 3.5363, Time: 19413.2s, Step: 32631, GPU: 4.9GB\n",
      "Epoch 1, Batch 32635, Loss: 2.5285, Time: 19416.6s, Step: 32636, GPU: 4.9GB\n",
      "Epoch 1, Batch 32640, Loss: 3.2978, Time: 19419.1s, Step: 32641, GPU: 4.9GB\n",
      "Epoch 1, Batch 32645, Loss: 2.7929, Time: 19422.6s, Step: 32646, GPU: 4.9GB\n",
      "Epoch 1, Batch 32650, Loss: 3.6916, Time: 19425.1s, Step: 32651, GPU: 4.9GB\n",
      "Epoch 1, Batch 32655, Loss: 3.9550, Time: 19428.5s, Step: 32656, GPU: 4.9GB\n",
      "Epoch 1, Batch 32660, Loss: 3.9006, Time: 19431.0s, Step: 32661, GPU: 4.9GB\n",
      "Epoch 1, Batch 32665, Loss: 3.3244, Time: 19434.4s, Step: 32666, GPU: 4.9GB\n",
      "Epoch 1, Batch 32670, Loss: 3.4965, Time: 19437.0s, Step: 32671, GPU: 4.9GB\n",
      "Epoch 1, Batch 32675, Loss: 3.9294, Time: 19440.4s, Step: 32676, GPU: 4.9GB\n",
      "Epoch 1, Batch 32680, Loss: 3.6902, Time: 19442.9s, Step: 32681, GPU: 4.9GB\n",
      "Epoch 1, Batch 32685, Loss: 3.2712, Time: 19446.3s, Step: 32686, GPU: 4.9GB\n",
      "Epoch 1, Batch 32690, Loss: 3.9961, Time: 19448.8s, Step: 32691, GPU: 4.9GB\n",
      "Epoch 1, Batch 32695, Loss: 4.8718, Time: 19452.2s, Step: 32696, GPU: 4.9GB\n",
      "Epoch 1, Batch 32700, Loss: 3.1570, Time: 19454.8s, Step: 32701, GPU: 4.9GB\n",
      "Epoch 1, Batch 32705, Loss: 3.8741, Time: 19458.2s, Step: 32706, GPU: 4.9GB\n",
      "Epoch 1, Batch 32710, Loss: 3.5708, Time: 19460.8s, Step: 32711, GPU: 4.9GB\n",
      "Epoch 1, Batch 32715, Loss: 3.4984, Time: 19464.2s, Step: 32716, GPU: 4.9GB\n",
      "Epoch 1, Batch 32720, Loss: 4.0308, Time: 19466.7s, Step: 32721, GPU: 4.9GB\n",
      "Epoch 1, Batch 32725, Loss: 3.3864, Time: 19470.1s, Step: 32726, GPU: 4.9GB\n",
      "Epoch 1, Batch 32730, Loss: 3.5367, Time: 19472.6s, Step: 32731, GPU: 4.9GB\n",
      "Epoch 1, Batch 32735, Loss: 3.6558, Time: 19476.1s, Step: 32736, GPU: 4.9GB\n",
      "Epoch 1, Batch 32740, Loss: 3.5473, Time: 19478.6s, Step: 32741, GPU: 4.9GB\n",
      "Epoch 1, Batch 32745, Loss: 3.5837, Time: 19482.0s, Step: 32746, GPU: 4.9GB\n",
      "Epoch 1, Batch 32750, Loss: 3.8694, Time: 19484.5s, Step: 32751, GPU: 4.9GB\n",
      "Epoch 1, Batch 32755, Loss: 3.3179, Time: 19487.9s, Step: 32756, GPU: 4.9GB\n",
      "Epoch 1, Batch 32760, Loss: 3.1423, Time: 19490.4s, Step: 32761, GPU: 4.9GB\n",
      "Epoch 1, Batch 32765, Loss: 2.8573, Time: 19493.8s, Step: 32766, GPU: 4.9GB\n",
      "Epoch 1, Batch 32770, Loss: 3.6002, Time: 19496.3s, Step: 32771, GPU: 4.9GB\n",
      "Epoch 1, Batch 32775, Loss: 3.7478, Time: 19499.7s, Step: 32776, GPU: 4.9GB\n",
      "Epoch 1, Batch 32780, Loss: 3.5220, Time: 19502.2s, Step: 32781, GPU: 4.9GB\n",
      "Epoch 1, Batch 32785, Loss: 3.5787, Time: 19505.6s, Step: 32786, GPU: 4.9GB\n",
      "Epoch 1, Batch 32790, Loss: 3.4597, Time: 19508.1s, Step: 32791, GPU: 4.9GB\n",
      "Epoch 1, Batch 32795, Loss: 4.0094, Time: 19511.6s, Step: 32796, GPU: 4.9GB\n",
      "Epoch 1, Batch 32800, Loss: 2.4006, Time: 19514.2s, Step: 32801, GPU: 4.9GB\n",
      "Epoch 1, Batch 32805, Loss: 3.6686, Time: 19517.6s, Step: 32806, GPU: 4.9GB\n",
      "Epoch 1, Batch 32810, Loss: 3.5546, Time: 19520.1s, Step: 32811, GPU: 4.9GB\n",
      "Epoch 1, Batch 32815, Loss: 3.0547, Time: 19523.5s, Step: 32816, GPU: 4.9GB\n",
      "Epoch 1, Batch 32820, Loss: 4.1040, Time: 19526.0s, Step: 32821, GPU: 4.9GB\n",
      "Epoch 1, Batch 32825, Loss: 3.2322, Time: 19529.4s, Step: 32826, GPU: 4.9GB\n",
      "Epoch 1, Batch 32830, Loss: 3.5750, Time: 19532.0s, Step: 32831, GPU: 4.9GB\n",
      "Epoch 1, Batch 32835, Loss: 3.3629, Time: 19535.4s, Step: 32836, GPU: 4.9GB\n",
      "Epoch 1, Batch 32840, Loss: 3.5386, Time: 19537.9s, Step: 32841, GPU: 4.9GB\n",
      "Epoch 1, Batch 32845, Loss: 3.1982, Time: 19541.3s, Step: 32846, GPU: 4.9GB\n",
      "Epoch 1, Batch 32850, Loss: 3.7974, Time: 19543.8s, Step: 32851, GPU: 4.9GB\n",
      "Epoch 1, Batch 32855, Loss: 3.4427, Time: 19547.2s, Step: 32856, GPU: 4.9GB\n",
      "Epoch 1, Batch 32860, Loss: 3.8040, Time: 19549.8s, Step: 32861, GPU: 4.9GB\n",
      "Epoch 1, Batch 32865, Loss: 3.7678, Time: 19553.2s, Step: 32866, GPU: 4.9GB\n",
      "Epoch 1, Batch 32870, Loss: 3.3012, Time: 19555.7s, Step: 32871, GPU: 4.9GB\n",
      "Epoch 1, Batch 32875, Loss: 3.8651, Time: 19559.1s, Step: 32876, GPU: 4.9GB\n",
      "Epoch 1, Batch 32880, Loss: 3.4477, Time: 19561.7s, Step: 32881, GPU: 4.9GB\n",
      "Epoch 1, Batch 32885, Loss: 3.3302, Time: 19565.1s, Step: 32886, GPU: 4.9GB\n",
      "Epoch 1, Batch 32890, Loss: 2.7734, Time: 19567.6s, Step: 32891, GPU: 4.9GB\n",
      "Epoch 1, Batch 32895, Loss: 3.4149, Time: 19571.1s, Step: 32896, GPU: 4.9GB\n",
      "Epoch 1, Batch 32900, Loss: 3.7346, Time: 19573.6s, Step: 32901, GPU: 4.9GB\n",
      "Epoch 1, Batch 32905, Loss: 3.7749, Time: 19577.1s, Step: 32906, GPU: 4.9GB\n",
      "Epoch 1, Batch 32910, Loss: 3.5399, Time: 19579.6s, Step: 32911, GPU: 4.9GB\n",
      "Epoch 1, Batch 32915, Loss: 3.4351, Time: 19583.1s, Step: 32916, GPU: 4.9GB\n",
      "Epoch 1, Batch 32920, Loss: 4.3521, Time: 19585.6s, Step: 32921, GPU: 4.9GB\n",
      "Epoch 1, Batch 32925, Loss: 3.1498, Time: 19589.0s, Step: 32926, GPU: 4.9GB\n",
      "Epoch 1, Batch 32930, Loss: 3.1885, Time: 19591.5s, Step: 32931, GPU: 4.9GB\n",
      "Epoch 1, Batch 32935, Loss: 3.4454, Time: 19595.0s, Step: 32936, GPU: 4.9GB\n",
      "Epoch 1, Batch 32940, Loss: 3.3172, Time: 19597.5s, Step: 32941, GPU: 4.9GB\n",
      "Epoch 1, Batch 32945, Loss: 3.2796, Time: 19600.9s, Step: 32946, GPU: 4.9GB\n",
      "Epoch 1, Batch 32950, Loss: 3.5628, Time: 19603.4s, Step: 32951, GPU: 4.9GB\n",
      "Epoch 1, Batch 32955, Loss: 2.7785, Time: 19606.8s, Step: 32956, GPU: 4.9GB\n",
      "Epoch 1, Batch 32960, Loss: 3.8646, Time: 19609.3s, Step: 32961, GPU: 4.9GB\n",
      "Epoch 1, Batch 32965, Loss: 4.1898, Time: 19612.7s, Step: 32966, GPU: 4.9GB\n",
      "Epoch 1, Batch 32970, Loss: 3.5005, Time: 19615.2s, Step: 32971, GPU: 4.9GB\n",
      "Epoch 1, Batch 32975, Loss: 3.2581, Time: 19618.7s, Step: 32976, GPU: 4.9GB\n",
      "Epoch 1, Batch 32980, Loss: 3.1311, Time: 19621.2s, Step: 32981, GPU: 4.9GB\n",
      "Epoch 1, Batch 32985, Loss: 3.4173, Time: 19624.6s, Step: 32986, GPU: 4.9GB\n",
      "Epoch 1, Batch 32990, Loss: 3.6307, Time: 19627.2s, Step: 32991, GPU: 4.9GB\n",
      "Epoch 1, Batch 32995, Loss: 4.0282, Time: 19630.6s, Step: 32996, GPU: 4.9GB\n",
      "Epoch 1, Batch 33000, Loss: 3.4677, Time: 19633.2s, Step: 33001, GPU: 4.9GB\n",
      "Epoch 1, Batch 33005, Loss: 3.1034, Time: 19636.7s, Step: 33006, GPU: 4.9GB\n",
      "Epoch 1, Batch 33010, Loss: 3.5958, Time: 19639.2s, Step: 33011, GPU: 4.9GB\n",
      "Epoch 1, Batch 33015, Loss: 3.3015, Time: 19642.6s, Step: 33016, GPU: 4.9GB\n",
      "Epoch 1, Batch 33020, Loss: 3.4823, Time: 19645.1s, Step: 33021, GPU: 4.9GB\n",
      "Epoch 1, Batch 33025, Loss: 3.1709, Time: 19648.6s, Step: 33026, GPU: 4.9GB\n",
      "Epoch 1, Batch 33030, Loss: 2.8814, Time: 19651.1s, Step: 33031, GPU: 4.9GB\n",
      "Epoch 1, Batch 33035, Loss: 3.7002, Time: 19654.5s, Step: 33036, GPU: 4.9GB\n",
      "Epoch 1, Batch 33040, Loss: 4.6857, Time: 19657.1s, Step: 33041, GPU: 4.9GB\n",
      "Epoch 1, Batch 33045, Loss: 3.8416, Time: 19660.5s, Step: 33046, GPU: 4.9GB\n",
      "Epoch 1, Batch 33050, Loss: 4.2931, Time: 19663.0s, Step: 33051, GPU: 4.9GB\n",
      "Epoch 1, Batch 33055, Loss: 4.2479, Time: 19666.5s, Step: 33056, GPU: 4.9GB\n",
      "Epoch 1, Batch 33060, Loss: 4.6252, Time: 19669.0s, Step: 33061, GPU: 4.9GB\n",
      "Epoch 1, Batch 33065, Loss: 4.2466, Time: 19672.4s, Step: 33066, GPU: 4.9GB\n",
      "Epoch 1, Batch 33070, Loss: 3.2807, Time: 19674.9s, Step: 33071, GPU: 4.9GB\n",
      "Epoch 1, Batch 33075, Loss: 3.4269, Time: 19678.3s, Step: 33076, GPU: 4.9GB\n",
      "Epoch 1, Batch 33080, Loss: 3.4630, Time: 19680.8s, Step: 33081, GPU: 4.9GB\n",
      "Epoch 1, Batch 33085, Loss: 3.7048, Time: 19684.3s, Step: 33086, GPU: 4.9GB\n",
      "Epoch 1, Batch 33090, Loss: 3.6038, Time: 19686.8s, Step: 33091, GPU: 4.9GB\n",
      "Epoch 1, Batch 33095, Loss: 3.2960, Time: 19690.2s, Step: 33096, GPU: 4.9GB\n",
      "Epoch 1, Batch 33100, Loss: 4.1469, Time: 19692.7s, Step: 33101, GPU: 4.9GB\n",
      "Epoch 1, Batch 33105, Loss: 3.6963, Time: 19696.1s, Step: 33106, GPU: 4.9GB\n",
      "Epoch 1, Batch 33110, Loss: 3.6202, Time: 19698.6s, Step: 33111, GPU: 4.9GB\n",
      "Epoch 1, Batch 33115, Loss: 4.2067, Time: 19702.0s, Step: 33116, GPU: 4.9GB\n",
      "Epoch 1, Batch 33120, Loss: 3.7880, Time: 19704.5s, Step: 33121, GPU: 4.9GB\n",
      "Epoch 1, Batch 33125, Loss: 2.8348, Time: 19707.9s, Step: 33126, GPU: 4.9GB\n",
      "Epoch 1, Batch 33130, Loss: 4.0638, Time: 19710.4s, Step: 33131, GPU: 4.9GB\n",
      "Epoch 1, Batch 33135, Loss: 3.2454, Time: 19713.9s, Step: 33136, GPU: 4.9GB\n",
      "Epoch 1, Batch 33140, Loss: 2.9774, Time: 19716.4s, Step: 33141, GPU: 4.9GB\n",
      "Epoch 1, Batch 33145, Loss: 2.9861, Time: 19719.8s, Step: 33146, GPU: 4.9GB\n",
      "Epoch 1, Batch 33150, Loss: 3.0663, Time: 19722.3s, Step: 33151, GPU: 4.9GB\n",
      "Epoch 1, Batch 33155, Loss: 3.0170, Time: 19725.7s, Step: 33156, GPU: 4.9GB\n",
      "Epoch 1, Batch 33160, Loss: 2.7815, Time: 19728.3s, Step: 33161, GPU: 4.9GB\n",
      "Epoch 1, Batch 33165, Loss: 3.9453, Time: 19731.7s, Step: 33166, GPU: 4.9GB\n",
      "Epoch 1, Batch 33170, Loss: 3.7322, Time: 19734.2s, Step: 33171, GPU: 4.9GB\n",
      "Epoch 1, Batch 33175, Loss: 3.1826, Time: 19737.6s, Step: 33176, GPU: 4.9GB\n",
      "Epoch 1, Batch 33180, Loss: 3.6826, Time: 19740.1s, Step: 33181, GPU: 4.9GB\n",
      "Epoch 1, Batch 33185, Loss: 3.4555, Time: 19743.6s, Step: 33186, GPU: 4.9GB\n",
      "Epoch 1, Batch 33190, Loss: 3.9217, Time: 19746.1s, Step: 33191, GPU: 4.9GB\n",
      "Epoch 1, Batch 33195, Loss: 2.9781, Time: 19749.5s, Step: 33196, GPU: 4.9GB\n",
      "Epoch 1, Batch 33200, Loss: 3.3021, Time: 19752.1s, Step: 33201, GPU: 4.9GB\n",
      "Epoch 1, Batch 33205, Loss: 3.1828, Time: 19755.5s, Step: 33206, GPU: 4.9GB\n",
      "Epoch 1, Batch 33210, Loss: 3.4051, Time: 19758.1s, Step: 33211, GPU: 4.9GB\n",
      "Epoch 1, Batch 33215, Loss: 2.8218, Time: 19761.5s, Step: 33216, GPU: 4.9GB\n",
      "Epoch 1, Batch 33220, Loss: 2.7127, Time: 19764.1s, Step: 33221, GPU: 4.9GB\n",
      "Epoch 1, Batch 33225, Loss: 3.4661, Time: 19767.5s, Step: 33226, GPU: 4.9GB\n",
      "Epoch 1, Batch 33230, Loss: 3.0565, Time: 19770.0s, Step: 33231, GPU: 4.9GB\n",
      "Epoch 1, Batch 33235, Loss: 3.0821, Time: 19773.5s, Step: 33236, GPU: 4.9GB\n",
      "Epoch 1, Batch 33240, Loss: 3.8600, Time: 19776.0s, Step: 33241, GPU: 4.9GB\n",
      "Epoch 1, Batch 33245, Loss: 3.2576, Time: 19779.4s, Step: 33246, GPU: 4.9GB\n",
      "Epoch 1, Batch 33250, Loss: 3.5899, Time: 19782.0s, Step: 33251, GPU: 4.9GB\n",
      "Epoch 1, Batch 33255, Loss: 2.7234, Time: 19785.4s, Step: 33256, GPU: 4.9GB\n",
      "Epoch 1, Batch 33260, Loss: 3.6381, Time: 19787.9s, Step: 33261, GPU: 4.9GB\n",
      "Epoch 1, Batch 33265, Loss: 3.5762, Time: 19791.3s, Step: 33266, GPU: 4.9GB\n",
      "Epoch 1, Batch 33270, Loss: 3.5049, Time: 19793.9s, Step: 33271, GPU: 4.9GB\n",
      "Epoch 1, Batch 33275, Loss: 2.7272, Time: 19797.3s, Step: 33276, GPU: 4.9GB\n",
      "Epoch 1, Batch 33280, Loss: 3.1745, Time: 19799.8s, Step: 33281, GPU: 4.9GB\n",
      "Epoch 1, Batch 33285, Loss: 3.3583, Time: 19803.2s, Step: 33286, GPU: 4.9GB\n",
      "Epoch 1, Batch 33290, Loss: 4.1111, Time: 19805.7s, Step: 33291, GPU: 4.9GB\n",
      "Epoch 1, Batch 33295, Loss: 2.5468, Time: 19809.1s, Step: 33296, GPU: 4.9GB\n",
      "Epoch 1, Batch 33300, Loss: 3.6508, Time: 19811.8s, Step: 33301, GPU: 4.9GB\n",
      "Epoch 1, Batch 33305, Loss: 4.2856, Time: 19815.2s, Step: 33306, GPU: 4.9GB\n",
      "Epoch 1, Batch 33310, Loss: 4.0787, Time: 19817.7s, Step: 33311, GPU: 4.9GB\n",
      "Epoch 1, Batch 33315, Loss: 3.1572, Time: 19821.1s, Step: 33316, GPU: 4.9GB\n",
      "Epoch 1, Batch 33320, Loss: 2.9042, Time: 19823.6s, Step: 33321, GPU: 4.9GB\n",
      "Epoch 1, Batch 33325, Loss: 3.1189, Time: 19827.0s, Step: 33326, GPU: 4.9GB\n",
      "Epoch 1, Batch 33330, Loss: 3.1082, Time: 19829.6s, Step: 33331, GPU: 4.9GB\n",
      "Epoch 1, Batch 33335, Loss: 3.8362, Time: 19832.9s, Step: 33336, GPU: 4.9GB\n",
      "Epoch 1, Batch 33340, Loss: 2.9754, Time: 19835.5s, Step: 33341, GPU: 4.9GB\n",
      "Epoch 1, Batch 33345, Loss: 2.9647, Time: 19838.9s, Step: 33346, GPU: 4.9GB\n",
      "Epoch 1, Batch 33350, Loss: 3.4770, Time: 19841.4s, Step: 33351, GPU: 4.9GB\n",
      "Epoch 1, Batch 33355, Loss: 3.2599, Time: 19844.8s, Step: 33356, GPU: 4.9GB\n",
      "Epoch 1, Batch 33360, Loss: 3.2603, Time: 19847.4s, Step: 33361, GPU: 4.9GB\n",
      "Epoch 1, Batch 33365, Loss: 3.0641, Time: 19850.8s, Step: 33366, GPU: 4.9GB\n",
      "Epoch 1, Batch 33370, Loss: 3.1613, Time: 19853.3s, Step: 33371, GPU: 4.9GB\n",
      "Epoch 1, Batch 33375, Loss: 3.3655, Time: 19856.7s, Step: 33376, GPU: 4.9GB\n",
      "Epoch 1, Batch 33380, Loss: 2.8946, Time: 19859.2s, Step: 33381, GPU: 4.9GB\n",
      "Epoch 1, Batch 33385, Loss: 3.5962, Time: 19862.7s, Step: 33386, GPU: 4.9GB\n",
      "Epoch 1, Batch 33390, Loss: 3.4938, Time: 19865.2s, Step: 33391, GPU: 4.9GB\n",
      "Epoch 1, Batch 33395, Loss: 2.6640, Time: 19868.6s, Step: 33396, GPU: 4.9GB\n",
      "Epoch 1, Batch 33400, Loss: 2.4556, Time: 19871.2s, Step: 33401, GPU: 4.9GB\n",
      "Epoch 1, Batch 33405, Loss: 4.4863, Time: 19874.7s, Step: 33406, GPU: 4.9GB\n",
      "Epoch 1, Batch 33410, Loss: 3.7285, Time: 19877.2s, Step: 33411, GPU: 4.9GB\n",
      "Epoch 1, Batch 33415, Loss: 3.0552, Time: 19880.6s, Step: 33416, GPU: 4.9GB\n",
      "Epoch 1, Batch 33420, Loss: 2.7275, Time: 19883.1s, Step: 33421, GPU: 4.9GB\n",
      "Epoch 1, Batch 33425, Loss: 3.5895, Time: 19886.5s, Step: 33426, GPU: 4.9GB\n",
      "Epoch 1, Batch 33430, Loss: 2.8183, Time: 19889.0s, Step: 33431, GPU: 4.9GB\n",
      "Epoch 1, Batch 33435, Loss: 3.6869, Time: 19892.4s, Step: 33436, GPU: 4.9GB\n",
      "Epoch 1, Batch 33440, Loss: 3.3255, Time: 19894.9s, Step: 33441, GPU: 4.9GB\n",
      "Epoch 1, Batch 33445, Loss: 4.1501, Time: 19898.4s, Step: 33446, GPU: 4.9GB\n",
      "Epoch 1, Batch 33450, Loss: 4.1829, Time: 19900.9s, Step: 33451, GPU: 4.9GB\n",
      "Epoch 1, Batch 33455, Loss: 3.3625, Time: 19904.3s, Step: 33456, GPU: 4.9GB\n",
      "Epoch 1, Batch 33460, Loss: 3.3235, Time: 19906.8s, Step: 33461, GPU: 4.9GB\n",
      "Epoch 1, Batch 33465, Loss: 2.7868, Time: 19910.1s, Step: 33466, GPU: 4.9GB\n",
      "Epoch 1, Batch 33470, Loss: 2.8674, Time: 19912.7s, Step: 33471, GPU: 4.9GB\n",
      "Epoch 1, Batch 33475, Loss: 3.5424, Time: 19916.1s, Step: 33476, GPU: 4.9GB\n",
      "Epoch 1, Batch 33480, Loss: 3.8328, Time: 19918.6s, Step: 33481, GPU: 4.9GB\n",
      "Epoch 1, Batch 33485, Loss: 2.8216, Time: 19922.0s, Step: 33486, GPU: 4.9GB\n",
      "Epoch 1, Batch 33490, Loss: 3.0480, Time: 19924.5s, Step: 33491, GPU: 4.9GB\n",
      "Epoch 1, Batch 33495, Loss: 3.7282, Time: 19927.9s, Step: 33496, GPU: 4.9GB\n",
      "Epoch 1, Batch 33500, Loss: 3.2321, Time: 19930.4s, Step: 33501, GPU: 4.9GB\n",
      "Epoch 1, Batch 33505, Loss: 3.6305, Time: 19933.9s, Step: 33506, GPU: 4.9GB\n",
      "Epoch 1, Batch 33510, Loss: 3.7624, Time: 19936.4s, Step: 33511, GPU: 4.9GB\n",
      "Epoch 1, Batch 33515, Loss: 2.9399, Time: 19939.8s, Step: 33516, GPU: 4.9GB\n",
      "Epoch 1, Batch 33520, Loss: 3.1328, Time: 19942.3s, Step: 33521, GPU: 4.9GB\n",
      "Epoch 1, Batch 33525, Loss: 3.3484, Time: 19945.8s, Step: 33526, GPU: 4.9GB\n",
      "Epoch 1, Batch 33530, Loss: 3.2195, Time: 19948.3s, Step: 33531, GPU: 4.9GB\n",
      "Epoch 1, Batch 33535, Loss: 4.1914, Time: 19951.7s, Step: 33536, GPU: 4.9GB\n",
      "Epoch 1, Batch 33540, Loss: 3.2727, Time: 19954.2s, Step: 33541, GPU: 4.9GB\n",
      "Epoch 1, Batch 33545, Loss: 3.4131, Time: 19957.8s, Step: 33546, GPU: 4.9GB\n",
      "Epoch 1, Batch 33550, Loss: 3.1935, Time: 19960.3s, Step: 33551, GPU: 4.9GB\n",
      "Epoch 1, Batch 33555, Loss: 3.6546, Time: 19963.7s, Step: 33556, GPU: 4.9GB\n",
      "Epoch 1, Batch 33560, Loss: 2.7657, Time: 19966.2s, Step: 33561, GPU: 4.9GB\n",
      "Epoch 1, Batch 33565, Loss: 2.8208, Time: 19969.6s, Step: 33566, GPU: 4.9GB\n",
      "Epoch 1, Batch 33570, Loss: 3.2205, Time: 19972.2s, Step: 33571, GPU: 4.9GB\n",
      "Epoch 1, Batch 33575, Loss: 3.6434, Time: 19975.6s, Step: 33576, GPU: 4.9GB\n",
      "Epoch 1, Batch 33580, Loss: 3.7479, Time: 19978.1s, Step: 33581, GPU: 4.9GB\n",
      "Epoch 1, Batch 33585, Loss: 2.9351, Time: 19981.5s, Step: 33586, GPU: 4.9GB\n",
      "Epoch 1, Batch 33590, Loss: 3.3319, Time: 19984.0s, Step: 33591, GPU: 4.9GB\n",
      "Epoch 1, Batch 33595, Loss: 3.4401, Time: 19987.5s, Step: 33596, GPU: 4.9GB\n",
      "Epoch 1, Batch 33600, Loss: 3.8066, Time: 19990.1s, Step: 33601, GPU: 4.9GB\n",
      "Epoch 1, Batch 33605, Loss: 3.5828, Time: 19993.5s, Step: 33606, GPU: 4.9GB\n",
      "Epoch 1, Batch 33610, Loss: 3.7478, Time: 19996.0s, Step: 33611, GPU: 4.9GB\n",
      "Epoch 1, Batch 33615, Loss: 2.9771, Time: 19999.5s, Step: 33616, GPU: 4.9GB\n",
      "Epoch 1, Batch 33620, Loss: 3.4413, Time: 20002.0s, Step: 33621, GPU: 4.9GB\n",
      "Epoch 1, Batch 33625, Loss: 3.0398, Time: 20005.4s, Step: 33626, GPU: 4.9GB\n",
      "Epoch 1, Batch 33630, Loss: 3.2869, Time: 20008.0s, Step: 33631, GPU: 4.9GB\n",
      "Epoch 1, Batch 33635, Loss: 3.0296, Time: 20011.4s, Step: 33636, GPU: 4.9GB\n",
      "Epoch 1, Batch 33640, Loss: 3.7527, Time: 20013.9s, Step: 33641, GPU: 4.9GB\n",
      "Epoch 1, Batch 33645, Loss: 3.8917, Time: 20017.3s, Step: 33646, GPU: 4.9GB\n",
      "Epoch 1, Batch 33650, Loss: 2.9668, Time: 20019.8s, Step: 33651, GPU: 4.9GB\n",
      "Epoch 1, Batch 33655, Loss: 2.7673, Time: 20023.2s, Step: 33656, GPU: 4.9GB\n",
      "Epoch 1, Batch 33660, Loss: 3.4927, Time: 20025.7s, Step: 33661, GPU: 4.9GB\n",
      "Epoch 1, Batch 33665, Loss: 3.3876, Time: 20029.1s, Step: 33666, GPU: 4.9GB\n",
      "Epoch 1, Batch 33670, Loss: 3.7387, Time: 20031.6s, Step: 33671, GPU: 4.9GB\n",
      "Epoch 1, Batch 33675, Loss: 3.2423, Time: 20035.1s, Step: 33676, GPU: 4.9GB\n",
      "Epoch 1, Batch 33680, Loss: 3.5535, Time: 20037.6s, Step: 33681, GPU: 4.9GB\n",
      "Epoch 1, Batch 33685, Loss: 3.8899, Time: 20041.0s, Step: 33686, GPU: 4.9GB\n",
      "Epoch 1, Batch 33690, Loss: 3.9596, Time: 20043.6s, Step: 33691, GPU: 4.9GB\n",
      "Epoch 1, Batch 33695, Loss: 3.1873, Time: 20047.0s, Step: 33696, GPU: 4.9GB\n",
      "Epoch 1, Batch 33700, Loss: 3.7001, Time: 20049.5s, Step: 33701, GPU: 4.9GB\n",
      "Epoch 1, Batch 33705, Loss: 2.9545, Time: 20052.9s, Step: 33706, GPU: 4.9GB\n",
      "Epoch 1, Batch 33710, Loss: 2.7357, Time: 20055.4s, Step: 33711, GPU: 4.9GB\n",
      "Epoch 1, Batch 33715, Loss: 3.2333, Time: 20058.9s, Step: 33716, GPU: 4.9GB\n",
      "Epoch 1, Batch 33720, Loss: 3.8811, Time: 20061.4s, Step: 33721, GPU: 4.9GB\n",
      "Epoch 1, Batch 33725, Loss: 3.0591, Time: 20064.8s, Step: 33726, GPU: 4.9GB\n",
      "Epoch 1, Batch 33730, Loss: 3.4624, Time: 20067.3s, Step: 33731, GPU: 4.9GB\n",
      "Epoch 1, Batch 33735, Loss: 3.2350, Time: 20070.7s, Step: 33736, GPU: 4.9GB\n",
      "Epoch 1, Batch 33740, Loss: 3.1864, Time: 20073.3s, Step: 33741, GPU: 4.9GB\n",
      "Epoch 1, Batch 33745, Loss: 4.3701, Time: 20076.7s, Step: 33746, GPU: 4.9GB\n",
      "Epoch 1, Batch 33750, Loss: 2.7279, Time: 20079.2s, Step: 33751, GPU: 4.9GB\n",
      "Epoch 1, Batch 33755, Loss: 4.3461, Time: 20082.6s, Step: 33756, GPU: 4.9GB\n",
      "Epoch 1, Batch 33760, Loss: 3.2184, Time: 20085.1s, Step: 33761, GPU: 4.9GB\n",
      "Epoch 1, Batch 33765, Loss: 3.0354, Time: 20088.6s, Step: 33766, GPU: 4.9GB\n",
      "Epoch 1, Batch 33770, Loss: 3.3251, Time: 20091.1s, Step: 33771, GPU: 4.9GB\n",
      "Epoch 1, Batch 33775, Loss: 2.3057, Time: 20094.5s, Step: 33776, GPU: 4.9GB\n",
      "Epoch 1, Batch 33780, Loss: 3.6884, Time: 20097.0s, Step: 33781, GPU: 4.9GB\n",
      "Epoch 1, Batch 33785, Loss: 2.5833, Time: 20100.4s, Step: 33786, GPU: 4.9GB\n",
      "Epoch 1, Batch 33790, Loss: 2.8541, Time: 20102.9s, Step: 33791, GPU: 4.9GB\n",
      "Epoch 1, Batch 33795, Loss: 2.9043, Time: 20106.4s, Step: 33796, GPU: 4.9GB\n",
      "Epoch 1, Batch 33800, Loss: 3.3950, Time: 20109.0s, Step: 33801, GPU: 4.9GB\n",
      "Epoch 1, Batch 33805, Loss: 3.6551, Time: 20112.4s, Step: 33806, GPU: 4.9GB\n",
      "Epoch 1, Batch 33810, Loss: 2.8478, Time: 20114.9s, Step: 33811, GPU: 4.9GB\n",
      "Epoch 1, Batch 33815, Loss: 3.4666, Time: 20118.3s, Step: 33816, GPU: 4.9GB\n",
      "Epoch 1, Batch 33820, Loss: 3.9490, Time: 20120.9s, Step: 33821, GPU: 4.9GB\n",
      "Epoch 1, Batch 33825, Loss: 3.8566, Time: 20124.3s, Step: 33826, GPU: 4.9GB\n",
      "Epoch 1, Batch 33830, Loss: 3.2538, Time: 20126.8s, Step: 33831, GPU: 4.9GB\n",
      "Epoch 1, Batch 33835, Loss: 3.6457, Time: 20130.2s, Step: 33836, GPU: 4.9GB\n",
      "Epoch 1, Batch 33840, Loss: 4.3875, Time: 20132.8s, Step: 33841, GPU: 4.9GB\n",
      "Epoch 1, Batch 33845, Loss: 2.2550, Time: 20136.2s, Step: 33846, GPU: 4.9GB\n",
      "Epoch 1, Batch 33850, Loss: 3.4407, Time: 20138.7s, Step: 33851, GPU: 4.9GB\n",
      "Epoch 1, Batch 33855, Loss: 3.7534, Time: 20142.2s, Step: 33856, GPU: 4.9GB\n",
      "Epoch 1, Batch 33860, Loss: 4.3224, Time: 20144.7s, Step: 33861, GPU: 4.9GB\n",
      "Epoch 1, Batch 33865, Loss: 3.8028, Time: 20148.1s, Step: 33866, GPU: 4.9GB\n",
      "Epoch 1, Batch 33870, Loss: 2.7051, Time: 20150.6s, Step: 33871, GPU: 4.9GB\n",
      "Epoch 1, Batch 33875, Loss: 2.8756, Time: 20154.1s, Step: 33876, GPU: 4.9GB\n",
      "Epoch 1, Batch 33880, Loss: 2.6595, Time: 20156.6s, Step: 33881, GPU: 4.9GB\n",
      "Epoch 1, Batch 33885, Loss: 3.4712, Time: 20160.0s, Step: 33886, GPU: 4.9GB\n",
      "Epoch 1, Batch 33890, Loss: 3.3298, Time: 20162.5s, Step: 33891, GPU: 4.9GB\n",
      "Epoch 1, Batch 33895, Loss: 4.0372, Time: 20166.0s, Step: 33896, GPU: 4.9GB\n",
      "Epoch 1, Batch 33900, Loss: 3.6781, Time: 20168.5s, Step: 33901, GPU: 4.9GB\n",
      "Epoch 1, Batch 33905, Loss: 3.5361, Time: 20171.9s, Step: 33906, GPU: 4.9GB\n",
      "Epoch 1, Batch 33910, Loss: 3.3492, Time: 20174.5s, Step: 33911, GPU: 4.9GB\n",
      "Epoch 1, Batch 33915, Loss: 2.9725, Time: 20177.9s, Step: 33916, GPU: 4.9GB\n",
      "Epoch 1, Batch 33920, Loss: 3.7424, Time: 20180.4s, Step: 33921, GPU: 4.9GB\n",
      "Epoch 1, Batch 33925, Loss: 2.6237, Time: 20183.9s, Step: 33926, GPU: 4.9GB\n",
      "Epoch 1, Batch 33930, Loss: 4.1610, Time: 20186.4s, Step: 33931, GPU: 4.9GB\n",
      "Epoch 1, Batch 33935, Loss: 2.7677, Time: 20189.8s, Step: 33936, GPU: 4.9GB\n",
      "Epoch 1, Batch 33940, Loss: 3.5425, Time: 20192.3s, Step: 33941, GPU: 4.9GB\n",
      "Epoch 1, Batch 33945, Loss: 2.8885, Time: 20195.7s, Step: 33946, GPU: 4.9GB\n",
      "Epoch 1, Batch 33950, Loss: 3.3759, Time: 20198.3s, Step: 33951, GPU: 4.9GB\n",
      "Epoch 1, Batch 33955, Loss: 3.9558, Time: 20201.7s, Step: 33956, GPU: 4.9GB\n",
      "Epoch 1, Batch 33960, Loss: 2.7558, Time: 20204.2s, Step: 33961, GPU: 4.9GB\n",
      "Epoch 1, Batch 33965, Loss: 2.4410, Time: 20207.6s, Step: 33966, GPU: 4.9GB\n",
      "Epoch 1, Batch 33970, Loss: 3.7346, Time: 20210.1s, Step: 33971, GPU: 4.9GB\n",
      "Epoch 1, Batch 33975, Loss: 3.1506, Time: 20213.5s, Step: 33976, GPU: 4.9GB\n",
      "Epoch 1, Batch 33980, Loss: 2.2993, Time: 20216.0s, Step: 33981, GPU: 4.9GB\n",
      "Epoch 1, Batch 33985, Loss: 2.9357, Time: 20219.4s, Step: 33986, GPU: 4.9GB\n",
      "Epoch 1, Batch 33990, Loss: 4.3442, Time: 20221.9s, Step: 33991, GPU: 4.9GB\n",
      "Epoch 1, Batch 33995, Loss: 3.8881, Time: 20225.3s, Step: 33996, GPU: 4.9GB\n",
      "Epoch 1, Batch 34000, Loss: 2.4155, Time: 20227.9s, Step: 34001, GPU: 4.9GB\n",
      "Epoch 1, Batch 34005, Loss: 2.8683, Time: 20231.3s, Step: 34006, GPU: 4.9GB\n",
      "Epoch 1, Batch 34010, Loss: 2.8075, Time: 20233.8s, Step: 34011, GPU: 4.9GB\n",
      "Epoch 1, Batch 34015, Loss: 3.7690, Time: 20237.3s, Step: 34016, GPU: 4.9GB\n",
      "Epoch 1, Batch 34020, Loss: 3.7460, Time: 20239.8s, Step: 34021, GPU: 4.9GB\n",
      "Epoch 1, Batch 34025, Loss: 3.4212, Time: 20243.2s, Step: 34026, GPU: 4.9GB\n",
      "Epoch 1, Batch 34030, Loss: 3.3983, Time: 20245.7s, Step: 34031, GPU: 4.9GB\n",
      "Epoch 1, Batch 34035, Loss: 3.7159, Time: 20249.1s, Step: 34036, GPU: 4.9GB\n",
      "Epoch 1, Batch 34040, Loss: 2.4815, Time: 20251.6s, Step: 34041, GPU: 4.9GB\n",
      "Epoch 1, Batch 34045, Loss: 3.1568, Time: 20255.0s, Step: 34046, GPU: 4.9GB\n",
      "Epoch 1, Batch 34050, Loss: 3.4209, Time: 20257.6s, Step: 34051, GPU: 4.9GB\n",
      "Epoch 1, Batch 34055, Loss: 3.9365, Time: 20261.0s, Step: 34056, GPU: 4.9GB\n",
      "Epoch 1, Batch 34060, Loss: 3.4474, Time: 20263.5s, Step: 34061, GPU: 4.9GB\n",
      "Epoch 1, Batch 34065, Loss: 3.7939, Time: 20266.9s, Step: 34066, GPU: 4.9GB\n",
      "Epoch 1, Batch 34070, Loss: 3.0838, Time: 20269.5s, Step: 34071, GPU: 4.9GB\n",
      "Epoch 1, Batch 34075, Loss: 3.2711, Time: 20272.9s, Step: 34076, GPU: 4.9GB\n",
      "Epoch 1, Batch 34080, Loss: 2.3126, Time: 20275.4s, Step: 34081, GPU: 4.9GB\n",
      "Epoch 1, Batch 34085, Loss: 3.1397, Time: 20278.8s, Step: 34086, GPU: 4.9GB\n",
      "Epoch 1, Batch 34090, Loss: 3.0592, Time: 20281.3s, Step: 34091, GPU: 4.9GB\n",
      "Epoch 1, Batch 34095, Loss: 3.4806, Time: 20284.7s, Step: 34096, GPU: 4.9GB\n",
      "Epoch 1, Batch 34100, Loss: 3.9184, Time: 20287.2s, Step: 34101, GPU: 4.9GB\n",
      "Epoch 1, Batch 34105, Loss: 3.6659, Time: 20290.7s, Step: 34106, GPU: 4.9GB\n",
      "Epoch 1, Batch 34110, Loss: 2.6171, Time: 20293.2s, Step: 34111, GPU: 4.9GB\n",
      "Epoch 1, Batch 34115, Loss: 3.3342, Time: 20296.6s, Step: 34116, GPU: 4.9GB\n",
      "Epoch 1, Batch 34120, Loss: 3.9292, Time: 20299.1s, Step: 34121, GPU: 4.9GB\n",
      "Epoch 1, Batch 34125, Loss: 3.7238, Time: 20302.5s, Step: 34126, GPU: 4.9GB\n",
      "Epoch 1, Batch 34130, Loss: 2.8671, Time: 20305.1s, Step: 34131, GPU: 4.9GB\n",
      "Epoch 1, Batch 34135, Loss: 3.6672, Time: 20308.5s, Step: 34136, GPU: 4.9GB\n",
      "Epoch 1, Batch 34140, Loss: 3.2137, Time: 20311.0s, Step: 34141, GPU: 4.9GB\n",
      "Epoch 1, Batch 34145, Loss: 3.5611, Time: 20314.4s, Step: 34146, GPU: 4.9GB\n",
      "Epoch 1, Batch 34150, Loss: 3.5186, Time: 20316.9s, Step: 34151, GPU: 4.9GB\n",
      "Epoch 1, Batch 34155, Loss: 3.2591, Time: 20320.3s, Step: 34156, GPU: 4.9GB\n",
      "Epoch 1, Batch 34160, Loss: 3.7911, Time: 20322.8s, Step: 34161, GPU: 4.9GB\n",
      "Epoch 1, Batch 34165, Loss: 3.1200, Time: 20326.3s, Step: 34166, GPU: 4.9GB\n",
      "Epoch 1, Batch 34170, Loss: 4.2643, Time: 20328.8s, Step: 34171, GPU: 4.9GB\n",
      "Epoch 1, Batch 34175, Loss: 3.7421, Time: 20332.2s, Step: 34176, GPU: 4.9GB\n",
      "Epoch 1, Batch 34180, Loss: 3.8177, Time: 20334.7s, Step: 34181, GPU: 4.9GB\n",
      "Epoch 1, Batch 34185, Loss: 3.2298, Time: 20338.1s, Step: 34186, GPU: 4.9GB\n",
      "Epoch 1, Batch 34190, Loss: 2.7869, Time: 20340.6s, Step: 34191, GPU: 4.9GB\n",
      "Epoch 1, Batch 34195, Loss: 2.9809, Time: 20344.0s, Step: 34196, GPU: 4.9GB\n",
      "Epoch 1, Batch 34200, Loss: 4.1042, Time: 20346.6s, Step: 34201, GPU: 4.9GB\n",
      "Epoch 1, Batch 34205, Loss: 3.8056, Time: 20350.1s, Step: 34206, GPU: 4.9GB\n",
      "Epoch 1, Batch 34210, Loss: 3.3366, Time: 20352.6s, Step: 34211, GPU: 4.9GB\n",
      "Epoch 1, Batch 34215, Loss: 3.1205, Time: 20356.1s, Step: 34216, GPU: 4.9GB\n",
      "Epoch 1, Batch 34220, Loss: 3.8618, Time: 20358.6s, Step: 34221, GPU: 4.9GB\n",
      "Epoch 1, Batch 34225, Loss: 3.2439, Time: 20362.0s, Step: 34226, GPU: 4.9GB\n",
      "Epoch 1, Batch 34230, Loss: 2.6028, Time: 20364.5s, Step: 34231, GPU: 4.9GB\n",
      "Epoch 1, Batch 34235, Loss: 3.0181, Time: 20368.0s, Step: 34236, GPU: 4.9GB\n",
      "Epoch 1, Batch 34240, Loss: 3.1827, Time: 20370.5s, Step: 34241, GPU: 4.9GB\n",
      "Epoch 1, Batch 34245, Loss: 3.7713, Time: 20373.9s, Step: 34246, GPU: 4.9GB\n",
      "Epoch 1, Batch 34250, Loss: 3.3876, Time: 20376.5s, Step: 34251, GPU: 4.9GB\n",
      "Epoch 1, Batch 34255, Loss: 3.1614, Time: 20379.9s, Step: 34256, GPU: 4.9GB\n",
      "Epoch 1, Batch 34260, Loss: 3.9961, Time: 20382.4s, Step: 34261, GPU: 4.9GB\n",
      "Epoch 1, Batch 34265, Loss: 3.4096, Time: 20385.8s, Step: 34266, GPU: 4.9GB\n",
      "Epoch 1, Batch 34270, Loss: 3.3408, Time: 20388.4s, Step: 34271, GPU: 4.9GB\n",
      "Epoch 1, Batch 34275, Loss: 3.6412, Time: 20391.8s, Step: 34276, GPU: 4.9GB\n",
      "Epoch 1, Batch 34280, Loss: 3.4065, Time: 20394.3s, Step: 34281, GPU: 4.9GB\n",
      "Epoch 1, Batch 34285, Loss: 3.7329, Time: 20397.7s, Step: 34286, GPU: 4.9GB\n",
      "Epoch 1, Batch 34290, Loss: 3.9746, Time: 20400.2s, Step: 34291, GPU: 4.9GB\n",
      "Epoch 1, Batch 34295, Loss: 3.3636, Time: 20403.6s, Step: 34296, GPU: 4.9GB\n",
      "Epoch 1, Batch 34300, Loss: 3.3130, Time: 20406.1s, Step: 34301, GPU: 4.9GB\n",
      "Epoch 1, Batch 34305, Loss: 3.2014, Time: 20409.6s, Step: 34306, GPU: 4.9GB\n",
      "Epoch 1, Batch 34310, Loss: 3.3001, Time: 20412.1s, Step: 34311, GPU: 4.9GB\n",
      "Epoch 1, Batch 34315, Loss: 3.3191, Time: 20415.5s, Step: 34316, GPU: 4.9GB\n",
      "Epoch 1, Batch 34320, Loss: 3.6095, Time: 20418.1s, Step: 34321, GPU: 4.9GB\n",
      "Epoch 1, Batch 34325, Loss: 3.6162, Time: 20421.5s, Step: 34326, GPU: 4.9GB\n",
      "Epoch 1, Batch 34330, Loss: 3.4653, Time: 20424.0s, Step: 34331, GPU: 4.9GB\n",
      "Epoch 1, Batch 34335, Loss: 3.5576, Time: 20427.4s, Step: 34336, GPU: 4.9GB\n",
      "Epoch 1, Batch 34340, Loss: 3.9465, Time: 20429.9s, Step: 34341, GPU: 4.9GB\n",
      "Epoch 1, Batch 34345, Loss: 3.8832, Time: 20433.3s, Step: 34346, GPU: 4.9GB\n",
      "Epoch 1, Batch 34350, Loss: 3.3752, Time: 20435.9s, Step: 34351, GPU: 4.9GB\n",
      "Epoch 1, Batch 34355, Loss: 2.7436, Time: 20439.3s, Step: 34356, GPU: 4.9GB\n",
      "Epoch 1, Batch 34360, Loss: 3.3439, Time: 20441.8s, Step: 34361, GPU: 4.9GB\n",
      "Epoch 1, Batch 34365, Loss: 4.1792, Time: 20445.2s, Step: 34366, GPU: 4.9GB\n",
      "Epoch 1, Batch 34370, Loss: 3.4909, Time: 20447.8s, Step: 34371, GPU: 4.9GB\n",
      "Epoch 1, Batch 34375, Loss: 4.1204, Time: 20451.2s, Step: 34376, GPU: 4.9GB\n",
      "Epoch 1, Batch 34380, Loss: 3.3371, Time: 20454.0s, Step: 34381, GPU: 4.9GB\n",
      "Epoch 1, Batch 34385, Loss: 3.2847, Time: 20457.4s, Step: 34386, GPU: 4.9GB\n",
      "Epoch 1, Batch 34390, Loss: 4.0901, Time: 20459.9s, Step: 34391, GPU: 4.9GB\n",
      "Epoch 1, Batch 34395, Loss: 3.6407, Time: 20463.3s, Step: 34396, GPU: 4.9GB\n",
      "Epoch 1, Batch 34400, Loss: 2.8792, Time: 20465.9s, Step: 34401, GPU: 4.9GB\n",
      "Epoch 1, Batch 34405, Loss: 3.7000, Time: 20469.4s, Step: 34406, GPU: 4.9GB\n",
      "Epoch 1, Batch 34410, Loss: 3.5956, Time: 20471.9s, Step: 34411, GPU: 4.9GB\n",
      "Epoch 1, Batch 34415, Loss: 2.9408, Time: 20475.3s, Step: 34416, GPU: 4.9GB\n",
      "Epoch 1, Batch 34420, Loss: 3.6840, Time: 20477.9s, Step: 34421, GPU: 4.9GB\n",
      "Epoch 1, Batch 34425, Loss: 3.3347, Time: 20481.3s, Step: 34426, GPU: 4.9GB\n",
      "Epoch 1, Batch 34430, Loss: 3.4746, Time: 20483.8s, Step: 34431, GPU: 4.9GB\n",
      "Epoch 1, Batch 34435, Loss: 2.8256, Time: 20487.2s, Step: 34436, GPU: 4.9GB\n",
      "Epoch 1, Batch 34440, Loss: 2.9134, Time: 20489.7s, Step: 34441, GPU: 4.9GB\n",
      "Epoch 1, Batch 34445, Loss: 3.7878, Time: 20493.1s, Step: 34446, GPU: 4.9GB\n",
      "Epoch 1, Batch 34450, Loss: 4.0493, Time: 20495.7s, Step: 34451, GPU: 4.9GB\n",
      "Epoch 1, Batch 34455, Loss: 4.1506, Time: 20499.1s, Step: 34456, GPU: 4.9GB\n",
      "Epoch 1, Batch 34460, Loss: 2.5809, Time: 20501.6s, Step: 34461, GPU: 4.9GB\n",
      "Epoch 1, Batch 34465, Loss: 3.2926, Time: 20505.1s, Step: 34466, GPU: 4.9GB\n",
      "Epoch 1, Batch 34470, Loss: 3.4099, Time: 20507.6s, Step: 34471, GPU: 4.9GB\n",
      "Epoch 1, Batch 34475, Loss: 3.7946, Time: 20511.0s, Step: 34476, GPU: 4.9GB\n",
      "Epoch 1, Batch 34480, Loss: 3.6941, Time: 20513.6s, Step: 34481, GPU: 4.9GB\n",
      "Epoch 1, Batch 34485, Loss: 3.5711, Time: 20517.0s, Step: 34486, GPU: 4.9GB\n",
      "Epoch 1, Batch 34490, Loss: 3.3059, Time: 20519.5s, Step: 34491, GPU: 4.9GB\n",
      "Epoch 1, Batch 34495, Loss: 2.6950, Time: 20522.9s, Step: 34496, GPU: 4.9GB\n",
      "Epoch 1, Batch 34500, Loss: 3.9614, Time: 20525.5s, Step: 34501, GPU: 4.9GB\n",
      "Epoch 1, Batch 34505, Loss: 2.8393, Time: 20528.9s, Step: 34506, GPU: 4.9GB\n",
      "Epoch 1, Batch 34510, Loss: 3.2293, Time: 20531.4s, Step: 34511, GPU: 4.9GB\n",
      "Epoch 1, Batch 34515, Loss: 2.9549, Time: 20534.8s, Step: 34516, GPU: 4.9GB\n",
      "Epoch 1, Batch 34520, Loss: 2.6353, Time: 20537.3s, Step: 34521, GPU: 4.9GB\n",
      "Epoch 1, Batch 34525, Loss: 2.9054, Time: 20540.7s, Step: 34526, GPU: 4.9GB\n",
      "Epoch 1, Batch 34530, Loss: 3.3039, Time: 20543.2s, Step: 34531, GPU: 4.9GB\n",
      "Epoch 1, Batch 34535, Loss: 3.3860, Time: 20546.6s, Step: 34536, GPU: 4.9GB\n",
      "Epoch 1, Batch 34540, Loss: 3.4080, Time: 20549.2s, Step: 34541, GPU: 4.9GB\n",
      "Epoch 1, Batch 34545, Loss: 3.9262, Time: 20552.6s, Step: 34546, GPU: 4.9GB\n",
      "Epoch 1, Batch 34550, Loss: 3.5948, Time: 20555.1s, Step: 34551, GPU: 4.9GB\n",
      "Epoch 1, Batch 34555, Loss: 3.7211, Time: 20558.5s, Step: 34556, GPU: 4.9GB\n",
      "Epoch 1, Batch 34560, Loss: 2.8956, Time: 20561.0s, Step: 34561, GPU: 4.9GB\n",
      "Epoch 1, Batch 34565, Loss: 3.3289, Time: 20564.4s, Step: 34566, GPU: 4.9GB\n",
      "Epoch 1, Batch 34570, Loss: 3.7033, Time: 20567.0s, Step: 34571, GPU: 4.9GB\n",
      "Epoch 1, Batch 34575, Loss: 3.0195, Time: 20570.4s, Step: 34576, GPU: 4.9GB\n",
      "Epoch 1, Batch 34580, Loss: 4.2946, Time: 20575.2s, Step: 34581, GPU: 4.9GB\n",
      "Epoch 1, Batch 34585, Loss: 3.1085, Time: 20578.6s, Step: 34586, GPU: 4.9GB\n",
      "Epoch 1, Batch 34590, Loss: 3.4097, Time: 20581.1s, Step: 34591, GPU: 4.9GB\n",
      "Epoch 1, Batch 34595, Loss: 3.4014, Time: 20584.5s, Step: 34596, GPU: 4.9GB\n",
      "Epoch 1, Batch 34600, Loss: 3.6768, Time: 20587.2s, Step: 34601, GPU: 4.9GB\n",
      "Epoch 1, Batch 34605, Loss: 3.5957, Time: 20590.6s, Step: 34606, GPU: 4.9GB\n",
      "Epoch 1, Batch 34610, Loss: 4.0108, Time: 20593.1s, Step: 34611, GPU: 4.9GB\n",
      "Epoch 1, Batch 34615, Loss: 2.8913, Time: 20596.5s, Step: 34616, GPU: 4.9GB\n",
      "Epoch 1, Batch 34620, Loss: 3.1007, Time: 20599.1s, Step: 34621, GPU: 4.9GB\n",
      "Epoch 1, Batch 34625, Loss: 3.5838, Time: 20602.4s, Step: 34626, GPU: 4.9GB\n",
      "Epoch 1, Batch 34630, Loss: 3.5174, Time: 20605.0s, Step: 34631, GPU: 4.9GB\n",
      "Epoch 1, Batch 34635, Loss: 3.6183, Time: 20608.4s, Step: 34636, GPU: 4.9GB\n",
      "Epoch 1, Batch 34640, Loss: 3.3498, Time: 20610.9s, Step: 34641, GPU: 4.9GB\n",
      "Epoch 1, Batch 34645, Loss: 3.3833, Time: 20614.3s, Step: 34646, GPU: 4.9GB\n",
      "Epoch 1, Batch 34650, Loss: 3.3136, Time: 20616.8s, Step: 34651, GPU: 4.9GB\n",
      "Epoch 1, Batch 34655, Loss: 3.3620, Time: 20620.2s, Step: 34656, GPU: 4.9GB\n",
      "Epoch 1, Batch 34660, Loss: 3.8284, Time: 20622.8s, Step: 34661, GPU: 4.9GB\n",
      "Epoch 1, Batch 34665, Loss: 2.7351, Time: 20626.2s, Step: 34666, GPU: 4.9GB\n",
      "Epoch 1, Batch 34670, Loss: 2.9459, Time: 20628.7s, Step: 34671, GPU: 4.9GB\n",
      "Epoch 1, Batch 34675, Loss: 3.1190, Time: 20632.1s, Step: 34676, GPU: 4.9GB\n",
      "Epoch 1, Batch 34680, Loss: 3.5659, Time: 20634.6s, Step: 34681, GPU: 4.9GB\n",
      "Epoch 1, Batch 34685, Loss: 3.0021, Time: 20638.0s, Step: 34686, GPU: 4.9GB\n",
      "Epoch 1, Batch 34690, Loss: 3.4979, Time: 20640.5s, Step: 34691, GPU: 4.9GB\n",
      "Epoch 1, Batch 34695, Loss: 3.2755, Time: 20644.0s, Step: 34696, GPU: 4.9GB\n",
      "Epoch 1, Batch 34700, Loss: 3.4655, Time: 20646.5s, Step: 34701, GPU: 4.9GB\n",
      "Epoch 1, Batch 34705, Loss: 2.7932, Time: 20649.9s, Step: 34706, GPU: 4.9GB\n",
      "Epoch 1, Batch 34710, Loss: 3.8318, Time: 20652.4s, Step: 34711, GPU: 4.9GB\n",
      "Epoch 1, Batch 34715, Loss: 3.5087, Time: 20655.9s, Step: 34716, GPU: 4.9GB\n",
      "Epoch 1, Batch 34720, Loss: 3.0243, Time: 20658.4s, Step: 34721, GPU: 4.9GB\n",
      "Epoch 1, Batch 34725, Loss: 3.5376, Time: 20661.9s, Step: 34726, GPU: 4.9GB\n",
      "Epoch 1, Batch 34730, Loss: 3.8688, Time: 20664.4s, Step: 34731, GPU: 4.9GB\n",
      "Epoch 1, Batch 34735, Loss: 3.6292, Time: 20667.8s, Step: 34736, GPU: 4.9GB\n",
      "Epoch 1, Batch 34740, Loss: 3.5127, Time: 20670.3s, Step: 34741, GPU: 4.9GB\n",
      "Epoch 1, Batch 34745, Loss: 3.3172, Time: 20673.7s, Step: 34746, GPU: 4.9GB\n",
      "Epoch 1, Batch 34750, Loss: 3.6522, Time: 20676.3s, Step: 34751, GPU: 4.9GB\n",
      "Epoch 1, Batch 34755, Loss: 3.9129, Time: 20679.7s, Step: 34756, GPU: 4.9GB\n",
      "Epoch 1, Batch 34760, Loss: 3.4428, Time: 20682.2s, Step: 34761, GPU: 4.9GB\n",
      "Epoch 1, Batch 34765, Loss: 4.5719, Time: 20685.6s, Step: 34766, GPU: 4.9GB\n",
      "Epoch 1, Batch 34770, Loss: 3.4683, Time: 20688.1s, Step: 34771, GPU: 4.9GB\n",
      "Epoch 1, Batch 34775, Loss: 2.8987, Time: 20691.5s, Step: 34776, GPU: 4.9GB\n",
      "Epoch 1, Batch 34780, Loss: 3.5858, Time: 20694.0s, Step: 34781, GPU: 4.9GB\n",
      "Epoch 1, Batch 34785, Loss: 2.5251, Time: 20697.4s, Step: 34786, GPU: 4.9GB\n",
      "Epoch 1, Batch 34790, Loss: 3.1541, Time: 20699.9s, Step: 34791, GPU: 4.9GB\n",
      "Epoch 1, Batch 34795, Loss: 4.6681, Time: 20703.4s, Step: 34796, GPU: 4.9GB\n",
      "Epoch 1, Batch 34800, Loss: 3.3029, Time: 20706.1s, Step: 34801, GPU: 4.9GB\n",
      "Epoch 1, Batch 34805, Loss: 3.9253, Time: 20709.5s, Step: 34806, GPU: 4.9GB\n",
      "Epoch 1, Batch 34810, Loss: 4.6644, Time: 20712.0s, Step: 34811, GPU: 4.9GB\n",
      "Epoch 1, Batch 34815, Loss: 4.0396, Time: 20715.5s, Step: 34816, GPU: 4.9GB\n",
      "Epoch 1, Batch 34820, Loss: 3.6395, Time: 20718.0s, Step: 34821, GPU: 4.9GB\n",
      "Epoch 1, Batch 34825, Loss: 4.0974, Time: 20721.4s, Step: 34826, GPU: 4.9GB\n",
      "Epoch 1, Batch 34830, Loss: 3.0189, Time: 20724.0s, Step: 34831, GPU: 4.9GB\n",
      "Epoch 1, Batch 34835, Loss: 3.0984, Time: 20727.4s, Step: 34836, GPU: 4.9GB\n",
      "Epoch 1, Batch 34840, Loss: 3.7994, Time: 20729.9s, Step: 34841, GPU: 4.9GB\n",
      "Epoch 1, Batch 34845, Loss: 3.0827, Time: 20733.3s, Step: 34846, GPU: 4.9GB\n",
      "Epoch 1, Batch 34850, Loss: 3.6404, Time: 20735.9s, Step: 34851, GPU: 4.9GB\n",
      "Epoch 1, Batch 34855, Loss: 3.1022, Time: 20739.3s, Step: 34856, GPU: 4.9GB\n",
      "Epoch 1, Batch 34860, Loss: 3.3185, Time: 20741.8s, Step: 34861, GPU: 4.9GB\n",
      "Epoch 1, Batch 34865, Loss: 4.0715, Time: 20745.3s, Step: 34866, GPU: 4.9GB\n",
      "Epoch 1, Batch 34870, Loss: 3.3622, Time: 20747.8s, Step: 34871, GPU: 4.9GB\n",
      "Epoch 1, Batch 34875, Loss: 3.7523, Time: 20751.2s, Step: 34876, GPU: 4.9GB\n",
      "Epoch 1, Batch 34880, Loss: 4.0928, Time: 20753.8s, Step: 34881, GPU: 4.9GB\n",
      "Epoch 1, Batch 34885, Loss: 3.1614, Time: 20757.2s, Step: 34886, GPU: 4.9GB\n",
      "Epoch 1, Batch 34890, Loss: 3.5208, Time: 20759.8s, Step: 34891, GPU: 4.9GB\n",
      "Epoch 1, Batch 34895, Loss: 3.2557, Time: 20763.2s, Step: 34896, GPU: 4.9GB\n",
      "Epoch 1, Batch 34900, Loss: 2.3857, Time: 20765.7s, Step: 34901, GPU: 4.9GB\n",
      "Epoch 1, Batch 34905, Loss: 3.7433, Time: 20769.2s, Step: 34906, GPU: 4.9GB\n",
      "Epoch 1, Batch 34910, Loss: 3.9077, Time: 20771.7s, Step: 34911, GPU: 4.9GB\n",
      "Epoch 1, Batch 34915, Loss: 3.1870, Time: 20775.1s, Step: 34916, GPU: 4.9GB\n",
      "Epoch 1, Batch 34920, Loss: 3.5993, Time: 20777.7s, Step: 34921, GPU: 4.9GB\n",
      "Epoch 1, Batch 34925, Loss: 3.5781, Time: 20781.1s, Step: 34926, GPU: 4.9GB\n",
      "Epoch 1, Batch 34930, Loss: 3.3285, Time: 20783.6s, Step: 34931, GPU: 4.9GB\n",
      "Epoch 1, Batch 34935, Loss: 3.2885, Time: 20787.0s, Step: 34936, GPU: 4.9GB\n",
      "Epoch 1, Batch 34940, Loss: 3.3391, Time: 20789.6s, Step: 34941, GPU: 4.9GB\n",
      "Epoch 1, Batch 34945, Loss: 3.1705, Time: 20793.0s, Step: 34946, GPU: 4.9GB\n",
      "Epoch 1, Batch 34950, Loss: 2.7426, Time: 20795.5s, Step: 34951, GPU: 4.9GB\n",
      "Epoch 1, Batch 34955, Loss: 3.8154, Time: 20799.0s, Step: 34956, GPU: 4.9GB\n",
      "Epoch 1, Batch 34960, Loss: 3.5905, Time: 20801.5s, Step: 34961, GPU: 4.9GB\n",
      "Epoch 1, Batch 34965, Loss: 3.7707, Time: 20804.9s, Step: 34966, GPU: 4.9GB\n",
      "Epoch 1, Batch 34970, Loss: 2.7273, Time: 20807.8s, Step: 34971, GPU: 4.9GB\n",
      "Epoch 1, Batch 34975, Loss: 3.7238, Time: 20811.3s, Step: 34976, GPU: 4.9GB\n",
      "Epoch 1, Batch 34980, Loss: 3.7583, Time: 20813.8s, Step: 34981, GPU: 4.9GB\n",
      "Epoch 1, Batch 34985, Loss: 3.2801, Time: 20817.2s, Step: 34986, GPU: 4.9GB\n",
      "Epoch 1, Batch 34990, Loss: 3.9828, Time: 20819.7s, Step: 34991, GPU: 4.9GB\n",
      "Epoch 1, Batch 34995, Loss: 3.0015, Time: 20823.2s, Step: 34996, GPU: 4.9GB\n",
      "Epoch 1, Batch 35000, Loss: 3.0725, Time: 20825.8s, Step: 35001, GPU: 4.9GB\n",
      "Epoch 1, Batch 35005, Loss: 4.5638, Time: 20829.2s, Step: 35006, GPU: 4.9GB\n",
      "Epoch 1, Batch 35010, Loss: 4.1370, Time: 20831.7s, Step: 35011, GPU: 4.9GB\n",
      "Epoch 1, Batch 35015, Loss: 3.6890, Time: 20835.1s, Step: 35016, GPU: 4.9GB\n",
      "Epoch 1, Batch 35020, Loss: 3.7507, Time: 20837.6s, Step: 35021, GPU: 4.9GB\n",
      "Epoch 1, Batch 35025, Loss: 3.1724, Time: 20841.1s, Step: 35026, GPU: 4.9GB\n",
      "Epoch 1, Batch 35030, Loss: 3.2680, Time: 20843.6s, Step: 35031, GPU: 4.9GB\n",
      "Epoch 1, Batch 35035, Loss: 4.1569, Time: 20847.0s, Step: 35036, GPU: 4.9GB\n",
      "Epoch 1, Batch 35040, Loss: 3.7858, Time: 20849.6s, Step: 35041, GPU: 4.9GB\n",
      "Epoch 1, Batch 35045, Loss: 3.6631, Time: 20853.0s, Step: 35046, GPU: 4.9GB\n",
      "Epoch 1, Batch 35050, Loss: 3.2661, Time: 20855.5s, Step: 35051, GPU: 4.9GB\n",
      "Epoch 1, Batch 35055, Loss: 3.4658, Time: 20858.9s, Step: 35056, GPU: 4.9GB\n",
      "Epoch 1, Batch 35060, Loss: 3.0445, Time: 20861.4s, Step: 35061, GPU: 4.9GB\n",
      "Epoch 1, Batch 35065, Loss: 4.0693, Time: 20864.9s, Step: 35066, GPU: 4.9GB\n",
      "Epoch 1, Batch 35070, Loss: 4.0970, Time: 20867.5s, Step: 35071, GPU: 4.9GB\n",
      "Epoch 1, Batch 35075, Loss: 3.3922, Time: 20870.9s, Step: 35076, GPU: 4.9GB\n",
      "Epoch 1, Batch 35080, Loss: 3.3603, Time: 20873.4s, Step: 35081, GPU: 4.9GB\n",
      "Epoch 1, Batch 35085, Loss: 3.3838, Time: 20876.9s, Step: 35086, GPU: 4.9GB\n",
      "Epoch 1, Batch 35090, Loss: 4.2663, Time: 20879.4s, Step: 35091, GPU: 4.9GB\n",
      "Epoch 1, Batch 35095, Loss: 4.0428, Time: 20882.8s, Step: 35096, GPU: 4.9GB\n",
      "Epoch 1, Batch 35100, Loss: 3.2412, Time: 20885.3s, Step: 35101, GPU: 4.9GB\n",
      "Epoch 1, Batch 35105, Loss: 4.0949, Time: 20888.7s, Step: 35106, GPU: 4.9GB\n",
      "Epoch 1, Batch 35110, Loss: 2.4049, Time: 20891.2s, Step: 35111, GPU: 4.9GB\n",
      "Epoch 1, Batch 35115, Loss: 3.4749, Time: 20894.7s, Step: 35116, GPU: 4.9GB\n",
      "Epoch 1, Batch 35120, Loss: 3.6441, Time: 20897.2s, Step: 35121, GPU: 4.9GB\n",
      "Epoch 1, Batch 35125, Loss: 3.6566, Time: 20900.7s, Step: 35126, GPU: 4.9GB\n",
      "Epoch 1, Batch 35130, Loss: 3.0022, Time: 20903.2s, Step: 35131, GPU: 4.9GB\n",
      "Epoch 1, Batch 35135, Loss: 4.2666, Time: 20906.6s, Step: 35136, GPU: 4.9GB\n",
      "Epoch 1, Batch 35140, Loss: 2.8460, Time: 20909.2s, Step: 35141, GPU: 4.9GB\n",
      "Epoch 1, Batch 35145, Loss: 2.8365, Time: 20912.6s, Step: 35146, GPU: 4.9GB\n",
      "Epoch 1, Batch 35150, Loss: 3.4806, Time: 20915.1s, Step: 35151, GPU: 4.9GB\n",
      "Epoch 1, Batch 35155, Loss: 3.6411, Time: 20918.5s, Step: 35156, GPU: 4.9GB\n",
      "Epoch 1, Batch 35160, Loss: 2.7886, Time: 20921.1s, Step: 35161, GPU: 4.9GB\n",
      "Epoch 1, Batch 35165, Loss: 2.5525, Time: 20924.5s, Step: 35166, GPU: 4.9GB\n",
      "Epoch 1, Batch 35170, Loss: 2.5074, Time: 20927.0s, Step: 35171, GPU: 4.9GB\n",
      "Epoch 1, Batch 35175, Loss: 3.0263, Time: 20930.4s, Step: 35176, GPU: 4.9GB\n",
      "Epoch 1, Batch 35180, Loss: 3.2615, Time: 20933.0s, Step: 35181, GPU: 4.9GB\n",
      "Epoch 1, Batch 35185, Loss: 3.6489, Time: 20936.4s, Step: 35186, GPU: 4.9GB\n",
      "Epoch 1, Batch 35190, Loss: 3.6517, Time: 20938.9s, Step: 35191, GPU: 4.9GB\n",
      "Epoch 1, Batch 35195, Loss: 3.0020, Time: 20942.4s, Step: 35196, GPU: 4.9GB\n",
      "Epoch 1, Batch 35200, Loss: 3.0721, Time: 20944.9s, Step: 35201, GPU: 4.9GB\n",
      "Epoch 1, Batch 35205, Loss: 3.9482, Time: 20948.3s, Step: 35206, GPU: 4.9GB\n",
      "Epoch 1, Batch 35210, Loss: 3.9063, Time: 20950.9s, Step: 35211, GPU: 4.9GB\n",
      "Epoch 1, Batch 35215, Loss: 4.0127, Time: 20954.3s, Step: 35216, GPU: 4.9GB\n",
      "Epoch 1, Batch 35220, Loss: 1.9706, Time: 20956.8s, Step: 35221, GPU: 4.9GB\n",
      "Epoch 1, Batch 35225, Loss: 3.0061, Time: 20960.2s, Step: 35226, GPU: 4.9GB\n",
      "Epoch 1, Batch 35230, Loss: 2.9482, Time: 20962.8s, Step: 35231, GPU: 4.9GB\n",
      "Epoch 1, Batch 35235, Loss: 3.0968, Time: 20966.2s, Step: 35236, GPU: 4.9GB\n",
      "Epoch 1, Batch 35240, Loss: 4.0426, Time: 20968.7s, Step: 35241, GPU: 4.9GB\n",
      "Epoch 1, Batch 35245, Loss: 3.8894, Time: 20972.1s, Step: 35246, GPU: 4.9GB\n",
      "Epoch 1, Batch 35250, Loss: 3.3634, Time: 20974.6s, Step: 35251, GPU: 4.9GB\n",
      "Epoch 1, Batch 35255, Loss: 2.9651, Time: 20978.0s, Step: 35256, GPU: 4.9GB\n",
      "Epoch 1, Batch 35260, Loss: 3.2347, Time: 20980.5s, Step: 35261, GPU: 4.9GB\n",
      "Epoch 1, Batch 35265, Loss: 3.7955, Time: 20983.9s, Step: 35266, GPU: 4.9GB\n",
      "Epoch 1, Batch 35270, Loss: 3.0833, Time: 20986.4s, Step: 35271, GPU: 4.9GB\n",
      "Epoch 1, Batch 35275, Loss: 3.7189, Time: 20989.8s, Step: 35276, GPU: 4.9GB\n",
      "Epoch 1, Batch 35280, Loss: 3.0787, Time: 20992.3s, Step: 35281, GPU: 4.9GB\n",
      "Epoch 1, Batch 35285, Loss: 3.1644, Time: 20995.7s, Step: 35286, GPU: 4.9GB\n",
      "Epoch 1, Batch 35290, Loss: 3.6789, Time: 20998.2s, Step: 35291, GPU: 4.9GB\n",
      "Epoch 1, Batch 35295, Loss: 2.9131, Time: 21001.6s, Step: 35296, GPU: 4.9GB\n",
      "Epoch 1, Batch 35300, Loss: 3.2306, Time: 21004.1s, Step: 35301, GPU: 4.9GB\n",
      "Epoch 1, Batch 35305, Loss: 3.2042, Time: 21007.5s, Step: 35306, GPU: 4.9GB\n",
      "Epoch 1, Batch 35310, Loss: 3.7870, Time: 21010.0s, Step: 35311, GPU: 4.9GB\n",
      "Epoch 1, Batch 35315, Loss: 3.5911, Time: 21013.4s, Step: 35316, GPU: 4.9GB\n",
      "Epoch 1, Batch 35320, Loss: 4.1558, Time: 21016.0s, Step: 35321, GPU: 4.9GB\n",
      "Epoch 1, Batch 35325, Loss: 2.7115, Time: 21019.4s, Step: 35326, GPU: 4.9GB\n",
      "Epoch 1, Batch 35330, Loss: 2.7394, Time: 21021.9s, Step: 35331, GPU: 4.9GB\n",
      "Epoch 1, Batch 35335, Loss: 3.7256, Time: 21025.3s, Step: 35336, GPU: 4.9GB\n",
      "Epoch 1, Batch 35340, Loss: 4.0801, Time: 21027.8s, Step: 35341, GPU: 4.9GB\n",
      "Epoch 1, Batch 35345, Loss: 3.3019, Time: 21031.2s, Step: 35346, GPU: 4.9GB\n",
      "Epoch 1, Batch 35350, Loss: 3.7881, Time: 21033.8s, Step: 35351, GPU: 4.9GB\n",
      "Epoch 1, Batch 35355, Loss: 2.5716, Time: 21037.2s, Step: 35356, GPU: 4.9GB\n",
      "Epoch 1, Batch 35360, Loss: 3.5122, Time: 21039.7s, Step: 35361, GPU: 4.9GB\n",
      "Epoch 1, Batch 35365, Loss: 3.1584, Time: 21043.1s, Step: 35366, GPU: 4.9GB\n",
      "Epoch 1, Batch 35370, Loss: 3.6572, Time: 21045.6s, Step: 35371, GPU: 4.9GB\n",
      "Epoch 1, Batch 35375, Loss: 2.9644, Time: 21049.0s, Step: 35376, GPU: 4.9GB\n",
      "Epoch 1, Batch 35380, Loss: 3.8784, Time: 21051.5s, Step: 35381, GPU: 4.9GB\n",
      "Epoch 1, Batch 35385, Loss: 3.0334, Time: 21054.9s, Step: 35386, GPU: 4.9GB\n",
      "Epoch 1, Batch 35390, Loss: 3.5897, Time: 21057.4s, Step: 35391, GPU: 4.9GB\n",
      "Epoch 1, Batch 35395, Loss: 3.6183, Time: 21060.9s, Step: 35396, GPU: 4.9GB\n",
      "Epoch 1, Batch 35400, Loss: 3.5300, Time: 21063.5s, Step: 35401, GPU: 4.9GB\n",
      "Epoch 1, Batch 35405, Loss: 3.4139, Time: 21066.9s, Step: 35406, GPU: 4.9GB\n",
      "Epoch 1, Batch 35410, Loss: 3.7836, Time: 21069.4s, Step: 35411, GPU: 4.9GB\n",
      "Epoch 1, Batch 35415, Loss: 4.1220, Time: 21072.8s, Step: 35416, GPU: 4.9GB\n",
      "Epoch 1, Batch 35420, Loss: 3.7343, Time: 21075.3s, Step: 35421, GPU: 4.9GB\n",
      "Epoch 1, Batch 35425, Loss: 3.2174, Time: 21078.8s, Step: 35426, GPU: 4.9GB\n",
      "Epoch 1, Batch 35430, Loss: 3.5206, Time: 21081.3s, Step: 35431, GPU: 4.9GB\n",
      "Epoch 1, Batch 35435, Loss: 3.3666, Time: 21084.7s, Step: 35436, GPU: 4.9GB\n",
      "Epoch 1, Batch 35440, Loss: 3.8285, Time: 21087.3s, Step: 35441, GPU: 4.9GB\n",
      "Epoch 1, Batch 35445, Loss: 2.2282, Time: 21090.7s, Step: 35446, GPU: 4.9GB\n",
      "Epoch 1, Batch 35450, Loss: 3.5442, Time: 21093.2s, Step: 35451, GPU: 4.9GB\n",
      "Epoch 1, Batch 35455, Loss: 3.6779, Time: 21096.6s, Step: 35456, GPU: 4.9GB\n",
      "Epoch 1, Batch 35460, Loss: 3.7083, Time: 21099.1s, Step: 35461, GPU: 4.9GB\n",
      "Epoch 1, Batch 35465, Loss: 3.0510, Time: 21102.6s, Step: 35466, GPU: 4.9GB\n",
      "Epoch 1, Batch 35470, Loss: 2.8397, Time: 21105.1s, Step: 35471, GPU: 4.9GB\n",
      "Epoch 1, Batch 35475, Loss: 4.2023, Time: 21108.5s, Step: 35476, GPU: 4.9GB\n",
      "Epoch 1, Batch 35480, Loss: 2.9817, Time: 21111.0s, Step: 35481, GPU: 4.9GB\n",
      "Epoch 1, Batch 35485, Loss: 2.1480, Time: 21114.4s, Step: 35486, GPU: 4.9GB\n",
      "Epoch 1, Batch 35490, Loss: 3.9887, Time: 21117.0s, Step: 35491, GPU: 4.9GB\n",
      "Epoch 1, Batch 35495, Loss: 3.2077, Time: 21120.4s, Step: 35496, GPU: 4.9GB\n",
      "Epoch 1, Batch 35500, Loss: 3.6766, Time: 21122.9s, Step: 35501, GPU: 4.9GB\n",
      "Epoch 1, Batch 35505, Loss: 3.0215, Time: 21126.3s, Step: 35506, GPU: 4.9GB\n",
      "Epoch 1, Batch 35510, Loss: 3.2675, Time: 21128.8s, Step: 35511, GPU: 4.9GB\n",
      "Epoch 1, Batch 35515, Loss: 2.7416, Time: 21132.2s, Step: 35516, GPU: 4.9GB\n",
      "Epoch 1, Batch 35520, Loss: 3.2900, Time: 21134.8s, Step: 35521, GPU: 4.9GB\n",
      "Epoch 1, Batch 35525, Loss: 3.1045, Time: 21138.2s, Step: 35526, GPU: 4.9GB\n",
      "Epoch 1, Batch 35530, Loss: 3.3453, Time: 21140.7s, Step: 35531, GPU: 4.9GB\n",
      "Epoch 1, Batch 35535, Loss: 3.0113, Time: 21144.1s, Step: 35536, GPU: 4.9GB\n",
      "Epoch 1, Batch 35540, Loss: 3.8264, Time: 21146.7s, Step: 35541, GPU: 4.9GB\n",
      "Epoch 1, Batch 35545, Loss: 3.6900, Time: 21150.1s, Step: 35546, GPU: 4.9GB\n",
      "Epoch 1, Batch 35550, Loss: 3.1175, Time: 21152.6s, Step: 35551, GPU: 4.9GB\n",
      "Epoch 1, Batch 35555, Loss: 3.2011, Time: 21156.0s, Step: 35556, GPU: 4.9GB\n",
      "Epoch 1, Batch 35560, Loss: 3.2283, Time: 21158.5s, Step: 35561, GPU: 4.9GB\n",
      "Epoch 1, Batch 35565, Loss: 2.9227, Time: 21162.0s, Step: 35566, GPU: 4.9GB\n",
      "Epoch 1, Batch 35570, Loss: 2.9597, Time: 21164.5s, Step: 35571, GPU: 4.9GB\n",
      "Epoch 1, Batch 35575, Loss: 3.3477, Time: 21167.9s, Step: 35576, GPU: 4.9GB\n",
      "Epoch 1, Batch 35580, Loss: 3.4512, Time: 21170.4s, Step: 35581, GPU: 4.9GB\n",
      "Epoch 1, Batch 35585, Loss: 3.7386, Time: 21173.9s, Step: 35586, GPU: 4.9GB\n",
      "Epoch 1, Batch 35590, Loss: 2.7344, Time: 21176.4s, Step: 35591, GPU: 4.9GB\n",
      "Epoch 1, Batch 35595, Loss: 3.8348, Time: 21179.8s, Step: 35596, GPU: 4.9GB\n",
      "Epoch 1, Batch 35600, Loss: 3.4873, Time: 21182.4s, Step: 35601, GPU: 4.9GB\n",
      "Epoch 1, Batch 35605, Loss: 3.4090, Time: 21185.8s, Step: 35606, GPU: 4.9GB\n",
      "Epoch 1, Batch 35610, Loss: 3.4431, Time: 21188.4s, Step: 35611, GPU: 4.9GB\n",
      "Epoch 1, Batch 35615, Loss: 3.3088, Time: 21191.8s, Step: 35616, GPU: 4.9GB\n",
      "Epoch 1, Batch 35620, Loss: 3.5706, Time: 21194.3s, Step: 35621, GPU: 4.9GB\n",
      "Epoch 1, Batch 35625, Loss: 3.8385, Time: 21197.7s, Step: 35626, GPU: 4.9GB\n",
      "Epoch 1, Batch 35630, Loss: 4.8892, Time: 21200.2s, Step: 35631, GPU: 4.9GB\n",
      "Epoch 1, Batch 35635, Loss: 3.5910, Time: 21203.6s, Step: 35636, GPU: 4.9GB\n",
      "Epoch 1, Batch 35640, Loss: 3.7996, Time: 21206.2s, Step: 35641, GPU: 4.9GB\n",
      "Epoch 1, Batch 35645, Loss: 3.6690, Time: 21209.6s, Step: 35646, GPU: 4.9GB\n",
      "Epoch 1, Batch 35650, Loss: 2.6896, Time: 21212.1s, Step: 35651, GPU: 4.9GB\n",
      "Epoch 1, Batch 35655, Loss: 3.5222, Time: 21215.5s, Step: 35656, GPU: 4.9GB\n",
      "Epoch 1, Batch 35660, Loss: 2.9291, Time: 21218.0s, Step: 35661, GPU: 4.9GB\n",
      "Epoch 1, Batch 35665, Loss: 3.8351, Time: 21221.5s, Step: 35666, GPU: 4.9GB\n",
      "Epoch 1, Batch 35670, Loss: 4.0689, Time: 21224.0s, Step: 35671, GPU: 4.9GB\n",
      "Epoch 1, Batch 35675, Loss: 3.2900, Time: 21227.4s, Step: 35676, GPU: 4.9GB\n",
      "Epoch 1, Batch 35680, Loss: 3.1170, Time: 21229.9s, Step: 35681, GPU: 4.9GB\n",
      "Epoch 1, Batch 35685, Loss: 2.9100, Time: 21233.3s, Step: 35686, GPU: 4.9GB\n",
      "Epoch 1, Batch 35690, Loss: 3.7324, Time: 21235.8s, Step: 35691, GPU: 4.9GB\n",
      "Epoch 1, Batch 35695, Loss: 3.5306, Time: 21239.3s, Step: 35696, GPU: 4.9GB\n",
      "Epoch 1, Batch 35700, Loss: 3.0669, Time: 21241.8s, Step: 35701, GPU: 4.9GB\n",
      "Epoch 1, Batch 35705, Loss: 2.3926, Time: 21245.2s, Step: 35706, GPU: 4.9GB\n",
      "Epoch 1, Batch 35710, Loss: 2.9912, Time: 21247.7s, Step: 35711, GPU: 4.9GB\n",
      "Epoch 1, Batch 35715, Loss: 2.9721, Time: 21251.2s, Step: 35716, GPU: 4.9GB\n",
      "Epoch 1, Batch 35720, Loss: 2.8282, Time: 21253.7s, Step: 35721, GPU: 4.9GB\n",
      "Epoch 1, Batch 35725, Loss: 3.1157, Time: 21257.2s, Step: 35726, GPU: 4.9GB\n",
      "Epoch 1, Batch 35730, Loss: 4.5377, Time: 21259.7s, Step: 35731, GPU: 4.9GB\n",
      "Epoch 1, Batch 35735, Loss: 3.0585, Time: 21263.2s, Step: 35736, GPU: 4.9GB\n",
      "Epoch 1, Batch 35740, Loss: 4.1115, Time: 21265.7s, Step: 35741, GPU: 4.9GB\n",
      "Epoch 1, Batch 35745, Loss: 3.6409, Time: 21269.1s, Step: 35746, GPU: 4.9GB\n",
      "Epoch 1, Batch 35750, Loss: 3.9684, Time: 21271.7s, Step: 35751, GPU: 4.9GB\n",
      "Epoch 1, Batch 35755, Loss: 3.9759, Time: 21275.1s, Step: 35756, GPU: 4.9GB\n",
      "Epoch 1, Batch 35760, Loss: 4.1761, Time: 21277.6s, Step: 35761, GPU: 4.9GB\n",
      "Epoch 1, Batch 35765, Loss: 3.1671, Time: 21281.0s, Step: 35766, GPU: 4.9GB\n",
      "Epoch 1, Batch 35770, Loss: 2.9206, Time: 21283.6s, Step: 35771, GPU: 4.9GB\n",
      "Epoch 1, Batch 35775, Loss: 3.4218, Time: 21287.0s, Step: 35776, GPU: 4.9GB\n",
      "Epoch 1, Batch 35780, Loss: 2.8096, Time: 21289.6s, Step: 35781, GPU: 4.9GB\n",
      "Epoch 1, Batch 35785, Loss: 3.3101, Time: 21293.0s, Step: 35786, GPU: 4.9GB\n",
      "Epoch 1, Batch 35790, Loss: 3.2775, Time: 21295.5s, Step: 35791, GPU: 4.9GB\n",
      "Epoch 1, Batch 35795, Loss: 3.4027, Time: 21299.0s, Step: 35796, GPU: 4.9GB\n",
      "Epoch 1, Batch 35800, Loss: 4.2380, Time: 21301.6s, Step: 35801, GPU: 4.9GB\n",
      "Epoch 1, Batch 35805, Loss: 3.4684, Time: 21305.0s, Step: 35806, GPU: 4.9GB\n",
      "Epoch 1, Batch 35810, Loss: 3.9240, Time: 21307.5s, Step: 35811, GPU: 4.9GB\n",
      "Epoch 1, Batch 35815, Loss: 4.2025, Time: 21311.0s, Step: 35816, GPU: 4.9GB\n",
      "Epoch 1, Batch 35820, Loss: 4.5784, Time: 21313.5s, Step: 35821, GPU: 4.9GB\n",
      "Epoch 1, Batch 35825, Loss: 3.1680, Time: 21317.0s, Step: 35826, GPU: 4.9GB\n",
      "Epoch 1, Batch 35830, Loss: 3.1024, Time: 21319.5s, Step: 35831, GPU: 4.9GB\n",
      "Epoch 1, Batch 35835, Loss: 4.4737, Time: 21323.0s, Step: 35836, GPU: 4.9GB\n",
      "Epoch 1, Batch 35840, Loss: 2.8900, Time: 21325.5s, Step: 35841, GPU: 4.9GB\n",
      "Epoch 1, Batch 35845, Loss: 3.6196, Time: 21328.9s, Step: 35846, GPU: 4.9GB\n",
      "Epoch 1, Batch 35850, Loss: 3.3665, Time: 21331.5s, Step: 35851, GPU: 4.9GB\n",
      "Epoch 1, Batch 35855, Loss: 3.4425, Time: 21334.9s, Step: 35856, GPU: 4.9GB\n",
      "Epoch 1, Batch 35860, Loss: 4.1898, Time: 21337.4s, Step: 35861, GPU: 4.9GB\n",
      "Epoch 1, Batch 35865, Loss: 3.8628, Time: 21340.9s, Step: 35866, GPU: 4.9GB\n",
      "Epoch 1, Batch 35870, Loss: 4.0106, Time: 21343.4s, Step: 35871, GPU: 4.9GB\n",
      "Epoch 1, Batch 35875, Loss: 4.0525, Time: 21346.9s, Step: 35876, GPU: 4.9GB\n",
      "Epoch 1, Batch 35880, Loss: 3.3249, Time: 21349.4s, Step: 35881, GPU: 4.9GB\n",
      "Epoch 1, Batch 35885, Loss: 3.1283, Time: 21352.8s, Step: 35886, GPU: 4.9GB\n",
      "Epoch 1, Batch 35890, Loss: 3.5668, Time: 21355.4s, Step: 35891, GPU: 4.9GB\n",
      "Epoch 1, Batch 35895, Loss: 3.5113, Time: 21358.8s, Step: 35896, GPU: 4.9GB\n",
      "Epoch 1, Batch 35900, Loss: 3.4519, Time: 21361.3s, Step: 35901, GPU: 4.9GB\n",
      "Epoch 1, Batch 35905, Loss: 3.2059, Time: 21364.8s, Step: 35906, GPU: 4.9GB\n",
      "Epoch 1, Batch 35910, Loss: 3.1818, Time: 21367.3s, Step: 35911, GPU: 4.9GB\n",
      "Epoch 1, Batch 35915, Loss: 3.0410, Time: 21370.7s, Step: 35916, GPU: 4.9GB\n",
      "Epoch 1, Batch 35920, Loss: 3.3692, Time: 21373.3s, Step: 35921, GPU: 4.9GB\n",
      "Epoch 1, Batch 35925, Loss: 2.9617, Time: 21376.7s, Step: 35926, GPU: 4.9GB\n",
      "Epoch 1, Batch 35930, Loss: 3.3673, Time: 21379.2s, Step: 35931, GPU: 4.9GB\n",
      "Epoch 1, Batch 35935, Loss: 3.0333, Time: 21382.7s, Step: 35936, GPU: 4.9GB\n",
      "Epoch 1, Batch 35940, Loss: 3.3825, Time: 21385.2s, Step: 35941, GPU: 4.9GB\n",
      "Epoch 1, Batch 35945, Loss: 3.2633, Time: 21388.6s, Step: 35946, GPU: 4.9GB\n",
      "Epoch 1, Batch 35950, Loss: 4.4879, Time: 21391.1s, Step: 35951, GPU: 4.9GB\n",
      "Epoch 1, Batch 35955, Loss: 3.1322, Time: 21394.5s, Step: 35956, GPU: 4.9GB\n",
      "Epoch 1, Batch 35960, Loss: 3.3058, Time: 21397.1s, Step: 35961, GPU: 4.9GB\n",
      "Epoch 1, Batch 35965, Loss: 2.5876, Time: 21400.5s, Step: 35966, GPU: 4.9GB\n",
      "Epoch 1, Batch 35970, Loss: 3.0692, Time: 21403.0s, Step: 35971, GPU: 4.9GB\n",
      "Epoch 1, Batch 35975, Loss: 3.2315, Time: 21406.5s, Step: 35976, GPU: 4.9GB\n",
      "Epoch 1, Batch 35980, Loss: 3.6986, Time: 21409.0s, Step: 35981, GPU: 4.9GB\n",
      "Epoch 1, Batch 35985, Loss: 3.0177, Time: 21412.4s, Step: 35986, GPU: 4.9GB\n",
      "Epoch 1, Batch 35990, Loss: 2.7792, Time: 21414.9s, Step: 35991, GPU: 4.9GB\n",
      "Epoch 1, Batch 35995, Loss: 3.8559, Time: 21418.3s, Step: 35996, GPU: 4.9GB\n",
      "Epoch 1, Batch 36000, Loss: 3.0951, Time: 21420.9s, Step: 36001, GPU: 4.9GB\n",
      "Epoch 1, Batch 36005, Loss: 3.3598, Time: 21424.4s, Step: 36006, GPU: 4.9GB\n",
      "Epoch 1, Batch 36010, Loss: 3.6180, Time: 21426.9s, Step: 36011, GPU: 4.9GB\n",
      "Epoch 1, Batch 36015, Loss: 3.7289, Time: 21430.3s, Step: 36016, GPU: 4.9GB\n",
      "Epoch 1, Batch 36020, Loss: 3.4895, Time: 21432.8s, Step: 36021, GPU: 4.9GB\n",
      "Epoch 1, Batch 36025, Loss: 3.1466, Time: 21436.4s, Step: 36026, GPU: 4.9GB\n",
      "Epoch 1, Batch 36030, Loss: 3.5436, Time: 21438.9s, Step: 36031, GPU: 4.9GB\n",
      "Epoch 1, Batch 36035, Loss: 2.3916, Time: 21442.4s, Step: 36036, GPU: 4.9GB\n",
      "Epoch 1, Batch 36040, Loss: 3.0327, Time: 21444.9s, Step: 36041, GPU: 4.9GB\n",
      "Epoch 1, Batch 36045, Loss: 4.0716, Time: 21448.3s, Step: 36046, GPU: 4.9GB\n",
      "Epoch 1, Batch 36050, Loss: 3.4668, Time: 21450.8s, Step: 36051, GPU: 4.9GB\n",
      "Epoch 1, Batch 36055, Loss: 3.2395, Time: 21454.3s, Step: 36056, GPU: 4.9GB\n",
      "Epoch 1, Batch 36060, Loss: 3.8288, Time: 21456.8s, Step: 36061, GPU: 4.9GB\n",
      "Epoch 1, Batch 36065, Loss: 3.6649, Time: 21460.2s, Step: 36066, GPU: 4.9GB\n",
      "Epoch 1, Batch 36070, Loss: 3.8121, Time: 21462.7s, Step: 36071, GPU: 4.9GB\n",
      "Epoch 1, Batch 36075, Loss: 3.8237, Time: 21466.2s, Step: 36076, GPU: 4.9GB\n",
      "Epoch 1, Batch 36080, Loss: 3.0657, Time: 21468.7s, Step: 36081, GPU: 4.9GB\n",
      "Epoch 1, Batch 36085, Loss: 4.1435, Time: 21472.1s, Step: 36086, GPU: 4.9GB\n",
      "Epoch 1, Batch 36090, Loss: 2.4584, Time: 21474.7s, Step: 36091, GPU: 4.9GB\n",
      "Epoch 1, Batch 36095, Loss: 3.5257, Time: 21478.1s, Step: 36096, GPU: 4.9GB\n",
      "Epoch 1, Batch 36100, Loss: 3.4298, Time: 21480.6s, Step: 36101, GPU: 4.9GB\n",
      "Epoch 1, Batch 36105, Loss: 2.9473, Time: 21484.0s, Step: 36106, GPU: 4.9GB\n",
      "Epoch 1, Batch 36110, Loss: 2.4395, Time: 21486.5s, Step: 36111, GPU: 4.9GB\n",
      "Epoch 1, Batch 36115, Loss: 3.6483, Time: 21490.0s, Step: 36116, GPU: 4.9GB\n",
      "Epoch 1, Batch 36120, Loss: 3.7985, Time: 21492.5s, Step: 36121, GPU: 4.9GB\n",
      "Epoch 1, Batch 36125, Loss: 2.8241, Time: 21495.9s, Step: 36126, GPU: 4.9GB\n",
      "Epoch 1, Batch 36130, Loss: 3.0301, Time: 21498.4s, Step: 36131, GPU: 4.9GB\n",
      "Epoch 1, Batch 36135, Loss: 3.1924, Time: 21501.8s, Step: 36136, GPU: 4.9GB\n",
      "Epoch 1, Batch 36140, Loss: 3.5971, Time: 21504.6s, Step: 36141, GPU: 4.9GB\n",
      "Epoch 1, Batch 36145, Loss: 3.6528, Time: 21508.0s, Step: 36146, GPU: 4.9GB\n",
      "Epoch 1, Batch 36150, Loss: 4.0114, Time: 21510.5s, Step: 36151, GPU: 4.9GB\n",
      "Epoch 1, Batch 36155, Loss: 2.9558, Time: 21513.9s, Step: 36156, GPU: 4.9GB\n",
      "Epoch 1, Batch 36160, Loss: 3.0637, Time: 21516.4s, Step: 36161, GPU: 4.9GB\n",
      "Epoch 1, Batch 36165, Loss: 2.8789, Time: 21519.8s, Step: 36166, GPU: 4.9GB\n",
      "Epoch 1, Batch 36170, Loss: 2.3551, Time: 21522.4s, Step: 36171, GPU: 4.9GB\n",
      "Epoch 1, Batch 36175, Loss: 3.7563, Time: 21525.8s, Step: 36176, GPU: 4.9GB\n",
      "Epoch 1, Batch 36180, Loss: 3.2351, Time: 21528.3s, Step: 36181, GPU: 4.9GB\n",
      "Epoch 1, Batch 36185, Loss: 3.5727, Time: 21531.7s, Step: 36186, GPU: 4.9GB\n",
      "Epoch 1, Batch 36190, Loss: 2.8372, Time: 21534.2s, Step: 36191, GPU: 4.9GB\n",
      "Epoch 1, Batch 36195, Loss: 3.6331, Time: 21537.6s, Step: 36196, GPU: 4.9GB\n",
      "Epoch 1, Batch 36200, Loss: 3.4834, Time: 21540.1s, Step: 36201, GPU: 4.9GB\n",
      "Epoch 1, Batch 36205, Loss: 3.7472, Time: 21543.5s, Step: 36206, GPU: 4.9GB\n",
      "Epoch 1, Batch 36210, Loss: 4.0286, Time: 21546.0s, Step: 36211, GPU: 4.9GB\n",
      "Epoch 1, Batch 36215, Loss: 3.6601, Time: 21549.5s, Step: 36216, GPU: 4.9GB\n",
      "Epoch 1, Batch 36220, Loss: 3.6634, Time: 21552.0s, Step: 36221, GPU: 4.9GB\n",
      "Epoch 1, Batch 36225, Loss: 3.3834, Time: 21555.4s, Step: 36226, GPU: 4.9GB\n",
      "Epoch 1, Batch 36230, Loss: 2.5043, Time: 21557.9s, Step: 36231, GPU: 4.9GB\n",
      "Epoch 1, Batch 36235, Loss: 3.1607, Time: 21561.3s, Step: 36236, GPU: 4.9GB\n",
      "Epoch 1, Batch 36240, Loss: 3.7986, Time: 21563.8s, Step: 36241, GPU: 4.9GB\n",
      "Epoch 1, Batch 36245, Loss: 3.5742, Time: 21567.2s, Step: 36246, GPU: 4.9GB\n",
      "Epoch 1, Batch 36250, Loss: 3.5655, Time: 21569.7s, Step: 36251, GPU: 4.9GB\n",
      "Epoch 1, Batch 36255, Loss: 3.4115, Time: 21573.1s, Step: 36256, GPU: 4.9GB\n",
      "Epoch 1, Batch 36260, Loss: 3.5171, Time: 21575.7s, Step: 36261, GPU: 4.9GB\n",
      "Epoch 1, Batch 36265, Loss: 3.4600, Time: 21579.1s, Step: 36266, GPU: 4.9GB\n",
      "Epoch 1, Batch 36270, Loss: 2.9658, Time: 21581.6s, Step: 36271, GPU: 4.9GB\n",
      "Epoch 1, Batch 36275, Loss: 3.8663, Time: 21585.0s, Step: 36276, GPU: 4.9GB\n",
      "Epoch 1, Batch 36280, Loss: 3.6387, Time: 21587.5s, Step: 36281, GPU: 4.9GB\n",
      "Epoch 1, Batch 36285, Loss: 3.5385, Time: 21590.9s, Step: 36286, GPU: 4.9GB\n",
      "Epoch 1, Batch 36290, Loss: 2.9422, Time: 21593.5s, Step: 36291, GPU: 4.9GB\n",
      "Epoch 1, Batch 36295, Loss: 3.2184, Time: 21596.9s, Step: 36296, GPU: 4.9GB\n",
      "Epoch 1, Batch 36300, Loss: 2.8556, Time: 21599.4s, Step: 36301, GPU: 4.9GB\n",
      "\n",
      "🔄 Auto-saving checkpoint at epoch 1, batch 36302...\n",
      "💾 Checkpoint saved to: ./my_model_checkpoints/auto_checkpoint_epoch_1_step_36302.pt\n",
      "✅ Checkpoint saved successfully!\n",
      "\n",
      "Epoch 1, Batch 36305, Loss: 3.0162, Time: 21604.6s, Step: 36306, GPU: 4.9GB\n",
      "Epoch 1, Batch 36310, Loss: 3.2770, Time: 21607.1s, Step: 36311, GPU: 4.9GB\n",
      "Epoch 1, Batch 36315, Loss: 3.5277, Time: 21610.5s, Step: 36316, GPU: 4.9GB\n",
      "Epoch 1, Batch 36320, Loss: 2.9164, Time: 21613.0s, Step: 36321, GPU: 4.9GB\n",
      "Epoch 1, Batch 36325, Loss: 3.3964, Time: 21616.5s, Step: 36326, GPU: 4.9GB\n",
      "Epoch 1, Batch 36330, Loss: 3.8847, Time: 21619.0s, Step: 36331, GPU: 4.9GB\n",
      "Epoch 1, Batch 36335, Loss: 2.8363, Time: 21622.4s, Step: 36336, GPU: 4.9GB\n",
      "Epoch 1, Batch 36340, Loss: 3.2000, Time: 21624.9s, Step: 36341, GPU: 4.9GB\n",
      "Epoch 1, Batch 36345, Loss: 3.2220, Time: 21628.3s, Step: 36346, GPU: 4.9GB\n",
      "Epoch 1, Batch 36350, Loss: 3.2503, Time: 21630.8s, Step: 36351, GPU: 4.9GB\n",
      "Epoch 1, Batch 36355, Loss: 3.3474, Time: 21634.3s, Step: 36356, GPU: 4.9GB\n",
      "Epoch 1, Batch 36360, Loss: 3.7587, Time: 21636.8s, Step: 36361, GPU: 4.9GB\n",
      "Epoch 1, Batch 36365, Loss: 3.3154, Time: 21640.2s, Step: 36366, GPU: 4.9GB\n",
      "Epoch 1, Batch 36370, Loss: 2.9250, Time: 21642.8s, Step: 36371, GPU: 4.9GB\n",
      "Epoch 1, Batch 36375, Loss: 3.2710, Time: 21646.2s, Step: 36376, GPU: 4.9GB\n",
      "Epoch 1, Batch 36380, Loss: 3.1375, Time: 21648.7s, Step: 36381, GPU: 4.9GB\n",
      "Epoch 1, Batch 36385, Loss: 3.4399, Time: 21652.1s, Step: 36386, GPU: 4.9GB\n",
      "Epoch 1, Batch 36390, Loss: 3.2661, Time: 21654.6s, Step: 36391, GPU: 4.9GB\n",
      "Epoch 1, Batch 36395, Loss: 3.7413, Time: 21658.0s, Step: 36396, GPU: 4.9GB\n",
      "Epoch 1, Batch 36400, Loss: 3.4340, Time: 21660.6s, Step: 36401, GPU: 4.9GB\n",
      "Epoch 1, Batch 36405, Loss: 3.1408, Time: 21664.0s, Step: 36406, GPU: 4.9GB\n",
      "Epoch 1, Batch 36410, Loss: 3.4822, Time: 21666.5s, Step: 36411, GPU: 4.9GB\n",
      "Epoch 1, Batch 36415, Loss: 2.8305, Time: 21669.9s, Step: 36416, GPU: 4.9GB\n",
      "Epoch 1, Batch 36420, Loss: 2.9890, Time: 21672.4s, Step: 36421, GPU: 4.9GB\n",
      "Epoch 1, Batch 36425, Loss: 3.0478, Time: 21675.8s, Step: 36426, GPU: 4.9GB\n",
      "Epoch 1, Batch 36430, Loss: 3.6378, Time: 21678.3s, Step: 36431, GPU: 4.9GB\n",
      "Epoch 1, Batch 36435, Loss: 3.6480, Time: 21681.7s, Step: 36436, GPU: 4.9GB\n",
      "Epoch 1, Batch 36440, Loss: 2.4868, Time: 21684.2s, Step: 36441, GPU: 4.9GB\n",
      "Epoch 1, Batch 36445, Loss: 2.8642, Time: 21687.7s, Step: 36446, GPU: 4.9GB\n",
      "Epoch 1, Batch 36450, Loss: 3.6758, Time: 21690.2s, Step: 36451, GPU: 4.9GB\n",
      "Epoch 1, Batch 36455, Loss: 3.4225, Time: 21693.6s, Step: 36456, GPU: 4.9GB\n",
      "Epoch 1, Batch 36460, Loss: 3.0215, Time: 21696.1s, Step: 36461, GPU: 4.9GB\n",
      "Epoch 1, Batch 36465, Loss: 3.6140, Time: 21699.5s, Step: 36466, GPU: 4.9GB\n",
      "Epoch 1, Batch 36470, Loss: 3.6806, Time: 21702.0s, Step: 36471, GPU: 4.9GB\n",
      "Epoch 1, Batch 36475, Loss: 2.9026, Time: 21705.4s, Step: 36476, GPU: 4.9GB\n",
      "Epoch 1, Batch 36480, Loss: 4.2361, Time: 21707.9s, Step: 36481, GPU: 4.9GB\n",
      "Epoch 1, Batch 36485, Loss: 3.3602, Time: 21711.4s, Step: 36486, GPU: 4.9GB\n",
      "Epoch 1, Batch 36490, Loss: 2.9001, Time: 21713.9s, Step: 36491, GPU: 4.9GB\n",
      "Epoch 1, Batch 36495, Loss: 2.5744, Time: 21717.4s, Step: 36496, GPU: 4.9GB\n",
      "Epoch 1, Batch 36500, Loss: 3.3740, Time: 21719.9s, Step: 36501, GPU: 4.9GB\n",
      "Epoch 1, Batch 36505, Loss: 2.8660, Time: 21723.3s, Step: 36506, GPU: 4.9GB\n",
      "Epoch 1, Batch 36510, Loss: 4.1529, Time: 21725.8s, Step: 36511, GPU: 4.9GB\n",
      "Epoch 1, Batch 36515, Loss: 3.3174, Time: 21729.2s, Step: 36516, GPU: 4.9GB\n",
      "Epoch 1, Batch 36520, Loss: 3.0684, Time: 21731.7s, Step: 36521, GPU: 4.9GB\n",
      "Epoch 1, Batch 36525, Loss: 3.0418, Time: 21735.1s, Step: 36526, GPU: 4.9GB\n",
      "Epoch 1, Batch 36530, Loss: 3.6296, Time: 21737.6s, Step: 36531, GPU: 4.9GB\n",
      "Epoch 1, Batch 36535, Loss: 4.7174, Time: 21741.1s, Step: 36536, GPU: 4.9GB\n",
      "Epoch 1, Batch 36540, Loss: 3.0947, Time: 21743.6s, Step: 36541, GPU: 4.9GB\n",
      "Epoch 1, Batch 36545, Loss: 3.2191, Time: 21747.0s, Step: 36546, GPU: 4.9GB\n",
      "Epoch 1, Batch 36550, Loss: 3.5085, Time: 21749.6s, Step: 36551, GPU: 4.9GB\n",
      "Epoch 1, Batch 36555, Loss: 3.2985, Time: 21753.0s, Step: 36556, GPU: 4.9GB\n",
      "Epoch 1, Batch 36560, Loss: 3.4625, Time: 21755.5s, Step: 36561, GPU: 4.9GB\n",
      "Epoch 1, Batch 36565, Loss: 3.4766, Time: 21758.9s, Step: 36566, GPU: 4.9GB\n",
      "Epoch 1, Batch 36570, Loss: 3.6150, Time: 21761.5s, Step: 36571, GPU: 4.9GB\n",
      "Epoch 1, Batch 36575, Loss: 3.3797, Time: 21764.9s, Step: 36576, GPU: 4.9GB\n",
      "Epoch 1, Batch 36580, Loss: 3.7289, Time: 21767.4s, Step: 36581, GPU: 4.9GB\n",
      "Epoch 1, Batch 36585, Loss: 2.7459, Time: 21770.9s, Step: 36586, GPU: 4.9GB\n",
      "Epoch 1, Batch 36590, Loss: 4.1682, Time: 21773.4s, Step: 36591, GPU: 4.9GB\n",
      "Epoch 1, Batch 36595, Loss: 2.8620, Time: 21776.8s, Step: 36596, GPU: 4.9GB\n",
      "Epoch 1, Batch 36600, Loss: 3.7742, Time: 21779.4s, Step: 36601, GPU: 4.9GB\n",
      "Epoch 1, Batch 36605, Loss: 3.0709, Time: 21782.8s, Step: 36606, GPU: 4.9GB\n",
      "Epoch 1, Batch 36610, Loss: 2.8307, Time: 21785.4s, Step: 36611, GPU: 4.9GB\n",
      "Epoch 1, Batch 36615, Loss: 3.3918, Time: 21788.8s, Step: 36616, GPU: 4.9GB\n",
      "Epoch 1, Batch 36620, Loss: 2.9953, Time: 21791.3s, Step: 36621, GPU: 4.9GB\n",
      "Epoch 1, Batch 36625, Loss: 4.0368, Time: 21794.8s, Step: 36626, GPU: 4.9GB\n",
      "Epoch 1, Batch 36630, Loss: 3.4793, Time: 21797.3s, Step: 36631, GPU: 4.9GB\n",
      "Epoch 1, Batch 36635, Loss: 3.0488, Time: 21800.7s, Step: 36636, GPU: 4.9GB\n",
      "Epoch 1, Batch 36640, Loss: 3.1453, Time: 21803.2s, Step: 36641, GPU: 4.9GB\n",
      "Epoch 1, Batch 36645, Loss: 3.0077, Time: 21806.7s, Step: 36646, GPU: 4.9GB\n",
      "Epoch 1, Batch 36650, Loss: 3.2642, Time: 21809.2s, Step: 36651, GPU: 4.9GB\n",
      "Epoch 1, Batch 36655, Loss: 2.7412, Time: 21812.6s, Step: 36656, GPU: 4.9GB\n",
      "Epoch 1, Batch 36660, Loss: 3.7105, Time: 21815.1s, Step: 36661, GPU: 4.9GB\n",
      "Epoch 1, Batch 36665, Loss: 4.0397, Time: 21818.5s, Step: 36666, GPU: 4.9GB\n",
      "Epoch 1, Batch 36670, Loss: 3.1091, Time: 21821.1s, Step: 36671, GPU: 4.9GB\n",
      "Epoch 1, Batch 36675, Loss: 3.7646, Time: 21824.5s, Step: 36676, GPU: 4.9GB\n",
      "Epoch 1, Batch 36680, Loss: 3.7896, Time: 21827.0s, Step: 36681, GPU: 4.9GB\n",
      "Epoch 1, Batch 36685, Loss: 4.2409, Time: 21830.4s, Step: 36686, GPU: 4.9GB\n",
      "Epoch 1, Batch 36690, Loss: 4.0831, Time: 21832.9s, Step: 36691, GPU: 4.9GB\n",
      "Epoch 1, Batch 36695, Loss: 4.4881, Time: 21836.3s, Step: 36696, GPU: 4.9GB\n",
      "Epoch 1, Batch 36700, Loss: 3.4812, Time: 21838.9s, Step: 36701, GPU: 4.9GB\n",
      "Epoch 1, Batch 36705, Loss: 3.2931, Time: 21842.3s, Step: 36706, GPU: 4.9GB\n",
      "Epoch 1, Batch 36710, Loss: 3.1522, Time: 21844.8s, Step: 36711, GPU: 4.9GB\n",
      "Epoch 1, Batch 36715, Loss: 3.1133, Time: 21848.2s, Step: 36716, GPU: 4.9GB\n",
      "Epoch 1, Batch 36720, Loss: 3.2811, Time: 21850.7s, Step: 36721, GPU: 4.9GB\n",
      "Epoch 1, Batch 36725, Loss: 3.2370, Time: 21854.1s, Step: 36726, GPU: 4.9GB\n",
      "Epoch 1, Batch 36730, Loss: 3.4447, Time: 21856.6s, Step: 36731, GPU: 4.9GB\n",
      "Epoch 1, Batch 36735, Loss: 3.1163, Time: 21860.1s, Step: 36736, GPU: 4.9GB\n",
      "Epoch 1, Batch 36740, Loss: 3.5505, Time: 21862.6s, Step: 36741, GPU: 4.9GB\n",
      "Epoch 1, Batch 36745, Loss: 3.3147, Time: 21866.0s, Step: 36746, GPU: 4.9GB\n",
      "Epoch 1, Batch 36750, Loss: 3.8281, Time: 21868.5s, Step: 36751, GPU: 4.9GB\n",
      "Epoch 1, Batch 36755, Loss: 3.5058, Time: 21871.9s, Step: 36756, GPU: 4.9GB\n",
      "Epoch 1, Batch 36760, Loss: 3.3499, Time: 21874.4s, Step: 36761, GPU: 4.9GB\n",
      "Epoch 1, Batch 36765, Loss: 3.8050, Time: 21877.8s, Step: 36766, GPU: 4.9GB\n",
      "Epoch 1, Batch 36770, Loss: 3.2287, Time: 21880.3s, Step: 36771, GPU: 4.9GB\n",
      "Epoch 1, Batch 36775, Loss: 3.1744, Time: 21883.7s, Step: 36776, GPU: 4.9GB\n",
      "Epoch 1, Batch 36780, Loss: 2.9846, Time: 21886.2s, Step: 36781, GPU: 4.9GB\n",
      "Epoch 1, Batch 36785, Loss: 2.2934, Time: 21889.6s, Step: 36786, GPU: 4.9GB\n",
      "Epoch 1, Batch 36790, Loss: 2.6150, Time: 21892.2s, Step: 36791, GPU: 4.9GB\n",
      "Epoch 1, Batch 36795, Loss: 3.3253, Time: 21895.6s, Step: 36796, GPU: 4.9GB\n",
      "Epoch 1, Batch 36800, Loss: 3.5514, Time: 21898.2s, Step: 36801, GPU: 4.9GB\n",
      "Epoch 1, Batch 36805, Loss: 3.0262, Time: 21901.6s, Step: 36806, GPU: 4.9GB\n",
      "Epoch 1, Batch 36810, Loss: 3.5744, Time: 21904.1s, Step: 36811, GPU: 4.9GB\n",
      "Epoch 1, Batch 36815, Loss: 3.5588, Time: 21907.5s, Step: 36816, GPU: 4.9GB\n",
      "Epoch 1, Batch 36820, Loss: 2.9821, Time: 21910.1s, Step: 36821, GPU: 4.9GB\n",
      "Epoch 1, Batch 36825, Loss: 4.3091, Time: 21913.5s, Step: 36826, GPU: 4.9GB\n",
      "Epoch 1, Batch 36830, Loss: 3.4934, Time: 21916.0s, Step: 36831, GPU: 4.9GB\n",
      "Epoch 1, Batch 36835, Loss: 3.4228, Time: 21919.4s, Step: 36836, GPU: 4.9GB\n",
      "Epoch 1, Batch 36840, Loss: 2.7832, Time: 21921.9s, Step: 36841, GPU: 4.9GB\n",
      "Epoch 1, Batch 36845, Loss: 3.3958, Time: 21925.3s, Step: 36846, GPU: 4.9GB\n",
      "Epoch 1, Batch 36850, Loss: 4.0042, Time: 21927.8s, Step: 36851, GPU: 4.9GB\n",
      "Epoch 1, Batch 36855, Loss: 3.3175, Time: 21931.2s, Step: 36856, GPU: 4.9GB\n",
      "Epoch 1, Batch 36860, Loss: 3.2708, Time: 21933.7s, Step: 36861, GPU: 4.9GB\n",
      "Epoch 1, Batch 36865, Loss: 3.3541, Time: 21937.1s, Step: 36866, GPU: 4.9GB\n",
      "Epoch 1, Batch 36870, Loss: 3.1251, Time: 21939.6s, Step: 36871, GPU: 4.9GB\n",
      "Epoch 1, Batch 36875, Loss: 3.2225, Time: 21943.0s, Step: 36876, GPU: 4.9GB\n",
      "Epoch 1, Batch 36880, Loss: 3.5047, Time: 21945.5s, Step: 36881, GPU: 4.9GB\n",
      "Epoch 1, Batch 36885, Loss: 3.4028, Time: 21948.9s, Step: 36886, GPU: 4.9GB\n",
      "Epoch 1, Batch 36890, Loss: 3.1833, Time: 21951.5s, Step: 36891, GPU: 4.9GB\n",
      "Epoch 1, Batch 36895, Loss: 2.8529, Time: 21954.9s, Step: 36896, GPU: 4.9GB\n",
      "Epoch 1, Batch 36900, Loss: 3.7350, Time: 21957.4s, Step: 36901, GPU: 4.9GB\n",
      "Epoch 1, Batch 36905, Loss: 3.7359, Time: 21960.8s, Step: 36906, GPU: 4.9GB\n",
      "Epoch 1, Batch 36910, Loss: 3.6524, Time: 21963.3s, Step: 36911, GPU: 4.9GB\n",
      "Epoch 1, Batch 36915, Loss: 2.6749, Time: 21966.7s, Step: 36916, GPU: 4.9GB\n",
      "Epoch 1, Batch 36920, Loss: 3.9913, Time: 21969.3s, Step: 36921, GPU: 4.9GB\n",
      "Epoch 1, Batch 36925, Loss: 3.4299, Time: 21972.7s, Step: 36926, GPU: 4.9GB\n",
      "Epoch 1, Batch 36930, Loss: 3.7424, Time: 21975.2s, Step: 36931, GPU: 4.9GB\n",
      "Epoch 1, Batch 36935, Loss: 3.9992, Time: 21978.6s, Step: 36936, GPU: 4.9GB\n",
      "Epoch 1, Batch 36940, Loss: 4.2975, Time: 21981.2s, Step: 36941, GPU: 4.9GB\n",
      "Epoch 1, Batch 36945, Loss: 4.3742, Time: 21984.6s, Step: 36946, GPU: 4.9GB\n",
      "Epoch 1, Batch 36950, Loss: 4.1517, Time: 21987.1s, Step: 36951, GPU: 4.9GB\n",
      "Epoch 1, Batch 36955, Loss: 4.1371, Time: 21990.6s, Step: 36956, GPU: 4.9GB\n",
      "Epoch 1, Batch 36960, Loss: 3.6433, Time: 21993.1s, Step: 36961, GPU: 4.9GB\n",
      "Epoch 1, Batch 36965, Loss: 3.1202, Time: 21996.5s, Step: 36966, GPU: 4.9GB\n",
      "Epoch 1, Batch 36970, Loss: 3.8532, Time: 21999.1s, Step: 36971, GPU: 4.9GB\n",
      "Epoch 1, Batch 36975, Loss: 2.4511, Time: 22002.5s, Step: 36976, GPU: 4.9GB\n",
      "Epoch 1, Batch 36980, Loss: 3.7264, Time: 22005.0s, Step: 36981, GPU: 4.9GB\n",
      "Epoch 1, Batch 36985, Loss: 3.1633, Time: 22008.4s, Step: 36986, GPU: 4.9GB\n",
      "Epoch 1, Batch 36990, Loss: 4.1311, Time: 22011.0s, Step: 36991, GPU: 4.9GB\n",
      "Epoch 1, Batch 36995, Loss: 3.5893, Time: 22014.4s, Step: 36996, GPU: 4.9GB\n",
      "Epoch 1, Batch 37000, Loss: 2.8059, Time: 22017.0s, Step: 37001, GPU: 4.9GB\n",
      "Epoch 1, Batch 37005, Loss: 3.4331, Time: 22020.4s, Step: 37006, GPU: 4.9GB\n",
      "Epoch 1, Batch 37010, Loss: 3.1779, Time: 22022.9s, Step: 37011, GPU: 4.9GB\n",
      "Epoch 1, Batch 37015, Loss: 3.3918, Time: 22026.4s, Step: 37016, GPU: 4.9GB\n",
      "Epoch 1, Batch 37020, Loss: 3.2584, Time: 22028.9s, Step: 37021, GPU: 4.9GB\n",
      "Epoch 1, Batch 37025, Loss: 3.8017, Time: 22032.3s, Step: 37026, GPU: 4.9GB\n",
      "Epoch 1, Batch 37030, Loss: 3.9912, Time: 22034.9s, Step: 37031, GPU: 4.9GB\n",
      "Epoch 1, Batch 37035, Loss: 3.2651, Time: 22038.3s, Step: 37036, GPU: 4.9GB\n",
      "Epoch 1, Batch 37040, Loss: 2.9933, Time: 22040.8s, Step: 37041, GPU: 4.9GB\n",
      "Epoch 1, Batch 37045, Loss: 3.6793, Time: 22044.3s, Step: 37046, GPU: 4.9GB\n",
      "Epoch 1, Batch 37050, Loss: 3.6048, Time: 22046.8s, Step: 37051, GPU: 4.9GB\n",
      "Epoch 1, Batch 37055, Loss: 2.8828, Time: 22050.2s, Step: 37056, GPU: 4.9GB\n",
      "Epoch 1, Batch 37060, Loss: 3.4079, Time: 22052.8s, Step: 37061, GPU: 4.9GB\n",
      "Epoch 1, Batch 37065, Loss: 2.9751, Time: 22056.2s, Step: 37066, GPU: 4.9GB\n",
      "Epoch 1, Batch 37070, Loss: 3.1185, Time: 22058.7s, Step: 37071, GPU: 4.9GB\n",
      "Epoch 1, Batch 37075, Loss: 2.9187, Time: 22062.1s, Step: 37076, GPU: 4.9GB\n",
      "Epoch 1, Batch 37080, Loss: 3.1776, Time: 22064.6s, Step: 37081, GPU: 4.9GB\n",
      "Epoch 1, Batch 37085, Loss: 3.1717, Time: 22068.1s, Step: 37086, GPU: 4.9GB\n",
      "Epoch 1, Batch 37090, Loss: 3.9334, Time: 22070.6s, Step: 37091, GPU: 4.9GB\n",
      "Epoch 1, Batch 37095, Loss: 3.2650, Time: 22074.0s, Step: 37096, GPU: 4.9GB\n",
      "Epoch 1, Batch 37100, Loss: 3.3272, Time: 22076.5s, Step: 37101, GPU: 4.9GB\n",
      "Epoch 1, Batch 37105, Loss: 2.6383, Time: 22079.9s, Step: 37106, GPU: 4.9GB\n",
      "Epoch 1, Batch 37110, Loss: 3.7807, Time: 22082.4s, Step: 37111, GPU: 4.9GB\n",
      "Epoch 1, Batch 37115, Loss: 4.6348, Time: 22085.9s, Step: 37116, GPU: 4.9GB\n",
      "Epoch 1, Batch 37120, Loss: 2.7988, Time: 22088.4s, Step: 37121, GPU: 4.9GB\n",
      "Epoch 1, Batch 37125, Loss: 4.0261, Time: 22091.8s, Step: 37126, GPU: 4.9GB\n",
      "Epoch 1, Batch 37130, Loss: 3.1927, Time: 22094.3s, Step: 37131, GPU: 4.9GB\n",
      "Epoch 1, Batch 37135, Loss: 3.0248, Time: 22097.7s, Step: 37136, GPU: 4.9GB\n",
      "Epoch 1, Batch 37140, Loss: 4.0621, Time: 22100.3s, Step: 37141, GPU: 4.9GB\n",
      "Epoch 1, Batch 37145, Loss: 2.7597, Time: 22103.7s, Step: 37146, GPU: 4.9GB\n",
      "Epoch 1, Batch 37150, Loss: 3.7630, Time: 22106.2s, Step: 37151, GPU: 4.9GB\n",
      "Epoch 1, Batch 37155, Loss: 2.8018, Time: 22109.6s, Step: 37156, GPU: 4.9GB\n",
      "Epoch 1, Batch 37160, Loss: 3.3362, Time: 22112.1s, Step: 37161, GPU: 4.9GB\n",
      "Epoch 1, Batch 37165, Loss: 3.4121, Time: 22115.6s, Step: 37166, GPU: 4.9GB\n",
      "Epoch 1, Batch 37170, Loss: 3.0462, Time: 22118.1s, Step: 37171, GPU: 4.9GB\n",
      "Epoch 1, Batch 37175, Loss: 3.2202, Time: 22121.5s, Step: 37176, GPU: 4.9GB\n",
      "Epoch 1, Batch 37180, Loss: 3.7548, Time: 22124.0s, Step: 37181, GPU: 4.9GB\n",
      "Epoch 1, Batch 37185, Loss: 3.7424, Time: 22127.4s, Step: 37186, GPU: 4.9GB\n",
      "Epoch 1, Batch 37190, Loss: 4.1592, Time: 22130.0s, Step: 37191, GPU: 4.9GB\n",
      "Epoch 1, Batch 37195, Loss: 3.2896, Time: 22133.4s, Step: 37196, GPU: 4.9GB\n",
      "Epoch 1, Batch 37200, Loss: 3.5543, Time: 22136.0s, Step: 37201, GPU: 4.9GB\n",
      "Epoch 1, Batch 37205, Loss: 3.2828, Time: 22139.4s, Step: 37206, GPU: 4.9GB\n",
      "Epoch 1, Batch 37210, Loss: 3.3040, Time: 22141.9s, Step: 37211, GPU: 4.9GB\n",
      "Epoch 1, Batch 37215, Loss: 3.2416, Time: 22145.3s, Step: 37216, GPU: 4.9GB\n",
      "Epoch 1, Batch 37220, Loss: 4.1707, Time: 22147.8s, Step: 37221, GPU: 4.9GB\n",
      "Epoch 1, Batch 37225, Loss: 3.7775, Time: 22151.2s, Step: 37226, GPU: 4.9GB\n",
      "Epoch 1, Batch 37230, Loss: 2.8855, Time: 22153.8s, Step: 37231, GPU: 4.9GB\n",
      "Epoch 1, Batch 37235, Loss: 3.0166, Time: 22157.2s, Step: 37236, GPU: 4.9GB\n",
      "Epoch 1, Batch 37240, Loss: 3.7506, Time: 22159.7s, Step: 37241, GPU: 4.9GB\n",
      "Epoch 1, Batch 37245, Loss: 3.5499, Time: 22163.1s, Step: 37246, GPU: 4.9GB\n",
      "Epoch 1, Batch 37250, Loss: 3.8501, Time: 22165.6s, Step: 37251, GPU: 4.9GB\n",
      "Epoch 1, Batch 37255, Loss: 3.0276, Time: 22169.0s, Step: 37256, GPU: 4.9GB\n",
      "Epoch 1, Batch 37260, Loss: 3.5439, Time: 22171.5s, Step: 37261, GPU: 4.9GB\n",
      "Epoch 1, Batch 37265, Loss: 3.0852, Time: 22174.9s, Step: 37266, GPU: 4.9GB\n",
      "Epoch 1, Batch 37270, Loss: 3.1956, Time: 22177.5s, Step: 37271, GPU: 4.9GB\n",
      "Epoch 1, Batch 37275, Loss: 3.9857, Time: 22180.9s, Step: 37276, GPU: 4.9GB\n",
      "Epoch 1, Batch 37280, Loss: 3.0903, Time: 22183.4s, Step: 37281, GPU: 4.9GB\n",
      "Epoch 1, Batch 37285, Loss: 3.6468, Time: 22186.8s, Step: 37286, GPU: 4.9GB\n",
      "Epoch 1, Batch 37290, Loss: 4.2883, Time: 22189.3s, Step: 37291, GPU: 4.9GB\n",
      "Epoch 1, Batch 37295, Loss: 3.2106, Time: 22192.7s, Step: 37296, GPU: 4.9GB\n",
      "Epoch 1, Batch 37300, Loss: 3.4442, Time: 22195.2s, Step: 37301, GPU: 4.9GB\n",
      "Epoch 1, Batch 37305, Loss: 3.7732, Time: 22198.7s, Step: 37306, GPU: 4.9GB\n",
      "Epoch 1, Batch 37310, Loss: 4.3813, Time: 22201.2s, Step: 37311, GPU: 4.9GB\n",
      "Epoch 1, Batch 37315, Loss: 3.0705, Time: 22204.6s, Step: 37316, GPU: 4.9GB\n",
      "Epoch 1, Batch 37320, Loss: 3.5426, Time: 22207.2s, Step: 37321, GPU: 4.9GB\n",
      "Epoch 1, Batch 37325, Loss: 3.6756, Time: 22210.6s, Step: 37326, GPU: 4.9GB\n",
      "Epoch 1, Batch 37330, Loss: 2.9519, Time: 22213.1s, Step: 37331, GPU: 4.9GB\n",
      "Epoch 1, Batch 37335, Loss: 3.5832, Time: 22216.6s, Step: 37336, GPU: 4.9GB\n",
      "Epoch 1, Batch 37340, Loss: 3.4347, Time: 22219.1s, Step: 37341, GPU: 4.9GB\n",
      "Epoch 1, Batch 37345, Loss: 2.9415, Time: 22222.5s, Step: 37346, GPU: 4.9GB\n",
      "Epoch 1, Batch 37350, Loss: 3.2880, Time: 22225.0s, Step: 37351, GPU: 4.9GB\n",
      "Epoch 1, Batch 37355, Loss: 3.1290, Time: 22228.5s, Step: 37356, GPU: 4.9GB\n",
      "Epoch 1, Batch 37360, Loss: 3.6822, Time: 22231.0s, Step: 37361, GPU: 4.9GB\n",
      "Epoch 1, Batch 37365, Loss: 3.4137, Time: 22234.4s, Step: 37366, GPU: 4.9GB\n",
      "Epoch 1, Batch 37370, Loss: 3.0375, Time: 22237.0s, Step: 37371, GPU: 4.9GB\n",
      "Epoch 1, Batch 37375, Loss: 4.1951, Time: 22240.4s, Step: 37376, GPU: 4.9GB\n",
      "Epoch 1, Batch 37380, Loss: 3.3587, Time: 22242.9s, Step: 37381, GPU: 4.9GB\n",
      "Epoch 1, Batch 37385, Loss: 2.8974, Time: 22246.3s, Step: 37386, GPU: 4.9GB\n",
      "Epoch 1, Batch 37390, Loss: 3.1985, Time: 22248.9s, Step: 37391, GPU: 4.9GB\n",
      "Epoch 1, Batch 37395, Loss: 3.0494, Time: 22252.3s, Step: 37396, GPU: 4.9GB\n",
      "Epoch 1, Batch 37400, Loss: 3.8366, Time: 22254.9s, Step: 37401, GPU: 4.9GB\n",
      "Epoch 1, Batch 37405, Loss: 2.8031, Time: 22258.3s, Step: 37406, GPU: 4.9GB\n",
      "Epoch 1, Batch 37410, Loss: 2.8525, Time: 22260.8s, Step: 37411, GPU: 4.9GB\n",
      "Epoch 1, Batch 37415, Loss: 3.2207, Time: 22264.2s, Step: 37416, GPU: 4.9GB\n",
      "Epoch 1, Batch 37420, Loss: 3.5160, Time: 22266.7s, Step: 37421, GPU: 4.9GB\n",
      "Epoch 1, Batch 37425, Loss: 3.3427, Time: 22270.1s, Step: 37426, GPU: 4.9GB\n",
      "Epoch 1, Batch 37430, Loss: 3.8956, Time: 22272.7s, Step: 37431, GPU: 4.9GB\n",
      "Epoch 1, Batch 37435, Loss: 3.3594, Time: 22276.1s, Step: 37436, GPU: 4.9GB\n",
      "Epoch 1, Batch 37440, Loss: 3.7228, Time: 22278.6s, Step: 37441, GPU: 4.9GB\n",
      "Epoch 1, Batch 37445, Loss: 2.1412, Time: 22282.0s, Step: 37446, GPU: 4.9GB\n",
      "Epoch 1, Batch 37450, Loss: 3.1978, Time: 22284.5s, Step: 37451, GPU: 4.9GB\n",
      "Epoch 1, Batch 37455, Loss: 2.8130, Time: 22287.9s, Step: 37456, GPU: 4.9GB\n",
      "Epoch 1, Batch 37460, Loss: 2.7076, Time: 22290.4s, Step: 37461, GPU: 4.9GB\n",
      "Epoch 1, Batch 37465, Loss: 2.5518, Time: 22293.9s, Step: 37466, GPU: 4.9GB\n",
      "Epoch 1, Batch 37470, Loss: 2.9893, Time: 22296.5s, Step: 37471, GPU: 4.9GB\n",
      "Epoch 1, Batch 37475, Loss: 3.4763, Time: 22299.9s, Step: 37476, GPU: 4.9GB\n",
      "Epoch 1, Batch 37480, Loss: 3.2939, Time: 22302.4s, Step: 37481, GPU: 4.9GB\n",
      "Epoch 1, Batch 37485, Loss: 2.9928, Time: 22305.8s, Step: 37486, GPU: 4.9GB\n",
      "Epoch 1, Batch 37490, Loss: 2.4495, Time: 22308.3s, Step: 37491, GPU: 4.9GB\n",
      "Epoch 1, Batch 37495, Loss: 3.5415, Time: 22311.7s, Step: 37496, GPU: 4.9GB\n",
      "Epoch 1, Batch 37500, Loss: 4.3416, Time: 22314.3s, Step: 37501, GPU: 4.9GB\n",
      "Epoch 1, Batch 37505, Loss: 4.1393, Time: 22317.7s, Step: 37506, GPU: 4.9GB\n",
      "Epoch 1, Batch 37510, Loss: 3.0552, Time: 22320.2s, Step: 37511, GPU: 4.9GB\n",
      "Epoch 1, Batch 37515, Loss: 3.4696, Time: 22323.6s, Step: 37516, GPU: 4.9GB\n",
      "Epoch 1, Batch 37520, Loss: 2.7138, Time: 22326.1s, Step: 37521, GPU: 4.9GB\n",
      "Epoch 1, Batch 37525, Loss: 3.5815, Time: 22329.5s, Step: 37526, GPU: 4.9GB\n",
      "Epoch 1, Batch 37530, Loss: 3.1629, Time: 22332.0s, Step: 37531, GPU: 4.9GB\n",
      "Epoch 1, Batch 37535, Loss: 2.6401, Time: 22335.4s, Step: 37536, GPU: 4.9GB\n",
      "Epoch 1, Batch 37540, Loss: 3.3943, Time: 22337.9s, Step: 37541, GPU: 4.9GB\n",
      "Epoch 1, Batch 37545, Loss: 3.4705, Time: 22341.3s, Step: 37546, GPU: 4.9GB\n",
      "Epoch 1, Batch 37550, Loss: 2.9421, Time: 22343.8s, Step: 37551, GPU: 4.9GB\n",
      "Epoch 1, Batch 37555, Loss: 3.5941, Time: 22347.3s, Step: 37556, GPU: 4.9GB\n",
      "Epoch 1, Batch 37560, Loss: 3.5763, Time: 22349.8s, Step: 37561, GPU: 4.9GB\n",
      "Epoch 1, Batch 37565, Loss: 4.0479, Time: 22353.2s, Step: 37566, GPU: 4.9GB\n",
      "Epoch 1, Batch 37570, Loss: 3.1550, Time: 22355.7s, Step: 37571, GPU: 4.9GB\n",
      "Epoch 1, Batch 37575, Loss: 3.7342, Time: 22359.1s, Step: 37576, GPU: 4.9GB\n",
      "Epoch 1, Batch 37580, Loss: 4.0748, Time: 22361.7s, Step: 37581, GPU: 4.9GB\n",
      "Epoch 1, Batch 37585, Loss: 3.2322, Time: 22365.1s, Step: 37586, GPU: 4.9GB\n",
      "Epoch 1, Batch 37590, Loss: 3.0857, Time: 22367.6s, Step: 37591, GPU: 4.9GB\n",
      "Epoch 1, Batch 37595, Loss: 2.7562, Time: 22371.0s, Step: 37596, GPU: 4.9GB\n",
      "Epoch 1, Batch 37600, Loss: 3.3315, Time: 22373.5s, Step: 37601, GPU: 4.9GB\n",
      "Epoch 1, Batch 37605, Loss: 3.5068, Time: 22377.0s, Step: 37606, GPU: 4.9GB\n",
      "Epoch 1, Batch 37610, Loss: 2.2579, Time: 22379.5s, Step: 37611, GPU: 4.9GB\n",
      "Epoch 1, Batch 37615, Loss: 3.2902, Time: 22382.9s, Step: 37616, GPU: 4.9GB\n",
      "Epoch 1, Batch 37620, Loss: 3.0392, Time: 22385.4s, Step: 37621, GPU: 4.9GB\n",
      "Epoch 1, Batch 37625, Loss: 3.2883, Time: 22388.8s, Step: 37626, GPU: 4.9GB\n",
      "Epoch 1, Batch 37630, Loss: 3.0946, Time: 22391.3s, Step: 37631, GPU: 4.9GB\n",
      "Epoch 1, Batch 37635, Loss: 3.8181, Time: 22394.7s, Step: 37636, GPU: 4.9GB\n",
      "Epoch 1, Batch 37640, Loss: 3.6152, Time: 22397.3s, Step: 37641, GPU: 4.9GB\n",
      "Epoch 1, Batch 37645, Loss: 3.3772, Time: 22400.7s, Step: 37646, GPU: 4.9GB\n",
      "Epoch 1, Batch 37650, Loss: 3.7999, Time: 22403.2s, Step: 37651, GPU: 4.9GB\n",
      "Epoch 1, Batch 37655, Loss: 3.3628, Time: 22406.6s, Step: 37656, GPU: 4.9GB\n",
      "Epoch 1, Batch 37660, Loss: 3.7271, Time: 22409.2s, Step: 37661, GPU: 4.9GB\n",
      "Epoch 1, Batch 37665, Loss: 3.0107, Time: 22412.6s, Step: 37666, GPU: 4.9GB\n",
      "Epoch 1, Batch 37670, Loss: 3.2011, Time: 22415.2s, Step: 37671, GPU: 4.9GB\n",
      "Epoch 1, Batch 37675, Loss: 3.6049, Time: 22418.6s, Step: 37676, GPU: 4.9GB\n",
      "Epoch 1, Batch 37680, Loss: 3.6594, Time: 22421.1s, Step: 37681, GPU: 4.9GB\n",
      "Epoch 1, Batch 37685, Loss: 3.1633, Time: 22424.5s, Step: 37686, GPU: 4.9GB\n",
      "Epoch 1, Batch 37690, Loss: 2.7041, Time: 22427.1s, Step: 37691, GPU: 4.9GB\n",
      "Epoch 1, Batch 37695, Loss: 2.9690, Time: 22430.5s, Step: 37696, GPU: 4.9GB\n",
      "Epoch 1, Batch 37700, Loss: 3.7782, Time: 22433.0s, Step: 37701, GPU: 4.9GB\n",
      "Epoch 1, Batch 37705, Loss: 3.2705, Time: 22436.4s, Step: 37706, GPU: 4.9GB\n",
      "Epoch 1, Batch 37710, Loss: 3.4331, Time: 22438.9s, Step: 37711, GPU: 4.9GB\n",
      "Epoch 1, Batch 37715, Loss: 3.0592, Time: 22442.3s, Step: 37716, GPU: 4.9GB\n",
      "Epoch 1, Batch 37720, Loss: 3.3767, Time: 22444.9s, Step: 37721, GPU: 4.9GB\n",
      "Epoch 1, Batch 37725, Loss: 3.4153, Time: 22448.3s, Step: 37726, GPU: 4.9GB\n",
      "Epoch 1, Batch 37730, Loss: 2.9234, Time: 22450.8s, Step: 37731, GPU: 4.9GB\n",
      "Epoch 1, Batch 37735, Loss: 2.6824, Time: 22454.2s, Step: 37736, GPU: 4.9GB\n",
      "Epoch 1, Batch 37740, Loss: 4.0711, Time: 22456.7s, Step: 37741, GPU: 4.9GB\n",
      "Epoch 1, Batch 37745, Loss: 3.0872, Time: 22460.1s, Step: 37746, GPU: 4.9GB\n",
      "Epoch 1, Batch 37750, Loss: 2.7421, Time: 22462.6s, Step: 37751, GPU: 4.9GB\n",
      "Epoch 1, Batch 37755, Loss: 3.3718, Time: 22466.0s, Step: 37756, GPU: 4.9GB\n",
      "Epoch 1, Batch 37760, Loss: 2.6631, Time: 22468.5s, Step: 37761, GPU: 4.9GB\n",
      "Epoch 1, Batch 37765, Loss: 4.2169, Time: 22471.9s, Step: 37766, GPU: 4.9GB\n",
      "Epoch 1, Batch 37770, Loss: 3.9100, Time: 22474.4s, Step: 37771, GPU: 4.9GB\n",
      "Epoch 1, Batch 37775, Loss: 3.3132, Time: 22477.9s, Step: 37776, GPU: 4.9GB\n",
      "Epoch 1, Batch 37780, Loss: 4.0903, Time: 22480.4s, Step: 37781, GPU: 4.9GB\n",
      "Epoch 1, Batch 37785, Loss: 3.1366, Time: 22483.8s, Step: 37786, GPU: 4.9GB\n",
      "Epoch 1, Batch 37790, Loss: 2.8552, Time: 22486.4s, Step: 37791, GPU: 4.9GB\n",
      "Epoch 1, Batch 37795, Loss: 2.9629, Time: 22489.8s, Step: 37796, GPU: 4.9GB\n",
      "Epoch 1, Batch 37800, Loss: 2.9157, Time: 22492.4s, Step: 37801, GPU: 4.9GB\n",
      "Epoch 1, Batch 37805, Loss: 3.0613, Time: 22495.8s, Step: 37806, GPU: 4.9GB\n",
      "Epoch 1, Batch 37810, Loss: 3.7013, Time: 22498.4s, Step: 37811, GPU: 4.9GB\n",
      "Epoch 1, Batch 37815, Loss: 2.7370, Time: 22501.8s, Step: 37816, GPU: 4.9GB\n",
      "Epoch 1, Batch 37820, Loss: 3.4206, Time: 22504.4s, Step: 37821, GPU: 4.9GB\n",
      "Epoch 1, Batch 37825, Loss: 3.4496, Time: 22507.8s, Step: 37826, GPU: 4.9GB\n",
      "Epoch 1, Batch 37830, Loss: 3.0820, Time: 22510.3s, Step: 37831, GPU: 4.9GB\n",
      "Epoch 1, Batch 37835, Loss: 2.2775, Time: 22513.7s, Step: 37836, GPU: 4.9GB\n",
      "Epoch 1, Batch 37840, Loss: 3.9235, Time: 22516.3s, Step: 37841, GPU: 4.9GB\n",
      "Epoch 1, Batch 37845, Loss: 3.3025, Time: 22519.7s, Step: 37846, GPU: 4.9GB\n",
      "Epoch 1, Batch 37850, Loss: 3.2498, Time: 22522.2s, Step: 37851, GPU: 4.9GB\n",
      "Epoch 1, Batch 37855, Loss: 2.9079, Time: 22525.6s, Step: 37856, GPU: 4.9GB\n",
      "Epoch 1, Batch 37860, Loss: 3.4153, Time: 22528.2s, Step: 37861, GPU: 4.9GB\n",
      "Epoch 1, Batch 37865, Loss: 2.9704, Time: 22531.6s, Step: 37866, GPU: 4.9GB\n",
      "Epoch 1, Batch 37870, Loss: 2.9461, Time: 22534.1s, Step: 37871, GPU: 4.9GB\n",
      "Epoch 1, Batch 37875, Loss: 4.2755, Time: 22537.5s, Step: 37876, GPU: 4.9GB\n",
      "Epoch 1, Batch 37880, Loss: 3.3481, Time: 22540.0s, Step: 37881, GPU: 4.9GB\n",
      "Epoch 1, Batch 37885, Loss: 4.8037, Time: 22543.5s, Step: 37886, GPU: 4.9GB\n",
      "Epoch 1, Batch 37890, Loss: 3.0526, Time: 22546.0s, Step: 37891, GPU: 4.9GB\n",
      "Epoch 1, Batch 37895, Loss: 3.5408, Time: 22549.4s, Step: 37896, GPU: 4.9GB\n",
      "Epoch 1, Batch 37900, Loss: 4.0648, Time: 22552.0s, Step: 37901, GPU: 4.9GB\n",
      "Epoch 1, Batch 37905, Loss: 3.8417, Time: 22555.4s, Step: 37906, GPU: 4.9GB\n",
      "Epoch 1, Batch 37910, Loss: 3.8714, Time: 22557.9s, Step: 37911, GPU: 4.9GB\n",
      "Epoch 1, Batch 37915, Loss: 2.5593, Time: 22561.4s, Step: 37916, GPU: 4.9GB\n",
      "Epoch 1, Batch 37920, Loss: 3.1275, Time: 22563.9s, Step: 37921, GPU: 4.9GB\n",
      "Epoch 1, Batch 37925, Loss: 2.9382, Time: 22567.3s, Step: 37926, GPU: 4.9GB\n",
      "Epoch 1, Batch 37930, Loss: 2.8816, Time: 22569.8s, Step: 37931, GPU: 4.9GB\n",
      "Epoch 1, Batch 37935, Loss: 2.9609, Time: 22573.2s, Step: 37936, GPU: 4.9GB\n",
      "Epoch 1, Batch 37940, Loss: 3.6958, Time: 22575.8s, Step: 37941, GPU: 4.9GB\n",
      "Epoch 1, Batch 37945, Loss: 3.3109, Time: 22579.2s, Step: 37946, GPU: 4.9GB\n",
      "Epoch 1, Batch 37950, Loss: 2.5181, Time: 22581.7s, Step: 37951, GPU: 4.9GB\n",
      "Epoch 1, Batch 37955, Loss: 3.7508, Time: 22585.1s, Step: 37956, GPU: 4.9GB\n",
      "Epoch 1, Batch 37960, Loss: 2.7997, Time: 22587.6s, Step: 37961, GPU: 4.9GB\n",
      "Epoch 1, Batch 37965, Loss: 2.7062, Time: 22591.1s, Step: 37966, GPU: 4.9GB\n",
      "Epoch 1, Batch 37970, Loss: 3.7329, Time: 22593.6s, Step: 37971, GPU: 4.9GB\n",
      "Epoch 1, Batch 37975, Loss: 2.6620, Time: 22597.1s, Step: 37976, GPU: 4.9GB\n",
      "Epoch 1, Batch 37980, Loss: 2.9768, Time: 22599.6s, Step: 37981, GPU: 4.9GB\n",
      "Epoch 1, Batch 37985, Loss: 3.0666, Time: 22603.0s, Step: 37986, GPU: 4.9GB\n",
      "Epoch 1, Batch 37990, Loss: 3.8287, Time: 22605.5s, Step: 37991, GPU: 4.9GB\n",
      "Epoch 1, Batch 37995, Loss: 4.2183, Time: 22609.0s, Step: 37996, GPU: 4.9GB\n",
      "Epoch 1, Batch 38000, Loss: 3.5375, Time: 22611.6s, Step: 38001, GPU: 4.9GB\n",
      "Epoch 1, Batch 38005, Loss: 2.9388, Time: 22615.0s, Step: 38006, GPU: 4.9GB\n",
      "Epoch 1, Batch 38010, Loss: 2.5106, Time: 22617.5s, Step: 38011, GPU: 4.9GB\n",
      "Epoch 1, Batch 38015, Loss: 2.8798, Time: 22620.9s, Step: 38016, GPU: 4.9GB\n",
      "Epoch 1, Batch 38020, Loss: 3.1484, Time: 22623.4s, Step: 38021, GPU: 4.9GB\n",
      "Epoch 1, Batch 38025, Loss: 3.0497, Time: 22626.9s, Step: 38026, GPU: 4.9GB\n",
      "Epoch 1, Batch 38030, Loss: 2.8243, Time: 22629.4s, Step: 38031, GPU: 4.9GB\n",
      "Epoch 1, Batch 38035, Loss: 3.8421, Time: 22632.8s, Step: 38036, GPU: 4.9GB\n",
      "Epoch 1, Batch 38040, Loss: 3.3203, Time: 22635.3s, Step: 38041, GPU: 4.9GB\n",
      "Epoch 1, Batch 38045, Loss: 3.7604, Time: 22638.7s, Step: 38046, GPU: 4.9GB\n",
      "Epoch 1, Batch 38050, Loss: 3.1815, Time: 22641.3s, Step: 38051, GPU: 4.9GB\n",
      "Epoch 1, Batch 38055, Loss: 4.1811, Time: 22644.7s, Step: 38056, GPU: 4.9GB\n",
      "Epoch 1, Batch 38060, Loss: 2.6991, Time: 22647.3s, Step: 38061, GPU: 4.9GB\n",
      "Epoch 1, Batch 38065, Loss: 3.1531, Time: 22650.7s, Step: 38066, GPU: 4.9GB\n",
      "Epoch 1, Batch 38070, Loss: 3.4810, Time: 22653.2s, Step: 38071, GPU: 4.9GB\n",
      "Epoch 1, Batch 38075, Loss: 3.1658, Time: 22656.6s, Step: 38076, GPU: 4.9GB\n",
      "Epoch 1, Batch 38080, Loss: 3.2680, Time: 22659.1s, Step: 38081, GPU: 4.9GB\n",
      "Epoch 1, Batch 38085, Loss: 2.8684, Time: 22662.5s, Step: 38086, GPU: 4.9GB\n",
      "Epoch 1, Batch 38090, Loss: 3.1317, Time: 22665.1s, Step: 38091, GPU: 4.9GB\n",
      "Epoch 1, Batch 38095, Loss: 3.2485, Time: 22668.5s, Step: 38096, GPU: 4.9GB\n",
      "Epoch 1, Batch 38100, Loss: 3.5207, Time: 22671.0s, Step: 38101, GPU: 4.9GB\n",
      "Epoch 1, Batch 38105, Loss: 2.8682, Time: 22674.4s, Step: 38106, GPU: 4.9GB\n",
      "Epoch 1, Batch 38110, Loss: 2.8155, Time: 22677.0s, Step: 38111, GPU: 4.9GB\n",
      "Epoch 1, Batch 38115, Loss: 3.4692, Time: 22680.4s, Step: 38116, GPU: 4.9GB\n",
      "Epoch 1, Batch 38120, Loss: 3.1786, Time: 22683.0s, Step: 38121, GPU: 4.9GB\n",
      "Epoch 1, Batch 38125, Loss: 4.2648, Time: 22686.4s, Step: 38126, GPU: 4.9GB\n",
      "Epoch 1, Batch 38130, Loss: 3.3095, Time: 22688.9s, Step: 38131, GPU: 4.9GB\n",
      "Epoch 1, Batch 38135, Loss: 3.6271, Time: 22692.4s, Step: 38136, GPU: 4.9GB\n",
      "Epoch 1, Batch 38140, Loss: 3.1356, Time: 22694.9s, Step: 38141, GPU: 4.9GB\n",
      "Epoch 1, Batch 38145, Loss: 4.0058, Time: 22698.3s, Step: 38146, GPU: 4.9GB\n",
      "Epoch 1, Batch 38150, Loss: 3.5109, Time: 22700.8s, Step: 38151, GPU: 4.9GB\n",
      "Epoch 1, Batch 38155, Loss: 4.6935, Time: 22704.2s, Step: 38156, GPU: 4.9GB\n",
      "Epoch 1, Batch 38160, Loss: 3.0564, Time: 22706.8s, Step: 38161, GPU: 4.9GB\n",
      "Epoch 1, Batch 38165, Loss: 3.2597, Time: 22710.2s, Step: 38166, GPU: 4.9GB\n",
      "Epoch 1, Batch 38170, Loss: 4.0794, Time: 22712.7s, Step: 38171, GPU: 4.9GB\n",
      "Epoch 1, Batch 38175, Loss: 3.3468, Time: 22716.1s, Step: 38176, GPU: 4.9GB\n",
      "Epoch 1, Batch 38180, Loss: 3.2341, Time: 22718.6s, Step: 38181, GPU: 4.9GB\n",
      "Epoch 1, Batch 38185, Loss: 3.5044, Time: 22722.0s, Step: 38186, GPU: 4.9GB\n",
      "Epoch 1, Batch 38190, Loss: 3.5220, Time: 22724.5s, Step: 38191, GPU: 4.9GB\n",
      "Epoch 1, Batch 38195, Loss: 3.3643, Time: 22727.9s, Step: 38196, GPU: 4.9GB\n",
      "Epoch 1, Batch 38200, Loss: 3.1640, Time: 22730.5s, Step: 38201, GPU: 4.9GB\n",
      "Epoch 1, Batch 38205, Loss: 4.0385, Time: 22733.9s, Step: 38206, GPU: 4.9GB\n",
      "Epoch 1, Batch 38210, Loss: 3.4546, Time: 22736.4s, Step: 38211, GPU: 4.9GB\n",
      "Epoch 1, Batch 38215, Loss: 4.1261, Time: 22739.9s, Step: 38216, GPU: 4.9GB\n",
      "Epoch 1, Batch 38220, Loss: 3.3140, Time: 22742.4s, Step: 38221, GPU: 4.9GB\n",
      "Epoch 1, Batch 38225, Loss: 3.3239, Time: 22745.8s, Step: 38226, GPU: 4.9GB\n",
      "Epoch 1, Batch 38230, Loss: 3.6722, Time: 22748.3s, Step: 38231, GPU: 4.9GB\n",
      "Epoch 1, Batch 38235, Loss: 3.2513, Time: 22751.7s, Step: 38236, GPU: 4.9GB\n",
      "Epoch 1, Batch 38240, Loss: 3.5859, Time: 22754.3s, Step: 38241, GPU: 4.9GB\n",
      "Epoch 1, Batch 38245, Loss: 3.2019, Time: 22757.7s, Step: 38246, GPU: 4.9GB\n",
      "Epoch 1, Batch 38250, Loss: 3.0110, Time: 22760.2s, Step: 38251, GPU: 4.9GB\n",
      "Epoch 1, Batch 38255, Loss: 3.7568, Time: 22763.6s, Step: 38256, GPU: 4.9GB\n",
      "Epoch 1, Batch 38260, Loss: 3.6615, Time: 22766.2s, Step: 38261, GPU: 4.9GB\n",
      "Epoch 1, Batch 38265, Loss: 3.4354, Time: 22769.6s, Step: 38266, GPU: 4.9GB\n",
      "Epoch 1, Batch 38270, Loss: 3.9088, Time: 22772.1s, Step: 38271, GPU: 4.9GB\n",
      "Epoch 1, Batch 38275, Loss: 3.9754, Time: 22775.5s, Step: 38276, GPU: 4.9GB\n",
      "Epoch 1, Batch 38280, Loss: 3.3544, Time: 22778.1s, Step: 38281, GPU: 4.9GB\n",
      "Epoch 1, Batch 38285, Loss: 3.1124, Time: 22781.5s, Step: 38286, GPU: 4.9GB\n",
      "Epoch 1, Batch 38290, Loss: 3.1811, Time: 22784.0s, Step: 38291, GPU: 4.9GB\n",
      "Epoch 1, Batch 38295, Loss: 2.9501, Time: 22787.4s, Step: 38296, GPU: 4.9GB\n",
      "Epoch 1, Batch 38300, Loss: 3.8578, Time: 22789.9s, Step: 38301, GPU: 4.9GB\n",
      "Epoch 1, Batch 38305, Loss: 2.9049, Time: 22793.3s, Step: 38306, GPU: 4.9GB\n",
      "Epoch 1, Batch 38310, Loss: 3.3751, Time: 22795.8s, Step: 38311, GPU: 4.9GB\n",
      "Epoch 1, Batch 38315, Loss: 3.8340, Time: 22799.3s, Step: 38316, GPU: 4.9GB\n",
      "Epoch 1, Batch 38320, Loss: 3.6513, Time: 22801.8s, Step: 38321, GPU: 4.9GB\n",
      "Epoch 1, Batch 38325, Loss: 4.0601, Time: 22805.2s, Step: 38326, GPU: 4.9GB\n",
      "Epoch 1, Batch 38330, Loss: 2.9227, Time: 22807.7s, Step: 38331, GPU: 4.9GB\n",
      "Epoch 1, Batch 38335, Loss: 2.9812, Time: 22811.2s, Step: 38336, GPU: 4.9GB\n",
      "Epoch 1, Batch 38340, Loss: 2.8666, Time: 22813.7s, Step: 38341, GPU: 4.9GB\n",
      "Epoch 1, Batch 38345, Loss: 4.1374, Time: 22817.2s, Step: 38346, GPU: 4.9GB\n",
      "Epoch 1, Batch 38350, Loss: 3.7282, Time: 22819.7s, Step: 38351, GPU: 4.9GB\n",
      "Epoch 1, Batch 38355, Loss: 4.2251, Time: 22823.1s, Step: 38356, GPU: 4.9GB\n",
      "Epoch 1, Batch 38360, Loss: 3.2452, Time: 22825.6s, Step: 38361, GPU: 4.9GB\n",
      "Epoch 1, Batch 38365, Loss: 2.8384, Time: 22829.0s, Step: 38366, GPU: 4.9GB\n",
      "Epoch 1, Batch 38370, Loss: 2.8380, Time: 22831.5s, Step: 38371, GPU: 4.9GB\n",
      "Epoch 1, Batch 38375, Loss: 3.3562, Time: 22834.9s, Step: 38376, GPU: 4.9GB\n",
      "Epoch 1, Batch 38380, Loss: 2.7034, Time: 22837.5s, Step: 38381, GPU: 4.9GB\n",
      "Epoch 1, Batch 38385, Loss: 3.3731, Time: 22840.9s, Step: 38386, GPU: 4.9GB\n",
      "Epoch 1, Batch 38390, Loss: 3.3532, Time: 22843.4s, Step: 38391, GPU: 4.9GB\n",
      "Epoch 1, Batch 38395, Loss: 3.2286, Time: 22846.8s, Step: 38396, GPU: 4.9GB\n",
      "Epoch 1, Batch 38400, Loss: 3.1343, Time: 22849.4s, Step: 38401, GPU: 4.9GB\n",
      "Epoch 1, Batch 38405, Loss: 3.5605, Time: 22852.9s, Step: 38406, GPU: 4.9GB\n",
      "Epoch 1, Batch 38410, Loss: 3.1559, Time: 22855.4s, Step: 38411, GPU: 4.9GB\n",
      "Epoch 1, Batch 38415, Loss: 2.7189, Time: 22858.8s, Step: 38416, GPU: 4.9GB\n",
      "Epoch 1, Batch 38420, Loss: 3.5819, Time: 22861.3s, Step: 38421, GPU: 4.9GB\n",
      "Epoch 1, Batch 38425, Loss: 3.6076, Time: 22864.8s, Step: 38426, GPU: 4.9GB\n",
      "Epoch 1, Batch 38430, Loss: 2.8798, Time: 22867.3s, Step: 38431, GPU: 4.9GB\n",
      "Epoch 1, Batch 38435, Loss: 4.0594, Time: 22870.7s, Step: 38436, GPU: 4.9GB\n",
      "Epoch 1, Batch 38440, Loss: 3.9464, Time: 22873.2s, Step: 38441, GPU: 4.9GB\n",
      "Epoch 1, Batch 38445, Loss: 3.3404, Time: 22876.6s, Step: 38446, GPU: 4.9GB\n",
      "Epoch 1, Batch 38450, Loss: 2.7171, Time: 22879.1s, Step: 38451, GPU: 4.9GB\n",
      "Epoch 1, Batch 38455, Loss: 3.8918, Time: 22882.5s, Step: 38456, GPU: 4.9GB\n",
      "Epoch 1, Batch 38460, Loss: 3.4216, Time: 22885.1s, Step: 38461, GPU: 4.9GB\n",
      "Epoch 1, Batch 38465, Loss: 2.5630, Time: 22888.5s, Step: 38466, GPU: 4.9GB\n",
      "Epoch 1, Batch 38470, Loss: 3.1171, Time: 22891.1s, Step: 38471, GPU: 4.9GB\n",
      "Epoch 1, Batch 38475, Loss: 3.9413, Time: 22894.5s, Step: 38476, GPU: 4.9GB\n",
      "Epoch 1, Batch 38480, Loss: 3.9938, Time: 22897.0s, Step: 38481, GPU: 4.9GB\n",
      "Epoch 1, Batch 38485, Loss: 3.0311, Time: 22900.4s, Step: 38486, GPU: 4.9GB\n",
      "Epoch 1, Batch 38490, Loss: 3.3626, Time: 22903.0s, Step: 38491, GPU: 4.9GB\n",
      "Epoch 1, Batch 38495, Loss: 4.1984, Time: 22906.4s, Step: 38496, GPU: 4.9GB\n",
      "Epoch 1, Batch 38500, Loss: 3.5895, Time: 22908.9s, Step: 38501, GPU: 4.9GB\n",
      "Epoch 1, Batch 38505, Loss: 2.9996, Time: 22912.3s, Step: 38506, GPU: 4.9GB\n",
      "Epoch 1, Batch 38510, Loss: 3.8818, Time: 22914.9s, Step: 38511, GPU: 4.9GB\n",
      "Epoch 1, Batch 38515, Loss: 3.4226, Time: 22918.3s, Step: 38516, GPU: 4.9GB\n",
      "Epoch 1, Batch 38520, Loss: 3.1267, Time: 22920.8s, Step: 38521, GPU: 4.9GB\n",
      "Epoch 1, Batch 38525, Loss: 3.1469, Time: 22924.2s, Step: 38526, GPU: 4.9GB\n",
      "Epoch 1, Batch 38530, Loss: 3.1981, Time: 22926.8s, Step: 38531, GPU: 4.9GB\n",
      "Epoch 1, Batch 38535, Loss: 2.9059, Time: 22930.2s, Step: 38536, GPU: 4.9GB\n",
      "Epoch 1, Batch 38540, Loss: 3.7482, Time: 22932.7s, Step: 38541, GPU: 4.9GB\n",
      "Epoch 1, Batch 38545, Loss: 3.6824, Time: 22936.2s, Step: 38546, GPU: 4.9GB\n",
      "Epoch 1, Batch 38550, Loss: 3.1901, Time: 22938.7s, Step: 38551, GPU: 4.9GB\n",
      "Epoch 1, Batch 38555, Loss: 2.9294, Time: 22942.1s, Step: 38556, GPU: 4.9GB\n",
      "Epoch 1, Batch 38560, Loss: 4.1512, Time: 22944.6s, Step: 38561, GPU: 4.9GB\n",
      "Epoch 1, Batch 38565, Loss: 3.2403, Time: 22948.1s, Step: 38566, GPU: 4.9GB\n",
      "Epoch 1, Batch 38570, Loss: 4.8056, Time: 22950.6s, Step: 38571, GPU: 4.9GB\n",
      "Epoch 1, Batch 38575, Loss: 3.4114, Time: 22954.0s, Step: 38576, GPU: 4.9GB\n",
      "Epoch 1, Batch 38580, Loss: 2.9533, Time: 22956.5s, Step: 38581, GPU: 4.9GB\n",
      "Epoch 1, Batch 38585, Loss: 2.8397, Time: 22960.0s, Step: 38586, GPU: 4.9GB\n",
      "Epoch 1, Batch 38590, Loss: 3.0048, Time: 22962.5s, Step: 38591, GPU: 4.9GB\n",
      "Epoch 1, Batch 38595, Loss: 3.3550, Time: 22965.9s, Step: 38596, GPU: 4.9GB\n",
      "Epoch 1, Batch 38600, Loss: 3.3645, Time: 22968.5s, Step: 38601, GPU: 4.9GB\n",
      "Epoch 1, Batch 38605, Loss: 2.4696, Time: 22971.9s, Step: 38606, GPU: 4.9GB\n",
      "Epoch 1, Batch 38610, Loss: 3.4778, Time: 22974.5s, Step: 38611, GPU: 4.9GB\n",
      "Epoch 1, Batch 38615, Loss: 3.1224, Time: 22977.9s, Step: 38616, GPU: 4.9GB\n",
      "Epoch 1, Batch 38620, Loss: 2.9652, Time: 22980.4s, Step: 38621, GPU: 4.9GB\n",
      "Epoch 1, Batch 38625, Loss: 3.7643, Time: 22983.8s, Step: 38626, GPU: 4.9GB\n",
      "Epoch 1, Batch 38630, Loss: 3.1911, Time: 22986.3s, Step: 38631, GPU: 4.9GB\n",
      "Epoch 1, Batch 38635, Loss: 3.3750, Time: 22989.7s, Step: 38636, GPU: 4.9GB\n",
      "Epoch 1, Batch 38640, Loss: 2.7849, Time: 22992.3s, Step: 38641, GPU: 4.9GB\n",
      "Epoch 1, Batch 38645, Loss: 3.1111, Time: 22995.6s, Step: 38646, GPU: 4.9GB\n",
      "Epoch 1, Batch 38650, Loss: 2.8883, Time: 22998.2s, Step: 38651, GPU: 4.9GB\n",
      "Epoch 1, Batch 38655, Loss: 3.2526, Time: 23001.6s, Step: 38656, GPU: 4.9GB\n",
      "Epoch 1, Batch 38660, Loss: 2.8428, Time: 23004.1s, Step: 38661, GPU: 4.9GB\n",
      "Epoch 1, Batch 38665, Loss: 2.8574, Time: 23007.5s, Step: 38666, GPU: 4.9GB\n",
      "Epoch 1, Batch 38670, Loss: 3.5911, Time: 23010.0s, Step: 38671, GPU: 4.9GB\n",
      "Epoch 1, Batch 38675, Loss: 2.5034, Time: 23013.4s, Step: 38676, GPU: 4.9GB\n",
      "Epoch 1, Batch 38680, Loss: 3.6584, Time: 23015.9s, Step: 38681, GPU: 4.9GB\n",
      "Epoch 1, Batch 38685, Loss: 3.1186, Time: 23019.4s, Step: 38686, GPU: 4.9GB\n",
      "Epoch 1, Batch 38690, Loss: 3.1909, Time: 23021.9s, Step: 38691, GPU: 4.9GB\n",
      "Epoch 1, Batch 38695, Loss: 2.7988, Time: 23025.3s, Step: 38696, GPU: 4.9GB\n",
      "Epoch 1, Batch 38700, Loss: 3.5243, Time: 23027.8s, Step: 38701, GPU: 4.9GB\n",
      "Epoch 1, Batch 38705, Loss: 2.5672, Time: 23031.3s, Step: 38706, GPU: 4.9GB\n",
      "Epoch 1, Batch 38710, Loss: 3.8586, Time: 23033.8s, Step: 38711, GPU: 4.9GB\n",
      "Epoch 1, Batch 38715, Loss: 3.3393, Time: 23037.2s, Step: 38716, GPU: 4.9GB\n",
      "Epoch 1, Batch 38720, Loss: 3.1725, Time: 23039.8s, Step: 38721, GPU: 4.9GB\n",
      "Epoch 1, Batch 38725, Loss: 3.2094, Time: 23043.2s, Step: 38726, GPU: 4.9GB\n",
      "Epoch 1, Batch 38730, Loss: 2.9524, Time: 23045.8s, Step: 38731, GPU: 4.9GB\n",
      "Epoch 1, Batch 38735, Loss: 3.2339, Time: 23049.2s, Step: 38736, GPU: 4.9GB\n",
      "Epoch 1, Batch 38740, Loss: 3.6087, Time: 23051.7s, Step: 38741, GPU: 4.9GB\n",
      "Epoch 1, Batch 38745, Loss: 3.5473, Time: 23055.1s, Step: 38746, GPU: 4.9GB\n",
      "Epoch 1, Batch 38750, Loss: 2.8741, Time: 23057.6s, Step: 38751, GPU: 4.9GB\n",
      "Epoch 1, Batch 38755, Loss: 3.6903, Time: 23061.0s, Step: 38756, GPU: 4.9GB\n",
      "Epoch 1, Batch 38760, Loss: 3.2846, Time: 23063.6s, Step: 38761, GPU: 4.9GB\n",
      "Epoch 1, Batch 38765, Loss: 3.0400, Time: 23067.0s, Step: 38766, GPU: 4.9GB\n",
      "Epoch 1, Batch 38770, Loss: 1.3661, Time: 23069.5s, Step: 38771, GPU: 4.9GB\n",
      "Epoch 1, Batch 38775, Loss: 3.6825, Time: 23073.0s, Step: 38776, GPU: 4.9GB\n",
      "Epoch 1, Batch 38780, Loss: 2.6816, Time: 23075.6s, Step: 38781, GPU: 4.9GB\n",
      "Epoch 1, Batch 38785, Loss: 3.1869, Time: 23079.0s, Step: 38786, GPU: 4.9GB\n",
      "Epoch 1, Batch 38790, Loss: 4.4390, Time: 23081.5s, Step: 38791, GPU: 4.9GB\n",
      "Epoch 1, Batch 38795, Loss: 3.9300, Time: 23085.0s, Step: 38796, GPU: 4.9GB\n",
      "Epoch 1, Batch 38800, Loss: 3.3000, Time: 23087.6s, Step: 38801, GPU: 4.9GB\n",
      "Epoch 1, Batch 38805, Loss: 2.6979, Time: 23091.0s, Step: 38806, GPU: 4.9GB\n",
      "Epoch 1, Batch 38810, Loss: 3.2402, Time: 23093.5s, Step: 38811, GPU: 4.9GB\n",
      "Epoch 1, Batch 38815, Loss: 3.2399, Time: 23096.9s, Step: 38816, GPU: 4.9GB\n",
      "Epoch 1, Batch 38820, Loss: 2.2107, Time: 23099.4s, Step: 38821, GPU: 4.9GB\n",
      "Epoch 1, Batch 38825, Loss: 2.6969, Time: 23102.9s, Step: 38826, GPU: 4.9GB\n",
      "Epoch 1, Batch 38830, Loss: 3.2049, Time: 23105.4s, Step: 38831, GPU: 4.9GB\n",
      "Epoch 1, Batch 38835, Loss: 3.1899, Time: 23108.8s, Step: 38836, GPU: 4.9GB\n",
      "Epoch 1, Batch 38840, Loss: 2.7179, Time: 23111.3s, Step: 38841, GPU: 4.9GB\n",
      "Epoch 1, Batch 38845, Loss: 3.4711, Time: 23114.8s, Step: 38846, GPU: 4.9GB\n",
      "Epoch 1, Batch 38850, Loss: 3.5982, Time: 23117.3s, Step: 38851, GPU: 4.9GB\n",
      "Epoch 1, Batch 38855, Loss: 2.4813, Time: 23120.7s, Step: 38856, GPU: 4.9GB\n",
      "Epoch 1, Batch 38860, Loss: 3.8744, Time: 23123.2s, Step: 38861, GPU: 4.9GB\n",
      "Epoch 1, Batch 38865, Loss: 3.3685, Time: 23126.6s, Step: 38866, GPU: 4.9GB\n",
      "Epoch 1, Batch 38870, Loss: 3.5534, Time: 23129.1s, Step: 38871, GPU: 4.9GB\n",
      "Epoch 1, Batch 38875, Loss: 3.0602, Time: 23132.6s, Step: 38876, GPU: 4.9GB\n",
      "Epoch 1, Batch 38880, Loss: 3.3130, Time: 23135.1s, Step: 38881, GPU: 4.9GB\n",
      "Epoch 1, Batch 38885, Loss: 2.8325, Time: 23138.5s, Step: 38886, GPU: 4.9GB\n",
      "Epoch 1, Batch 38890, Loss: 3.4449, Time: 23141.0s, Step: 38891, GPU: 4.9GB\n",
      "Epoch 1, Batch 38895, Loss: 3.5024, Time: 23144.5s, Step: 38896, GPU: 4.9GB\n",
      "Epoch 1, Batch 38900, Loss: 3.0138, Time: 23147.0s, Step: 38901, GPU: 4.9GB\n",
      "Epoch 1, Batch 38905, Loss: 3.2869, Time: 23150.4s, Step: 38906, GPU: 4.9GB\n",
      "Epoch 1, Batch 38910, Loss: 2.9991, Time: 23152.9s, Step: 38911, GPU: 4.9GB\n",
      "Epoch 1, Batch 38915, Loss: 3.8664, Time: 23156.3s, Step: 38916, GPU: 4.9GB\n",
      "Epoch 1, Batch 38920, Loss: 3.7261, Time: 23158.9s, Step: 38921, GPU: 4.9GB\n",
      "Epoch 1, Batch 38925, Loss: 3.3953, Time: 23162.3s, Step: 38926, GPU: 4.9GB\n",
      "Epoch 1, Batch 38930, Loss: 2.5288, Time: 23164.8s, Step: 38931, GPU: 4.9GB\n",
      "Epoch 1, Batch 38935, Loss: 3.6847, Time: 23168.2s, Step: 38936, GPU: 4.9GB\n",
      "Epoch 1, Batch 38940, Loss: 3.0103, Time: 23170.8s, Step: 38941, GPU: 4.9GB\n",
      "Epoch 1, Batch 38945, Loss: 3.1722, Time: 23174.2s, Step: 38946, GPU: 4.9GB\n",
      "Epoch 1, Batch 38950, Loss: 3.4204, Time: 23176.7s, Step: 38951, GPU: 4.9GB\n",
      "Epoch 1, Batch 38955, Loss: 3.3971, Time: 23180.1s, Step: 38956, GPU: 4.9GB\n",
      "Epoch 1, Batch 38960, Loss: 3.9446, Time: 23182.6s, Step: 38961, GPU: 4.9GB\n",
      "Epoch 1, Batch 38965, Loss: 3.7883, Time: 23186.0s, Step: 38966, GPU: 4.9GB\n",
      "Epoch 1, Batch 38970, Loss: 4.0289, Time: 23188.6s, Step: 38971, GPU: 4.9GB\n",
      "Epoch 1, Batch 38975, Loss: 2.9570, Time: 23192.0s, Step: 38976, GPU: 4.9GB\n",
      "Epoch 1, Batch 38980, Loss: 3.5414, Time: 23194.5s, Step: 38981, GPU: 4.9GB\n",
      "Epoch 1, Batch 38985, Loss: 2.9707, Time: 23197.9s, Step: 38986, GPU: 4.9GB\n",
      "Epoch 1, Batch 38990, Loss: 3.3269, Time: 23200.4s, Step: 38991, GPU: 4.9GB\n",
      "Epoch 1, Batch 38995, Loss: 2.8811, Time: 23203.9s, Step: 38996, GPU: 4.9GB\n",
      "Epoch 1, Batch 39000, Loss: 3.0427, Time: 23206.5s, Step: 39001, GPU: 4.9GB\n",
      "Epoch 1, Batch 39005, Loss: 3.0292, Time: 23209.9s, Step: 39006, GPU: 4.9GB\n",
      "Epoch 1, Batch 39010, Loss: 3.7591, Time: 23212.4s, Step: 39011, GPU: 4.9GB\n",
      "Epoch 1, Batch 39015, Loss: 3.2015, Time: 23215.9s, Step: 39016, GPU: 4.9GB\n",
      "Epoch 1, Batch 39020, Loss: 3.2959, Time: 23218.4s, Step: 39021, GPU: 4.9GB\n",
      "Epoch 1, Batch 39025, Loss: 4.3300, Time: 23221.8s, Step: 39026, GPU: 4.9GB\n",
      "Epoch 1, Batch 39030, Loss: 3.1433, Time: 23224.3s, Step: 39031, GPU: 4.9GB\n",
      "Epoch 1, Batch 39035, Loss: 3.1089, Time: 23227.7s, Step: 39036, GPU: 4.9GB\n",
      "Epoch 1, Batch 39040, Loss: 3.1087, Time: 23230.3s, Step: 39041, GPU: 4.9GB\n",
      "Epoch 1, Batch 39045, Loss: 3.3692, Time: 23233.7s, Step: 39046, GPU: 4.9GB\n",
      "Epoch 1, Batch 39050, Loss: 3.7335, Time: 23236.2s, Step: 39051, GPU: 4.9GB\n",
      "Epoch 1, Batch 39055, Loss: 3.8622, Time: 23239.6s, Step: 39056, GPU: 4.9GB\n",
      "Epoch 1, Batch 39060, Loss: 3.0694, Time: 23242.1s, Step: 39061, GPU: 4.9GB\n",
      "Epoch 1, Batch 39065, Loss: 3.7957, Time: 23245.6s, Step: 39066, GPU: 4.9GB\n",
      "Epoch 1, Batch 39070, Loss: 3.8613, Time: 23248.1s, Step: 39071, GPU: 4.9GB\n",
      "Epoch 1, Batch 39075, Loss: 4.1160, Time: 23251.5s, Step: 39076, GPU: 4.9GB\n",
      "Epoch 1, Batch 39080, Loss: 2.7634, Time: 23254.0s, Step: 39081, GPU: 4.9GB\n",
      "Epoch 1, Batch 39085, Loss: 2.9638, Time: 23257.5s, Step: 39086, GPU: 4.9GB\n",
      "Epoch 1, Batch 39090, Loss: 3.6680, Time: 23260.0s, Step: 39091, GPU: 4.9GB\n",
      "Epoch 1, Batch 39095, Loss: 2.7467, Time: 23263.4s, Step: 39096, GPU: 4.9GB\n",
      "Epoch 1, Batch 39100, Loss: 3.2665, Time: 23265.9s, Step: 39101, GPU: 4.9GB\n",
      "Epoch 1, Batch 39105, Loss: 3.3777, Time: 23269.3s, Step: 39106, GPU: 4.9GB\n",
      "Epoch 1, Batch 39110, Loss: 3.1206, Time: 23271.8s, Step: 39111, GPU: 4.9GB\n",
      "Epoch 1, Batch 39115, Loss: 3.3987, Time: 23275.2s, Step: 39116, GPU: 4.9GB\n",
      "Epoch 1, Batch 39120, Loss: 3.6109, Time: 23277.7s, Step: 39121, GPU: 4.9GB\n",
      "Epoch 1, Batch 39125, Loss: 3.1513, Time: 23281.1s, Step: 39126, GPU: 4.9GB\n",
      "Epoch 1, Batch 39130, Loss: 2.4053, Time: 23283.6s, Step: 39131, GPU: 4.9GB\n",
      "Epoch 1, Batch 39135, Loss: 2.6139, Time: 23287.1s, Step: 39136, GPU: 4.9GB\n",
      "Epoch 1, Batch 39140, Loss: 3.1518, Time: 23289.6s, Step: 39141, GPU: 4.9GB\n",
      "Epoch 1, Batch 39145, Loss: 3.3753, Time: 23293.0s, Step: 39146, GPU: 4.9GB\n",
      "Epoch 1, Batch 39150, Loss: 3.2760, Time: 23295.5s, Step: 39151, GPU: 4.9GB\n",
      "Epoch 1, Batch 39155, Loss: 3.3225, Time: 23298.9s, Step: 39156, GPU: 4.9GB\n",
      "Epoch 1, Batch 39160, Loss: 2.5884, Time: 23301.4s, Step: 39161, GPU: 4.9GB\n",
      "Epoch 1, Batch 39165, Loss: 3.2246, Time: 23304.8s, Step: 39166, GPU: 4.9GB\n",
      "Epoch 1, Batch 39170, Loss: 3.2755, Time: 23307.3s, Step: 39171, GPU: 4.9GB\n",
      "Epoch 1, Batch 39175, Loss: 2.6656, Time: 23310.7s, Step: 39176, GPU: 4.9GB\n",
      "Epoch 1, Batch 39180, Loss: 3.4321, Time: 23313.3s, Step: 39181, GPU: 4.9GB\n",
      "Epoch 1, Batch 39185, Loss: 2.8726, Time: 23316.7s, Step: 39186, GPU: 4.9GB\n",
      "Epoch 1, Batch 39190, Loss: 2.6223, Time: 23319.2s, Step: 39191, GPU: 4.9GB\n",
      "Epoch 1, Batch 39195, Loss: 2.7142, Time: 23322.6s, Step: 39196, GPU: 4.9GB\n",
      "Epoch 1, Batch 39200, Loss: 3.9928, Time: 23325.2s, Step: 39201, GPU: 4.9GB\n",
      "Epoch 1, Batch 39205, Loss: 4.0213, Time: 23328.6s, Step: 39206, GPU: 4.9GB\n",
      "Epoch 1, Batch 39210, Loss: 2.5587, Time: 23331.1s, Step: 39211, GPU: 4.9GB\n",
      "Epoch 1, Batch 39215, Loss: 3.9230, Time: 23334.5s, Step: 39216, GPU: 4.9GB\n",
      "Epoch 1, Batch 39220, Loss: 3.3390, Time: 23337.0s, Step: 39221, GPU: 4.9GB\n",
      "Epoch 1, Batch 39225, Loss: 2.8757, Time: 23340.4s, Step: 39226, GPU: 4.9GB\n",
      "Epoch 1, Batch 39230, Loss: 2.5346, Time: 23342.9s, Step: 39231, GPU: 4.9GB\n",
      "Epoch 1, Batch 39235, Loss: 3.6932, Time: 23346.4s, Step: 39236, GPU: 4.9GB\n",
      "Epoch 1, Batch 39240, Loss: 2.8746, Time: 23348.9s, Step: 39241, GPU: 4.9GB\n",
      "Epoch 1, Batch 39245, Loss: 3.8826, Time: 23352.3s, Step: 39246, GPU: 4.9GB\n",
      "Epoch 1, Batch 39250, Loss: 3.7119, Time: 23354.8s, Step: 39251, GPU: 4.9GB\n",
      "Epoch 1, Batch 39255, Loss: 3.3046, Time: 23358.2s, Step: 39256, GPU: 4.9GB\n",
      "Epoch 1, Batch 39260, Loss: 3.2020, Time: 23360.7s, Step: 39261, GPU: 4.9GB\n",
      "Epoch 1, Batch 39265, Loss: 3.2475, Time: 23364.2s, Step: 39266, GPU: 4.9GB\n",
      "Epoch 1, Batch 39270, Loss: 3.0203, Time: 23366.7s, Step: 39271, GPU: 4.9GB\n",
      "Epoch 1, Batch 39275, Loss: 4.2221, Time: 23370.1s, Step: 39276, GPU: 4.9GB\n",
      "Epoch 1, Batch 39280, Loss: 3.1356, Time: 23372.7s, Step: 39281, GPU: 4.9GB\n",
      "Epoch 1, Batch 39285, Loss: 2.7736, Time: 23376.1s, Step: 39286, GPU: 4.9GB\n",
      "Epoch 1, Batch 39290, Loss: 3.1449, Time: 23378.6s, Step: 39291, GPU: 4.9GB\n",
      "Epoch 1, Batch 39295, Loss: 2.2750, Time: 23382.0s, Step: 39296, GPU: 4.9GB\n",
      "Epoch 1, Batch 39300, Loss: 3.8683, Time: 23384.6s, Step: 39301, GPU: 4.9GB\n",
      "Epoch 1, Batch 39305, Loss: 2.6847, Time: 23388.0s, Step: 39306, GPU: 4.9GB\n",
      "Epoch 1, Batch 39310, Loss: 3.0827, Time: 23390.5s, Step: 39311, GPU: 4.9GB\n",
      "Epoch 1, Batch 39315, Loss: 3.6492, Time: 23393.9s, Step: 39316, GPU: 4.9GB\n",
      "Epoch 1, Batch 39320, Loss: 4.1849, Time: 23396.4s, Step: 39321, GPU: 4.9GB\n",
      "Epoch 1, Batch 39325, Loss: 3.1816, Time: 23399.8s, Step: 39326, GPU: 4.9GB\n",
      "Epoch 1, Batch 39330, Loss: 3.6517, Time: 23402.3s, Step: 39331, GPU: 4.9GB\n",
      "Epoch 1, Batch 39335, Loss: 3.1557, Time: 23405.7s, Step: 39336, GPU: 4.9GB\n",
      "Epoch 1, Batch 39340, Loss: 3.4187, Time: 23408.3s, Step: 39341, GPU: 4.9GB\n",
      "Epoch 1, Batch 39345, Loss: 4.3917, Time: 23411.8s, Step: 39346, GPU: 4.9GB\n",
      "Epoch 1, Batch 39350, Loss: 3.0321, Time: 23414.3s, Step: 39351, GPU: 4.9GB\n",
      "Epoch 1, Batch 39355, Loss: 3.2181, Time: 23417.7s, Step: 39356, GPU: 4.9GB\n",
      "Epoch 1, Batch 39360, Loss: 3.8081, Time: 23420.2s, Step: 39361, GPU: 4.9GB\n",
      "Epoch 1, Batch 39365, Loss: 3.0932, Time: 23423.7s, Step: 39366, GPU: 4.9GB\n",
      "Epoch 1, Batch 39370, Loss: 2.4502, Time: 23426.2s, Step: 39371, GPU: 4.9GB\n",
      "Epoch 1, Batch 39375, Loss: 4.4133, Time: 23429.6s, Step: 39376, GPU: 4.9GB\n",
      "Epoch 1, Batch 39380, Loss: 3.5000, Time: 23432.1s, Step: 39381, GPU: 4.9GB\n",
      "Epoch 1, Batch 39385, Loss: 3.3135, Time: 23435.6s, Step: 39386, GPU: 4.9GB\n",
      "Epoch 1, Batch 39390, Loss: 2.8549, Time: 23438.1s, Step: 39391, GPU: 4.9GB\n",
      "Epoch 1, Batch 39395, Loss: 3.2892, Time: 23441.5s, Step: 39396, GPU: 4.9GB\n",
      "Epoch 1, Batch 39400, Loss: 2.9428, Time: 23444.1s, Step: 39401, GPU: 4.9GB\n",
      "Epoch 1, Batch 39405, Loss: 2.7080, Time: 23447.6s, Step: 39406, GPU: 4.9GB\n",
      "Epoch 1, Batch 39410, Loss: 4.4196, Time: 23450.1s, Step: 39411, GPU: 4.9GB\n",
      "Epoch 1, Batch 39415, Loss: 4.1490, Time: 23453.6s, Step: 39416, GPU: 4.9GB\n",
      "Epoch 1, Batch 39420, Loss: 3.4864, Time: 23456.1s, Step: 39421, GPU: 4.9GB\n",
      "Epoch 1, Batch 39425, Loss: 3.1536, Time: 23459.5s, Step: 39426, GPU: 4.9GB\n",
      "Epoch 1, Batch 39430, Loss: 2.5817, Time: 23462.1s, Step: 39431, GPU: 4.9GB\n",
      "Epoch 1, Batch 39435, Loss: 3.0613, Time: 23465.6s, Step: 39436, GPU: 4.9GB\n",
      "Epoch 1, Batch 39440, Loss: 3.8049, Time: 23468.1s, Step: 39441, GPU: 4.9GB\n",
      "Epoch 1, Batch 39445, Loss: 3.7542, Time: 23471.5s, Step: 39446, GPU: 4.9GB\n",
      "Epoch 1, Batch 39450, Loss: 3.3847, Time: 23474.0s, Step: 39451, GPU: 4.9GB\n",
      "Epoch 1, Batch 39455, Loss: 2.8644, Time: 23477.5s, Step: 39456, GPU: 4.9GB\n",
      "Epoch 1, Batch 39460, Loss: 3.5922, Time: 23480.0s, Step: 39461, GPU: 4.9GB\n",
      "Epoch 1, Batch 39465, Loss: 3.7351, Time: 23483.4s, Step: 39466, GPU: 4.9GB\n",
      "Epoch 1, Batch 39470, Loss: 3.6270, Time: 23485.9s, Step: 39471, GPU: 4.9GB\n",
      "Epoch 1, Batch 39475, Loss: 3.4624, Time: 23489.4s, Step: 39476, GPU: 4.9GB\n",
      "Epoch 1, Batch 39480, Loss: 3.2590, Time: 23491.9s, Step: 39481, GPU: 4.9GB\n",
      "Epoch 1, Batch 39485, Loss: 4.1720, Time: 23495.3s, Step: 39486, GPU: 4.9GB\n",
      "Epoch 1, Batch 39490, Loss: 3.3540, Time: 23497.8s, Step: 39491, GPU: 4.9GB\n",
      "Epoch 1, Batch 39495, Loss: 2.4455, Time: 23501.3s, Step: 39496, GPU: 4.9GB\n",
      "Epoch 1, Batch 39500, Loss: 2.8752, Time: 23503.8s, Step: 39501, GPU: 4.9GB\n",
      "Epoch 1, Batch 39505, Loss: 3.1210, Time: 23507.2s, Step: 39506, GPU: 4.9GB\n",
      "Epoch 1, Batch 39510, Loss: 3.0034, Time: 23509.7s, Step: 39511, GPU: 4.9GB\n",
      "Epoch 1, Batch 39515, Loss: 3.5852, Time: 23513.1s, Step: 39516, GPU: 4.9GB\n",
      "Epoch 1, Batch 39520, Loss: 3.1135, Time: 23515.6s, Step: 39521, GPU: 4.9GB\n",
      "Epoch 1, Batch 39525, Loss: 2.8579, Time: 23519.0s, Step: 39526, GPU: 4.9GB\n",
      "Epoch 1, Batch 39530, Loss: 2.8122, Time: 23521.6s, Step: 39531, GPU: 4.9GB\n",
      "Epoch 1, Batch 39535, Loss: 3.7339, Time: 23525.0s, Step: 39536, GPU: 4.9GB\n",
      "Epoch 1, Batch 39540, Loss: 2.9570, Time: 23527.5s, Step: 39541, GPU: 4.9GB\n",
      "Epoch 1, Batch 39545, Loss: 2.8449, Time: 23531.0s, Step: 39546, GPU: 4.9GB\n",
      "Epoch 1, Batch 39550, Loss: 3.9985, Time: 23533.5s, Step: 39551, GPU: 4.9GB\n",
      "Epoch 1, Batch 39555, Loss: 3.5460, Time: 23536.9s, Step: 39556, GPU: 4.9GB\n",
      "Epoch 1, Batch 39560, Loss: 3.3691, Time: 23539.5s, Step: 39561, GPU: 4.9GB\n",
      "Epoch 1, Batch 39565, Loss: 3.2847, Time: 23542.9s, Step: 39566, GPU: 4.9GB\n",
      "Epoch 1, Batch 39570, Loss: 3.4717, Time: 23545.4s, Step: 39571, GPU: 4.9GB\n",
      "Epoch 1, Batch 39575, Loss: 3.0070, Time: 23548.8s, Step: 39576, GPU: 4.9GB\n",
      "Epoch 1, Batch 39580, Loss: 3.9157, Time: 23551.3s, Step: 39581, GPU: 4.9GB\n",
      "Epoch 1, Batch 39585, Loss: 3.0698, Time: 23554.8s, Step: 39586, GPU: 4.9GB\n",
      "Epoch 1, Batch 39590, Loss: 3.5356, Time: 23557.3s, Step: 39591, GPU: 4.9GB\n",
      "Epoch 1, Batch 39595, Loss: 3.6409, Time: 23560.7s, Step: 39596, GPU: 4.9GB\n",
      "Epoch 1, Batch 39600, Loss: 2.8335, Time: 23563.3s, Step: 39601, GPU: 4.9GB\n",
      "Epoch 1, Batch 39605, Loss: 3.1167, Time: 23566.7s, Step: 39606, GPU: 4.9GB\n",
      "Epoch 1, Batch 39610, Loss: 3.5426, Time: 23569.2s, Step: 39611, GPU: 4.9GB\n",
      "Epoch 1, Batch 39615, Loss: 3.2818, Time: 23573.3s, Step: 39616, GPU: 4.9GB\n",
      "Epoch 1, Batch 39620, Loss: 3.0506, Time: 23575.9s, Step: 39621, GPU: 4.9GB\n",
      "Epoch 1, Batch 39625, Loss: 2.3213, Time: 23579.3s, Step: 39626, GPU: 4.9GB\n",
      "Epoch 1, Batch 39630, Loss: 3.8735, Time: 23581.8s, Step: 39631, GPU: 4.9GB\n",
      "Epoch 1, Batch 39635, Loss: 3.2075, Time: 23585.2s, Step: 39636, GPU: 4.9GB\n",
      "Epoch 1, Batch 39640, Loss: 3.4958, Time: 23587.7s, Step: 39641, GPU: 4.9GB\n",
      "Epoch 1, Batch 39645, Loss: 3.8109, Time: 23591.2s, Step: 39646, GPU: 4.9GB\n",
      "Epoch 1, Batch 39650, Loss: 3.5805, Time: 23593.7s, Step: 39651, GPU: 4.9GB\n",
      "Epoch 1, Batch 39655, Loss: 3.0762, Time: 23597.1s, Step: 39656, GPU: 4.9GB\n",
      "Epoch 1, Batch 39660, Loss: 4.3083, Time: 23599.6s, Step: 39661, GPU: 4.9GB\n",
      "Epoch 1, Batch 39665, Loss: 3.1360, Time: 23603.0s, Step: 39666, GPU: 4.9GB\n",
      "Epoch 1, Batch 39670, Loss: 3.2671, Time: 23605.6s, Step: 39671, GPU: 4.9GB\n",
      "Epoch 1, Batch 39675, Loss: 2.7625, Time: 23609.0s, Step: 39676, GPU: 4.9GB\n",
      "Epoch 1, Batch 39680, Loss: 3.0341, Time: 23611.5s, Step: 39681, GPU: 4.9GB\n",
      "Epoch 1, Batch 39685, Loss: 3.3315, Time: 23614.9s, Step: 39686, GPU: 4.9GB\n",
      "Epoch 1, Batch 39690, Loss: 2.9567, Time: 23617.4s, Step: 39691, GPU: 4.9GB\n",
      "Epoch 1, Batch 39695, Loss: 3.5443, Time: 23620.8s, Step: 39696, GPU: 4.9GB\n",
      "Epoch 1, Batch 39700, Loss: 3.2286, Time: 23623.3s, Step: 39701, GPU: 4.9GB\n",
      "Epoch 1, Batch 39705, Loss: 3.7264, Time: 23626.7s, Step: 39706, GPU: 4.9GB\n",
      "Epoch 1, Batch 39710, Loss: 2.9932, Time: 23629.2s, Step: 39711, GPU: 4.9GB\n",
      "Epoch 1, Batch 39715, Loss: 3.7822, Time: 23632.6s, Step: 39716, GPU: 4.9GB\n",
      "Epoch 1, Batch 39720, Loss: 3.6638, Time: 23635.1s, Step: 39721, GPU: 4.9GB\n",
      "Epoch 1, Batch 39725, Loss: 3.2285, Time: 23638.5s, Step: 39726, GPU: 4.9GB\n",
      "Epoch 1, Batch 39730, Loss: 2.5250, Time: 23641.0s, Step: 39731, GPU: 4.9GB\n",
      "Epoch 1, Batch 39735, Loss: 3.2431, Time: 23644.4s, Step: 39736, GPU: 4.9GB\n",
      "Epoch 1, Batch 39740, Loss: 3.3961, Time: 23647.0s, Step: 39741, GPU: 4.9GB\n",
      "Epoch 1, Batch 39745, Loss: 2.1697, Time: 23650.4s, Step: 39746, GPU: 4.9GB\n",
      "Epoch 1, Batch 39750, Loss: 3.7816, Time: 23652.9s, Step: 39751, GPU: 4.9GB\n",
      "Epoch 1, Batch 39755, Loss: 3.1963, Time: 23656.3s, Step: 39756, GPU: 4.9GB\n",
      "Epoch 1, Batch 39760, Loss: 3.0544, Time: 23658.8s, Step: 39761, GPU: 4.9GB\n",
      "Epoch 1, Batch 39765, Loss: 2.8831, Time: 23662.2s, Step: 39766, GPU: 4.9GB\n",
      "Epoch 1, Batch 39770, Loss: 3.6120, Time: 23664.7s, Step: 39771, GPU: 4.9GB\n",
      "Epoch 1, Batch 39775, Loss: 3.5589, Time: 23668.2s, Step: 39776, GPU: 4.9GB\n",
      "Epoch 1, Batch 39780, Loss: 4.4598, Time: 23670.7s, Step: 39781, GPU: 4.9GB\n",
      "Epoch 1, Batch 39785, Loss: 3.5859, Time: 23674.1s, Step: 39786, GPU: 4.9GB\n",
      "Epoch 1, Batch 39790, Loss: 3.3288, Time: 23676.6s, Step: 39791, GPU: 4.9GB\n",
      "Epoch 1, Batch 39795, Loss: 3.3768, Time: 23680.0s, Step: 39796, GPU: 4.9GB\n",
      "Epoch 1, Batch 39800, Loss: 3.1467, Time: 23682.6s, Step: 39801, GPU: 4.9GB\n",
      "Epoch 1, Batch 39805, Loss: 2.7053, Time: 23686.0s, Step: 39806, GPU: 4.9GB\n",
      "Epoch 1, Batch 39810, Loss: 3.4939, Time: 23688.5s, Step: 39811, GPU: 4.9GB\n",
      "Epoch 1, Batch 39815, Loss: 2.9212, Time: 23691.9s, Step: 39816, GPU: 4.9GB\n",
      "Epoch 1, Batch 39820, Loss: 3.4208, Time: 23694.4s, Step: 39821, GPU: 4.9GB\n",
      "Epoch 1, Batch 39825, Loss: 3.3977, Time: 23697.8s, Step: 39826, GPU: 4.9GB\n",
      "Epoch 1, Batch 39830, Loss: 3.4441, Time: 23700.4s, Step: 39831, GPU: 4.9GB\n",
      "Epoch 1, Batch 39835, Loss: 2.7531, Time: 23703.8s, Step: 39836, GPU: 4.9GB\n",
      "Epoch 1, Batch 39840, Loss: 3.4918, Time: 23706.4s, Step: 39841, GPU: 4.9GB\n",
      "Epoch 1, Batch 39845, Loss: 3.3353, Time: 23709.8s, Step: 39846, GPU: 4.9GB\n",
      "Epoch 1, Batch 39850, Loss: 3.8445, Time: 23712.4s, Step: 39851, GPU: 4.9GB\n",
      "Epoch 1, Batch 39855, Loss: 3.3491, Time: 23715.8s, Step: 39856, GPU: 4.9GB\n",
      "Epoch 1, Batch 39860, Loss: 2.9815, Time: 23718.3s, Step: 39861, GPU: 4.9GB\n",
      "Epoch 1, Batch 39865, Loss: 3.4160, Time: 23721.8s, Step: 39866, GPU: 4.9GB\n",
      "Epoch 1, Batch 39870, Loss: 3.8889, Time: 23724.3s, Step: 39871, GPU: 4.9GB\n",
      "Epoch 1, Batch 39875, Loss: 3.0762, Time: 23727.7s, Step: 39876, GPU: 4.9GB\n",
      "Epoch 1, Batch 39880, Loss: 2.9026, Time: 23730.3s, Step: 39881, GPU: 4.9GB\n",
      "Epoch 1, Batch 39885, Loss: 2.3502, Time: 23733.7s, Step: 39886, GPU: 4.9GB\n",
      "Epoch 1, Batch 39890, Loss: 2.8924, Time: 23736.3s, Step: 39891, GPU: 4.9GB\n",
      "Epoch 1, Batch 39895, Loss: 4.3514, Time: 23739.7s, Step: 39896, GPU: 4.9GB\n",
      "Epoch 1, Batch 39900, Loss: 3.4525, Time: 23742.2s, Step: 39901, GPU: 4.9GB\n",
      "Epoch 1, Batch 39905, Loss: 3.5255, Time: 23745.7s, Step: 39906, GPU: 4.9GB\n",
      "Epoch 1, Batch 39910, Loss: 3.1726, Time: 23748.2s, Step: 39911, GPU: 4.9GB\n",
      "Epoch 1, Batch 39915, Loss: 3.8183, Time: 23751.6s, Step: 39916, GPU: 4.9GB\n",
      "Epoch 1, Batch 39920, Loss: 2.8934, Time: 23754.1s, Step: 39921, GPU: 4.9GB\n",
      "Epoch 1, Batch 39925, Loss: 4.9501, Time: 23757.6s, Step: 39926, GPU: 4.9GB\n",
      "Epoch 1, Batch 39930, Loss: 3.2517, Time: 23760.1s, Step: 39931, GPU: 4.9GB\n",
      "Epoch 1, Batch 39935, Loss: 2.6228, Time: 23763.5s, Step: 39936, GPU: 4.9GB\n",
      "Epoch 1, Batch 39940, Loss: 3.8764, Time: 23766.1s, Step: 39941, GPU: 4.9GB\n",
      "Epoch 1, Batch 39945, Loss: 3.4951, Time: 23769.5s, Step: 39946, GPU: 4.9GB\n",
      "Epoch 1, Batch 39950, Loss: 3.3916, Time: 23772.0s, Step: 39951, GPU: 4.9GB\n",
      "Epoch 1, Batch 39955, Loss: 3.1800, Time: 23775.4s, Step: 39956, GPU: 4.9GB\n",
      "Epoch 1, Batch 39960, Loss: 4.1294, Time: 23778.0s, Step: 39961, GPU: 4.9GB\n",
      "Epoch 1, Batch 39965, Loss: 3.2576, Time: 23781.4s, Step: 39966, GPU: 4.9GB\n",
      "Epoch 1, Batch 39970, Loss: 3.9009, Time: 23783.9s, Step: 39971, GPU: 4.9GB\n",
      "Epoch 1, Batch 39975, Loss: 3.3948, Time: 23787.3s, Step: 39976, GPU: 4.9GB\n",
      "Epoch 1, Batch 39980, Loss: 3.8381, Time: 23789.8s, Step: 39981, GPU: 4.9GB\n",
      "Epoch 1, Batch 39985, Loss: 3.1369, Time: 23793.2s, Step: 39986, GPU: 4.9GB\n",
      "Epoch 1, Batch 39990, Loss: 4.3956, Time: 23795.8s, Step: 39991, GPU: 4.9GB\n",
      "Epoch 1, Batch 39995, Loss: 3.5721, Time: 23799.2s, Step: 39996, GPU: 4.9GB\n",
      "Epoch 1, Batch 40000, Loss: 3.5218, Time: 23801.8s, Step: 40001, GPU: 4.9GB\n",
      "Epoch 1, Batch 40005, Loss: 3.1898, Time: 23805.2s, Step: 40006, GPU: 4.9GB\n",
      "Epoch 1, Batch 40010, Loss: 3.3276, Time: 23807.8s, Step: 40011, GPU: 4.9GB\n",
      "Epoch 1, Batch 40015, Loss: 3.3947, Time: 23811.2s, Step: 40016, GPU: 4.9GB\n",
      "Epoch 1, Batch 40020, Loss: 2.9174, Time: 23813.8s, Step: 40021, GPU: 4.9GB\n",
      "Epoch 1, Batch 40025, Loss: 3.0475, Time: 23817.2s, Step: 40026, GPU: 4.9GB\n",
      "Epoch 1, Batch 40030, Loss: 3.8720, Time: 23819.7s, Step: 40031, GPU: 4.9GB\n",
      "Epoch 1, Batch 40035, Loss: 4.0161, Time: 23823.1s, Step: 40036, GPU: 4.9GB\n",
      "Epoch 1, Batch 40040, Loss: 3.7215, Time: 23825.7s, Step: 40041, GPU: 4.9GB\n",
      "Epoch 1, Batch 40045, Loss: 3.2615, Time: 23829.1s, Step: 40046, GPU: 4.9GB\n",
      "Epoch 1, Batch 40050, Loss: 3.3435, Time: 23831.7s, Step: 40051, GPU: 4.9GB\n",
      "Epoch 1, Batch 40055, Loss: 2.5185, Time: 23835.1s, Step: 40056, GPU: 4.9GB\n",
      "Epoch 1, Batch 40060, Loss: 3.5592, Time: 23837.6s, Step: 40061, GPU: 4.9GB\n",
      "Epoch 1, Batch 40065, Loss: 3.8427, Time: 23841.1s, Step: 40066, GPU: 4.9GB\n",
      "Epoch 1, Batch 40070, Loss: 2.9690, Time: 23843.6s, Step: 40071, GPU: 4.9GB\n",
      "Epoch 1, Batch 40075, Loss: 3.2252, Time: 23847.0s, Step: 40076, GPU: 4.9GB\n",
      "Epoch 1, Batch 40080, Loss: 3.3149, Time: 23849.5s, Step: 40081, GPU: 4.9GB\n",
      "Epoch 1, Batch 40085, Loss: 3.6260, Time: 23852.9s, Step: 40086, GPU: 4.9GB\n",
      "Epoch 1, Batch 40090, Loss: 2.8121, Time: 23855.4s, Step: 40091, GPU: 4.9GB\n",
      "Epoch 1, Batch 40095, Loss: 3.2608, Time: 23858.8s, Step: 40096, GPU: 4.9GB\n",
      "Epoch 1, Batch 40100, Loss: 2.1823, Time: 23861.4s, Step: 40101, GPU: 4.9GB\n",
      "Epoch 1, Batch 40105, Loss: 3.3183, Time: 23864.7s, Step: 40106, GPU: 4.9GB\n",
      "Epoch 1, Batch 40110, Loss: 2.9569, Time: 23867.3s, Step: 40111, GPU: 4.9GB\n",
      "Epoch 1, Batch 40115, Loss: 3.7044, Time: 23870.7s, Step: 40116, GPU: 4.9GB\n",
      "Epoch 1, Batch 40120, Loss: 3.5423, Time: 23873.2s, Step: 40121, GPU: 4.9GB\n",
      "Epoch 1, Batch 40125, Loss: 3.4285, Time: 23876.6s, Step: 40126, GPU: 4.9GB\n",
      "Epoch 1, Batch 40130, Loss: 3.4136, Time: 23879.1s, Step: 40131, GPU: 4.9GB\n",
      "Epoch 1, Batch 40135, Loss: 3.2548, Time: 23882.5s, Step: 40136, GPU: 4.9GB\n",
      "Epoch 1, Batch 40140, Loss: 4.3230, Time: 23885.1s, Step: 40141, GPU: 4.9GB\n",
      "Epoch 1, Batch 40145, Loss: 2.7015, Time: 23888.5s, Step: 40146, GPU: 4.9GB\n",
      "Epoch 1, Batch 40150, Loss: 3.2389, Time: 23891.0s, Step: 40151, GPU: 4.9GB\n",
      "Epoch 1, Batch 40155, Loss: 3.3748, Time: 23894.4s, Step: 40156, GPU: 4.9GB\n",
      "Epoch 1, Batch 40160, Loss: 3.0367, Time: 23897.0s, Step: 40161, GPU: 4.9GB\n",
      "Epoch 1, Batch 40165, Loss: 3.1749, Time: 23900.4s, Step: 40166, GPU: 4.9GB\n",
      "Epoch 1, Batch 40170, Loss: 3.9109, Time: 23903.0s, Step: 40171, GPU: 4.9GB\n",
      "Epoch 1, Batch 40175, Loss: 3.9800, Time: 23906.4s, Step: 40176, GPU: 4.9GB\n",
      "Epoch 1, Batch 40180, Loss: 3.8116, Time: 23909.0s, Step: 40181, GPU: 4.9GB\n",
      "Epoch 1, Batch 40185, Loss: 2.3630, Time: 23912.4s, Step: 40186, GPU: 4.9GB\n",
      "Epoch 1, Batch 40190, Loss: 3.2902, Time: 23914.9s, Step: 40191, GPU: 4.9GB\n",
      "Epoch 1, Batch 40195, Loss: 3.6070, Time: 23918.4s, Step: 40196, GPU: 4.9GB\n",
      "Epoch 1, Batch 40200, Loss: 3.7277, Time: 23921.0s, Step: 40201, GPU: 4.9GB\n",
      "Epoch 1, Batch 40205, Loss: 2.8316, Time: 23924.5s, Step: 40206, GPU: 4.9GB\n",
      "Epoch 1, Batch 40210, Loss: 4.0257, Time: 23927.0s, Step: 40211, GPU: 4.9GB\n",
      "Epoch 1, Batch 40215, Loss: 3.7729, Time: 23930.4s, Step: 40216, GPU: 4.9GB\n",
      "Epoch 1, Batch 40220, Loss: 3.5723, Time: 23932.9s, Step: 40221, GPU: 4.9GB\n",
      "Epoch 1, Batch 40225, Loss: 3.2528, Time: 23936.3s, Step: 40226, GPU: 4.9GB\n",
      "Epoch 1, Batch 40230, Loss: 3.1165, Time: 23938.9s, Step: 40231, GPU: 4.9GB\n",
      "Epoch 1, Batch 40235, Loss: 3.3521, Time: 23942.3s, Step: 40236, GPU: 4.9GB\n",
      "Epoch 1, Batch 40240, Loss: 3.1381, Time: 23944.8s, Step: 40241, GPU: 4.9GB\n",
      "Epoch 1, Batch 40245, Loss: 3.0703, Time: 23948.2s, Step: 40246, GPU: 4.9GB\n",
      "Epoch 1, Batch 40250, Loss: 3.0278, Time: 23950.8s, Step: 40251, GPU: 4.9GB\n",
      "Epoch 1, Batch 40255, Loss: 4.7068, Time: 23954.2s, Step: 40256, GPU: 4.9GB\n",
      "Epoch 1, Batch 40260, Loss: 3.5876, Time: 23956.7s, Step: 40261, GPU: 4.9GB\n",
      "Epoch 1, Batch 40265, Loss: 3.2619, Time: 23960.1s, Step: 40266, GPU: 4.9GB\n",
      "Epoch 1, Batch 40270, Loss: 3.5323, Time: 23962.7s, Step: 40271, GPU: 4.9GB\n",
      "Epoch 1, Batch 40275, Loss: 3.5583, Time: 23966.1s, Step: 40276, GPU: 4.9GB\n",
      "Epoch 1, Batch 40280, Loss: 3.4432, Time: 23968.6s, Step: 40281, GPU: 4.9GB\n",
      "Epoch 1, Batch 40285, Loss: 3.2657, Time: 23972.0s, Step: 40286, GPU: 4.9GB\n",
      "Epoch 1, Batch 40290, Loss: 3.2132, Time: 23974.5s, Step: 40291, GPU: 4.9GB\n",
      "Epoch 1, Batch 40295, Loss: 3.9563, Time: 23977.9s, Step: 40296, GPU: 4.9GB\n",
      "Epoch 1, Batch 40300, Loss: 4.0445, Time: 23980.4s, Step: 40301, GPU: 4.9GB\n",
      "Epoch 1, Batch 40305, Loss: 2.7521, Time: 23983.9s, Step: 40306, GPU: 4.9GB\n",
      "Epoch 1, Batch 40310, Loss: 3.6099, Time: 23986.4s, Step: 40311, GPU: 4.9GB\n",
      "Epoch 1, Batch 40315, Loss: 3.7221, Time: 23989.8s, Step: 40316, GPU: 4.9GB\n",
      "Epoch 1, Batch 40320, Loss: 3.1910, Time: 23992.3s, Step: 40321, GPU: 4.9GB\n",
      "Epoch 1, Batch 40325, Loss: 4.2269, Time: 23995.7s, Step: 40326, GPU: 4.9GB\n",
      "Epoch 1, Batch 40330, Loss: 3.3794, Time: 23998.2s, Step: 40331, GPU: 4.9GB\n",
      "Epoch 1, Batch 40335, Loss: 3.6203, Time: 24001.7s, Step: 40336, GPU: 4.9GB\n",
      "Epoch 1, Batch 40340, Loss: 2.5788, Time: 24004.2s, Step: 40341, GPU: 4.9GB\n",
      "Epoch 1, Batch 40345, Loss: 3.1428, Time: 24007.6s, Step: 40346, GPU: 4.9GB\n",
      "Epoch 1, Batch 40350, Loss: 2.4978, Time: 24010.1s, Step: 40351, GPU: 4.9GB\n",
      "Epoch 1, Batch 40355, Loss: 3.2222, Time: 24013.5s, Step: 40356, GPU: 4.9GB\n",
      "Epoch 1, Batch 40360, Loss: 3.6579, Time: 24016.0s, Step: 40361, GPU: 4.9GB\n",
      "Epoch 1, Batch 40365, Loss: 3.1081, Time: 24019.4s, Step: 40366, GPU: 4.9GB\n",
      "Epoch 1, Batch 40370, Loss: 2.9452, Time: 24021.9s, Step: 40371, GPU: 4.9GB\n",
      "Epoch 1, Batch 40375, Loss: 3.5425, Time: 24025.3s, Step: 40376, GPU: 4.9GB\n",
      "Epoch 1, Batch 40380, Loss: 3.2829, Time: 24027.8s, Step: 40381, GPU: 4.9GB\n",
      "Epoch 1, Batch 40385, Loss: 3.4309, Time: 24031.3s, Step: 40386, GPU: 4.9GB\n",
      "Epoch 1, Batch 40390, Loss: 4.0940, Time: 24033.8s, Step: 40391, GPU: 4.9GB\n",
      "Epoch 1, Batch 40395, Loss: 3.3936, Time: 24037.2s, Step: 40396, GPU: 4.9GB\n",
      "Epoch 1, Batch 40400, Loss: 2.7002, Time: 24039.8s, Step: 40401, GPU: 4.9GB\n",
      "Epoch 1, Batch 40405, Loss: 3.2081, Time: 24043.3s, Step: 40406, GPU: 4.9GB\n",
      "Epoch 1, Batch 40410, Loss: 3.5065, Time: 24045.8s, Step: 40411, GPU: 4.9GB\n",
      "Epoch 1, Batch 40415, Loss: 2.9928, Time: 24049.2s, Step: 40416, GPU: 4.9GB\n",
      "Epoch 1, Batch 40420, Loss: 3.5087, Time: 24051.7s, Step: 40421, GPU: 4.9GB\n",
      "Epoch 1, Batch 40425, Loss: 3.7137, Time: 24055.1s, Step: 40426, GPU: 4.9GB\n",
      "Epoch 1, Batch 40430, Loss: 3.6123, Time: 24057.6s, Step: 40431, GPU: 4.9GB\n",
      "Epoch 1, Batch 40435, Loss: 3.3136, Time: 24061.0s, Step: 40436, GPU: 4.9GB\n",
      "Epoch 1, Batch 40440, Loss: 3.5665, Time: 24063.5s, Step: 40441, GPU: 4.9GB\n",
      "Epoch 1, Batch 40445, Loss: 3.7092, Time: 24066.9s, Step: 40446, GPU: 4.9GB\n",
      "Epoch 1, Batch 40450, Loss: 3.3014, Time: 24069.5s, Step: 40451, GPU: 4.9GB\n",
      "Epoch 1, Batch 40455, Loss: 2.0730, Time: 24072.9s, Step: 40456, GPU: 4.9GB\n",
      "Epoch 1, Batch 40460, Loss: 3.3091, Time: 24075.4s, Step: 40461, GPU: 4.9GB\n",
      "Epoch 1, Batch 40465, Loss: 3.2535, Time: 24078.8s, Step: 40466, GPU: 4.9GB\n",
      "Epoch 1, Batch 40470, Loss: 3.5432, Time: 24081.3s, Step: 40471, GPU: 4.9GB\n",
      "Epoch 1, Batch 40475, Loss: 3.6014, Time: 24084.8s, Step: 40476, GPU: 4.9GB\n",
      "Epoch 1, Batch 40480, Loss: 2.8841, Time: 24087.3s, Step: 40481, GPU: 4.9GB\n",
      "Epoch 1, Batch 40485, Loss: 3.6546, Time: 24090.7s, Step: 40486, GPU: 4.9GB\n",
      "Epoch 1, Batch 40490, Loss: 3.2292, Time: 24093.2s, Step: 40491, GPU: 4.9GB\n",
      "Epoch 1, Batch 40495, Loss: 4.2739, Time: 24096.6s, Step: 40496, GPU: 4.9GB\n",
      "Epoch 1, Batch 40500, Loss: 2.9452, Time: 24099.2s, Step: 40501, GPU: 4.9GB\n",
      "Epoch 1, Batch 40505, Loss: 2.6964, Time: 24102.6s, Step: 40506, GPU: 4.9GB\n",
      "Epoch 1, Batch 40510, Loss: 3.4411, Time: 24105.1s, Step: 40511, GPU: 4.9GB\n",
      "Epoch 1, Batch 40515, Loss: 3.0484, Time: 24108.6s, Step: 40516, GPU: 4.9GB\n",
      "Epoch 1, Batch 40520, Loss: 3.6694, Time: 24111.1s, Step: 40521, GPU: 4.9GB\n",
      "Epoch 1, Batch 40525, Loss: 2.5058, Time: 24114.5s, Step: 40526, GPU: 4.9GB\n",
      "Epoch 1, Batch 40530, Loss: 3.1311, Time: 24117.0s, Step: 40531, GPU: 4.9GB\n",
      "Epoch 1, Batch 40535, Loss: 3.8753, Time: 24120.4s, Step: 40536, GPU: 4.9GB\n",
      "Epoch 1, Batch 40540, Loss: 2.4140, Time: 24122.9s, Step: 40541, GPU: 4.9GB\n",
      "Epoch 1, Batch 40545, Loss: 3.5723, Time: 24126.4s, Step: 40546, GPU: 4.9GB\n",
      "Epoch 1, Batch 40550, Loss: 3.6467, Time: 24128.9s, Step: 40551, GPU: 4.9GB\n",
      "Epoch 1, Batch 40555, Loss: 3.4884, Time: 24132.3s, Step: 40556, GPU: 4.9GB\n",
      "Epoch 1, Batch 40560, Loss: 3.0841, Time: 24134.9s, Step: 40561, GPU: 4.9GB\n",
      "Epoch 1, Batch 40565, Loss: 2.8806, Time: 24138.3s, Step: 40566, GPU: 4.9GB\n",
      "Epoch 1, Batch 40570, Loss: 2.8039, Time: 24140.8s, Step: 40571, GPU: 4.9GB\n",
      "Epoch 1, Batch 40575, Loss: 3.7672, Time: 24144.2s, Step: 40576, GPU: 4.9GB\n",
      "Epoch 1, Batch 40580, Loss: 3.0887, Time: 24146.7s, Step: 40581, GPU: 4.9GB\n",
      "Epoch 1, Batch 40585, Loss: 3.7682, Time: 24150.1s, Step: 40586, GPU: 4.9GB\n",
      "Epoch 1, Batch 40590, Loss: 3.6098, Time: 24152.6s, Step: 40591, GPU: 4.9GB\n",
      "Epoch 1, Batch 40595, Loss: 3.3581, Time: 24156.0s, Step: 40596, GPU: 4.9GB\n",
      "Epoch 1, Batch 40600, Loss: 3.7318, Time: 24158.6s, Step: 40601, GPU: 4.9GB\n",
      "Epoch 1, Batch 40605, Loss: 3.2743, Time: 24162.1s, Step: 40606, GPU: 4.9GB\n",
      "Epoch 1, Batch 40610, Loss: 3.2759, Time: 24164.6s, Step: 40611, GPU: 4.9GB\n",
      "Epoch 1, Batch 40615, Loss: 4.1761, Time: 24168.0s, Step: 40616, GPU: 4.9GB\n",
      "Epoch 1, Batch 40620, Loss: 3.2048, Time: 24170.5s, Step: 40621, GPU: 4.9GB\n",
      "Epoch 1, Batch 40625, Loss: 4.9615, Time: 24173.9s, Step: 40626, GPU: 4.9GB\n",
      "Epoch 1, Batch 40630, Loss: 4.3264, Time: 24176.4s, Step: 40631, GPU: 4.9GB\n",
      "Epoch 1, Batch 40635, Loss: 2.4064, Time: 24179.9s, Step: 40636, GPU: 4.9GB\n",
      "Epoch 1, Batch 40640, Loss: 3.4476, Time: 24182.4s, Step: 40641, GPU: 4.9GB\n",
      "Epoch 1, Batch 40645, Loss: 3.2455, Time: 24185.8s, Step: 40646, GPU: 4.9GB\n",
      "Epoch 1, Batch 40650, Loss: 2.9827, Time: 24188.3s, Step: 40651, GPU: 4.9GB\n",
      "Epoch 1, Batch 40655, Loss: 3.1859, Time: 24191.7s, Step: 40656, GPU: 4.9GB\n",
      "Epoch 1, Batch 40660, Loss: 2.7234, Time: 24194.2s, Step: 40661, GPU: 4.9GB\n",
      "Epoch 1, Batch 40665, Loss: 3.4832, Time: 24197.7s, Step: 40666, GPU: 4.9GB\n",
      "Epoch 1, Batch 40670, Loss: 2.9631, Time: 24200.2s, Step: 40671, GPU: 4.9GB\n",
      "Epoch 1, Batch 40675, Loss: 3.5783, Time: 24203.6s, Step: 40676, GPU: 4.9GB\n",
      "Epoch 1, Batch 40680, Loss: 3.0834, Time: 24206.1s, Step: 40681, GPU: 4.9GB\n",
      "Epoch 1, Batch 40685, Loss: 2.4760, Time: 24209.6s, Step: 40686, GPU: 4.9GB\n",
      "Epoch 1, Batch 40690, Loss: 2.8401, Time: 24212.1s, Step: 40691, GPU: 4.9GB\n",
      "Epoch 1, Batch 40695, Loss: 3.3358, Time: 24215.5s, Step: 40696, GPU: 4.9GB\n",
      "Epoch 1, Batch 40700, Loss: 3.9282, Time: 24218.0s, Step: 40701, GPU: 4.9GB\n",
      "Epoch 1, Batch 40705, Loss: 3.0900, Time: 24221.5s, Step: 40706, GPU: 4.9GB\n",
      "Epoch 1, Batch 40710, Loss: 3.4036, Time: 24224.0s, Step: 40711, GPU: 4.9GB\n",
      "Epoch 1, Batch 40715, Loss: 3.0923, Time: 24227.4s, Step: 40716, GPU: 4.9GB\n",
      "Epoch 1, Batch 40720, Loss: 3.1733, Time: 24229.9s, Step: 40721, GPU: 4.9GB\n",
      "Epoch 1, Batch 40725, Loss: 3.5386, Time: 24233.4s, Step: 40726, GPU: 4.9GB\n",
      "Epoch 1, Batch 40730, Loss: 3.4589, Time: 24235.9s, Step: 40731, GPU: 4.9GB\n",
      "Epoch 1, Batch 40735, Loss: 3.6069, Time: 24239.3s, Step: 40736, GPU: 4.9GB\n",
      "Epoch 1, Batch 40740, Loss: 3.5714, Time: 24241.8s, Step: 40741, GPU: 4.9GB\n",
      "Epoch 1, Batch 40745, Loss: 3.2109, Time: 24245.3s, Step: 40746, GPU: 4.9GB\n",
      "Epoch 1, Batch 40750, Loss: 2.3226, Time: 24247.8s, Step: 40751, GPU: 4.9GB\n",
      "Epoch 1, Batch 40755, Loss: 3.0595, Time: 24251.2s, Step: 40756, GPU: 4.9GB\n",
      "Epoch 1, Batch 40760, Loss: 3.0938, Time: 24253.7s, Step: 40761, GPU: 4.9GB\n",
      "Epoch 1, Batch 40765, Loss: 3.4564, Time: 24257.2s, Step: 40766, GPU: 4.9GB\n",
      "Epoch 1, Batch 40770, Loss: 4.0685, Time: 24259.7s, Step: 40771, GPU: 4.9GB\n",
      "Epoch 1, Batch 40775, Loss: 2.2265, Time: 24263.1s, Step: 40776, GPU: 4.9GB\n",
      "Epoch 1, Batch 40780, Loss: 3.5288, Time: 24265.6s, Step: 40781, GPU: 4.9GB\n",
      "Epoch 1, Batch 40785, Loss: 2.6502, Time: 24269.0s, Step: 40786, GPU: 4.9GB\n",
      "Epoch 1, Batch 40790, Loss: 2.7808, Time: 24271.5s, Step: 40791, GPU: 4.9GB\n",
      "Epoch 1, Batch 40795, Loss: 3.3339, Time: 24275.0s, Step: 40796, GPU: 4.9GB\n",
      "Epoch 1, Batch 40800, Loss: 2.9993, Time: 24277.5s, Step: 40801, GPU: 4.9GB\n",
      "Epoch 1, Batch 40805, Loss: 3.2379, Time: 24281.0s, Step: 40806, GPU: 4.9GB\n",
      "Epoch 1, Batch 40810, Loss: 3.4787, Time: 24283.5s, Step: 40811, GPU: 4.9GB\n",
      "Epoch 1, Batch 40815, Loss: 2.8116, Time: 24287.0s, Step: 40816, GPU: 4.9GB\n",
      "Epoch 1, Batch 40820, Loss: 3.4947, Time: 24289.5s, Step: 40821, GPU: 4.9GB\n",
      "Epoch 1, Batch 40825, Loss: 3.0477, Time: 24292.9s, Step: 40826, GPU: 4.9GB\n",
      "Epoch 1, Batch 40830, Loss: 3.8812, Time: 24295.4s, Step: 40831, GPU: 4.9GB\n",
      "Epoch 1, Batch 40835, Loss: 3.4609, Time: 24298.9s, Step: 40836, GPU: 4.9GB\n",
      "Epoch 1, Batch 40840, Loss: 3.6183, Time: 24301.4s, Step: 40841, GPU: 4.9GB\n",
      "Epoch 1, Batch 40845, Loss: 3.7442, Time: 24304.8s, Step: 40846, GPU: 4.9GB\n",
      "Epoch 1, Batch 40850, Loss: 3.2272, Time: 24307.3s, Step: 40851, GPU: 4.9GB\n",
      "Epoch 1, Batch 40855, Loss: 2.6358, Time: 24310.7s, Step: 40856, GPU: 4.9GB\n",
      "Epoch 1, Batch 40860, Loss: 3.0264, Time: 24313.2s, Step: 40861, GPU: 4.9GB\n",
      "Epoch 1, Batch 40865, Loss: 4.3023, Time: 24316.7s, Step: 40866, GPU: 4.9GB\n",
      "Epoch 1, Batch 40870, Loss: 2.8935, Time: 24319.2s, Step: 40871, GPU: 4.9GB\n",
      "Epoch 1, Batch 40875, Loss: 3.2363, Time: 24322.6s, Step: 40876, GPU: 4.9GB\n",
      "Epoch 1, Batch 40880, Loss: 3.2236, Time: 24325.1s, Step: 40881, GPU: 4.9GB\n",
      "Epoch 1, Batch 40885, Loss: 4.3436, Time: 24328.5s, Step: 40886, GPU: 4.9GB\n",
      "Epoch 1, Batch 40890, Loss: 3.5622, Time: 24331.1s, Step: 40891, GPU: 4.9GB\n",
      "Epoch 1, Batch 40895, Loss: 2.7418, Time: 24334.5s, Step: 40896, GPU: 4.9GB\n",
      "Epoch 1, Batch 40900, Loss: 4.7420, Time: 24337.0s, Step: 40901, GPU: 4.9GB\n",
      "Epoch 1, Batch 40905, Loss: 3.7347, Time: 24340.5s, Step: 40906, GPU: 4.9GB\n",
      "Epoch 1, Batch 40910, Loss: 2.8572, Time: 24343.0s, Step: 40911, GPU: 4.9GB\n",
      "Epoch 1, Batch 40915, Loss: 3.5344, Time: 24346.4s, Step: 40916, GPU: 4.9GB\n",
      "Epoch 1, Batch 40920, Loss: 2.8538, Time: 24348.9s, Step: 40921, GPU: 4.9GB\n",
      "Epoch 1, Batch 40925, Loss: 3.4195, Time: 24352.4s, Step: 40926, GPU: 4.9GB\n",
      "Epoch 1, Batch 40930, Loss: 2.5723, Time: 24354.9s, Step: 40931, GPU: 4.9GB\n",
      "Epoch 1, Batch 40935, Loss: 2.4162, Time: 24358.3s, Step: 40936, GPU: 4.9GB\n",
      "Epoch 1, Batch 40940, Loss: 2.8874, Time: 24360.8s, Step: 40941, GPU: 4.9GB\n",
      "Epoch 1, Batch 40945, Loss: 2.8102, Time: 24364.2s, Step: 40946, GPU: 4.9GB\n",
      "Epoch 1, Batch 40950, Loss: 3.7393, Time: 24366.7s, Step: 40951, GPU: 4.9GB\n",
      "Epoch 1, Batch 40955, Loss: 3.7862, Time: 24370.2s, Step: 40956, GPU: 4.9GB\n",
      "Epoch 1, Batch 40960, Loss: 3.1159, Time: 24372.7s, Step: 40961, GPU: 4.9GB\n",
      "Epoch 1, Batch 40965, Loss: 3.1554, Time: 24376.1s, Step: 40966, GPU: 4.9GB\n",
      "Epoch 1, Batch 40970, Loss: 2.7806, Time: 24378.7s, Step: 40971, GPU: 4.9GB\n",
      "Epoch 1, Batch 40975, Loss: 2.7579, Time: 24382.1s, Step: 40976, GPU: 4.9GB\n",
      "Epoch 1, Batch 40980, Loss: 3.8489, Time: 24384.7s, Step: 40981, GPU: 4.9GB\n",
      "Epoch 1, Batch 40985, Loss: 3.3167, Time: 24388.1s, Step: 40986, GPU: 4.9GB\n",
      "Epoch 1, Batch 40990, Loss: 3.7956, Time: 24390.6s, Step: 40991, GPU: 4.9GB\n",
      "Epoch 1, Batch 40995, Loss: 4.2210, Time: 24394.1s, Step: 40996, GPU: 4.9GB\n",
      "Epoch 1, Batch 41000, Loss: 2.7730, Time: 24396.7s, Step: 41001, GPU: 4.9GB\n",
      "Epoch 1, Batch 41005, Loss: 3.0267, Time: 24400.1s, Step: 41006, GPU: 4.9GB\n",
      "Epoch 1, Batch 41010, Loss: 3.2401, Time: 24402.6s, Step: 41011, GPU: 4.9GB\n",
      "Epoch 1, Batch 41015, Loss: 3.3476, Time: 24406.1s, Step: 41016, GPU: 4.9GB\n",
      "Epoch 1, Batch 41020, Loss: 3.4390, Time: 24408.6s, Step: 41021, GPU: 4.9GB\n",
      "Epoch 1, Batch 41025, Loss: 3.7197, Time: 24412.0s, Step: 41026, GPU: 4.9GB\n",
      "Epoch 1, Batch 41030, Loss: 3.1924, Time: 24414.5s, Step: 41031, GPU: 4.9GB\n",
      "Epoch 1, Batch 41035, Loss: 3.9745, Time: 24417.9s, Step: 41036, GPU: 4.9GB\n",
      "Epoch 1, Batch 41040, Loss: 3.1879, Time: 24420.5s, Step: 41041, GPU: 4.9GB\n",
      "Epoch 1, Batch 41045, Loss: 3.0784, Time: 24423.9s, Step: 41046, GPU: 4.9GB\n",
      "Epoch 1, Batch 41050, Loss: 3.3385, Time: 24426.4s, Step: 41051, GPU: 4.9GB\n",
      "Epoch 1, Batch 41055, Loss: 3.1981, Time: 24429.8s, Step: 41056, GPU: 4.9GB\n",
      "Epoch 1, Batch 41060, Loss: 3.8517, Time: 24432.3s, Step: 41061, GPU: 4.9GB\n",
      "Epoch 1, Batch 41065, Loss: 2.5599, Time: 24435.7s, Step: 41066, GPU: 4.9GB\n",
      "Epoch 1, Batch 41070, Loss: 3.4824, Time: 24438.3s, Step: 41071, GPU: 4.9GB\n",
      "Epoch 1, Batch 41075, Loss: 2.9251, Time: 24441.7s, Step: 41076, GPU: 4.9GB\n",
      "Epoch 1, Batch 41080, Loss: 3.5610, Time: 24444.2s, Step: 41081, GPU: 4.9GB\n",
      "Epoch 1, Batch 41085, Loss: 2.9734, Time: 24447.6s, Step: 41086, GPU: 4.9GB\n",
      "Epoch 1, Batch 41090, Loss: 2.3085, Time: 24450.1s, Step: 41091, GPU: 4.9GB\n",
      "Epoch 1, Batch 41095, Loss: 3.3292, Time: 24453.5s, Step: 41096, GPU: 4.9GB\n",
      "Epoch 1, Batch 41100, Loss: 3.5163, Time: 24456.1s, Step: 41101, GPU: 4.9GB\n",
      "Epoch 1, Batch 41105, Loss: 3.3504, Time: 24459.5s, Step: 41106, GPU: 4.9GB\n",
      "Epoch 1, Batch 41110, Loss: 3.5928, Time: 24462.0s, Step: 41111, GPU: 4.9GB\n",
      "Epoch 1, Batch 41115, Loss: 3.5554, Time: 24465.4s, Step: 41116, GPU: 4.9GB\n",
      "Epoch 1, Batch 41120, Loss: 3.6072, Time: 24467.9s, Step: 41121, GPU: 4.9GB\n",
      "Epoch 1, Batch 41125, Loss: 3.2467, Time: 24471.3s, Step: 41126, GPU: 4.9GB\n",
      "Epoch 1, Batch 41130, Loss: 3.9979, Time: 24473.8s, Step: 41131, GPU: 4.9GB\n",
      "Epoch 1, Batch 41135, Loss: 2.9235, Time: 24477.2s, Step: 41136, GPU: 4.9GB\n",
      "Epoch 1, Batch 41140, Loss: 3.2646, Time: 24479.8s, Step: 41141, GPU: 4.9GB\n",
      "Epoch 1, Batch 41145, Loss: 2.9034, Time: 24483.2s, Step: 41146, GPU: 4.9GB\n",
      "Epoch 1, Batch 41150, Loss: 3.6205, Time: 24485.7s, Step: 41151, GPU: 4.9GB\n",
      "Epoch 1, Batch 41155, Loss: 3.7426, Time: 24489.1s, Step: 41156, GPU: 4.9GB\n",
      "Epoch 1, Batch 41160, Loss: 3.1515, Time: 24491.6s, Step: 41161, GPU: 4.9GB\n",
      "Epoch 1, Batch 41165, Loss: 3.5795, Time: 24495.0s, Step: 41166, GPU: 4.9GB\n",
      "Epoch 1, Batch 41170, Loss: 3.6108, Time: 24497.6s, Step: 41171, GPU: 4.9GB\n",
      "Epoch 1, Batch 41175, Loss: 3.6304, Time: 24501.0s, Step: 41176, GPU: 4.9GB\n",
      "Epoch 1, Batch 41180, Loss: 3.1448, Time: 24503.6s, Step: 41181, GPU: 4.9GB\n",
      "Epoch 1, Batch 41185, Loss: 2.9149, Time: 24508.0s, Step: 41186, GPU: 4.9GB\n",
      "Epoch 1, Batch 41190, Loss: 3.0636, Time: 24510.6s, Step: 41191, GPU: 4.9GB\n",
      "Epoch 1, Batch 41195, Loss: 3.6014, Time: 24514.0s, Step: 41196, GPU: 4.9GB\n",
      "Epoch 1, Batch 41200, Loss: 3.0949, Time: 24516.6s, Step: 41201, GPU: 4.9GB\n",
      "Epoch 1, Batch 41205, Loss: 3.0851, Time: 24520.0s, Step: 41206, GPU: 4.9GB\n",
      "Epoch 1, Batch 41210, Loss: 3.2266, Time: 24522.6s, Step: 41211, GPU: 4.9GB\n",
      "Epoch 1, Batch 41215, Loss: 2.7344, Time: 24526.0s, Step: 41216, GPU: 4.9GB\n",
      "Epoch 1, Batch 41220, Loss: 4.1072, Time: 24528.5s, Step: 41221, GPU: 4.9GB\n",
      "Epoch 1, Batch 41225, Loss: 2.3112, Time: 24532.0s, Step: 41226, GPU: 4.9GB\n",
      "Epoch 1, Batch 41230, Loss: 3.8939, Time: 24534.5s, Step: 41231, GPU: 4.9GB\n",
      "Epoch 1, Batch 41235, Loss: 3.1765, Time: 24537.9s, Step: 41236, GPU: 4.9GB\n",
      "Epoch 1, Batch 41240, Loss: 3.5552, Time: 24540.5s, Step: 41241, GPU: 4.9GB\n",
      "Epoch 1, Batch 41245, Loss: 3.5883, Time: 24543.9s, Step: 41246, GPU: 4.9GB\n",
      "Epoch 1, Batch 41250, Loss: 3.1169, Time: 24546.4s, Step: 41251, GPU: 4.9GB\n",
      "Epoch 1, Batch 41255, Loss: 3.5895, Time: 24549.8s, Step: 41256, GPU: 4.9GB\n",
      "Epoch 1, Batch 41260, Loss: 3.2095, Time: 24552.4s, Step: 41261, GPU: 4.9GB\n",
      "Epoch 1, Batch 41265, Loss: 3.7897, Time: 24555.8s, Step: 41266, GPU: 4.9GB\n",
      "Epoch 1, Batch 41270, Loss: 2.6896, Time: 24558.3s, Step: 41271, GPU: 4.9GB\n",
      "Epoch 1, Batch 41275, Loss: 2.8579, Time: 24561.7s, Step: 41276, GPU: 4.9GB\n",
      "Epoch 1, Batch 41280, Loss: 3.7354, Time: 24564.2s, Step: 41281, GPU: 4.9GB\n",
      "Epoch 1, Batch 41285, Loss: 3.6051, Time: 24567.7s, Step: 41286, GPU: 4.9GB\n",
      "Epoch 1, Batch 41290, Loss: 3.8275, Time: 24570.2s, Step: 41291, GPU: 4.9GB\n",
      "Epoch 1, Batch 41295, Loss: 3.0081, Time: 24573.6s, Step: 41296, GPU: 4.9GB\n",
      "Epoch 1, Batch 41300, Loss: 3.5298, Time: 24576.1s, Step: 41301, GPU: 4.9GB\n",
      "Epoch 1, Batch 41305, Loss: 3.9659, Time: 24579.5s, Step: 41306, GPU: 4.9GB\n",
      "Epoch 1, Batch 41310, Loss: 3.3794, Time: 24582.1s, Step: 41311, GPU: 4.9GB\n",
      "Epoch 1, Batch 41315, Loss: 3.2585, Time: 24585.5s, Step: 41316, GPU: 4.9GB\n",
      "Epoch 1, Batch 41320, Loss: 3.3919, Time: 24588.0s, Step: 41321, GPU: 4.9GB\n",
      "Epoch 1, Batch 41325, Loss: 3.8721, Time: 24591.5s, Step: 41326, GPU: 4.9GB\n",
      "Epoch 1, Batch 41330, Loss: 2.7286, Time: 24594.0s, Step: 41331, GPU: 4.9GB\n",
      "Epoch 1, Batch 41335, Loss: 3.9656, Time: 24597.4s, Step: 41336, GPU: 4.9GB\n",
      "Epoch 1, Batch 41340, Loss: 2.8061, Time: 24599.9s, Step: 41341, GPU: 4.9GB\n",
      "Epoch 1, Batch 41345, Loss: 2.6882, Time: 24603.4s, Step: 41346, GPU: 4.9GB\n",
      "Epoch 1, Batch 41350, Loss: 3.2531, Time: 24605.9s, Step: 41351, GPU: 4.9GB\n",
      "Epoch 1, Batch 41355, Loss: 3.3416, Time: 24609.3s, Step: 41356, GPU: 4.9GB\n",
      "Epoch 1, Batch 41360, Loss: 2.7613, Time: 24611.8s, Step: 41361, GPU: 4.9GB\n",
      "Epoch 1, Batch 41365, Loss: 2.7429, Time: 24615.3s, Step: 41366, GPU: 4.9GB\n",
      "Epoch 1, Batch 41370, Loss: 3.6876, Time: 24617.8s, Step: 41371, GPU: 4.9GB\n",
      "Epoch 1, Batch 41375, Loss: 3.3257, Time: 24621.2s, Step: 41376, GPU: 4.9GB\n",
      "Epoch 1, Batch 41380, Loss: 3.3416, Time: 24623.8s, Step: 41381, GPU: 4.9GB\n",
      "Epoch 1, Batch 41385, Loss: 4.1155, Time: 24627.2s, Step: 41386, GPU: 4.9GB\n",
      "Epoch 1, Batch 41390, Loss: 2.8415, Time: 24629.7s, Step: 41391, GPU: 4.9GB\n",
      "Epoch 1, Batch 41395, Loss: 2.8838, Time: 24633.1s, Step: 41396, GPU: 4.9GB\n",
      "Epoch 1, Batch 41400, Loss: 3.0940, Time: 24635.7s, Step: 41401, GPU: 4.9GB\n",
      "Epoch 1, Batch 41405, Loss: 3.1806, Time: 24639.1s, Step: 41406, GPU: 4.9GB\n",
      "Epoch 1, Batch 41410, Loss: 3.6707, Time: 24641.7s, Step: 41411, GPU: 4.9GB\n",
      "Epoch 1, Batch 41415, Loss: 3.3260, Time: 24645.1s, Step: 41416, GPU: 4.9GB\n",
      "Epoch 1, Batch 41420, Loss: 3.5403, Time: 24647.6s, Step: 41421, GPU: 4.9GB\n",
      "Epoch 1, Batch 41425, Loss: 2.9354, Time: 24651.0s, Step: 41426, GPU: 4.9GB\n",
      "Epoch 1, Batch 41430, Loss: 2.6511, Time: 24653.5s, Step: 41431, GPU: 4.9GB\n",
      "Epoch 1, Batch 41435, Loss: 3.3729, Time: 24656.9s, Step: 41436, GPU: 4.9GB\n",
      "Epoch 1, Batch 41440, Loss: 3.2005, Time: 24659.5s, Step: 41441, GPU: 4.9GB\n",
      "Epoch 1, Batch 41445, Loss: 3.4999, Time: 24662.9s, Step: 41446, GPU: 4.9GB\n",
      "Epoch 1, Batch 41450, Loss: 3.9082, Time: 24665.4s, Step: 41451, GPU: 4.9GB\n",
      "Epoch 1, Batch 41455, Loss: 3.3763, Time: 24668.8s, Step: 41456, GPU: 4.9GB\n",
      "Epoch 1, Batch 41460, Loss: 3.2193, Time: 24671.3s, Step: 41461, GPU: 4.9GB\n",
      "Epoch 1, Batch 41465, Loss: 3.0279, Time: 24674.7s, Step: 41466, GPU: 4.9GB\n",
      "Epoch 1, Batch 41470, Loss: 2.9827, Time: 24677.3s, Step: 41471, GPU: 4.9GB\n",
      "Epoch 1, Batch 41475, Loss: 3.1457, Time: 24680.6s, Step: 41476, GPU: 4.9GB\n",
      "Epoch 1, Batch 41480, Loss: 3.4788, Time: 24683.2s, Step: 41481, GPU: 4.9GB\n",
      "Epoch 1, Batch 41485, Loss: 3.8627, Time: 24686.5s, Step: 41486, GPU: 4.9GB\n",
      "Epoch 1, Batch 41490, Loss: 3.1019, Time: 24689.0s, Step: 41491, GPU: 4.9GB\n",
      "Epoch 1, Batch 41495, Loss: 3.5001, Time: 24692.5s, Step: 41496, GPU: 4.9GB\n",
      "Epoch 1, Batch 41500, Loss: 3.6809, Time: 24695.0s, Step: 41501, GPU: 4.9GB\n",
      "Epoch 1, Batch 41505, Loss: 3.2216, Time: 24698.4s, Step: 41506, GPU: 4.9GB\n",
      "Epoch 1, Batch 41510, Loss: 4.0939, Time: 24700.9s, Step: 41511, GPU: 4.9GB\n",
      "Epoch 1, Batch 41515, Loss: 2.8186, Time: 24704.4s, Step: 41516, GPU: 4.9GB\n",
      "Epoch 1, Batch 41520, Loss: 3.6766, Time: 24706.9s, Step: 41521, GPU: 4.9GB\n",
      "Epoch 1, Batch 41525, Loss: 2.9769, Time: 24710.4s, Step: 41526, GPU: 4.9GB\n",
      "Epoch 1, Batch 41530, Loss: 3.2774, Time: 24712.9s, Step: 41531, GPU: 4.9GB\n",
      "Epoch 1, Batch 41535, Loss: 3.2583, Time: 24716.3s, Step: 41536, GPU: 4.9GB\n",
      "Epoch 1, Batch 41540, Loss: 2.5068, Time: 24718.8s, Step: 41541, GPU: 4.9GB\n",
      "Epoch 1, Batch 41545, Loss: 3.0422, Time: 24722.3s, Step: 41546, GPU: 4.9GB\n",
      "Epoch 1, Batch 41550, Loss: 3.1329, Time: 24724.8s, Step: 41551, GPU: 4.9GB\n",
      "Epoch 1, Batch 41555, Loss: 3.1842, Time: 24728.2s, Step: 41556, GPU: 4.9GB\n",
      "Epoch 1, Batch 41560, Loss: 2.9127, Time: 24730.7s, Step: 41561, GPU: 4.9GB\n",
      "Epoch 1, Batch 41565, Loss: 3.2717, Time: 24734.2s, Step: 41566, GPU: 4.9GB\n",
      "Epoch 1, Batch 41570, Loss: 3.1234, Time: 24736.7s, Step: 41571, GPU: 4.9GB\n",
      "Epoch 1, Batch 41575, Loss: 3.6482, Time: 24740.2s, Step: 41576, GPU: 4.9GB\n",
      "Epoch 1, Batch 41580, Loss: 3.5227, Time: 24742.7s, Step: 41581, GPU: 4.9GB\n",
      "Epoch 1, Batch 41585, Loss: 3.6189, Time: 24746.1s, Step: 41586, GPU: 4.9GB\n",
      "Epoch 1, Batch 41590, Loss: 3.0296, Time: 24748.7s, Step: 41591, GPU: 4.9GB\n",
      "Epoch 1, Batch 41595, Loss: 2.6171, Time: 24752.1s, Step: 41596, GPU: 4.9GB\n",
      "Epoch 1, Batch 41600, Loss: 2.5620, Time: 24754.7s, Step: 41601, GPU: 4.9GB\n",
      "Epoch 1, Batch 41605, Loss: 3.8480, Time: 24758.2s, Step: 41606, GPU: 4.9GB\n",
      "Epoch 1, Batch 41610, Loss: 3.2364, Time: 24760.7s, Step: 41611, GPU: 4.9GB\n",
      "Epoch 1, Batch 41615, Loss: 3.9426, Time: 24764.1s, Step: 41616, GPU: 4.9GB\n",
      "Epoch 1, Batch 41620, Loss: 3.8539, Time: 24766.7s, Step: 41621, GPU: 4.9GB\n",
      "Epoch 1, Batch 41625, Loss: 3.6778, Time: 24770.1s, Step: 41626, GPU: 4.9GB\n",
      "Epoch 1, Batch 41630, Loss: 2.5701, Time: 24772.6s, Step: 41631, GPU: 4.9GB\n",
      "Epoch 1, Batch 41635, Loss: 3.2171, Time: 24776.0s, Step: 41636, GPU: 4.9GB\n",
      "Epoch 1, Batch 41640, Loss: 3.2674, Time: 24778.6s, Step: 41641, GPU: 4.9GB\n",
      "Epoch 1, Batch 41645, Loss: 3.1165, Time: 24782.0s, Step: 41646, GPU: 4.9GB\n",
      "Epoch 1, Batch 41650, Loss: 3.1514, Time: 24784.5s, Step: 41651, GPU: 4.9GB\n",
      "Epoch 1, Batch 41655, Loss: 3.2252, Time: 24787.9s, Step: 41656, GPU: 4.9GB\n",
      "Epoch 1, Batch 41660, Loss: 3.0885, Time: 24790.4s, Step: 41661, GPU: 4.9GB\n",
      "Epoch 1, Batch 41665, Loss: 3.0365, Time: 24793.8s, Step: 41666, GPU: 4.9GB\n",
      "Epoch 1, Batch 41670, Loss: 3.6793, Time: 24796.3s, Step: 41671, GPU: 4.9GB\n",
      "Epoch 1, Batch 41675, Loss: 3.7881, Time: 24799.7s, Step: 41676, GPU: 4.9GB\n",
      "Epoch 1, Batch 41680, Loss: 3.6093, Time: 24802.2s, Step: 41681, GPU: 4.9GB\n",
      "Epoch 1, Batch 41685, Loss: 3.4422, Time: 24805.7s, Step: 41686, GPU: 4.9GB\n",
      "Epoch 1, Batch 41690, Loss: 3.2928, Time: 24808.2s, Step: 41691, GPU: 4.9GB\n",
      "Epoch 1, Batch 41695, Loss: 2.7213, Time: 24811.6s, Step: 41696, GPU: 4.9GB\n",
      "Epoch 1, Batch 41700, Loss: 3.4479, Time: 24814.1s, Step: 41701, GPU: 4.9GB\n",
      "Epoch 1, Batch 41705, Loss: 3.5262, Time: 24817.6s, Step: 41706, GPU: 4.9GB\n",
      "Epoch 1, Batch 41710, Loss: 3.4880, Time: 24820.1s, Step: 41711, GPU: 4.9GB\n",
      "Epoch 1, Batch 41715, Loss: 3.5149, Time: 24823.6s, Step: 41716, GPU: 4.9GB\n",
      "Epoch 1, Batch 41720, Loss: 3.5701, Time: 24826.1s, Step: 41721, GPU: 4.9GB\n",
      "Epoch 1, Batch 41725, Loss: 3.7073, Time: 24829.5s, Step: 41726, GPU: 4.9GB\n",
      "Epoch 1, Batch 41730, Loss: 3.4409, Time: 24832.0s, Step: 41731, GPU: 4.9GB\n",
      "Epoch 1, Batch 41735, Loss: 3.3886, Time: 24835.4s, Step: 41736, GPU: 4.9GB\n",
      "Epoch 1, Batch 41740, Loss: 2.8079, Time: 24838.0s, Step: 41741, GPU: 4.9GB\n",
      "Epoch 1, Batch 41745, Loss: 3.4119, Time: 24841.4s, Step: 41746, GPU: 4.9GB\n",
      "Epoch 1, Batch 41750, Loss: 3.2279, Time: 24843.9s, Step: 41751, GPU: 4.9GB\n",
      "Epoch 1, Batch 41755, Loss: 2.6771, Time: 24847.4s, Step: 41756, GPU: 4.9GB\n",
      "Epoch 1, Batch 41760, Loss: 3.3242, Time: 24849.9s, Step: 41761, GPU: 4.9GB\n",
      "Epoch 1, Batch 41765, Loss: 3.4854, Time: 24853.3s, Step: 41766, GPU: 4.9GB\n",
      "Epoch 1, Batch 41770, Loss: 3.8179, Time: 24855.8s, Step: 41771, GPU: 4.9GB\n",
      "Epoch 1, Batch 41775, Loss: 2.9348, Time: 24859.3s, Step: 41776, GPU: 4.9GB\n",
      "Epoch 1, Batch 41780, Loss: 3.5692, Time: 24861.8s, Step: 41781, GPU: 4.9GB\n",
      "Epoch 1, Batch 41785, Loss: 3.2451, Time: 24865.2s, Step: 41786, GPU: 4.9GB\n",
      "Epoch 1, Batch 41790, Loss: 3.7833, Time: 24867.7s, Step: 41791, GPU: 4.9GB\n",
      "Epoch 1, Batch 41795, Loss: 3.0298, Time: 24871.1s, Step: 41796, GPU: 4.9GB\n",
      "Epoch 1, Batch 41800, Loss: 3.4134, Time: 24873.7s, Step: 41801, GPU: 4.9GB\n",
      "Epoch 1, Batch 41805, Loss: 3.1694, Time: 24877.2s, Step: 41806, GPU: 4.9GB\n",
      "Epoch 1, Batch 41810, Loss: 3.8833, Time: 24879.7s, Step: 41811, GPU: 4.9GB\n",
      "Epoch 1, Batch 41815, Loss: 3.4436, Time: 24883.1s, Step: 41816, GPU: 4.9GB\n",
      "Epoch 1, Batch 41820, Loss: 2.5196, Time: 24885.7s, Step: 41821, GPU: 4.9GB\n",
      "Epoch 1, Batch 41825, Loss: 3.7866, Time: 24889.1s, Step: 41826, GPU: 4.9GB\n",
      "Epoch 1, Batch 41830, Loss: 2.9410, Time: 24891.6s, Step: 41831, GPU: 4.9GB\n",
      "Epoch 1, Batch 41835, Loss: 3.8682, Time: 24895.1s, Step: 41836, GPU: 4.9GB\n",
      "Epoch 1, Batch 41840, Loss: 2.9422, Time: 24897.6s, Step: 41841, GPU: 4.9GB\n",
      "Epoch 1, Batch 41845, Loss: 3.0986, Time: 24901.0s, Step: 41846, GPU: 4.9GB\n",
      "Epoch 1, Batch 41850, Loss: 3.3482, Time: 24903.5s, Step: 41851, GPU: 4.9GB\n",
      "Epoch 1, Batch 41855, Loss: 4.1476, Time: 24906.9s, Step: 41856, GPU: 4.9GB\n",
      "Epoch 1, Batch 41860, Loss: 2.6387, Time: 24909.5s, Step: 41861, GPU: 4.9GB\n",
      "Epoch 1, Batch 41865, Loss: 2.4898, Time: 24912.9s, Step: 41866, GPU: 4.9GB\n",
      "Epoch 1, Batch 41870, Loss: 3.3414, Time: 24915.4s, Step: 41871, GPU: 4.9GB\n",
      "Epoch 1, Batch 41875, Loss: 3.3759, Time: 24918.8s, Step: 41876, GPU: 4.9GB\n",
      "Epoch 1, Batch 41880, Loss: 3.8403, Time: 24921.3s, Step: 41881, GPU: 4.9GB\n",
      "Epoch 1, Batch 41885, Loss: 3.3750, Time: 24924.8s, Step: 41886, GPU: 4.9GB\n",
      "Epoch 1, Batch 41890, Loss: 3.5805, Time: 24927.3s, Step: 41891, GPU: 4.9GB\n",
      "Epoch 1, Batch 41895, Loss: 3.2846, Time: 24930.7s, Step: 41896, GPU: 4.9GB\n",
      "Epoch 1, Batch 41900, Loss: 2.8917, Time: 24933.2s, Step: 41901, GPU: 4.9GB\n",
      "Epoch 1, Batch 41905, Loss: 3.8393, Time: 24936.6s, Step: 41906, GPU: 4.9GB\n",
      "Epoch 1, Batch 41910, Loss: 2.9495, Time: 24939.1s, Step: 41911, GPU: 4.9GB\n",
      "Epoch 1, Batch 41915, Loss: 3.5216, Time: 24942.6s, Step: 41916, GPU: 4.9GB\n",
      "Epoch 1, Batch 41920, Loss: 3.2297, Time: 24945.1s, Step: 41921, GPU: 4.9GB\n",
      "Epoch 1, Batch 41925, Loss: 3.5405, Time: 24948.5s, Step: 41926, GPU: 4.9GB\n",
      "Epoch 1, Batch 41930, Loss: 4.2292, Time: 24951.0s, Step: 41931, GPU: 4.9GB\n",
      "Epoch 1, Batch 41935, Loss: 3.3376, Time: 24954.4s, Step: 41936, GPU: 4.9GB\n",
      "Epoch 1, Batch 41940, Loss: 3.6709, Time: 24956.9s, Step: 41941, GPU: 4.9GB\n",
      "Epoch 1, Batch 41945, Loss: 4.1161, Time: 24960.4s, Step: 41946, GPU: 4.9GB\n",
      "Epoch 1, Batch 41950, Loss: 3.4183, Time: 24962.9s, Step: 41951, GPU: 4.9GB\n",
      "Epoch 1, Batch 41955, Loss: 2.2603, Time: 24966.3s, Step: 41956, GPU: 4.9GB\n",
      "Epoch 1, Batch 41960, Loss: 2.6422, Time: 24968.8s, Step: 41961, GPU: 4.9GB\n",
      "Epoch 1, Batch 41965, Loss: 2.6289, Time: 24972.3s, Step: 41966, GPU: 4.9GB\n",
      "Epoch 1, Batch 41970, Loss: 2.5643, Time: 24974.8s, Step: 41971, GPU: 4.9GB\n",
      "Epoch 1, Batch 41975, Loss: 3.5073, Time: 24978.3s, Step: 41976, GPU: 4.9GB\n",
      "Epoch 1, Batch 41980, Loss: 3.2642, Time: 24980.8s, Step: 41981, GPU: 4.9GB\n",
      "Epoch 1, Batch 41985, Loss: 4.0854, Time: 24985.4s, Step: 41986, GPU: 4.9GB\n",
      "Epoch 1, Batch 41990, Loss: 3.4733, Time: 24988.0s, Step: 41991, GPU: 4.9GB\n",
      "Epoch 1, Batch 41995, Loss: 2.7547, Time: 24991.4s, Step: 41996, GPU: 4.9GB\n",
      "Epoch 1, Batch 42000, Loss: 2.9754, Time: 24994.0s, Step: 42001, GPU: 4.9GB\n",
      "Epoch 1, Batch 42005, Loss: 3.0321, Time: 24997.4s, Step: 42006, GPU: 4.9GB\n",
      "Epoch 1, Batch 42010, Loss: 3.0192, Time: 24999.9s, Step: 42011, GPU: 4.9GB\n",
      "Epoch 1, Batch 42015, Loss: 3.7019, Time: 25003.4s, Step: 42016, GPU: 4.9GB\n",
      "Epoch 1, Batch 42020, Loss: 2.8517, Time: 25005.9s, Step: 42021, GPU: 4.9GB\n",
      "Epoch 1, Batch 42025, Loss: 3.6033, Time: 25009.3s, Step: 42026, GPU: 4.9GB\n",
      "Epoch 1, Batch 42030, Loss: 3.7660, Time: 25011.8s, Step: 42031, GPU: 4.9GB\n",
      "Epoch 1, Batch 42035, Loss: 3.3847, Time: 25015.2s, Step: 42036, GPU: 4.9GB\n",
      "Epoch 1, Batch 42040, Loss: 3.5463, Time: 25017.8s, Step: 42041, GPU: 4.9GB\n",
      "Epoch 1, Batch 42045, Loss: 2.3430, Time: 25021.2s, Step: 42046, GPU: 4.9GB\n",
      "Epoch 1, Batch 42050, Loss: 3.7047, Time: 25023.7s, Step: 42051, GPU: 4.9GB\n",
      "Epoch 1, Batch 42055, Loss: 3.0174, Time: 25027.2s, Step: 42056, GPU: 4.9GB\n",
      "Epoch 1, Batch 42060, Loss: 2.9397, Time: 25029.7s, Step: 42061, GPU: 4.9GB\n",
      "Epoch 1, Batch 42065, Loss: 3.1167, Time: 25033.1s, Step: 42066, GPU: 4.9GB\n",
      "Epoch 1, Batch 42070, Loss: 3.5656, Time: 25035.7s, Step: 42071, GPU: 4.9GB\n",
      "Epoch 1, Batch 42075, Loss: 3.5532, Time: 25039.1s, Step: 42076, GPU: 4.9GB\n",
      "Epoch 1, Batch 42080, Loss: 4.1384, Time: 25041.6s, Step: 42081, GPU: 4.9GB\n",
      "Epoch 1, Batch 42085, Loss: 2.8394, Time: 25045.0s, Step: 42086, GPU: 4.9GB\n",
      "Epoch 1, Batch 42090, Loss: 3.0161, Time: 25047.6s, Step: 42091, GPU: 4.9GB\n",
      "Epoch 1, Batch 42095, Loss: 3.0288, Time: 25051.0s, Step: 42096, GPU: 4.9GB\n",
      "Epoch 1, Batch 42100, Loss: 3.5932, Time: 25053.5s, Step: 42101, GPU: 4.9GB\n",
      "Epoch 1, Batch 42105, Loss: 3.1384, Time: 25056.9s, Step: 42106, GPU: 4.9GB\n",
      "Epoch 1, Batch 42110, Loss: 2.6945, Time: 25059.4s, Step: 42111, GPU: 4.9GB\n",
      "Epoch 1, Batch 42115, Loss: 3.4649, Time: 25062.9s, Step: 42116, GPU: 4.9GB\n",
      "Epoch 1, Batch 42120, Loss: 3.6601, Time: 25065.4s, Step: 42121, GPU: 4.9GB\n",
      "Epoch 1, Batch 42125, Loss: 2.2196, Time: 25068.8s, Step: 42126, GPU: 4.9GB\n",
      "Epoch 1, Batch 42130, Loss: 3.1647, Time: 25071.3s, Step: 42131, GPU: 4.9GB\n",
      "Epoch 1, Batch 42135, Loss: 3.1749, Time: 25074.7s, Step: 42136, GPU: 4.9GB\n",
      "Epoch 1, Batch 42140, Loss: 2.6557, Time: 25077.2s, Step: 42141, GPU: 4.9GB\n",
      "Epoch 1, Batch 42145, Loss: 1.9465, Time: 25080.6s, Step: 42146, GPU: 4.9GB\n",
      "Epoch 1, Batch 42150, Loss: 3.2392, Time: 25083.2s, Step: 42151, GPU: 4.9GB\n",
      "Epoch 1, Batch 42155, Loss: 3.8968, Time: 25086.6s, Step: 42156, GPU: 4.9GB\n",
      "Epoch 1, Batch 42160, Loss: 2.7072, Time: 25089.1s, Step: 42161, GPU: 4.9GB\n",
      "Epoch 1, Batch 42165, Loss: 3.4724, Time: 25092.5s, Step: 42166, GPU: 4.9GB\n",
      "Epoch 1, Batch 42170, Loss: 3.3244, Time: 25095.0s, Step: 42171, GPU: 4.9GB\n",
      "Epoch 1, Batch 42175, Loss: 2.8887, Time: 25098.5s, Step: 42176, GPU: 4.9GB\n",
      "Epoch 1, Batch 42180, Loss: 2.8042, Time: 25101.0s, Step: 42181, GPU: 4.9GB\n",
      "Epoch 1, Batch 42185, Loss: 3.1094, Time: 25104.4s, Step: 42186, GPU: 4.9GB\n",
      "Epoch 1, Batch 42190, Loss: 3.1128, Time: 25106.9s, Step: 42191, GPU: 4.9GB\n",
      "Epoch 1, Batch 42195, Loss: 2.7942, Time: 25110.3s, Step: 42196, GPU: 4.9GB\n",
      "Epoch 1, Batch 42200, Loss: 3.2954, Time: 25112.9s, Step: 42201, GPU: 4.9GB\n",
      "Epoch 1, Batch 42205, Loss: 2.8497, Time: 25116.3s, Step: 42206, GPU: 4.9GB\n",
      "Epoch 1, Batch 42210, Loss: 2.9788, Time: 25118.8s, Step: 42211, GPU: 4.9GB\n",
      "Epoch 1, Batch 42215, Loss: 2.9785, Time: 25122.3s, Step: 42216, GPU: 4.9GB\n",
      "Epoch 1, Batch 42220, Loss: 3.3049, Time: 25124.8s, Step: 42221, GPU: 4.9GB\n",
      "Epoch 1, Batch 42225, Loss: 3.0901, Time: 25128.3s, Step: 42226, GPU: 4.9GB\n",
      "Epoch 1, Batch 42230, Loss: 3.0327, Time: 25130.8s, Step: 42231, GPU: 4.9GB\n",
      "Epoch 1, Batch 42235, Loss: 2.9682, Time: 25134.2s, Step: 42236, GPU: 4.9GB\n",
      "Epoch 1, Batch 42240, Loss: 3.2517, Time: 25136.7s, Step: 42241, GPU: 4.9GB\n",
      "Epoch 1, Batch 42245, Loss: 4.3635, Time: 25140.1s, Step: 42246, GPU: 4.9GB\n",
      "Epoch 1, Batch 42250, Loss: 3.0471, Time: 25142.7s, Step: 42251, GPU: 4.9GB\n",
      "Epoch 1, Batch 42255, Loss: 2.7332, Time: 25146.1s, Step: 42256, GPU: 4.9GB\n",
      "Epoch 1, Batch 42260, Loss: 3.4540, Time: 25148.6s, Step: 42261, GPU: 4.9GB\n",
      "Epoch 1, Batch 42265, Loss: 2.8342, Time: 25152.0s, Step: 42266, GPU: 4.9GB\n",
      "Epoch 1, Batch 42270, Loss: 3.9470, Time: 25154.5s, Step: 42271, GPU: 4.9GB\n",
      "Epoch 1, Batch 42275, Loss: 2.6708, Time: 25158.0s, Step: 42276, GPU: 4.9GB\n",
      "Epoch 1, Batch 42280, Loss: 3.0212, Time: 25160.6s, Step: 42281, GPU: 4.9GB\n",
      "Epoch 1, Batch 42285, Loss: 3.4055, Time: 25164.0s, Step: 42286, GPU: 4.9GB\n",
      "Epoch 1, Batch 42290, Loss: 3.2026, Time: 25166.5s, Step: 42291, GPU: 4.9GB\n",
      "Epoch 1, Batch 42295, Loss: 3.3346, Time: 25169.9s, Step: 42296, GPU: 4.9GB\n",
      "Epoch 1, Batch 42300, Loss: 3.8664, Time: 25172.4s, Step: 42301, GPU: 4.9GB\n",
      "Epoch 1, Batch 42305, Loss: 4.2423, Time: 25175.9s, Step: 42306, GPU: 4.9GB\n",
      "Epoch 1, Batch 42310, Loss: 2.8287, Time: 25178.4s, Step: 42311, GPU: 4.9GB\n",
      "Epoch 1, Batch 42315, Loss: 1.9593, Time: 25181.8s, Step: 42316, GPU: 4.9GB\n",
      "Epoch 1, Batch 42320, Loss: 3.2753, Time: 25184.3s, Step: 42321, GPU: 4.9GB\n",
      "Epoch 1, Batch 42325, Loss: 2.7856, Time: 25187.7s, Step: 42326, GPU: 4.9GB\n",
      "Epoch 1, Batch 42330, Loss: 3.4982, Time: 25190.3s, Step: 42331, GPU: 4.9GB\n",
      "Epoch 1, Batch 42335, Loss: 4.1107, Time: 25193.7s, Step: 42336, GPU: 4.9GB\n",
      "Epoch 1, Batch 42340, Loss: 3.1829, Time: 25196.2s, Step: 42341, GPU: 4.9GB\n",
      "Epoch 1, Batch 42345, Loss: 3.9360, Time: 25199.6s, Step: 42346, GPU: 4.9GB\n",
      "Epoch 1, Batch 42350, Loss: 3.4344, Time: 25202.1s, Step: 42351, GPU: 4.9GB\n",
      "Epoch 1, Batch 42355, Loss: 2.0788, Time: 25205.6s, Step: 42356, GPU: 4.9GB\n",
      "Epoch 1, Batch 42360, Loss: 3.2325, Time: 25208.1s, Step: 42361, GPU: 4.9GB\n",
      "Epoch 1, Batch 42365, Loss: 2.8527, Time: 25211.6s, Step: 42366, GPU: 4.9GB\n",
      "Epoch 1, Batch 42370, Loss: 3.3675, Time: 25214.1s, Step: 42371, GPU: 4.9GB\n",
      "Epoch 1, Batch 42375, Loss: 3.2009, Time: 25217.5s, Step: 42376, GPU: 4.9GB\n",
      "Epoch 1, Batch 42380, Loss: 3.5984, Time: 25220.1s, Step: 42381, GPU: 4.9GB\n",
      "Epoch 1, Batch 42385, Loss: 3.2402, Time: 25223.5s, Step: 42386, GPU: 4.9GB\n",
      "Epoch 1, Batch 42390, Loss: 3.3634, Time: 25226.0s, Step: 42391, GPU: 4.9GB\n",
      "Epoch 1, Batch 42395, Loss: 3.2614, Time: 25229.4s, Step: 42396, GPU: 4.9GB\n",
      "Epoch 1, Batch 42400, Loss: 3.3004, Time: 25232.0s, Step: 42401, GPU: 4.9GB\n",
      "Epoch 1, Batch 42405, Loss: 3.3989, Time: 25235.4s, Step: 42406, GPU: 4.9GB\n",
      "Epoch 1, Batch 42410, Loss: 3.4727, Time: 25238.0s, Step: 42411, GPU: 4.9GB\n",
      "Epoch 1, Batch 42415, Loss: 3.0783, Time: 25241.4s, Step: 42416, GPU: 4.9GB\n",
      "Epoch 1, Batch 42420, Loss: 3.4269, Time: 25243.9s, Step: 42421, GPU: 4.9GB\n",
      "Epoch 1, Batch 42425, Loss: 3.6068, Time: 25247.4s, Step: 42426, GPU: 4.9GB\n",
      "Epoch 1, Batch 42430, Loss: 3.5101, Time: 25249.9s, Step: 42431, GPU: 4.9GB\n",
      "Epoch 1, Batch 42435, Loss: 3.2640, Time: 25253.3s, Step: 42436, GPU: 4.9GB\n",
      "Epoch 1, Batch 42440, Loss: 3.0670, Time: 25255.8s, Step: 42441, GPU: 4.9GB\n",
      "Epoch 1, Batch 42445, Loss: 3.4012, Time: 25259.2s, Step: 42446, GPU: 4.9GB\n",
      "Epoch 1, Batch 42450, Loss: 3.4784, Time: 25261.7s, Step: 42451, GPU: 4.9GB\n",
      "Epoch 1, Batch 42455, Loss: 2.9564, Time: 25265.1s, Step: 42456, GPU: 4.9GB\n",
      "Epoch 1, Batch 42460, Loss: 3.7467, Time: 25267.6s, Step: 42461, GPU: 4.9GB\n",
      "Epoch 1, Batch 42465, Loss: 3.6098, Time: 25271.1s, Step: 42466, GPU: 4.9GB\n",
      "Epoch 1, Batch 42470, Loss: 3.0364, Time: 25273.6s, Step: 42471, GPU: 4.9GB\n",
      "Epoch 1, Batch 42475, Loss: 2.7348, Time: 25277.0s, Step: 42476, GPU: 4.9GB\n",
      "Epoch 1, Batch 42480, Loss: 2.4188, Time: 25279.6s, Step: 42481, GPU: 4.9GB\n",
      "Epoch 1, Batch 42485, Loss: 4.3100, Time: 25283.0s, Step: 42486, GPU: 4.9GB\n",
      "Epoch 1, Batch 42490, Loss: 3.2720, Time: 25285.5s, Step: 42491, GPU: 4.9GB\n",
      "Epoch 1, Batch 42495, Loss: 4.7502, Time: 25289.0s, Step: 42496, GPU: 4.9GB\n",
      "Epoch 1, Batch 42500, Loss: 3.3513, Time: 25291.5s, Step: 42501, GPU: 4.9GB\n",
      "Epoch 1, Batch 42505, Loss: 3.1091, Time: 25294.9s, Step: 42506, GPU: 4.9GB\n",
      "Epoch 1, Batch 42510, Loss: 3.5547, Time: 25297.4s, Step: 42511, GPU: 4.9GB\n",
      "Epoch 1, Batch 42515, Loss: 3.0632, Time: 25300.9s, Step: 42516, GPU: 4.9GB\n",
      "Epoch 1, Batch 42520, Loss: 2.5226, Time: 25303.4s, Step: 42521, GPU: 4.9GB\n",
      "Epoch 1, Batch 42525, Loss: 3.4123, Time: 25306.8s, Step: 42526, GPU: 4.9GB\n",
      "Epoch 1, Batch 42530, Loss: 2.4533, Time: 25309.3s, Step: 42531, GPU: 4.9GB\n",
      "Epoch 1, Batch 42535, Loss: 2.6703, Time: 25312.7s, Step: 42536, GPU: 4.9GB\n",
      "Epoch 1, Batch 42540, Loss: 3.4620, Time: 25315.2s, Step: 42541, GPU: 4.9GB\n",
      "Epoch 1, Batch 42545, Loss: 2.9679, Time: 25318.6s, Step: 42546, GPU: 4.9GB\n",
      "Epoch 1, Batch 42550, Loss: 3.7960, Time: 25321.2s, Step: 42551, GPU: 4.9GB\n",
      "Epoch 1, Batch 42555, Loss: 4.4194, Time: 25324.6s, Step: 42556, GPU: 4.9GB\n",
      "Epoch 1, Batch 42560, Loss: 2.5913, Time: 25327.1s, Step: 42561, GPU: 4.9GB\n",
      "Epoch 1, Batch 42565, Loss: 3.2881, Time: 25330.5s, Step: 42566, GPU: 4.9GB\n",
      "Epoch 1, Batch 42570, Loss: 2.8466, Time: 25333.0s, Step: 42571, GPU: 4.9GB\n",
      "Epoch 1, Batch 42575, Loss: 4.3064, Time: 25336.5s, Step: 42576, GPU: 4.9GB\n",
      "Epoch 1, Batch 42580, Loss: 3.5254, Time: 25339.0s, Step: 42581, GPU: 4.9GB\n",
      "Epoch 1, Batch 42585, Loss: 3.1200, Time: 25342.4s, Step: 42586, GPU: 4.9GB\n",
      "Epoch 1, Batch 42590, Loss: 3.2611, Time: 25344.9s, Step: 42591, GPU: 4.9GB\n",
      "Epoch 1, Batch 42595, Loss: 3.3355, Time: 25348.4s, Step: 42596, GPU: 4.9GB\n",
      "Epoch 1, Batch 42600, Loss: 3.6724, Time: 25351.0s, Step: 42601, GPU: 4.9GB\n",
      "Epoch 1, Batch 42605, Loss: 2.5220, Time: 25354.4s, Step: 42606, GPU: 4.9GB\n",
      "Epoch 1, Batch 42610, Loss: 3.5729, Time: 25357.0s, Step: 42611, GPU: 4.9GB\n",
      "Epoch 1, Batch 42615, Loss: 2.9823, Time: 25360.4s, Step: 42616, GPU: 4.9GB\n",
      "Epoch 1, Batch 42620, Loss: 3.5133, Time: 25362.9s, Step: 42621, GPU: 4.9GB\n",
      "Epoch 1, Batch 42625, Loss: 3.2204, Time: 25366.4s, Step: 42626, GPU: 4.9GB\n",
      "Epoch 1, Batch 42630, Loss: 3.3337, Time: 25368.9s, Step: 42631, GPU: 4.9GB\n",
      "Epoch 1, Batch 42635, Loss: 3.2368, Time: 25372.4s, Step: 42636, GPU: 4.9GB\n",
      "Epoch 1, Batch 42640, Loss: 2.7751, Time: 25374.9s, Step: 42641, GPU: 4.9GB\n",
      "Epoch 1, Batch 42645, Loss: 2.7862, Time: 25378.3s, Step: 42646, GPU: 4.9GB\n",
      "Epoch 1, Batch 42650, Loss: 2.5952, Time: 25380.8s, Step: 42651, GPU: 4.9GB\n",
      "Epoch 1, Batch 42655, Loss: 3.7258, Time: 25384.3s, Step: 42656, GPU: 4.9GB\n",
      "Epoch 1, Batch 42660, Loss: 3.6525, Time: 25386.8s, Step: 42661, GPU: 4.9GB\n",
      "Epoch 1, Batch 42665, Loss: 3.2005, Time: 25390.2s, Step: 42666, GPU: 4.9GB\n",
      "Epoch 1, Batch 42670, Loss: 3.2541, Time: 25392.7s, Step: 42671, GPU: 4.9GB\n",
      "Epoch 1, Batch 42675, Loss: 3.7366, Time: 25396.2s, Step: 42676, GPU: 4.9GB\n",
      "Epoch 1, Batch 42680, Loss: 3.4280, Time: 25398.7s, Step: 42681, GPU: 4.9GB\n",
      "Epoch 1, Batch 42685, Loss: 3.0923, Time: 25402.1s, Step: 42686, GPU: 4.9GB\n",
      "Epoch 1, Batch 42690, Loss: 3.2878, Time: 25404.6s, Step: 42691, GPU: 4.9GB\n",
      "Epoch 1, Batch 42695, Loss: 3.8891, Time: 25408.1s, Step: 42696, GPU: 4.9GB\n",
      "Epoch 1, Batch 42700, Loss: 2.9401, Time: 25410.6s, Step: 42701, GPU: 4.9GB\n",
      "Epoch 1, Batch 42705, Loss: 3.3285, Time: 25414.0s, Step: 42706, GPU: 4.9GB\n",
      "Epoch 1, Batch 42710, Loss: 3.8920, Time: 25416.6s, Step: 42711, GPU: 4.9GB\n",
      "Epoch 1, Batch 42715, Loss: 3.2826, Time: 25420.0s, Step: 42716, GPU: 4.9GB\n",
      "Epoch 1, Batch 42720, Loss: 2.8898, Time: 25422.6s, Step: 42721, GPU: 4.9GB\n",
      "Epoch 1, Batch 42725, Loss: 3.9018, Time: 25426.0s, Step: 42726, GPU: 4.9GB\n",
      "Epoch 1, Batch 42730, Loss: 3.2663, Time: 25428.6s, Step: 42731, GPU: 4.9GB\n",
      "Epoch 1, Batch 42735, Loss: 2.9336, Time: 25432.0s, Step: 42736, GPU: 4.9GB\n",
      "Epoch 1, Batch 42740, Loss: 3.7647, Time: 25434.5s, Step: 42741, GPU: 4.9GB\n",
      "Epoch 1, Batch 42745, Loss: 2.9420, Time: 25438.0s, Step: 42746, GPU: 4.9GB\n",
      "Epoch 1, Batch 42750, Loss: 3.0011, Time: 25440.5s, Step: 42751, GPU: 4.9GB\n",
      "Epoch 1, Batch 42755, Loss: 3.8203, Time: 25443.9s, Step: 42756, GPU: 4.9GB\n",
      "Epoch 1, Batch 42760, Loss: 3.6245, Time: 25446.4s, Step: 42761, GPU: 4.9GB\n",
      "Epoch 1, Batch 42765, Loss: 3.0526, Time: 25449.8s, Step: 42766, GPU: 4.9GB\n",
      "Epoch 1, Batch 42770, Loss: 4.1028, Time: 25452.4s, Step: 42771, GPU: 4.9GB\n",
      "Epoch 1, Batch 42775, Loss: 3.1387, Time: 25455.8s, Step: 42776, GPU: 4.9GB\n",
      "Epoch 1, Batch 42780, Loss: 3.3798, Time: 25458.3s, Step: 42781, GPU: 4.9GB\n",
      "Epoch 1, Batch 42785, Loss: 3.4938, Time: 25461.7s, Step: 42786, GPU: 4.9GB\n",
      "Epoch 1, Batch 42790, Loss: 3.3027, Time: 25464.2s, Step: 42791, GPU: 4.9GB\n",
      "Epoch 1, Batch 42795, Loss: 3.6666, Time: 25467.7s, Step: 42796, GPU: 4.9GB\n",
      "Epoch 1, Batch 42800, Loss: 3.8694, Time: 25470.3s, Step: 42801, GPU: 4.9GB\n",
      "Epoch 1, Batch 42805, Loss: 2.8213, Time: 25473.7s, Step: 42806, GPU: 4.9GB\n",
      "Epoch 1, Batch 42810, Loss: 3.2608, Time: 25476.2s, Step: 42811, GPU: 4.9GB\n",
      "Epoch 1, Batch 42815, Loss: 3.0083, Time: 25479.6s, Step: 42816, GPU: 4.9GB\n",
      "Epoch 1, Batch 42820, Loss: 3.1459, Time: 25482.1s, Step: 42821, GPU: 4.9GB\n",
      "Epoch 1, Batch 42825, Loss: 3.0250, Time: 25485.5s, Step: 42826, GPU: 4.9GB\n",
      "Epoch 1, Batch 42830, Loss: 2.9481, Time: 25488.1s, Step: 42831, GPU: 4.9GB\n",
      "Epoch 1, Batch 42835, Loss: 2.6682, Time: 25491.5s, Step: 42836, GPU: 4.9GB\n",
      "Epoch 1, Batch 42840, Loss: 3.3242, Time: 25494.0s, Step: 42841, GPU: 4.9GB\n",
      "Epoch 1, Batch 42845, Loss: 2.9183, Time: 25497.5s, Step: 42846, GPU: 4.9GB\n",
      "Epoch 1, Batch 42850, Loss: 2.9780, Time: 25500.0s, Step: 42851, GPU: 4.9GB\n",
      "Epoch 1, Batch 42855, Loss: 3.1221, Time: 25503.4s, Step: 42856, GPU: 4.9GB\n",
      "Epoch 1, Batch 42860, Loss: 3.5986, Time: 25505.9s, Step: 42861, GPU: 4.9GB\n",
      "Epoch 1, Batch 42865, Loss: 3.5107, Time: 25509.4s, Step: 42866, GPU: 4.9GB\n",
      "Epoch 1, Batch 42870, Loss: 2.2964, Time: 25511.9s, Step: 42871, GPU: 4.9GB\n",
      "Epoch 1, Batch 42875, Loss: 3.7052, Time: 25515.3s, Step: 42876, GPU: 4.9GB\n",
      "Epoch 1, Batch 42880, Loss: 3.1585, Time: 25517.8s, Step: 42881, GPU: 4.9GB\n",
      "Epoch 1, Batch 42885, Loss: 2.6263, Time: 25521.2s, Step: 42886, GPU: 4.9GB\n",
      "Epoch 1, Batch 42890, Loss: 2.3696, Time: 25523.8s, Step: 42891, GPU: 4.9GB\n",
      "Epoch 1, Batch 42895, Loss: 2.7414, Time: 25527.2s, Step: 42896, GPU: 4.9GB\n",
      "Epoch 1, Batch 42900, Loss: 2.7194, Time: 25529.7s, Step: 42901, GPU: 4.9GB\n",
      "Epoch 1, Batch 42905, Loss: 3.3986, Time: 25533.1s, Step: 42906, GPU: 4.9GB\n",
      "Epoch 1, Batch 42910, Loss: 3.0323, Time: 25535.6s, Step: 42911, GPU: 4.9GB\n",
      "Epoch 1, Batch 42915, Loss: 3.5849, Time: 25539.0s, Step: 42916, GPU: 4.9GB\n",
      "Epoch 1, Batch 42920, Loss: 3.5796, Time: 25541.5s, Step: 42921, GPU: 4.9GB\n",
      "Epoch 1, Batch 42925, Loss: 3.1362, Time: 25545.0s, Step: 42926, GPU: 4.9GB\n",
      "Epoch 1, Batch 42930, Loss: 3.5849, Time: 25547.5s, Step: 42931, GPU: 4.9GB\n",
      "Epoch 1, Batch 42935, Loss: 3.3297, Time: 25550.9s, Step: 42936, GPU: 4.9GB\n",
      "Epoch 1, Batch 42940, Loss: 3.3744, Time: 25553.4s, Step: 42941, GPU: 4.9GB\n",
      "Epoch 1, Batch 42945, Loss: 2.8641, Time: 25556.8s, Step: 42946, GPU: 4.9GB\n",
      "Epoch 1, Batch 42950, Loss: 2.8979, Time: 25559.3s, Step: 42951, GPU: 4.9GB\n",
      "Epoch 1, Batch 42955, Loss: 3.1547, Time: 25562.7s, Step: 42956, GPU: 4.9GB\n",
      "Epoch 1, Batch 42960, Loss: 4.0410, Time: 25565.3s, Step: 42961, GPU: 4.9GB\n",
      "Epoch 1, Batch 42965, Loss: 2.7011, Time: 25568.6s, Step: 42966, GPU: 4.9GB\n",
      "Epoch 1, Batch 42970, Loss: 3.5312, Time: 25571.1s, Step: 42971, GPU: 4.9GB\n",
      "Epoch 1, Batch 42975, Loss: 2.7090, Time: 25574.5s, Step: 42976, GPU: 4.9GB\n",
      "Epoch 1, Batch 42980, Loss: 3.0744, Time: 25577.1s, Step: 42981, GPU: 4.9GB\n",
      "Epoch 1, Batch 42985, Loss: 2.7262, Time: 25580.4s, Step: 42986, GPU: 4.9GB\n",
      "Epoch 1, Batch 42990, Loss: 2.4197, Time: 25583.0s, Step: 42991, GPU: 4.9GB\n",
      "Epoch 1, Batch 42995, Loss: 3.4448, Time: 25586.4s, Step: 42996, GPU: 4.9GB\n",
      "Epoch 1, Batch 43000, Loss: 2.8748, Time: 25589.0s, Step: 43001, GPU: 4.9GB\n",
      "Epoch 1, Batch 43005, Loss: 2.6561, Time: 25592.4s, Step: 43006, GPU: 4.9GB\n",
      "Epoch 1, Batch 43010, Loss: 3.5620, Time: 25594.9s, Step: 43011, GPU: 4.9GB\n",
      "Epoch 1, Batch 43015, Loss: 2.5228, Time: 25598.3s, Step: 43016, GPU: 4.9GB\n",
      "Epoch 1, Batch 43020, Loss: 3.4804, Time: 25600.8s, Step: 43021, GPU: 4.9GB\n",
      "Epoch 1, Batch 43025, Loss: 3.8083, Time: 25604.2s, Step: 43026, GPU: 4.9GB\n",
      "Epoch 1, Batch 43030, Loss: 3.2670, Time: 25606.7s, Step: 43031, GPU: 4.9GB\n",
      "Epoch 1, Batch 43035, Loss: 3.1780, Time: 25610.2s, Step: 43036, GPU: 4.9GB\n",
      "Epoch 1, Batch 43040, Loss: 3.7823, Time: 25612.7s, Step: 43041, GPU: 4.9GB\n",
      "Epoch 1, Batch 43045, Loss: 3.4889, Time: 25616.1s, Step: 43046, GPU: 4.9GB\n",
      "Epoch 1, Batch 43050, Loss: 2.8118, Time: 25618.6s, Step: 43051, GPU: 4.9GB\n",
      "Epoch 1, Batch 43055, Loss: 3.8847, Time: 25622.0s, Step: 43056, GPU: 4.9GB\n",
      "Epoch 1, Batch 43060, Loss: 3.6974, Time: 25624.5s, Step: 43061, GPU: 4.9GB\n",
      "Epoch 1, Batch 43065, Loss: 3.0719, Time: 25627.9s, Step: 43066, GPU: 4.9GB\n",
      "Epoch 1, Batch 43070, Loss: 3.3591, Time: 25630.5s, Step: 43071, GPU: 4.9GB\n",
      "Epoch 1, Batch 43075, Loss: 3.7680, Time: 25633.9s, Step: 43076, GPU: 4.9GB\n",
      "Epoch 1, Batch 43080, Loss: 2.9535, Time: 25636.4s, Step: 43081, GPU: 4.9GB\n",
      "Epoch 1, Batch 43085, Loss: 2.8821, Time: 25639.8s, Step: 43086, GPU: 4.9GB\n",
      "Epoch 1, Batch 43090, Loss: 2.7970, Time: 25642.4s, Step: 43091, GPU: 4.9GB\n",
      "Epoch 1, Batch 43095, Loss: 3.3563, Time: 25645.8s, Step: 43096, GPU: 4.9GB\n",
      "Epoch 1, Batch 43100, Loss: 3.7468, Time: 25648.3s, Step: 43101, GPU: 4.9GB\n",
      "Epoch 1, Batch 43105, Loss: 2.9576, Time: 25651.8s, Step: 43106, GPU: 4.9GB\n",
      "Epoch 1, Batch 43110, Loss: 3.3901, Time: 25654.3s, Step: 43111, GPU: 4.9GB\n",
      "Epoch 1, Batch 43115, Loss: 3.2139, Time: 25657.7s, Step: 43116, GPU: 4.9GB\n",
      "Epoch 1, Batch 43120, Loss: 2.9814, Time: 25660.2s, Step: 43121, GPU: 4.9GB\n",
      "Epoch 1, Batch 43125, Loss: 3.1149, Time: 25663.6s, Step: 43126, GPU: 4.9GB\n",
      "Epoch 1, Batch 43130, Loss: 2.8094, Time: 25666.2s, Step: 43131, GPU: 4.9GB\n",
      "Epoch 1, Batch 43135, Loss: 2.3798, Time: 25669.6s, Step: 43136, GPU: 4.9GB\n",
      "Epoch 1, Batch 43140, Loss: 3.1408, Time: 25672.1s, Step: 43141, GPU: 4.9GB\n",
      "Epoch 1, Batch 43145, Loss: 2.7270, Time: 25675.6s, Step: 43146, GPU: 4.9GB\n",
      "Epoch 1, Batch 43150, Loss: 3.5673, Time: 25678.1s, Step: 43151, GPU: 4.9GB\n",
      "Epoch 1, Batch 43155, Loss: 2.8188, Time: 25681.5s, Step: 43156, GPU: 4.9GB\n",
      "Epoch 1, Batch 43160, Loss: 3.2054, Time: 25684.0s, Step: 43161, GPU: 4.9GB\n",
      "Epoch 1, Batch 43165, Loss: 3.1072, Time: 25687.4s, Step: 43166, GPU: 4.9GB\n",
      "Epoch 1, Batch 43170, Loss: 3.4701, Time: 25690.0s, Step: 43171, GPU: 4.9GB\n",
      "Epoch 1, Batch 43175, Loss: 2.4967, Time: 25693.4s, Step: 43176, GPU: 4.9GB\n",
      "Epoch 1, Batch 43180, Loss: 4.2624, Time: 25695.9s, Step: 43181, GPU: 4.9GB\n",
      "Epoch 1, Batch 43185, Loss: 2.3147, Time: 25699.3s, Step: 43186, GPU: 4.9GB\n",
      "Epoch 1, Batch 43190, Loss: 2.5651, Time: 25701.8s, Step: 43191, GPU: 4.9GB\n",
      "Epoch 1, Batch 43195, Loss: 2.5226, Time: 25705.2s, Step: 43196, GPU: 4.9GB\n",
      "Epoch 1, Batch 43200, Loss: 3.4282, Time: 25707.8s, Step: 43201, GPU: 4.9GB\n",
      "Epoch 1, Batch 43205, Loss: 3.0592, Time: 25711.3s, Step: 43206, GPU: 4.9GB\n",
      "Epoch 1, Batch 43210, Loss: 3.8347, Time: 25713.8s, Step: 43211, GPU: 4.9GB\n",
      "Epoch 1, Batch 43215, Loss: 1.9531, Time: 25717.2s, Step: 43216, GPU: 4.9GB\n",
      "Epoch 1, Batch 43220, Loss: 3.7750, Time: 25719.7s, Step: 43221, GPU: 4.9GB\n",
      "Epoch 1, Batch 43225, Loss: 2.0943, Time: 25723.2s, Step: 43226, GPU: 4.9GB\n",
      "Epoch 1, Batch 43230, Loss: 3.6174, Time: 25725.7s, Step: 43231, GPU: 4.9GB\n",
      "Epoch 1, Batch 43235, Loss: 3.1501, Time: 25729.1s, Step: 43236, GPU: 4.9GB\n",
      "Epoch 1, Batch 43240, Loss: 3.6371, Time: 25731.7s, Step: 43241, GPU: 4.9GB\n",
      "Epoch 1, Batch 43245, Loss: 3.3034, Time: 25735.1s, Step: 43246, GPU: 4.9GB\n",
      "Epoch 1, Batch 43250, Loss: 2.8420, Time: 25737.6s, Step: 43251, GPU: 4.9GB\n",
      "Epoch 1, Batch 43255, Loss: 3.4413, Time: 25741.0s, Step: 43256, GPU: 4.9GB\n",
      "Epoch 1, Batch 43260, Loss: 3.3084, Time: 25743.5s, Step: 43261, GPU: 4.9GB\n",
      "Epoch 1, Batch 43265, Loss: 3.2601, Time: 25746.9s, Step: 43266, GPU: 4.9GB\n",
      "Epoch 1, Batch 43270, Loss: 2.9340, Time: 25749.5s, Step: 43271, GPU: 4.9GB\n",
      "Epoch 1, Batch 43275, Loss: 3.2960, Time: 25752.9s, Step: 43276, GPU: 4.9GB\n",
      "Epoch 1, Batch 43280, Loss: 3.0404, Time: 25755.4s, Step: 43281, GPU: 4.9GB\n",
      "Epoch 1, Batch 43285, Loss: 3.7432, Time: 25758.8s, Step: 43286, GPU: 4.9GB\n",
      "Epoch 1, Batch 43290, Loss: 3.4160, Time: 25761.3s, Step: 43291, GPU: 4.9GB\n",
      "Epoch 1, Batch 43295, Loss: 3.3030, Time: 25764.7s, Step: 43296, GPU: 4.9GB\n",
      "Epoch 1, Batch 43300, Loss: 3.2320, Time: 25767.3s, Step: 43301, GPU: 4.9GB\n",
      "Epoch 1, Batch 43305, Loss: 3.1989, Time: 25770.7s, Step: 43306, GPU: 4.9GB\n",
      "Epoch 1, Batch 43310, Loss: 3.4930, Time: 25773.2s, Step: 43311, GPU: 4.9GB\n",
      "Epoch 1, Batch 43315, Loss: 3.0839, Time: 25776.6s, Step: 43316, GPU: 4.9GB\n",
      "Epoch 1, Batch 43320, Loss: 2.7730, Time: 25779.1s, Step: 43321, GPU: 4.9GB\n",
      "Epoch 1, Batch 43325, Loss: 2.4904, Time: 25782.5s, Step: 43326, GPU: 4.9GB\n",
      "Epoch 1, Batch 43330, Loss: 3.1691, Time: 25785.1s, Step: 43331, GPU: 4.9GB\n",
      "Epoch 1, Batch 43335, Loss: 3.6719, Time: 25788.5s, Step: 43336, GPU: 4.9GB\n",
      "Epoch 1, Batch 43340, Loss: 4.3548, Time: 25791.0s, Step: 43341, GPU: 4.9GB\n",
      "Epoch 1, Batch 43345, Loss: 3.9346, Time: 25794.4s, Step: 43346, GPU: 4.9GB\n",
      "Epoch 1, Batch 43350, Loss: 3.1050, Time: 25796.9s, Step: 43351, GPU: 4.9GB\n",
      "Epoch 1, Batch 43355, Loss: 3.7667, Time: 25800.3s, Step: 43356, GPU: 4.9GB\n",
      "Epoch 1, Batch 43360, Loss: 3.7719, Time: 25802.9s, Step: 43361, GPU: 4.9GB\n",
      "Epoch 1, Batch 43365, Loss: 3.3880, Time: 25806.3s, Step: 43366, GPU: 4.9GB\n",
      "Epoch 1, Batch 43370, Loss: 2.6070, Time: 25808.8s, Step: 43371, GPU: 4.9GB\n",
      "Epoch 1, Batch 43375, Loss: 3.3381, Time: 25812.2s, Step: 43376, GPU: 4.9GB\n",
      "Epoch 1, Batch 43380, Loss: 3.0583, Time: 25814.7s, Step: 43381, GPU: 4.9GB\n",
      "Epoch 1, Batch 43385, Loss: 3.7624, Time: 25818.1s, Step: 43386, GPU: 4.9GB\n",
      "Epoch 1, Batch 43390, Loss: 3.2071, Time: 25820.6s, Step: 43391, GPU: 4.9GB\n",
      "Epoch 1, Batch 43395, Loss: 3.2829, Time: 25824.0s, Step: 43396, GPU: 4.9GB\n",
      "Epoch 1, Batch 43400, Loss: 2.9834, Time: 25826.6s, Step: 43401, GPU: 4.9GB\n",
      "Epoch 1, Batch 43405, Loss: 3.6093, Time: 25830.1s, Step: 43406, GPU: 4.9GB\n",
      "Epoch 1, Batch 43410, Loss: 4.1618, Time: 25832.6s, Step: 43411, GPU: 4.9GB\n",
      "Epoch 1, Batch 43415, Loss: 2.1647, Time: 25836.0s, Step: 43416, GPU: 4.9GB\n",
      "Epoch 1, Batch 43420, Loss: 2.8824, Time: 25838.5s, Step: 43421, GPU: 4.9GB\n",
      "Epoch 1, Batch 43425, Loss: 3.3511, Time: 25841.9s, Step: 43426, GPU: 4.9GB\n",
      "Epoch 1, Batch 43430, Loss: 3.0091, Time: 25844.4s, Step: 43431, GPU: 4.9GB\n",
      "Epoch 1, Batch 43435, Loss: 3.4527, Time: 25847.8s, Step: 43436, GPU: 4.9GB\n",
      "Epoch 1, Batch 43440, Loss: 3.2447, Time: 25850.3s, Step: 43441, GPU: 4.9GB\n",
      "Epoch 1, Batch 43445, Loss: 3.9788, Time: 25853.7s, Step: 43446, GPU: 4.9GB\n",
      "Epoch 1, Batch 43450, Loss: 3.2135, Time: 25856.2s, Step: 43451, GPU: 4.9GB\n",
      "Epoch 1, Batch 43455, Loss: 2.8040, Time: 25859.7s, Step: 43456, GPU: 4.9GB\n",
      "Epoch 1, Batch 43460, Loss: 3.2478, Time: 25862.2s, Step: 43461, GPU: 4.9GB\n",
      "Epoch 1, Batch 43465, Loss: 3.1817, Time: 25865.6s, Step: 43466, GPU: 4.9GB\n",
      "Epoch 1, Batch 43470, Loss: 3.5183, Time: 25868.1s, Step: 43471, GPU: 4.9GB\n",
      "Epoch 1, Batch 43475, Loss: 3.5793, Time: 25871.5s, Step: 43476, GPU: 4.9GB\n",
      "Epoch 1, Batch 43480, Loss: 3.1243, Time: 25874.1s, Step: 43481, GPU: 4.9GB\n",
      "Epoch 1, Batch 43485, Loss: 3.4635, Time: 25877.5s, Step: 43486, GPU: 4.9GB\n",
      "Epoch 1, Batch 43490, Loss: 3.3849, Time: 25880.0s, Step: 43491, GPU: 4.9GB\n",
      "Epoch 1, Batch 43495, Loss: 3.9926, Time: 25883.4s, Step: 43496, GPU: 4.9GB\n",
      "Epoch 1, Batch 43500, Loss: 3.5144, Time: 25886.0s, Step: 43501, GPU: 4.9GB\n",
      "Epoch 1, Batch 43505, Loss: 3.3870, Time: 25889.4s, Step: 43506, GPU: 4.9GB\n",
      "Epoch 1, Batch 43510, Loss: 3.5565, Time: 25891.9s, Step: 43511, GPU: 4.9GB\n",
      "Epoch 1, Batch 43515, Loss: 3.4653, Time: 25895.3s, Step: 43516, GPU: 4.9GB\n",
      "Epoch 1, Batch 43520, Loss: 2.8917, Time: 25897.8s, Step: 43521, GPU: 4.9GB\n",
      "Epoch 1, Batch 43525, Loss: 3.2344, Time: 25901.3s, Step: 43526, GPU: 4.9GB\n",
      "Epoch 1, Batch 43530, Loss: 3.7304, Time: 25903.8s, Step: 43531, GPU: 4.9GB\n",
      "Epoch 1, Batch 43535, Loss: 2.9270, Time: 25907.2s, Step: 43536, GPU: 4.9GB\n",
      "Epoch 1, Batch 43540, Loss: 2.8737, Time: 25909.7s, Step: 43541, GPU: 4.9GB\n",
      "Epoch 1, Batch 43545, Loss: 3.0097, Time: 25913.1s, Step: 43546, GPU: 4.9GB\n",
      "Epoch 1, Batch 43550, Loss: 3.2724, Time: 25915.6s, Step: 43551, GPU: 4.9GB\n",
      "Epoch 1, Batch 43555, Loss: 2.8294, Time: 25919.1s, Step: 43556, GPU: 4.9GB\n",
      "Epoch 1, Batch 43560, Loss: 3.3635, Time: 25921.6s, Step: 43561, GPU: 4.9GB\n",
      "Epoch 1, Batch 43565, Loss: 3.3443, Time: 25925.0s, Step: 43566, GPU: 4.9GB\n",
      "Epoch 1, Batch 43570, Loss: 2.8394, Time: 25927.5s, Step: 43571, GPU: 4.9GB\n",
      "Epoch 1, Batch 43575, Loss: 3.4844, Time: 25931.0s, Step: 43576, GPU: 4.9GB\n",
      "Epoch 1, Batch 43580, Loss: 2.8774, Time: 25933.5s, Step: 43581, GPU: 4.9GB\n",
      "Epoch 1, Batch 43585, Loss: 3.2820, Time: 25936.9s, Step: 43586, GPU: 4.9GB\n",
      "Epoch 1, Batch 43590, Loss: 3.0760, Time: 25939.5s, Step: 43591, GPU: 4.9GB\n",
      "Epoch 1, Batch 43595, Loss: 3.2201, Time: 25942.9s, Step: 43596, GPU: 4.9GB\n",
      "Epoch 1, Batch 43600, Loss: 3.5274, Time: 25945.5s, Step: 43601, GPU: 4.9GB\n",
      "Epoch 1, Batch 43605, Loss: 3.3688, Time: 25948.9s, Step: 43606, GPU: 4.9GB\n",
      "Epoch 1, Batch 43610, Loss: 3.3020, Time: 25951.5s, Step: 43611, GPU: 4.9GB\n",
      "Epoch 1, Batch 43615, Loss: 2.6009, Time: 25954.9s, Step: 43616, GPU: 4.9GB\n",
      "Epoch 1, Batch 43620, Loss: 2.1083, Time: 25957.4s, Step: 43621, GPU: 4.9GB\n",
      "Epoch 1, Batch 43625, Loss: 3.0079, Time: 25960.9s, Step: 43626, GPU: 4.9GB\n",
      "Epoch 1, Batch 43630, Loss: 3.8644, Time: 25963.4s, Step: 43631, GPU: 4.9GB\n",
      "Epoch 1, Batch 43635, Loss: 2.8528, Time: 25966.8s, Step: 43636, GPU: 4.9GB\n",
      "Epoch 1, Batch 43640, Loss: 3.4947, Time: 25969.4s, Step: 43641, GPU: 4.9GB\n",
      "Epoch 1, Batch 43645, Loss: 3.2273, Time: 25972.8s, Step: 43646, GPU: 4.9GB\n",
      "Epoch 1, Batch 43650, Loss: 3.0527, Time: 25975.3s, Step: 43651, GPU: 4.9GB\n",
      "Epoch 1, Batch 43655, Loss: 3.6141, Time: 25978.7s, Step: 43656, GPU: 4.9GB\n",
      "Epoch 1, Batch 43660, Loss: 2.5255, Time: 25981.2s, Step: 43661, GPU: 4.9GB\n",
      "Epoch 1, Batch 43665, Loss: 2.9154, Time: 25984.6s, Step: 43666, GPU: 4.9GB\n",
      "Epoch 1, Batch 43670, Loss: 3.1429, Time: 25987.2s, Step: 43671, GPU: 4.9GB\n",
      "Epoch 1, Batch 43675, Loss: 3.6575, Time: 25990.6s, Step: 43676, GPU: 4.9GB\n",
      "Epoch 1, Batch 43680, Loss: 3.0533, Time: 25993.1s, Step: 43681, GPU: 4.9GB\n",
      "Epoch 1, Batch 43685, Loss: 3.1176, Time: 25996.5s, Step: 43686, GPU: 4.9GB\n",
      "Epoch 1, Batch 43690, Loss: 3.7553, Time: 25999.0s, Step: 43691, GPU: 4.9GB\n",
      "Epoch 1, Batch 43695, Loss: 3.3239, Time: 26002.4s, Step: 43696, GPU: 4.9GB\n",
      "Epoch 1, Batch 43700, Loss: 3.1134, Time: 26004.9s, Step: 43701, GPU: 4.9GB\n",
      "Epoch 1, Batch 43705, Loss: 3.2736, Time: 26008.3s, Step: 43706, GPU: 4.9GB\n",
      "Epoch 1, Batch 43710, Loss: 3.3778, Time: 26010.8s, Step: 43711, GPU: 4.9GB\n",
      "Epoch 1, Batch 43715, Loss: 4.1591, Time: 26014.2s, Step: 43716, GPU: 4.9GB\n",
      "Epoch 1, Batch 43720, Loss: 3.5745, Time: 26016.7s, Step: 43721, GPU: 4.9GB\n",
      "Epoch 1, Batch 43725, Loss: 3.0262, Time: 26020.1s, Step: 43726, GPU: 4.9GB\n",
      "Epoch 1, Batch 43730, Loss: 3.1119, Time: 26022.7s, Step: 43731, GPU: 4.9GB\n",
      "Epoch 1, Batch 43735, Loss: 2.5975, Time: 26026.1s, Step: 43736, GPU: 4.9GB\n",
      "Epoch 1, Batch 43740, Loss: 2.9296, Time: 26028.6s, Step: 43741, GPU: 4.9GB\n",
      "Epoch 1, Batch 43745, Loss: 3.2887, Time: 26032.0s, Step: 43746, GPU: 4.9GB\n",
      "Epoch 1, Batch 43750, Loss: 2.8564, Time: 26034.5s, Step: 43751, GPU: 4.9GB\n",
      "Epoch 1, Batch 43755, Loss: 3.2358, Time: 26038.0s, Step: 43756, GPU: 4.9GB\n",
      "Epoch 1, Batch 43760, Loss: 3.3025, Time: 26040.5s, Step: 43761, GPU: 4.9GB\n",
      "Epoch 1, Batch 43765, Loss: 3.6943, Time: 26043.9s, Step: 43766, GPU: 4.9GB\n",
      "Epoch 1, Batch 43770, Loss: 2.6022, Time: 26046.4s, Step: 43771, GPU: 4.9GB\n",
      "Epoch 1, Batch 43775, Loss: 4.1686, Time: 26049.8s, Step: 43776, GPU: 4.9GB\n",
      "Epoch 1, Batch 43780, Loss: 3.7569, Time: 26052.3s, Step: 43781, GPU: 4.9GB\n",
      "Epoch 1, Batch 43785, Loss: 3.4910, Time: 26055.7s, Step: 43786, GPU: 4.9GB\n",
      "Epoch 1, Batch 43790, Loss: 3.4344, Time: 26058.2s, Step: 43791, GPU: 4.9GB\n",
      "Epoch 1, Batch 43795, Loss: 4.1249, Time: 26061.6s, Step: 43796, GPU: 4.9GB\n",
      "Epoch 1, Batch 43800, Loss: 2.8912, Time: 26064.2s, Step: 43801, GPU: 4.9GB\n",
      "Epoch 1, Batch 43805, Loss: 2.9054, Time: 26068.1s, Step: 43806, GPU: 4.9GB\n",
      "Epoch 1, Batch 43810, Loss: 3.2922, Time: 26070.6s, Step: 43811, GPU: 4.9GB\n",
      "Epoch 1, Batch 43815, Loss: 3.8739, Time: 26074.0s, Step: 43816, GPU: 4.9GB\n",
      "Epoch 1, Batch 43820, Loss: 3.6304, Time: 26076.5s, Step: 43821, GPU: 4.9GB\n",
      "Epoch 1, Batch 43825, Loss: 2.5024, Time: 26080.0s, Step: 43826, GPU: 4.9GB\n",
      "Epoch 1, Batch 43830, Loss: 2.5940, Time: 26082.5s, Step: 43831, GPU: 4.9GB\n",
      "Epoch 1, Batch 43835, Loss: 3.1353, Time: 26085.9s, Step: 43836, GPU: 4.9GB\n",
      "Epoch 1, Batch 43840, Loss: 3.2929, Time: 26088.5s, Step: 43841, GPU: 4.9GB\n",
      "Epoch 1, Batch 43845, Loss: 3.4480, Time: 26091.9s, Step: 43846, GPU: 4.9GB\n",
      "Epoch 1, Batch 43850, Loss: 2.7215, Time: 26094.4s, Step: 43851, GPU: 4.9GB\n",
      "Epoch 1, Batch 43855, Loss: 2.6660, Time: 26097.8s, Step: 43856, GPU: 4.9GB\n",
      "Epoch 1, Batch 43860, Loss: 3.8342, Time: 26100.4s, Step: 43861, GPU: 4.9GB\n",
      "Epoch 1, Batch 43865, Loss: 3.2611, Time: 26103.8s, Step: 43866, GPU: 4.9GB\n",
      "Epoch 1, Batch 43870, Loss: 3.2925, Time: 26106.3s, Step: 43871, GPU: 4.9GB\n",
      "Epoch 1, Batch 43875, Loss: 3.5235, Time: 26109.7s, Step: 43876, GPU: 4.9GB\n",
      "Epoch 1, Batch 43880, Loss: 3.1098, Time: 26112.3s, Step: 43881, GPU: 4.9GB\n",
      "Epoch 1, Batch 43885, Loss: 3.3852, Time: 26115.7s, Step: 43886, GPU: 4.9GB\n",
      "Epoch 1, Batch 43890, Loss: 2.3683, Time: 26118.2s, Step: 43891, GPU: 4.9GB\n",
      "Epoch 1, Batch 43895, Loss: 3.2851, Time: 26121.6s, Step: 43896, GPU: 4.9GB\n",
      "Epoch 1, Batch 43900, Loss: 3.5611, Time: 26124.1s, Step: 43901, GPU: 4.9GB\n",
      "Epoch 1, Batch 43905, Loss: 4.2453, Time: 26127.5s, Step: 43906, GPU: 4.9GB\n",
      "Epoch 1, Batch 43910, Loss: 3.1766, Time: 26130.0s, Step: 43911, GPU: 4.9GB\n",
      "Epoch 1, Batch 43915, Loss: 3.2332, Time: 26133.4s, Step: 43916, GPU: 4.9GB\n",
      "Epoch 1, Batch 43920, Loss: 3.0810, Time: 26135.9s, Step: 43921, GPU: 4.9GB\n",
      "Epoch 1, Batch 43925, Loss: 3.9155, Time: 26139.3s, Step: 43926, GPU: 4.9GB\n",
      "Epoch 1, Batch 43930, Loss: 3.0452, Time: 26141.9s, Step: 43931, GPU: 4.9GB\n",
      "Epoch 1, Batch 43935, Loss: 3.0549, Time: 26145.3s, Step: 43936, GPU: 4.9GB\n",
      "Epoch 1, Batch 43940, Loss: 3.0830, Time: 26147.8s, Step: 43941, GPU: 4.9GB\n",
      "Epoch 1, Batch 43945, Loss: 3.0774, Time: 26151.2s, Step: 43946, GPU: 4.9GB\n",
      "Epoch 1, Batch 43950, Loss: 3.1896, Time: 26153.7s, Step: 43951, GPU: 4.9GB\n",
      "Epoch 1, Batch 43955, Loss: 3.1383, Time: 26157.1s, Step: 43956, GPU: 4.9GB\n",
      "Epoch 1, Batch 43960, Loss: 3.1772, Time: 26159.7s, Step: 43961, GPU: 4.9GB\n",
      "Epoch 1, Batch 43965, Loss: 2.6397, Time: 26163.1s, Step: 43966, GPU: 4.9GB\n",
      "Epoch 1, Batch 43970, Loss: 2.7311, Time: 26165.6s, Step: 43971, GPU: 4.9GB\n",
      "Epoch 1, Batch 43975, Loss: 2.7553, Time: 26169.0s, Step: 43976, GPU: 4.9GB\n",
      "Epoch 1, Batch 43980, Loss: 2.8240, Time: 26171.5s, Step: 43981, GPU: 4.9GB\n",
      "Epoch 1, Batch 43985, Loss: 3.1652, Time: 26174.9s, Step: 43986, GPU: 4.9GB\n",
      "Epoch 1, Batch 43990, Loss: 2.6025, Time: 26177.4s, Step: 43991, GPU: 4.9GB\n",
      "Epoch 1, Batch 43995, Loss: 3.9157, Time: 26180.9s, Step: 43996, GPU: 4.9GB\n",
      "Epoch 1, Batch 44000, Loss: 2.9824, Time: 26183.5s, Step: 44001, GPU: 4.9GB\n",
      "Epoch 1, Batch 44005, Loss: 3.6034, Time: 26187.5s, Step: 44006, GPU: 4.9GB\n",
      "Epoch 1, Batch 44010, Loss: 3.3691, Time: 26190.0s, Step: 44011, GPU: 4.9GB\n",
      "Epoch 1, Batch 44015, Loss: 3.3153, Time: 26193.4s, Step: 44016, GPU: 4.9GB\n",
      "Epoch 1, Batch 44020, Loss: 3.4112, Time: 26195.9s, Step: 44021, GPU: 4.9GB\n",
      "Epoch 1, Batch 44025, Loss: 3.0059, Time: 26199.3s, Step: 44026, GPU: 4.9GB\n",
      "Epoch 1, Batch 44030, Loss: 3.6020, Time: 26201.8s, Step: 44031, GPU: 4.9GB\n",
      "Epoch 1, Batch 44035, Loss: 3.3252, Time: 26205.2s, Step: 44036, GPU: 4.9GB\n",
      "Epoch 1, Batch 44040, Loss: 3.3332, Time: 26207.7s, Step: 44041, GPU: 4.9GB\n",
      "Epoch 1, Batch 44045, Loss: 3.5923, Time: 26211.1s, Step: 44046, GPU: 4.9GB\n",
      "Epoch 1, Batch 44050, Loss: 2.5300, Time: 26213.6s, Step: 44051, GPU: 4.9GB\n",
      "Epoch 1, Batch 44055, Loss: 2.9027, Time: 26217.0s, Step: 44056, GPU: 4.9GB\n",
      "Epoch 1, Batch 44060, Loss: 3.5557, Time: 26219.6s, Step: 44061, GPU: 4.9GB\n",
      "Epoch 1, Batch 44065, Loss: 3.1402, Time: 26223.0s, Step: 44066, GPU: 4.9GB\n",
      "Epoch 1, Batch 44070, Loss: 3.3151, Time: 26225.5s, Step: 44071, GPU: 4.9GB\n",
      "Epoch 1, Batch 44075, Loss: 2.4791, Time: 26228.9s, Step: 44076, GPU: 4.9GB\n",
      "Epoch 1, Batch 44080, Loss: 2.8741, Time: 26231.4s, Step: 44081, GPU: 4.9GB\n",
      "Epoch 1, Batch 44085, Loss: 2.2606, Time: 26234.8s, Step: 44086, GPU: 4.9GB\n",
      "Epoch 1, Batch 44090, Loss: 3.1760, Time: 26237.3s, Step: 44091, GPU: 4.9GB\n",
      "Epoch 1, Batch 44095, Loss: 4.5854, Time: 26240.7s, Step: 44096, GPU: 4.9GB\n",
      "Epoch 1, Batch 44100, Loss: 3.4964, Time: 26243.2s, Step: 44101, GPU: 4.9GB\n",
      "Epoch 1, Batch 44105, Loss: 3.5199, Time: 26246.7s, Step: 44106, GPU: 4.9GB\n",
      "Epoch 1, Batch 44110, Loss: 2.8106, Time: 26249.2s, Step: 44111, GPU: 4.9GB\n",
      "Epoch 1, Batch 44115, Loss: 3.4148, Time: 26252.6s, Step: 44116, GPU: 4.9GB\n",
      "Epoch 1, Batch 44120, Loss: 3.9626, Time: 26255.1s, Step: 44121, GPU: 4.9GB\n",
      "Epoch 1, Batch 44125, Loss: 3.5591, Time: 26258.5s, Step: 44126, GPU: 4.9GB\n",
      "Epoch 1, Batch 44130, Loss: 3.7397, Time: 26261.0s, Step: 44131, GPU: 4.9GB\n",
      "Epoch 1, Batch 44135, Loss: 2.5086, Time: 26264.4s, Step: 44136, GPU: 4.9GB\n",
      "Epoch 1, Batch 44140, Loss: 2.7655, Time: 26266.9s, Step: 44141, GPU: 4.9GB\n",
      "Epoch 1, Batch 44145, Loss: 3.1756, Time: 26270.4s, Step: 44146, GPU: 4.9GB\n",
      "Epoch 1, Batch 44150, Loss: 3.3826, Time: 26272.9s, Step: 44151, GPU: 4.9GB\n",
      "Epoch 1, Batch 44155, Loss: 3.0690, Time: 26276.3s, Step: 44156, GPU: 4.9GB\n",
      "Epoch 1, Batch 44160, Loss: 2.9573, Time: 26278.8s, Step: 44161, GPU: 4.9GB\n",
      "Epoch 1, Batch 44165, Loss: 3.3202, Time: 26282.2s, Step: 44166, GPU: 4.9GB\n",
      "Epoch 1, Batch 44170, Loss: 3.1790, Time: 26284.7s, Step: 44171, GPU: 4.9GB\n",
      "Epoch 1, Batch 44175, Loss: 2.2095, Time: 26288.1s, Step: 44176, GPU: 4.9GB\n",
      "Epoch 1, Batch 44180, Loss: 3.8879, Time: 26290.6s, Step: 44181, GPU: 4.9GB\n",
      "Epoch 1, Batch 44185, Loss: 2.9315, Time: 26294.0s, Step: 44186, GPU: 4.9GB\n",
      "Epoch 1, Batch 44190, Loss: 3.8416, Time: 26296.5s, Step: 44191, GPU: 4.9GB\n",
      "Epoch 1, Batch 44195, Loss: 3.3933, Time: 26299.9s, Step: 44196, GPU: 4.9GB\n",
      "Epoch 1, Batch 44200, Loss: 3.4856, Time: 26302.5s, Step: 44201, GPU: 4.9GB\n",
      "Epoch 1, Batch 44205, Loss: 3.7826, Time: 26305.9s, Step: 44206, GPU: 4.9GB\n",
      "Epoch 1, Batch 44210, Loss: 3.4175, Time: 26308.4s, Step: 44211, GPU: 4.9GB\n",
      "Epoch 1, Batch 44215, Loss: 2.8220, Time: 26311.8s, Step: 44216, GPU: 4.9GB\n",
      "Epoch 1, Batch 44220, Loss: 3.0308, Time: 26314.3s, Step: 44221, GPU: 4.9GB\n",
      "Epoch 1, Batch 44225, Loss: 4.3351, Time: 26317.7s, Step: 44226, GPU: 4.9GB\n",
      "Epoch 1, Batch 44230, Loss: 2.9772, Time: 26320.2s, Step: 44231, GPU: 4.9GB\n",
      "Epoch 1, Batch 44235, Loss: 3.4196, Time: 26323.6s, Step: 44236, GPU: 4.9GB\n",
      "Epoch 1, Batch 44240, Loss: 3.3204, Time: 26326.1s, Step: 44241, GPU: 4.9GB\n",
      "Epoch 1, Batch 44245, Loss: 3.2478, Time: 26329.6s, Step: 44246, GPU: 4.9GB\n",
      "Epoch 1, Batch 44250, Loss: 4.0006, Time: 26332.1s, Step: 44251, GPU: 4.9GB\n",
      "Epoch 1, Batch 44255, Loss: 2.8072, Time: 26335.5s, Step: 44256, GPU: 4.9GB\n",
      "Epoch 1, Batch 44260, Loss: 3.4066, Time: 26338.0s, Step: 44261, GPU: 4.9GB\n",
      "Epoch 1, Batch 44265, Loss: 2.1721, Time: 26341.4s, Step: 44266, GPU: 4.9GB\n",
      "Epoch 1, Batch 44270, Loss: 2.8510, Time: 26343.9s, Step: 44271, GPU: 4.9GB\n",
      "Epoch 1, Batch 44275, Loss: 3.2873, Time: 26347.4s, Step: 44276, GPU: 4.9GB\n",
      "Epoch 1, Batch 44280, Loss: 3.4548, Time: 26349.9s, Step: 44281, GPU: 4.9GB\n",
      "Epoch 1, Batch 44285, Loss: 3.6753, Time: 26353.3s, Step: 44286, GPU: 4.9GB\n",
      "Epoch 1, Batch 44290, Loss: 3.0382, Time: 26355.8s, Step: 44291, GPU: 4.9GB\n",
      "Epoch 1, Batch 44295, Loss: 2.9610, Time: 26359.2s, Step: 44296, GPU: 4.9GB\n",
      "Epoch 1, Batch 44300, Loss: 3.5571, Time: 26361.7s, Step: 44301, GPU: 4.9GB\n",
      "Epoch 1, Batch 44305, Loss: 3.0433, Time: 26365.2s, Step: 44306, GPU: 4.9GB\n",
      "Epoch 1, Batch 44310, Loss: 3.0162, Time: 26367.7s, Step: 44311, GPU: 4.9GB\n",
      "Epoch 1, Batch 44315, Loss: 3.7153, Time: 26371.1s, Step: 44316, GPU: 4.9GB\n",
      "Epoch 1, Batch 44320, Loss: 4.0741, Time: 26373.6s, Step: 44321, GPU: 4.9GB\n",
      "Epoch 1, Batch 44325, Loss: 3.5581, Time: 26377.0s, Step: 44326, GPU: 4.9GB\n",
      "Epoch 1, Batch 44330, Loss: 2.9508, Time: 26379.6s, Step: 44331, GPU: 4.9GB\n",
      "Epoch 1, Batch 44335, Loss: 3.3333, Time: 26383.0s, Step: 44336, GPU: 4.9GB\n",
      "Epoch 1, Batch 44340, Loss: 3.3354, Time: 26385.5s, Step: 44341, GPU: 4.9GB\n",
      "Epoch 1, Batch 44345, Loss: 3.3853, Time: 26389.0s, Step: 44346, GPU: 4.9GB\n",
      "Epoch 1, Batch 44350, Loss: 3.6898, Time: 26391.5s, Step: 44351, GPU: 4.9GB\n",
      "Epoch 1, Batch 44355, Loss: 3.0761, Time: 26394.9s, Step: 44356, GPU: 4.9GB\n",
      "Epoch 1, Batch 44360, Loss: 2.8597, Time: 26397.4s, Step: 44361, GPU: 4.9GB\n",
      "Epoch 1, Batch 44365, Loss: 3.1121, Time: 26400.9s, Step: 44366, GPU: 4.9GB\n",
      "Epoch 1, Batch 44370, Loss: 3.2390, Time: 26403.4s, Step: 44371, GPU: 4.9GB\n",
      "Epoch 1, Batch 44375, Loss: 3.1673, Time: 26406.8s, Step: 44376, GPU: 4.9GB\n",
      "Epoch 1, Batch 44380, Loss: 2.8406, Time: 26409.3s, Step: 44381, GPU: 4.9GB\n",
      "Epoch 1, Batch 44385, Loss: 3.1226, Time: 26412.8s, Step: 44386, GPU: 4.9GB\n",
      "Epoch 1, Batch 44390, Loss: 2.7993, Time: 26415.3s, Step: 44391, GPU: 4.9GB\n",
      "Epoch 1, Batch 44395, Loss: 2.8821, Time: 26418.7s, Step: 44396, GPU: 4.9GB\n",
      "Epoch 1, Batch 44400, Loss: 3.2436, Time: 26421.3s, Step: 44401, GPU: 4.9GB\n",
      "Epoch 1, Batch 44405, Loss: 3.3390, Time: 26424.7s, Step: 44406, GPU: 4.9GB\n",
      "Epoch 1, Batch 44410, Loss: 2.7343, Time: 26427.3s, Step: 44411, GPU: 4.9GB\n",
      "Epoch 1, Batch 44415, Loss: 2.9266, Time: 26430.7s, Step: 44416, GPU: 4.9GB\n",
      "Epoch 1, Batch 44420, Loss: 2.7979, Time: 26433.2s, Step: 44421, GPU: 4.9GB\n",
      "Epoch 1, Batch 44425, Loss: 4.0891, Time: 26436.6s, Step: 44426, GPU: 4.9GB\n",
      "Epoch 1, Batch 44430, Loss: 3.6306, Time: 26439.1s, Step: 44431, GPU: 4.9GB\n",
      "Epoch 1, Batch 44435, Loss: 3.6060, Time: 26442.6s, Step: 44436, GPU: 4.9GB\n",
      "Epoch 1, Batch 44440, Loss: 4.5293, Time: 26445.1s, Step: 44441, GPU: 4.9GB\n",
      "Epoch 1, Batch 44445, Loss: 2.7052, Time: 26448.6s, Step: 44446, GPU: 4.9GB\n",
      "Epoch 1, Batch 44450, Loss: 3.1141, Time: 26451.1s, Step: 44451, GPU: 4.9GB\n",
      "Epoch 1, Batch 44455, Loss: 2.9547, Time: 26454.5s, Step: 44456, GPU: 4.9GB\n",
      "Epoch 1, Batch 44460, Loss: 2.1093, Time: 26457.0s, Step: 44461, GPU: 4.9GB\n",
      "Epoch 1, Batch 44465, Loss: 3.9901, Time: 26460.5s, Step: 44466, GPU: 4.9GB\n",
      "Epoch 1, Batch 44470, Loss: 3.4417, Time: 26463.0s, Step: 44471, GPU: 4.9GB\n",
      "Epoch 1, Batch 44475, Loss: 2.7056, Time: 26466.4s, Step: 44476, GPU: 4.9GB\n",
      "Epoch 1, Batch 44480, Loss: 3.2745, Time: 26468.9s, Step: 44481, GPU: 4.9GB\n",
      "Epoch 1, Batch 44485, Loss: 4.2369, Time: 26472.4s, Step: 44486, GPU: 4.9GB\n",
      "Epoch 1, Batch 44490, Loss: 3.3901, Time: 26474.9s, Step: 44491, GPU: 4.9GB\n",
      "Epoch 1, Batch 44495, Loss: 2.9710, Time: 26478.3s, Step: 44496, GPU: 4.9GB\n",
      "Epoch 1, Batch 44500, Loss: 2.4330, Time: 26480.9s, Step: 44501, GPU: 4.9GB\n",
      "Epoch 1, Batch 44505, Loss: 2.9184, Time: 26484.3s, Step: 44506, GPU: 4.9GB\n",
      "Epoch 1, Batch 44510, Loss: 3.2700, Time: 26486.9s, Step: 44511, GPU: 4.9GB\n",
      "Epoch 1, Batch 44515, Loss: 3.1644, Time: 26490.3s, Step: 44516, GPU: 4.9GB\n",
      "Epoch 1, Batch 44520, Loss: 2.3119, Time: 26492.8s, Step: 44521, GPU: 4.9GB\n",
      "Epoch 1, Batch 44525, Loss: 3.0921, Time: 26496.2s, Step: 44526, GPU: 4.9GB\n",
      "Epoch 1, Batch 44530, Loss: 3.0991, Time: 26498.7s, Step: 44531, GPU: 4.9GB\n",
      "Epoch 1, Batch 44535, Loss: 3.1698, Time: 26502.1s, Step: 44536, GPU: 4.9GB\n",
      "Epoch 1, Batch 44540, Loss: 3.3367, Time: 26504.7s, Step: 44541, GPU: 4.9GB\n",
      "Epoch 1, Batch 44545, Loss: 3.1005, Time: 26508.1s, Step: 44546, GPU: 4.9GB\n",
      "Epoch 1, Batch 44550, Loss: 3.1321, Time: 26510.7s, Step: 44551, GPU: 4.9GB\n",
      "Epoch 1, Batch 44555, Loss: 2.8847, Time: 26514.1s, Step: 44556, GPU: 4.9GB\n",
      "Epoch 1, Batch 44560, Loss: 3.3201, Time: 26516.6s, Step: 44561, GPU: 4.9GB\n",
      "Epoch 1, Batch 44565, Loss: 2.7038, Time: 26520.0s, Step: 44566, GPU: 4.9GB\n",
      "Epoch 1, Batch 44570, Loss: 3.5934, Time: 26522.5s, Step: 44571, GPU: 4.9GB\n",
      "Epoch 1, Batch 44575, Loss: 4.5931, Time: 26525.9s, Step: 44576, GPU: 4.9GB\n",
      "Epoch 1, Batch 44580, Loss: 3.3097, Time: 26528.4s, Step: 44581, GPU: 4.9GB\n",
      "Epoch 1, Batch 44585, Loss: 3.2722, Time: 26531.8s, Step: 44586, GPU: 4.9GB\n",
      "Epoch 1, Batch 44590, Loss: 3.5121, Time: 26534.3s, Step: 44591, GPU: 4.9GB\n",
      "Epoch 1, Batch 44595, Loss: 3.2039, Time: 26537.7s, Step: 44596, GPU: 4.9GB\n",
      "Epoch 1, Batch 44600, Loss: 2.4532, Time: 26540.4s, Step: 44601, GPU: 4.9GB\n",
      "Epoch 1, Batch 44605, Loss: 3.4444, Time: 26543.8s, Step: 44606, GPU: 4.9GB\n",
      "Epoch 1, Batch 44610, Loss: 3.8357, Time: 26546.3s, Step: 44611, GPU: 4.9GB\n",
      "Epoch 1, Batch 44615, Loss: 2.8655, Time: 26549.7s, Step: 44616, GPU: 4.9GB\n",
      "Epoch 1, Batch 44620, Loss: 3.4347, Time: 26552.2s, Step: 44621, GPU: 4.9GB\n",
      "Epoch 1, Batch 44625, Loss: 3.5495, Time: 26555.6s, Step: 44626, GPU: 4.9GB\n",
      "Epoch 1, Batch 44630, Loss: 3.5251, Time: 26558.1s, Step: 44631, GPU: 4.9GB\n",
      "Epoch 1, Batch 44635, Loss: 3.3336, Time: 26561.5s, Step: 44636, GPU: 4.9GB\n",
      "Epoch 1, Batch 44640, Loss: 3.4875, Time: 26564.0s, Step: 44641, GPU: 4.9GB\n",
      "Epoch 1, Batch 44645, Loss: 3.4277, Time: 26567.4s, Step: 44646, GPU: 4.9GB\n",
      "Epoch 1, Batch 44650, Loss: 3.5022, Time: 26570.0s, Step: 44651, GPU: 4.9GB\n",
      "Epoch 1, Batch 44655, Loss: 3.7385, Time: 26573.3s, Step: 44656, GPU: 4.9GB\n",
      "Epoch 1, Batch 44660, Loss: 3.3506, Time: 26575.9s, Step: 44661, GPU: 4.9GB\n",
      "Epoch 1, Batch 44665, Loss: 3.3881, Time: 26579.3s, Step: 44666, GPU: 4.9GB\n",
      "Epoch 1, Batch 44670, Loss: 4.0830, Time: 26581.8s, Step: 44671, GPU: 4.9GB\n",
      "Epoch 1, Batch 44675, Loss: 3.7645, Time: 26585.2s, Step: 44676, GPU: 4.9GB\n",
      "Epoch 1, Batch 44680, Loss: 3.7390, Time: 26587.7s, Step: 44681, GPU: 4.9GB\n",
      "Epoch 1, Batch 44685, Loss: 3.3488, Time: 26591.1s, Step: 44686, GPU: 4.9GB\n",
      "Epoch 1, Batch 44690, Loss: 2.8720, Time: 26593.7s, Step: 44691, GPU: 4.9GB\n",
      "Epoch 1, Batch 44695, Loss: 3.5103, Time: 26597.1s, Step: 44696, GPU: 4.9GB\n",
      "Epoch 1, Batch 44700, Loss: 2.7614, Time: 26599.6s, Step: 44701, GPU: 4.9GB\n",
      "Epoch 1, Batch 44705, Loss: 3.1277, Time: 26603.0s, Step: 44706, GPU: 4.9GB\n",
      "Epoch 1, Batch 44710, Loss: 2.7750, Time: 26605.5s, Step: 44711, GPU: 4.9GB\n",
      "Epoch 1, Batch 44715, Loss: 2.5395, Time: 26609.0s, Step: 44716, GPU: 4.9GB\n",
      "Epoch 1, Batch 44720, Loss: 3.1244, Time: 26611.5s, Step: 44721, GPU: 4.9GB\n",
      "Epoch 1, Batch 44725, Loss: 2.5596, Time: 26614.9s, Step: 44726, GPU: 4.9GB\n",
      "Epoch 1, Batch 44730, Loss: 3.4435, Time: 26617.5s, Step: 44731, GPU: 4.9GB\n",
      "Epoch 1, Batch 44735, Loss: 3.6329, Time: 26620.9s, Step: 44736, GPU: 4.9GB\n",
      "Epoch 1, Batch 44740, Loss: 3.2636, Time: 26623.4s, Step: 44741, GPU: 4.9GB\n",
      "Epoch 1, Batch 44745, Loss: 2.4610, Time: 26626.8s, Step: 44746, GPU: 4.9GB\n",
      "Epoch 1, Batch 44750, Loss: 3.0144, Time: 26629.4s, Step: 44751, GPU: 4.9GB\n",
      "Epoch 1, Batch 44755, Loss: 3.5321, Time: 26632.8s, Step: 44756, GPU: 4.9GB\n",
      "Epoch 1, Batch 44760, Loss: 3.6033, Time: 26635.3s, Step: 44761, GPU: 4.9GB\n",
      "Epoch 1, Batch 44765, Loss: 3.2258, Time: 26638.8s, Step: 44766, GPU: 4.9GB\n",
      "Epoch 1, Batch 44770, Loss: 3.1516, Time: 26641.3s, Step: 44771, GPU: 4.9GB\n",
      "Epoch 1, Batch 44775, Loss: 3.2882, Time: 26644.7s, Step: 44776, GPU: 4.9GB\n",
      "Epoch 1, Batch 44780, Loss: 2.8796, Time: 26647.2s, Step: 44781, GPU: 4.9GB\n",
      "Epoch 1, Batch 44785, Loss: 2.5699, Time: 26650.6s, Step: 44786, GPU: 4.9GB\n",
      "Epoch 1, Batch 44790, Loss: 2.9468, Time: 26653.2s, Step: 44791, GPU: 4.9GB\n",
      "Epoch 1, Batch 44795, Loss: 2.3641, Time: 26656.6s, Step: 44796, GPU: 4.9GB\n",
      "Epoch 1, Batch 44800, Loss: 3.0314, Time: 26659.2s, Step: 44801, GPU: 4.9GB\n",
      "Epoch 1, Batch 44805, Loss: 2.9645, Time: 26662.6s, Step: 44806, GPU: 4.9GB\n",
      "Epoch 1, Batch 44810, Loss: 2.5852, Time: 26665.1s, Step: 44811, GPU: 4.9GB\n",
      "Epoch 1, Batch 44815, Loss: 3.3649, Time: 26668.6s, Step: 44816, GPU: 4.9GB\n",
      "Epoch 1, Batch 44820, Loss: 3.2213, Time: 26671.1s, Step: 44821, GPU: 4.9GB\n",
      "Epoch 1, Batch 44825, Loss: 3.0521, Time: 26674.5s, Step: 44826, GPU: 4.9GB\n",
      "Epoch 1, Batch 44830, Loss: 3.0169, Time: 26677.1s, Step: 44831, GPU: 4.9GB\n",
      "Epoch 1, Batch 44835, Loss: 3.4557, Time: 26680.5s, Step: 44836, GPU: 4.9GB\n",
      "Epoch 1, Batch 44840, Loss: 3.1171, Time: 26683.0s, Step: 44841, GPU: 4.9GB\n",
      "Epoch 1, Batch 44845, Loss: 3.2765, Time: 26686.4s, Step: 44846, GPU: 4.9GB\n",
      "Epoch 1, Batch 44850, Loss: 3.1874, Time: 26689.0s, Step: 44851, GPU: 4.9GB\n",
      "Epoch 1, Batch 44855, Loss: 2.6304, Time: 26692.4s, Step: 44856, GPU: 4.9GB\n",
      "Epoch 1, Batch 44860, Loss: 4.1995, Time: 26695.0s, Step: 44861, GPU: 4.9GB\n",
      "Epoch 1, Batch 44865, Loss: 3.6047, Time: 26698.4s, Step: 44866, GPU: 4.9GB\n",
      "Epoch 1, Batch 44870, Loss: 3.3689, Time: 26700.9s, Step: 44871, GPU: 4.9GB\n",
      "Epoch 1, Batch 44875, Loss: 3.2166, Time: 26704.3s, Step: 44876, GPU: 4.9GB\n",
      "Epoch 1, Batch 44880, Loss: 2.8896, Time: 26706.8s, Step: 44881, GPU: 4.9GB\n",
      "Epoch 1, Batch 44885, Loss: 3.8662, Time: 26710.2s, Step: 44886, GPU: 4.9GB\n",
      "Epoch 1, Batch 44890, Loss: 3.5102, Time: 26712.8s, Step: 44891, GPU: 4.9GB\n",
      "Epoch 1, Batch 44895, Loss: 3.0420, Time: 26716.2s, Step: 44896, GPU: 4.9GB\n",
      "Epoch 1, Batch 44900, Loss: 2.6682, Time: 26718.7s, Step: 44901, GPU: 4.9GB\n",
      "Epoch 1, Batch 44905, Loss: 3.1164, Time: 26722.1s, Step: 44906, GPU: 4.9GB\n",
      "Epoch 1, Batch 44910, Loss: 4.3419, Time: 26724.6s, Step: 44911, GPU: 4.9GB\n",
      "Epoch 1, Batch 44915, Loss: 4.7149, Time: 26728.0s, Step: 44916, GPU: 4.9GB\n",
      "Epoch 1, Batch 44920, Loss: 3.3275, Time: 26730.5s, Step: 44921, GPU: 4.9GB\n",
      "Epoch 1, Batch 44925, Loss: 3.2244, Time: 26734.0s, Step: 44926, GPU: 4.9GB\n",
      "Epoch 1, Batch 44930, Loss: 3.1121, Time: 26736.5s, Step: 44931, GPU: 4.9GB\n",
      "Epoch 1, Batch 44935, Loss: 4.1367, Time: 26739.9s, Step: 44936, GPU: 4.9GB\n",
      "Epoch 1, Batch 44940, Loss: 3.0740, Time: 26742.4s, Step: 44941, GPU: 4.9GB\n",
      "Epoch 1, Batch 44945, Loss: 2.6627, Time: 26745.9s, Step: 44946, GPU: 4.9GB\n",
      "Epoch 1, Batch 44950, Loss: 2.6067, Time: 26748.5s, Step: 44951, GPU: 4.9GB\n",
      "Epoch 1, Batch 44955, Loss: 3.2048, Time: 26751.9s, Step: 44956, GPU: 4.9GB\n",
      "Epoch 1, Batch 44960, Loss: 3.2857, Time: 26754.4s, Step: 44961, GPU: 4.9GB\n",
      "Epoch 1, Batch 44965, Loss: 3.2395, Time: 26757.8s, Step: 44966, GPU: 4.9GB\n",
      "Epoch 1, Batch 44970, Loss: 3.0848, Time: 26760.4s, Step: 44971, GPU: 4.9GB\n",
      "Epoch 1, Batch 44975, Loss: 3.8072, Time: 26763.8s, Step: 44976, GPU: 4.9GB\n",
      "Epoch 1, Batch 44980, Loss: 4.0306, Time: 26766.3s, Step: 44981, GPU: 4.9GB\n",
      "Epoch 1, Batch 44985, Loss: 3.6405, Time: 26769.7s, Step: 44986, GPU: 4.9GB\n",
      "Epoch 1, Batch 44990, Loss: 3.6838, Time: 26772.2s, Step: 44991, GPU: 4.9GB\n",
      "Epoch 1, Batch 44995, Loss: 3.0913, Time: 26775.7s, Step: 44996, GPU: 4.9GB\n",
      "Epoch 1, Batch 45000, Loss: 2.9293, Time: 26778.3s, Step: 45001, GPU: 4.9GB\n",
      "Epoch 1, Batch 45005, Loss: 4.3946, Time: 26781.7s, Step: 45006, GPU: 4.9GB\n",
      "Epoch 1, Batch 45010, Loss: 3.2121, Time: 26784.2s, Step: 45011, GPU: 4.9GB\n",
      "Epoch 1, Batch 45015, Loss: 3.6022, Time: 26787.6s, Step: 45016, GPU: 4.9GB\n",
      "Epoch 1, Batch 45020, Loss: 4.0046, Time: 26790.1s, Step: 45021, GPU: 4.9GB\n",
      "Epoch 1, Batch 45025, Loss: 3.7881, Time: 26793.5s, Step: 45026, GPU: 4.9GB\n",
      "Epoch 1, Batch 45030, Loss: 3.1923, Time: 26796.0s, Step: 45031, GPU: 4.9GB\n",
      "Epoch 1, Batch 45035, Loss: 4.2387, Time: 26799.5s, Step: 45036, GPU: 4.9GB\n",
      "Epoch 1, Batch 45040, Loss: 3.1133, Time: 26802.0s, Step: 45041, GPU: 4.9GB\n",
      "Epoch 1, Batch 45045, Loss: 2.5030, Time: 26805.4s, Step: 45046, GPU: 4.9GB\n",
      "Epoch 1, Batch 45050, Loss: 2.3057, Time: 26807.9s, Step: 45051, GPU: 4.9GB\n",
      "Epoch 1, Batch 45055, Loss: 3.0893, Time: 26811.4s, Step: 45056, GPU: 4.9GB\n",
      "Epoch 1, Batch 45060, Loss: 3.6901, Time: 26813.9s, Step: 45061, GPU: 4.9GB\n",
      "Epoch 1, Batch 45065, Loss: 2.8877, Time: 26817.3s, Step: 45066, GPU: 4.9GB\n",
      "Epoch 1, Batch 45070, Loss: 3.4197, Time: 26819.9s, Step: 45071, GPU: 4.9GB\n",
      "Epoch 1, Batch 45075, Loss: 3.0176, Time: 26823.3s, Step: 45076, GPU: 4.9GB\n",
      "Epoch 1, Batch 45080, Loss: 4.1673, Time: 26825.8s, Step: 45081, GPU: 4.9GB\n",
      "Epoch 1, Batch 45085, Loss: 2.9467, Time: 26829.2s, Step: 45086, GPU: 4.9GB\n",
      "Epoch 1, Batch 45090, Loss: 3.9820, Time: 26831.8s, Step: 45091, GPU: 4.9GB\n",
      "Epoch 1, Batch 45095, Loss: 3.0972, Time: 26835.2s, Step: 45096, GPU: 4.9GB\n",
      "Epoch 1, Batch 45100, Loss: 2.9945, Time: 26837.7s, Step: 45101, GPU: 4.9GB\n",
      "Epoch 1, Batch 45105, Loss: 3.7944, Time: 26841.0s, Step: 45106, GPU: 4.9GB\n",
      "Epoch 1, Batch 45110, Loss: 2.9807, Time: 26843.6s, Step: 45111, GPU: 4.9GB\n",
      "Epoch 1, Batch 45115, Loss: 2.5224, Time: 26847.0s, Step: 45116, GPU: 4.9GB\n",
      "Epoch 1, Batch 45120, Loss: 3.2547, Time: 26849.5s, Step: 45121, GPU: 4.9GB\n",
      "Epoch 1, Batch 45125, Loss: 3.1856, Time: 26852.9s, Step: 45126, GPU: 4.9GB\n",
      "Epoch 1, Batch 45130, Loss: 3.5780, Time: 26855.4s, Step: 45131, GPU: 4.9GB\n",
      "Epoch 1, Batch 45135, Loss: 3.2595, Time: 26858.9s, Step: 45136, GPU: 4.9GB\n",
      "Epoch 1, Batch 45140, Loss: 2.9106, Time: 26861.4s, Step: 45141, GPU: 4.9GB\n",
      "Epoch 1, Batch 45145, Loss: 2.8112, Time: 26864.8s, Step: 45146, GPU: 4.9GB\n",
      "Epoch 1, Batch 45150, Loss: 4.3915, Time: 26867.3s, Step: 45151, GPU: 4.9GB\n",
      "Epoch 1, Batch 45155, Loss: 3.3350, Time: 26870.7s, Step: 45156, GPU: 4.9GB\n",
      "Epoch 1, Batch 45160, Loss: 2.2941, Time: 26873.3s, Step: 45161, GPU: 4.9GB\n",
      "Epoch 1, Batch 45165, Loss: 3.2628, Time: 26876.7s, Step: 45166, GPU: 4.9GB\n",
      "Epoch 1, Batch 45170, Loss: 4.2603, Time: 26879.2s, Step: 45171, GPU: 4.9GB\n",
      "Epoch 1, Batch 45175, Loss: 4.1609, Time: 26882.6s, Step: 45176, GPU: 4.9GB\n",
      "Epoch 1, Batch 45180, Loss: 3.6498, Time: 26885.1s, Step: 45181, GPU: 4.9GB\n",
      "Epoch 1, Batch 45185, Loss: 3.4993, Time: 26888.5s, Step: 45186, GPU: 4.9GB\n",
      "Epoch 1, Batch 45190, Loss: 2.4648, Time: 26891.0s, Step: 45191, GPU: 4.9GB\n",
      "Epoch 1, Batch 45195, Loss: 2.7317, Time: 26894.4s, Step: 45196, GPU: 4.9GB\n",
      "Epoch 1, Batch 45200, Loss: 2.6407, Time: 26897.0s, Step: 45201, GPU: 4.9GB\n",
      "Epoch 1, Batch 45205, Loss: 2.8748, Time: 26900.4s, Step: 45206, GPU: 4.9GB\n",
      "Epoch 1, Batch 45210, Loss: 3.4080, Time: 26902.9s, Step: 45211, GPU: 4.9GB\n",
      "Epoch 1, Batch 45215, Loss: 3.3452, Time: 26906.3s, Step: 45216, GPU: 4.9GB\n",
      "Epoch 1, Batch 45220, Loss: 2.5466, Time: 26908.8s, Step: 45221, GPU: 4.9GB\n",
      "Epoch 1, Batch 45225, Loss: 3.1791, Time: 26912.2s, Step: 45226, GPU: 4.9GB\n",
      "Epoch 1, Batch 45230, Loss: 3.1412, Time: 26914.7s, Step: 45231, GPU: 4.9GB\n",
      "Epoch 1, Batch 45235, Loss: 3.6135, Time: 26918.2s, Step: 45236, GPU: 4.9GB\n",
      "Epoch 1, Batch 45240, Loss: 2.8624, Time: 26920.7s, Step: 45241, GPU: 4.9GB\n",
      "Epoch 1, Batch 45245, Loss: 3.3852, Time: 26924.1s, Step: 45246, GPU: 4.9GB\n",
      "Epoch 1, Batch 45250, Loss: 3.0790, Time: 26926.6s, Step: 45251, GPU: 4.9GB\n",
      "Epoch 1, Batch 45255, Loss: 4.4271, Time: 26930.0s, Step: 45256, GPU: 4.9GB\n",
      "Epoch 1, Batch 45260, Loss: 3.1402, Time: 26932.6s, Step: 45261, GPU: 4.9GB\n",
      "Epoch 1, Batch 45265, Loss: 2.5796, Time: 26936.0s, Step: 45266, GPU: 4.9GB\n",
      "Epoch 1, Batch 45270, Loss: 3.5857, Time: 26938.5s, Step: 45271, GPU: 4.9GB\n",
      "Epoch 1, Batch 45275, Loss: 3.1565, Time: 26942.0s, Step: 45276, GPU: 4.9GB\n",
      "Epoch 1, Batch 45280, Loss: 3.6236, Time: 26944.5s, Step: 45281, GPU: 4.9GB\n",
      "Epoch 1, Batch 45285, Loss: 3.8216, Time: 26947.9s, Step: 45286, GPU: 4.9GB\n",
      "Epoch 1, Batch 45290, Loss: 3.2603, Time: 26950.4s, Step: 45291, GPU: 4.9GB\n",
      "Epoch 1, Batch 45295, Loss: 3.6759, Time: 26953.8s, Step: 45296, GPU: 4.9GB\n",
      "Epoch 1, Batch 45300, Loss: 3.8540, Time: 26956.4s, Step: 45301, GPU: 4.9GB\n",
      "Epoch 1, Batch 45305, Loss: 3.0405, Time: 26959.8s, Step: 45306, GPU: 4.9GB\n",
      "Epoch 1, Batch 45310, Loss: 3.3156, Time: 26962.3s, Step: 45311, GPU: 4.9GB\n",
      "Epoch 1, Batch 45315, Loss: 2.9721, Time: 26965.7s, Step: 45316, GPU: 4.9GB\n",
      "Epoch 1, Batch 45320, Loss: 3.1320, Time: 26968.3s, Step: 45321, GPU: 4.9GB\n",
      "Epoch 1, Batch 45325, Loss: 3.2228, Time: 26971.6s, Step: 45326, GPU: 4.9GB\n",
      "Epoch 1, Batch 45330, Loss: 3.0747, Time: 26974.1s, Step: 45331, GPU: 4.9GB\n",
      "Epoch 1, Batch 45335, Loss: 3.3059, Time: 26977.5s, Step: 45336, GPU: 4.9GB\n",
      "Epoch 1, Batch 45340, Loss: 3.6754, Time: 26980.0s, Step: 45341, GPU: 4.9GB\n",
      "Epoch 1, Batch 45345, Loss: 2.7474, Time: 26983.4s, Step: 45346, GPU: 4.9GB\n",
      "Epoch 1, Batch 45350, Loss: 3.2074, Time: 26985.9s, Step: 45351, GPU: 4.9GB\n",
      "Epoch 1, Batch 45355, Loss: 3.5648, Time: 26989.3s, Step: 45356, GPU: 4.9GB\n",
      "Epoch 1, Batch 45360, Loss: 3.4836, Time: 26991.8s, Step: 45361, GPU: 4.9GB\n",
      "Epoch 1, Batch 45365, Loss: 3.0600, Time: 26995.3s, Step: 45366, GPU: 4.9GB\n",
      "Epoch 1, Batch 45370, Loss: 3.4763, Time: 26997.8s, Step: 45371, GPU: 4.9GB\n",
      "Epoch 1, Batch 45375, Loss: 3.4493, Time: 27001.2s, Step: 45376, GPU: 4.9GB\n",
      "Epoch 1, Batch 45380, Loss: 2.5623, Time: 27003.7s, Step: 45381, GPU: 4.9GB\n",
      "Epoch 1, Batch 45385, Loss: 1.9328, Time: 27007.1s, Step: 45386, GPU: 4.9GB\n",
      "Epoch 1, Batch 45390, Loss: 4.6044, Time: 27009.6s, Step: 45391, GPU: 4.9GB\n",
      "Epoch 1, Batch 45395, Loss: 3.1288, Time: 27013.1s, Step: 45396, GPU: 4.9GB\n",
      "Epoch 1, Batch 45400, Loss: 3.3372, Time: 27015.7s, Step: 45401, GPU: 4.9GB\n",
      "Epoch 1, Batch 45405, Loss: 4.1018, Time: 27019.1s, Step: 45406, GPU: 4.9GB\n",
      "Epoch 1, Batch 45410, Loss: 3.5363, Time: 27021.7s, Step: 45411, GPU: 4.9GB\n",
      "Epoch 1, Batch 45415, Loss: 2.8669, Time: 27025.1s, Step: 45416, GPU: 4.9GB\n",
      "Epoch 1, Batch 45420, Loss: 3.5504, Time: 27027.6s, Step: 45421, GPU: 4.9GB\n",
      "Epoch 1, Batch 45425, Loss: 3.7747, Time: 27031.0s, Step: 45426, GPU: 4.9GB\n",
      "Epoch 1, Batch 45430, Loss: 2.3935, Time: 27033.6s, Step: 45431, GPU: 4.9GB\n",
      "Epoch 1, Batch 45435, Loss: 3.0056, Time: 27037.0s, Step: 45436, GPU: 4.9GB\n",
      "Epoch 1, Batch 45440, Loss: 3.2314, Time: 27039.5s, Step: 45441, GPU: 4.9GB\n",
      "Epoch 1, Batch 45445, Loss: 3.5565, Time: 27043.0s, Step: 45446, GPU: 4.9GB\n",
      "Epoch 1, Batch 45450, Loss: 2.8171, Time: 27045.5s, Step: 45451, GPU: 4.9GB\n",
      "Epoch 1, Batch 45455, Loss: 4.3608, Time: 27048.9s, Step: 45456, GPU: 4.9GB\n",
      "Epoch 1, Batch 45460, Loss: 2.1517, Time: 27051.4s, Step: 45461, GPU: 4.9GB\n",
      "Epoch 1, Batch 45465, Loss: 2.8484, Time: 27054.9s, Step: 45466, GPU: 4.9GB\n",
      "Epoch 1, Batch 45470, Loss: 2.9026, Time: 27057.4s, Step: 45471, GPU: 4.9GB\n",
      "Epoch 1, Batch 45475, Loss: 3.1074, Time: 27060.8s, Step: 45476, GPU: 4.9GB\n",
      "Epoch 1, Batch 45480, Loss: 3.1877, Time: 27063.3s, Step: 45481, GPU: 4.9GB\n",
      "Epoch 1, Batch 45485, Loss: 3.0783, Time: 27066.7s, Step: 45486, GPU: 4.9GB\n",
      "Epoch 1, Batch 45490, Loss: 3.5357, Time: 27069.3s, Step: 45491, GPU: 4.9GB\n",
      "Epoch 1, Batch 45495, Loss: 2.9835, Time: 27072.7s, Step: 45496, GPU: 4.9GB\n",
      "Epoch 1, Batch 45500, Loss: 2.9817, Time: 27075.2s, Step: 45501, GPU: 4.9GB\n",
      "Epoch 1, Batch 45505, Loss: 3.6345, Time: 27078.6s, Step: 45506, GPU: 4.9GB\n",
      "Epoch 1, Batch 45510, Loss: 3.0275, Time: 27081.1s, Step: 45511, GPU: 4.9GB\n",
      "Epoch 1, Batch 45515, Loss: 3.0204, Time: 27084.6s, Step: 45516, GPU: 4.9GB\n",
      "Epoch 1, Batch 45520, Loss: 3.1953, Time: 27087.1s, Step: 45521, GPU: 4.9GB\n",
      "Epoch 1, Batch 45525, Loss: 2.7978, Time: 27090.5s, Step: 45526, GPU: 4.9GB\n",
      "Epoch 1, Batch 45530, Loss: 3.1177, Time: 27093.1s, Step: 45531, GPU: 4.9GB\n",
      "Epoch 1, Batch 45535, Loss: 3.1148, Time: 27096.5s, Step: 45536, GPU: 4.9GB\n",
      "Epoch 1, Batch 45540, Loss: 3.5598, Time: 27099.0s, Step: 45541, GPU: 4.9GB\n",
      "Epoch 1, Batch 45545, Loss: 3.1339, Time: 27102.4s, Step: 45546, GPU: 4.9GB\n",
      "Epoch 1, Batch 45550, Loss: 2.9905, Time: 27104.9s, Step: 45551, GPU: 4.9GB\n",
      "Epoch 1, Batch 45555, Loss: 3.0069, Time: 27108.3s, Step: 45556, GPU: 4.9GB\n",
      "Epoch 1, Batch 45560, Loss: 3.0456, Time: 27110.9s, Step: 45561, GPU: 4.9GB\n",
      "Epoch 1, Batch 45565, Loss: 2.9726, Time: 27114.3s, Step: 45566, GPU: 4.9GB\n",
      "Epoch 1, Batch 45570, Loss: 3.5181, Time: 27116.9s, Step: 45571, GPU: 4.9GB\n",
      "Epoch 1, Batch 45575, Loss: 3.6196, Time: 27120.3s, Step: 45576, GPU: 4.9GB\n",
      "Epoch 1, Batch 45580, Loss: 3.5603, Time: 27122.8s, Step: 45581, GPU: 4.9GB\n",
      "Epoch 1, Batch 45585, Loss: 3.3444, Time: 27126.2s, Step: 45586, GPU: 4.9GB\n",
      "Epoch 1, Batch 45590, Loss: 4.0605, Time: 27128.7s, Step: 45591, GPU: 4.9GB\n",
      "Epoch 1, Batch 45595, Loss: 3.1775, Time: 27132.1s, Step: 45596, GPU: 4.9GB\n",
      "Epoch 1, Batch 45600, Loss: 3.7566, Time: 27134.7s, Step: 45601, GPU: 4.9GB\n",
      "Epoch 1, Batch 45605, Loss: 3.5715, Time: 27138.1s, Step: 45606, GPU: 4.9GB\n",
      "Epoch 1, Batch 45610, Loss: 3.5351, Time: 27140.6s, Step: 45611, GPU: 4.9GB\n",
      "Epoch 1, Batch 45615, Loss: 2.6511, Time: 27144.1s, Step: 45616, GPU: 4.9GB\n",
      "Epoch 1, Batch 45620, Loss: 2.3214, Time: 27146.6s, Step: 45621, GPU: 4.9GB\n",
      "Epoch 1, Batch 45625, Loss: 3.2652, Time: 27150.0s, Step: 45626, GPU: 4.9GB\n",
      "Epoch 1, Batch 45630, Loss: 3.1165, Time: 27152.5s, Step: 45631, GPU: 4.9GB\n",
      "Epoch 1, Batch 45635, Loss: 3.4925, Time: 27155.9s, Step: 45636, GPU: 4.9GB\n",
      "Epoch 1, Batch 45640, Loss: 3.4163, Time: 27158.5s, Step: 45641, GPU: 4.9GB\n",
      "Epoch 1, Batch 45645, Loss: 2.0010, Time: 27161.9s, Step: 45646, GPU: 4.9GB\n",
      "Epoch 1, Batch 45650, Loss: 2.9875, Time: 27164.4s, Step: 45651, GPU: 4.9GB\n",
      "Epoch 1, Batch 45655, Loss: 2.8202, Time: 27167.8s, Step: 45656, GPU: 4.9GB\n",
      "Epoch 1, Batch 45660, Loss: 3.0326, Time: 27170.3s, Step: 45661, GPU: 4.9GB\n",
      "Epoch 1, Batch 45665, Loss: 3.7685, Time: 27173.7s, Step: 45666, GPU: 4.9GB\n",
      "Epoch 1, Batch 45670, Loss: 3.2157, Time: 27176.2s, Step: 45671, GPU: 4.9GB\n",
      "Epoch 1, Batch 45675, Loss: 2.7080, Time: 27179.7s, Step: 45676, GPU: 4.9GB\n",
      "Epoch 1, Batch 45680, Loss: 3.9031, Time: 27182.2s, Step: 45681, GPU: 4.9GB\n",
      "Epoch 1, Batch 45685, Loss: 3.1588, Time: 27185.6s, Step: 45686, GPU: 4.9GB\n",
      "Epoch 1, Batch 45690, Loss: 3.0519, Time: 27188.1s, Step: 45691, GPU: 4.9GB\n",
      "Epoch 1, Batch 45695, Loss: 3.3198, Time: 27191.6s, Step: 45696, GPU: 4.9GB\n",
      "Epoch 1, Batch 45700, Loss: 3.0769, Time: 27194.1s, Step: 45701, GPU: 4.9GB\n",
      "Epoch 1, Batch 45705, Loss: 3.8086, Time: 27197.5s, Step: 45706, GPU: 4.9GB\n",
      "Epoch 1, Batch 45710, Loss: 3.5743, Time: 27200.1s, Step: 45711, GPU: 4.9GB\n",
      "Epoch 1, Batch 45715, Loss: 3.3386, Time: 27203.5s, Step: 45716, GPU: 4.9GB\n",
      "Epoch 1, Batch 45720, Loss: 2.5835, Time: 27206.0s, Step: 45721, GPU: 4.9GB\n",
      "Epoch 1, Batch 45725, Loss: 3.0643, Time: 27209.5s, Step: 45726, GPU: 4.9GB\n",
      "Epoch 1, Batch 45730, Loss: 3.4843, Time: 27212.0s, Step: 45731, GPU: 4.9GB\n",
      "Epoch 1, Batch 45735, Loss: 3.1946, Time: 27215.4s, Step: 45736, GPU: 4.9GB\n",
      "Epoch 1, Batch 45740, Loss: 3.5363, Time: 27218.0s, Step: 45741, GPU: 4.9GB\n",
      "Epoch 1, Batch 45745, Loss: 3.5817, Time: 27221.4s, Step: 45746, GPU: 4.9GB\n",
      "Epoch 1, Batch 45750, Loss: 2.4728, Time: 27223.9s, Step: 45751, GPU: 4.9GB\n",
      "Epoch 1, Batch 45755, Loss: 2.8175, Time: 27227.4s, Step: 45756, GPU: 4.9GB\n",
      "Epoch 1, Batch 45760, Loss: 3.5268, Time: 27229.9s, Step: 45761, GPU: 4.9GB\n",
      "Epoch 1, Batch 45765, Loss: 3.1159, Time: 27233.3s, Step: 45766, GPU: 4.9GB\n",
      "Epoch 1, Batch 45770, Loss: 3.9994, Time: 27235.9s, Step: 45771, GPU: 4.9GB\n",
      "Epoch 1, Batch 45775, Loss: 4.7020, Time: 27239.3s, Step: 45776, GPU: 4.9GB\n",
      "Epoch 1, Batch 45780, Loss: 3.2801, Time: 27241.8s, Step: 45781, GPU: 4.9GB\n",
      "Epoch 1, Batch 45785, Loss: 2.9816, Time: 27245.2s, Step: 45786, GPU: 4.9GB\n",
      "Epoch 1, Batch 45790, Loss: 2.5441, Time: 27247.8s, Step: 45791, GPU: 4.9GB\n",
      "Epoch 1, Batch 45795, Loss: 2.5392, Time: 27251.2s, Step: 45796, GPU: 4.9GB\n",
      "Epoch 1, Batch 45800, Loss: 3.6528, Time: 27253.8s, Step: 45801, GPU: 4.9GB\n",
      "Epoch 1, Batch 45805, Loss: 2.7799, Time: 27257.2s, Step: 45806, GPU: 4.9GB\n",
      "Epoch 1, Batch 45810, Loss: 2.8456, Time: 27259.7s, Step: 45811, GPU: 4.9GB\n",
      "Epoch 1, Batch 45815, Loss: 3.9197, Time: 27263.1s, Step: 45816, GPU: 4.9GB\n",
      "Epoch 1, Batch 45820, Loss: 3.5643, Time: 27265.6s, Step: 45821, GPU: 4.9GB\n",
      "Epoch 1, Batch 45825, Loss: 3.3565, Time: 27269.0s, Step: 45826, GPU: 4.9GB\n",
      "Epoch 1, Batch 45830, Loss: 2.6544, Time: 27271.5s, Step: 45831, GPU: 4.9GB\n",
      "Epoch 1, Batch 45835, Loss: 3.3151, Time: 27274.9s, Step: 45836, GPU: 4.9GB\n",
      "Epoch 1, Batch 45840, Loss: 3.2996, Time: 27277.5s, Step: 45841, GPU: 4.9GB\n",
      "Epoch 1, Batch 45845, Loss: 3.1643, Time: 27280.9s, Step: 45846, GPU: 4.9GB\n",
      "Epoch 1, Batch 45850, Loss: 3.3255, Time: 27283.4s, Step: 45851, GPU: 4.9GB\n",
      "Epoch 1, Batch 45855, Loss: 2.5640, Time: 27286.8s, Step: 45856, GPU: 4.9GB\n",
      "Epoch 1, Batch 45860, Loss: 3.7144, Time: 27289.3s, Step: 45861, GPU: 4.9GB\n",
      "Epoch 1, Batch 45865, Loss: 2.9042, Time: 27292.7s, Step: 45866, GPU: 4.9GB\n",
      "Epoch 1, Batch 45870, Loss: 3.2934, Time: 27295.2s, Step: 45871, GPU: 4.9GB\n",
      "Epoch 1, Batch 45875, Loss: 4.1052, Time: 27298.7s, Step: 45876, GPU: 4.9GB\n",
      "Epoch 1, Batch 45880, Loss: 3.4998, Time: 27301.2s, Step: 45881, GPU: 4.9GB\n",
      "Epoch 1, Batch 45885, Loss: 3.3420, Time: 27304.6s, Step: 45886, GPU: 4.9GB\n",
      "Epoch 1, Batch 45890, Loss: 1.6108, Time: 27307.1s, Step: 45891, GPU: 4.9GB\n",
      "Epoch 1, Batch 45895, Loss: 3.7436, Time: 27310.5s, Step: 45896, GPU: 4.9GB\n",
      "Epoch 1, Batch 45900, Loss: 2.8925, Time: 27313.0s, Step: 45901, GPU: 4.9GB\n",
      "Epoch 1, Batch 45905, Loss: 2.7873, Time: 27316.4s, Step: 45906, GPU: 4.9GB\n",
      "Epoch 1, Batch 45910, Loss: 3.0691, Time: 27319.0s, Step: 45911, GPU: 4.9GB\n",
      "Epoch 1, Batch 45915, Loss: 2.8486, Time: 27322.4s, Step: 45916, GPU: 4.9GB\n",
      "Epoch 1, Batch 45920, Loss: 3.6515, Time: 27324.9s, Step: 45921, GPU: 4.9GB\n",
      "Epoch 1, Batch 45925, Loss: 3.3825, Time: 27328.3s, Step: 45926, GPU: 4.9GB\n",
      "Epoch 1, Batch 45930, Loss: 3.0618, Time: 27330.8s, Step: 45931, GPU: 4.9GB\n",
      "Epoch 1, Batch 45935, Loss: 2.9710, Time: 27334.2s, Step: 45936, GPU: 4.9GB\n",
      "Epoch 1, Batch 45940, Loss: 3.7473, Time: 27336.8s, Step: 45941, GPU: 4.9GB\n",
      "Epoch 1, Batch 45945, Loss: 4.7322, Time: 27340.2s, Step: 45946, GPU: 4.9GB\n",
      "Epoch 1, Batch 45950, Loss: 3.3785, Time: 27342.7s, Step: 45951, GPU: 4.9GB\n",
      "Epoch 1, Batch 45955, Loss: 3.2903, Time: 27346.1s, Step: 45956, GPU: 4.9GB\n",
      "Epoch 1, Batch 45960, Loss: 4.1418, Time: 27348.6s, Step: 45961, GPU: 4.9GB\n",
      "Epoch 1, Batch 45965, Loss: 2.8037, Time: 27352.1s, Step: 45966, GPU: 4.9GB\n",
      "Epoch 1, Batch 45970, Loss: 2.6201, Time: 27354.6s, Step: 45971, GPU: 4.9GB\n",
      "Epoch 1, Batch 45975, Loss: 3.3101, Time: 27358.0s, Step: 45976, GPU: 4.9GB\n",
      "Epoch 1, Batch 45980, Loss: 2.9912, Time: 27360.5s, Step: 45981, GPU: 4.9GB\n",
      "Epoch 1, Batch 45985, Loss: 3.6048, Time: 27363.9s, Step: 45986, GPU: 4.9GB\n",
      "Epoch 1, Batch 45990, Loss: 2.5489, Time: 27366.5s, Step: 45991, GPU: 4.9GB\n",
      "Epoch 1, Batch 45995, Loss: 3.2644, Time: 27369.9s, Step: 45996, GPU: 4.9GB\n",
      "Epoch 1, Batch 46000, Loss: 2.4898, Time: 27372.5s, Step: 46001, GPU: 4.9GB\n",
      "Epoch 1, Batch 46005, Loss: 3.0187, Time: 27375.9s, Step: 46006, GPU: 4.9GB\n",
      "Epoch 1, Batch 46010, Loss: 3.4726, Time: 27378.4s, Step: 46011, GPU: 4.9GB\n",
      "Epoch 1, Batch 46015, Loss: 3.2408, Time: 27381.8s, Step: 46016, GPU: 4.9GB\n",
      "Epoch 1, Batch 46020, Loss: 2.5497, Time: 27384.3s, Step: 46021, GPU: 4.9GB\n",
      "Epoch 1, Batch 46025, Loss: 2.5628, Time: 27387.7s, Step: 46026, GPU: 4.9GB\n",
      "Epoch 1, Batch 46030, Loss: 3.3661, Time: 27390.2s, Step: 46031, GPU: 4.9GB\n",
      "Epoch 1, Batch 46035, Loss: 3.0113, Time: 27393.6s, Step: 46036, GPU: 4.9GB\n",
      "Epoch 1, Batch 46040, Loss: 2.7076, Time: 27396.2s, Step: 46041, GPU: 4.9GB\n",
      "Epoch 1, Batch 46045, Loss: 2.6986, Time: 27399.6s, Step: 46046, GPU: 4.9GB\n",
      "Epoch 1, Batch 46050, Loss: 3.2707, Time: 27402.1s, Step: 46051, GPU: 4.9GB\n",
      "Epoch 1, Batch 46055, Loss: 3.1252, Time: 27405.5s, Step: 46056, GPU: 4.9GB\n",
      "Epoch 1, Batch 46060, Loss: 3.2346, Time: 27408.0s, Step: 46061, GPU: 4.9GB\n",
      "Epoch 1, Batch 46065, Loss: 3.7089, Time: 27411.4s, Step: 46066, GPU: 4.9GB\n",
      "Epoch 1, Batch 46070, Loss: 3.6456, Time: 27414.0s, Step: 46071, GPU: 4.9GB\n",
      "Epoch 1, Batch 46075, Loss: 2.8472, Time: 27417.4s, Step: 46076, GPU: 4.9GB\n",
      "Epoch 1, Batch 46080, Loss: 2.5017, Time: 27419.9s, Step: 46081, GPU: 4.9GB\n",
      "Epoch 1, Batch 46085, Loss: 3.5720, Time: 27423.3s, Step: 46086, GPU: 4.9GB\n",
      "Epoch 1, Batch 46090, Loss: 2.8370, Time: 27425.8s, Step: 46091, GPU: 4.9GB\n",
      "Epoch 1, Batch 46095, Loss: 4.1790, Time: 27429.2s, Step: 46096, GPU: 4.9GB\n",
      "Epoch 1, Batch 46100, Loss: 3.0678, Time: 27431.7s, Step: 46101, GPU: 4.9GB\n",
      "Epoch 1, Batch 46105, Loss: 3.3789, Time: 27435.1s, Step: 46106, GPU: 4.9GB\n",
      "Epoch 1, Batch 46110, Loss: 3.4453, Time: 27437.6s, Step: 46111, GPU: 4.9GB\n",
      "Epoch 1, Batch 46115, Loss: 3.1879, Time: 27441.1s, Step: 46116, GPU: 4.9GB\n",
      "Epoch 1, Batch 46120, Loss: 3.1710, Time: 27443.6s, Step: 46121, GPU: 4.9GB\n",
      "Epoch 1, Batch 46125, Loss: 2.6626, Time: 27447.0s, Step: 46126, GPU: 4.9GB\n",
      "Epoch 1, Batch 46130, Loss: 3.5455, Time: 27449.5s, Step: 46131, GPU: 4.9GB\n",
      "Epoch 1, Batch 46135, Loss: 2.9247, Time: 27453.0s, Step: 46136, GPU: 4.9GB\n",
      "Epoch 1, Batch 46140, Loss: 4.3277, Time: 27455.5s, Step: 46141, GPU: 4.9GB\n",
      "Epoch 1, Batch 46145, Loss: 2.6026, Time: 27459.0s, Step: 46146, GPU: 4.9GB\n",
      "Epoch 1, Batch 46150, Loss: 2.9837, Time: 27461.5s, Step: 46151, GPU: 4.9GB\n",
      "Epoch 1, Batch 46155, Loss: 2.5678, Time: 27464.9s, Step: 46156, GPU: 4.9GB\n",
      "Epoch 1, Batch 46160, Loss: 2.9401, Time: 27467.4s, Step: 46161, GPU: 4.9GB\n",
      "Epoch 1, Batch 46165, Loss: 3.7310, Time: 27470.9s, Step: 46166, GPU: 4.9GB\n",
      "Epoch 1, Batch 46170, Loss: 2.9655, Time: 27473.4s, Step: 46171, GPU: 4.9GB\n",
      "Epoch 1, Batch 46175, Loss: 3.4510, Time: 27476.8s, Step: 46176, GPU: 4.9GB\n",
      "Epoch 1, Batch 46180, Loss: 3.3864, Time: 27479.4s, Step: 46181, GPU: 4.9GB\n",
      "Epoch 1, Batch 46185, Loss: 2.7651, Time: 27482.7s, Step: 46186, GPU: 4.9GB\n",
      "Epoch 1, Batch 46190, Loss: 3.3283, Time: 27485.3s, Step: 46191, GPU: 4.9GB\n",
      "Epoch 1, Batch 46195, Loss: 2.7923, Time: 27488.7s, Step: 46196, GPU: 4.9GB\n",
      "Epoch 1, Batch 46200, Loss: 3.1895, Time: 27491.3s, Step: 46201, GPU: 4.9GB\n",
      "Epoch 1, Batch 46205, Loss: 2.4507, Time: 27494.7s, Step: 46206, GPU: 4.9GB\n",
      "Epoch 1, Batch 46210, Loss: 2.5052, Time: 27497.2s, Step: 46211, GPU: 4.9GB\n",
      "Epoch 1, Batch 46215, Loss: 3.6273, Time: 27500.7s, Step: 46216, GPU: 4.9GB\n",
      "Epoch 1, Batch 46220, Loss: 3.6702, Time: 27503.2s, Step: 46221, GPU: 4.9GB\n",
      "Epoch 1, Batch 46225, Loss: 3.3404, Time: 27506.6s, Step: 46226, GPU: 4.9GB\n",
      "Epoch 1, Batch 46230, Loss: 4.3195, Time: 27509.1s, Step: 46231, GPU: 4.9GB\n",
      "Epoch 1, Batch 46235, Loss: 3.4822, Time: 27512.5s, Step: 46236, GPU: 4.9GB\n",
      "Epoch 1, Batch 46240, Loss: 3.4965, Time: 27515.1s, Step: 46241, GPU: 4.9GB\n",
      "Epoch 1, Batch 46245, Loss: 3.2685, Time: 27518.5s, Step: 46246, GPU: 4.9GB\n",
      "Epoch 1, Batch 46250, Loss: 2.9645, Time: 27521.0s, Step: 46251, GPU: 4.9GB\n",
      "Epoch 1, Batch 46255, Loss: 2.6472, Time: 27524.4s, Step: 46256, GPU: 4.9GB\n",
      "Epoch 1, Batch 46260, Loss: 3.5724, Time: 27526.9s, Step: 46261, GPU: 4.9GB\n",
      "Epoch 1, Batch 46265, Loss: 2.3754, Time: 27530.3s, Step: 46266, GPU: 4.9GB\n",
      "Epoch 1, Batch 46270, Loss: 3.8123, Time: 27532.9s, Step: 46271, GPU: 4.9GB\n",
      "Epoch 1, Batch 46275, Loss: 2.7839, Time: 27536.3s, Step: 46276, GPU: 4.9GB\n",
      "Epoch 1, Batch 46280, Loss: 3.5668, Time: 27538.8s, Step: 46281, GPU: 4.9GB\n",
      "Epoch 1, Batch 46285, Loss: 2.8352, Time: 27542.2s, Step: 46286, GPU: 4.9GB\n",
      "Epoch 1, Batch 46290, Loss: 3.1503, Time: 27544.8s, Step: 46291, GPU: 4.9GB\n",
      "Epoch 1, Batch 46295, Loss: 3.3345, Time: 27548.2s, Step: 46296, GPU: 4.9GB\n",
      "Epoch 1, Batch 46300, Loss: 4.1009, Time: 27550.7s, Step: 46301, GPU: 4.9GB\n",
      "Epoch 1, Batch 46305, Loss: 3.2083, Time: 27554.1s, Step: 46306, GPU: 4.9GB\n",
      "Epoch 1, Batch 46310, Loss: 2.9324, Time: 27556.7s, Step: 46311, GPU: 4.9GB\n",
      "Epoch 1, Batch 46315, Loss: 2.7347, Time: 27560.1s, Step: 46316, GPU: 4.9GB\n",
      "Epoch 1, Batch 46320, Loss: 2.7370, Time: 27562.6s, Step: 46321, GPU: 4.9GB\n",
      "Epoch 1, Batch 46325, Loss: 3.0663, Time: 27566.0s, Step: 46326, GPU: 4.9GB\n",
      "Epoch 1, Batch 46330, Loss: 2.8757, Time: 27568.5s, Step: 46331, GPU: 4.9GB\n",
      "Epoch 1, Batch 46335, Loss: 3.7168, Time: 27571.9s, Step: 46336, GPU: 4.9GB\n",
      "Epoch 1, Batch 46340, Loss: 3.6932, Time: 27574.5s, Step: 46341, GPU: 4.9GB\n",
      "Epoch 1, Batch 46345, Loss: 2.8663, Time: 27577.9s, Step: 46346, GPU: 4.9GB\n",
      "Epoch 1, Batch 46350, Loss: 3.4161, Time: 27580.4s, Step: 46351, GPU: 4.9GB\n",
      "Epoch 1, Batch 46355, Loss: 3.1382, Time: 27583.8s, Step: 46356, GPU: 4.9GB\n",
      "Epoch 1, Batch 46360, Loss: 2.9476, Time: 27586.3s, Step: 46361, GPU: 4.9GB\n",
      "Epoch 1, Batch 46365, Loss: 3.6107, Time: 27589.7s, Step: 46366, GPU: 4.9GB\n",
      "Epoch 1, Batch 46370, Loss: 3.1786, Time: 27592.2s, Step: 46371, GPU: 4.9GB\n",
      "Epoch 1, Batch 46375, Loss: 3.1300, Time: 27595.6s, Step: 46376, GPU: 4.9GB\n",
      "Epoch 1, Batch 46380, Loss: 3.1921, Time: 27598.1s, Step: 46381, GPU: 4.9GB\n",
      "Epoch 1, Batch 46385, Loss: 3.4220, Time: 27601.5s, Step: 46386, GPU: 4.9GB\n",
      "Epoch 1, Batch 46390, Loss: 2.9445, Time: 27604.0s, Step: 46391, GPU: 4.9GB\n",
      "Epoch 1, Batch 46395, Loss: 3.2971, Time: 27607.4s, Step: 46396, GPU: 4.9GB\n",
      "Epoch 1, Batch 46400, Loss: 2.9406, Time: 27610.0s, Step: 46401, GPU: 4.9GB\n",
      "Epoch 1, Batch 46405, Loss: 3.7123, Time: 27613.4s, Step: 46406, GPU: 4.9GB\n",
      "Epoch 1, Batch 46410, Loss: 3.3930, Time: 27616.0s, Step: 46411, GPU: 4.9GB\n",
      "Epoch 1, Batch 46415, Loss: 2.9079, Time: 27619.4s, Step: 46416, GPU: 4.9GB\n",
      "Epoch 1, Batch 46420, Loss: 2.9139, Time: 27621.9s, Step: 46421, GPU: 4.9GB\n",
      "Epoch 1, Batch 46425, Loss: 3.6351, Time: 27625.3s, Step: 46426, GPU: 4.9GB\n",
      "Epoch 1, Batch 46430, Loss: 3.6870, Time: 27627.9s, Step: 46431, GPU: 4.9GB\n",
      "Epoch 1, Batch 46435, Loss: 2.6851, Time: 27631.3s, Step: 46436, GPU: 4.9GB\n",
      "Epoch 1, Batch 46440, Loss: 3.3704, Time: 27633.8s, Step: 46441, GPU: 4.9GB\n",
      "Epoch 1, Batch 46445, Loss: 2.4316, Time: 27637.2s, Step: 46446, GPU: 4.9GB\n",
      "Epoch 1, Batch 46450, Loss: 3.0925, Time: 27639.7s, Step: 46451, GPU: 4.9GB\n",
      "Epoch 1, Batch 46455, Loss: 3.6974, Time: 27643.1s, Step: 46456, GPU: 4.9GB\n",
      "Epoch 1, Batch 46460, Loss: 2.9003, Time: 27645.7s, Step: 46461, GPU: 4.9GB\n",
      "Epoch 1, Batch 46465, Loss: 3.1607, Time: 27649.1s, Step: 46466, GPU: 4.9GB\n",
      "Epoch 1, Batch 46470, Loss: 2.9491, Time: 27651.6s, Step: 46471, GPU: 4.9GB\n",
      "Epoch 1, Batch 46475, Loss: 3.5239, Time: 27655.0s, Step: 46476, GPU: 4.9GB\n",
      "Epoch 1, Batch 46480, Loss: 2.7625, Time: 27657.5s, Step: 46481, GPU: 4.9GB\n",
      "Epoch 1, Batch 46485, Loss: 3.3914, Time: 27660.9s, Step: 46486, GPU: 4.9GB\n",
      "Epoch 1, Batch 46490, Loss: 3.0406, Time: 27663.4s, Step: 46491, GPU: 4.9GB\n",
      "Epoch 1, Batch 46495, Loss: 2.9228, Time: 27666.9s, Step: 46496, GPU: 4.9GB\n",
      "Epoch 1, Batch 46500, Loss: 2.8872, Time: 27669.4s, Step: 46501, GPU: 4.9GB\n",
      "Epoch 1, Batch 46505, Loss: 2.8075, Time: 27672.8s, Step: 46506, GPU: 4.9GB\n",
      "Epoch 1, Batch 46510, Loss: 3.1421, Time: 27675.3s, Step: 46511, GPU: 4.9GB\n",
      "Epoch 1, Batch 46515, Loss: 3.9076, Time: 27678.7s, Step: 46516, GPU: 4.9GB\n",
      "Epoch 1, Batch 46520, Loss: 3.0720, Time: 27681.2s, Step: 46521, GPU: 4.9GB\n",
      "Epoch 1, Batch 46525, Loss: 3.2835, Time: 27684.6s, Step: 46526, GPU: 4.9GB\n",
      "Epoch 1, Batch 46530, Loss: 2.8696, Time: 27687.1s, Step: 46531, GPU: 4.9GB\n",
      "Epoch 1, Batch 46535, Loss: 3.5281, Time: 27690.5s, Step: 46536, GPU: 4.9GB\n",
      "Epoch 1, Batch 46540, Loss: 2.9450, Time: 27693.0s, Step: 46541, GPU: 4.9GB\n",
      "Epoch 1, Batch 46545, Loss: 3.6074, Time: 27696.4s, Step: 46546, GPU: 4.9GB\n",
      "Epoch 1, Batch 46550, Loss: 3.0978, Time: 27698.9s, Step: 46551, GPU: 4.9GB\n",
      "Epoch 1, Batch 46555, Loss: 3.3285, Time: 27702.4s, Step: 46556, GPU: 4.9GB\n",
      "Epoch 1, Batch 46560, Loss: 2.6953, Time: 27704.9s, Step: 46561, GPU: 4.9GB\n",
      "Epoch 1, Batch 46565, Loss: 3.2423, Time: 27708.3s, Step: 46566, GPU: 4.9GB\n",
      "Epoch 1, Batch 46570, Loss: 2.8698, Time: 27710.8s, Step: 46571, GPU: 4.9GB\n",
      "Epoch 1, Batch 46575, Loss: 3.0246, Time: 27714.3s, Step: 46576, GPU: 4.9GB\n",
      "Epoch 1, Batch 46580, Loss: 3.1194, Time: 27716.8s, Step: 46581, GPU: 4.9GB\n",
      "Epoch 1, Batch 46585, Loss: 2.0622, Time: 27720.2s, Step: 46586, GPU: 4.9GB\n",
      "Epoch 1, Batch 46590, Loss: 3.2781, Time: 27722.7s, Step: 46591, GPU: 4.9GB\n",
      "Epoch 1, Batch 46595, Loss: 3.1246, Time: 27726.2s, Step: 46596, GPU: 4.9GB\n",
      "Epoch 1, Batch 46600, Loss: 2.4444, Time: 27728.8s, Step: 46601, GPU: 4.9GB\n",
      "Epoch 1, Batch 46605, Loss: 3.2790, Time: 27732.2s, Step: 46606, GPU: 4.9GB\n",
      "Epoch 1, Batch 46610, Loss: 2.6478, Time: 27734.7s, Step: 46611, GPU: 4.9GB\n",
      "Epoch 1, Batch 46615, Loss: 2.5763, Time: 27738.1s, Step: 46616, GPU: 4.9GB\n",
      "Epoch 1, Batch 46620, Loss: 3.3941, Time: 27740.6s, Step: 46621, GPU: 4.9GB\n",
      "Epoch 1, Batch 46625, Loss: 2.1058, Time: 27744.0s, Step: 46626, GPU: 4.9GB\n",
      "Epoch 1, Batch 46630, Loss: 3.6054, Time: 27746.6s, Step: 46631, GPU: 4.9GB\n",
      "Epoch 1, Batch 46635, Loss: 2.7044, Time: 27750.0s, Step: 46636, GPU: 4.9GB\n",
      "Epoch 1, Batch 46640, Loss: 3.8367, Time: 27752.5s, Step: 46641, GPU: 4.9GB\n",
      "Epoch 1, Batch 46645, Loss: 3.4184, Time: 27755.9s, Step: 46646, GPU: 4.9GB\n",
      "Epoch 1, Batch 46650, Loss: 2.8386, Time: 27758.4s, Step: 46651, GPU: 4.9GB\n",
      "Epoch 1, Batch 46655, Loss: 3.5302, Time: 27761.8s, Step: 46656, GPU: 4.9GB\n",
      "Epoch 1, Batch 46660, Loss: 2.3267, Time: 27764.4s, Step: 46661, GPU: 4.9GB\n",
      "Epoch 1, Batch 46665, Loss: 3.0676, Time: 27767.8s, Step: 46666, GPU: 4.9GB\n",
      "Epoch 1, Batch 46670, Loss: 3.5547, Time: 27770.3s, Step: 46671, GPU: 4.9GB\n",
      "Epoch 1, Batch 46675, Loss: 2.8737, Time: 27773.7s, Step: 46676, GPU: 4.9GB\n",
      "Epoch 1, Batch 46680, Loss: 3.5790, Time: 27776.2s, Step: 46681, GPU: 4.9GB\n",
      "Epoch 1, Batch 46685, Loss: 2.9778, Time: 27779.7s, Step: 46686, GPU: 4.9GB\n",
      "Epoch 1, Batch 46690, Loss: 3.5716, Time: 27782.2s, Step: 46691, GPU: 4.9GB\n",
      "Epoch 1, Batch 46695, Loss: 3.4597, Time: 27785.6s, Step: 46696, GPU: 4.9GB\n",
      "Epoch 1, Batch 46700, Loss: 3.1619, Time: 27788.1s, Step: 46701, GPU: 4.9GB\n",
      "Epoch 1, Batch 46705, Loss: 3.3014, Time: 27791.5s, Step: 46706, GPU: 4.9GB\n",
      "Epoch 1, Batch 46710, Loss: 3.1502, Time: 27794.1s, Step: 46711, GPU: 4.9GB\n",
      "Epoch 1, Batch 46715, Loss: 3.1932, Time: 27797.5s, Step: 46716, GPU: 4.9GB\n",
      "Epoch 1, Batch 46720, Loss: 3.3057, Time: 27800.0s, Step: 46721, GPU: 4.9GB\n",
      "Epoch 1, Batch 46725, Loss: 2.4562, Time: 27803.4s, Step: 46726, GPU: 4.9GB\n",
      "Epoch 1, Batch 46730, Loss: 3.8492, Time: 27806.0s, Step: 46731, GPU: 4.9GB\n",
      "Epoch 1, Batch 46735, Loss: 3.9424, Time: 27809.4s, Step: 46736, GPU: 4.9GB\n",
      "Epoch 1, Batch 46740, Loss: 3.4493, Time: 27811.9s, Step: 46741, GPU: 4.9GB\n",
      "Epoch 1, Batch 46745, Loss: 3.0885, Time: 27815.4s, Step: 46746, GPU: 4.9GB\n",
      "Epoch 1, Batch 46750, Loss: 3.2856, Time: 27817.9s, Step: 46751, GPU: 4.9GB\n",
      "Epoch 1, Batch 46755, Loss: 3.8199, Time: 27821.3s, Step: 46756, GPU: 4.9GB\n",
      "Epoch 1, Batch 46760, Loss: 2.7671, Time: 27823.8s, Step: 46761, GPU: 4.9GB\n",
      "Epoch 1, Batch 46765, Loss: 2.6390, Time: 27827.2s, Step: 46766, GPU: 4.9GB\n",
      "Epoch 1, Batch 46770, Loss: 3.2604, Time: 27829.7s, Step: 46771, GPU: 4.9GB\n",
      "Epoch 1, Batch 46775, Loss: 3.7477, Time: 27833.2s, Step: 46776, GPU: 4.9GB\n",
      "Epoch 1, Batch 46780, Loss: 3.5474, Time: 27835.7s, Step: 46781, GPU: 4.9GB\n",
      "Epoch 1, Batch 46785, Loss: 2.8901, Time: 27839.1s, Step: 46786, GPU: 4.9GB\n",
      "Epoch 1, Batch 46790, Loss: 3.6859, Time: 27841.6s, Step: 46791, GPU: 4.9GB\n",
      "Epoch 1, Batch 46795, Loss: 3.6189, Time: 27845.0s, Step: 46796, GPU: 4.9GB\n",
      "Epoch 1, Batch 46800, Loss: 3.7785, Time: 27847.6s, Step: 46801, GPU: 4.9GB\n",
      "Epoch 1, Batch 46805, Loss: 3.6355, Time: 27851.0s, Step: 46806, GPU: 4.9GB\n",
      "Epoch 1, Batch 46810, Loss: 3.6292, Time: 27853.6s, Step: 46811, GPU: 4.9GB\n",
      "Epoch 1, Batch 46815, Loss: 3.7019, Time: 27857.0s, Step: 46816, GPU: 4.9GB\n",
      "Epoch 1, Batch 46820, Loss: 2.5363, Time: 27859.5s, Step: 46821, GPU: 4.9GB\n",
      "Epoch 1, Batch 46825, Loss: 3.6486, Time: 27862.9s, Step: 46826, GPU: 4.9GB\n",
      "Epoch 1, Batch 46830, Loss: 3.2412, Time: 27865.4s, Step: 46831, GPU: 4.9GB\n",
      "Epoch 1, Batch 46835, Loss: 2.8303, Time: 27868.8s, Step: 46836, GPU: 4.9GB\n",
      "Epoch 1, Batch 46840, Loss: 3.2349, Time: 27871.4s, Step: 46841, GPU: 4.9GB\n",
      "Epoch 1, Batch 46845, Loss: 3.4063, Time: 27874.8s, Step: 46846, GPU: 4.9GB\n",
      "Epoch 1, Batch 46850, Loss: 3.2100, Time: 27877.3s, Step: 46851, GPU: 4.9GB\n",
      "Epoch 1, Batch 46855, Loss: 2.6590, Time: 27880.7s, Step: 46856, GPU: 4.9GB\n",
      "Epoch 1, Batch 46860, Loss: 2.4082, Time: 27883.3s, Step: 46861, GPU: 4.9GB\n",
      "Epoch 1, Batch 46865, Loss: 2.8104, Time: 27886.7s, Step: 46866, GPU: 4.9GB\n",
      "Epoch 1, Batch 46870, Loss: 3.5937, Time: 27889.3s, Step: 46871, GPU: 4.9GB\n",
      "Epoch 1, Batch 46875, Loss: 2.9533, Time: 27892.8s, Step: 46876, GPU: 4.9GB\n",
      "Epoch 1, Batch 46880, Loss: 4.1602, Time: 27895.3s, Step: 46881, GPU: 4.9GB\n",
      "Epoch 1, Batch 46885, Loss: 3.4731, Time: 27898.7s, Step: 46886, GPU: 4.9GB\n",
      "Epoch 1, Batch 46890, Loss: 2.9860, Time: 27901.2s, Step: 46891, GPU: 4.9GB\n",
      "Epoch 1, Batch 46895, Loss: 2.9756, Time: 27904.7s, Step: 46896, GPU: 4.9GB\n",
      "Epoch 1, Batch 46900, Loss: 3.5762, Time: 27907.2s, Step: 46901, GPU: 4.9GB\n",
      "Epoch 1, Batch 46905, Loss: 3.2066, Time: 27910.6s, Step: 46906, GPU: 4.9GB\n",
      "Epoch 1, Batch 46910, Loss: 2.6697, Time: 27913.1s, Step: 46911, GPU: 4.9GB\n",
      "Epoch 1, Batch 46915, Loss: 3.4475, Time: 27916.5s, Step: 46916, GPU: 4.9GB\n",
      "Epoch 1, Batch 46920, Loss: 2.4754, Time: 27919.1s, Step: 46921, GPU: 4.9GB\n",
      "Epoch 1, Batch 46925, Loss: 3.4736, Time: 27922.5s, Step: 46926, GPU: 4.9GB\n",
      "Epoch 1, Batch 46930, Loss: 3.0569, Time: 27925.0s, Step: 46931, GPU: 4.9GB\n",
      "Epoch 1, Batch 46935, Loss: 2.8277, Time: 27928.4s, Step: 46936, GPU: 4.9GB\n",
      "Epoch 1, Batch 46940, Loss: 4.5433, Time: 27930.9s, Step: 46941, GPU: 4.9GB\n",
      "Epoch 1, Batch 46945, Loss: 3.2322, Time: 27934.4s, Step: 46946, GPU: 4.9GB\n",
      "Epoch 1, Batch 46950, Loss: 3.5487, Time: 27936.9s, Step: 46951, GPU: 4.9GB\n",
      "Epoch 1, Batch 46955, Loss: 2.9525, Time: 27940.3s, Step: 46956, GPU: 4.9GB\n",
      "Epoch 1, Batch 46960, Loss: 3.1259, Time: 27942.9s, Step: 46961, GPU: 4.9GB\n",
      "Epoch 1, Batch 46965, Loss: 4.1893, Time: 27946.3s, Step: 46966, GPU: 4.9GB\n",
      "Epoch 1, Batch 46970, Loss: 3.2408, Time: 27948.8s, Step: 46971, GPU: 4.9GB\n",
      "Epoch 1, Batch 46975, Loss: 2.6796, Time: 27952.2s, Step: 46976, GPU: 4.9GB\n",
      "Epoch 1, Batch 46980, Loss: 2.5274, Time: 27954.7s, Step: 46981, GPU: 4.9GB\n",
      "Epoch 1, Batch 46985, Loss: 2.3953, Time: 27958.1s, Step: 46986, GPU: 4.9GB\n",
      "Epoch 1, Batch 46990, Loss: 2.8553, Time: 27960.7s, Step: 46991, GPU: 4.9GB\n",
      "Epoch 1, Batch 46995, Loss: 2.7376, Time: 27964.1s, Step: 46996, GPU: 4.9GB\n",
      "Epoch 1, Batch 47000, Loss: 3.5957, Time: 27966.7s, Step: 47001, GPU: 4.9GB\n",
      "Epoch 1, Batch 47005, Loss: 3.3265, Time: 27970.1s, Step: 47006, GPU: 4.9GB\n",
      "Epoch 1, Batch 47010, Loss: 2.7258, Time: 27972.7s, Step: 47011, GPU: 4.9GB\n",
      "Epoch 1, Batch 47015, Loss: 4.1707, Time: 27976.1s, Step: 47016, GPU: 4.9GB\n",
      "Epoch 1, Batch 47020, Loss: 4.0646, Time: 27978.7s, Step: 47021, GPU: 4.9GB\n",
      "Epoch 1, Batch 47025, Loss: 2.4057, Time: 27982.1s, Step: 47026, GPU: 4.9GB\n",
      "Epoch 1, Batch 47030, Loss: 3.0547, Time: 27984.6s, Step: 47031, GPU: 4.9GB\n",
      "Epoch 1, Batch 47035, Loss: 2.9403, Time: 27988.0s, Step: 47036, GPU: 4.9GB\n",
      "Epoch 1, Batch 47040, Loss: 3.2994, Time: 27990.5s, Step: 47041, GPU: 4.9GB\n",
      "Epoch 1, Batch 47045, Loss: 3.3243, Time: 27993.9s, Step: 47046, GPU: 4.9GB\n",
      "Epoch 1, Batch 47050, Loss: 2.6679, Time: 27996.5s, Step: 47051, GPU: 4.9GB\n",
      "Epoch 1, Batch 47055, Loss: 2.4428, Time: 27999.9s, Step: 47056, GPU: 4.9GB\n",
      "Epoch 1, Batch 47060, Loss: 3.5455, Time: 28002.4s, Step: 47061, GPU: 4.9GB\n",
      "Epoch 1, Batch 47065, Loss: 2.6666, Time: 28005.8s, Step: 47066, GPU: 4.9GB\n",
      "Epoch 1, Batch 47070, Loss: 3.8851, Time: 28008.3s, Step: 47071, GPU: 4.9GB\n",
      "Epoch 1, Batch 47075, Loss: 3.5804, Time: 28011.7s, Step: 47076, GPU: 4.9GB\n",
      "Epoch 1, Batch 47080, Loss: 3.1740, Time: 28014.2s, Step: 47081, GPU: 4.9GB\n",
      "Epoch 1, Batch 47085, Loss: 4.0282, Time: 28017.7s, Step: 47086, GPU: 4.9GB\n",
      "Epoch 1, Batch 47090, Loss: 2.9891, Time: 28020.2s, Step: 47091, GPU: 4.9GB\n",
      "Epoch 1, Batch 47095, Loss: 2.3972, Time: 28023.6s, Step: 47096, GPU: 4.9GB\n",
      "Epoch 1, Batch 47100, Loss: 3.1975, Time: 28026.2s, Step: 47101, GPU: 4.9GB\n",
      "Epoch 1, Batch 47105, Loss: 3.2539, Time: 28029.6s, Step: 47106, GPU: 4.9GB\n",
      "Epoch 1, Batch 47110, Loss: 3.3727, Time: 28032.1s, Step: 47111, GPU: 4.9GB\n",
      "Epoch 1, Batch 47115, Loss: 3.9313, Time: 28035.5s, Step: 47116, GPU: 4.9GB\n",
      "Epoch 1, Batch 47120, Loss: 3.2099, Time: 28038.1s, Step: 47121, GPU: 4.9GB\n",
      "Epoch 1, Batch 47125, Loss: 3.3265, Time: 28041.5s, Step: 47126, GPU: 4.9GB\n",
      "Epoch 1, Batch 47130, Loss: 3.0211, Time: 28044.0s, Step: 47131, GPU: 4.9GB\n",
      "Epoch 1, Batch 47135, Loss: 3.4431, Time: 28047.4s, Step: 47136, GPU: 4.9GB\n",
      "Epoch 1, Batch 47140, Loss: 3.3956, Time: 28049.9s, Step: 47141, GPU: 4.9GB\n",
      "Epoch 1, Batch 47145, Loss: 2.2338, Time: 28053.4s, Step: 47146, GPU: 4.9GB\n",
      "Epoch 1, Batch 47150, Loss: 2.8678, Time: 28055.9s, Step: 47151, GPU: 4.9GB\n",
      "Epoch 1, Batch 47155, Loss: 2.8106, Time: 28059.3s, Step: 47156, GPU: 4.9GB\n",
      "Epoch 1, Batch 47160, Loss: 3.1667, Time: 28061.8s, Step: 47161, GPU: 4.9GB\n",
      "Epoch 1, Batch 47165, Loss: 3.3705, Time: 28065.3s, Step: 47166, GPU: 4.9GB\n",
      "Epoch 1, Batch 47170, Loss: 3.5793, Time: 28067.8s, Step: 47171, GPU: 4.9GB\n",
      "Epoch 1, Batch 47175, Loss: 3.7731, Time: 28071.2s, Step: 47176, GPU: 4.9GB\n",
      "Epoch 1, Batch 47180, Loss: 2.5322, Time: 28073.7s, Step: 47181, GPU: 4.9GB\n",
      "Epoch 1, Batch 47185, Loss: 3.3818, Time: 28077.1s, Step: 47186, GPU: 4.9GB\n",
      "Epoch 1, Batch 47190, Loss: 3.6042, Time: 28079.7s, Step: 47191, GPU: 4.9GB\n",
      "Epoch 1, Batch 47195, Loss: 3.6034, Time: 28083.1s, Step: 47196, GPU: 4.9GB\n",
      "Epoch 1, Batch 47200, Loss: 3.4429, Time: 28085.7s, Step: 47201, GPU: 4.9GB\n",
      "Epoch 1, Batch 47205, Loss: 3.0672, Time: 28089.1s, Step: 47206, GPU: 4.9GB\n",
      "Epoch 1, Batch 47210, Loss: 3.5202, Time: 28091.6s, Step: 47211, GPU: 4.9GB\n",
      "Epoch 1, Batch 47215, Loss: 3.4703, Time: 28095.0s, Step: 47216, GPU: 4.9GB\n",
      "Epoch 1, Batch 47220, Loss: 3.3507, Time: 28097.6s, Step: 47221, GPU: 4.9GB\n",
      "Epoch 1, Batch 47225, Loss: 2.7631, Time: 28101.0s, Step: 47226, GPU: 4.9GB\n",
      "Epoch 1, Batch 47230, Loss: 2.4949, Time: 28103.5s, Step: 47231, GPU: 4.9GB\n",
      "Epoch 1, Batch 47235, Loss: 3.1520, Time: 28106.9s, Step: 47236, GPU: 4.9GB\n",
      "Epoch 1, Batch 47240, Loss: 3.2638, Time: 28109.5s, Step: 47241, GPU: 4.9GB\n",
      "Epoch 1, Batch 47245, Loss: 3.2517, Time: 28112.9s, Step: 47246, GPU: 4.9GB\n",
      "Epoch 1, Batch 47250, Loss: 3.4429, Time: 28115.4s, Step: 47251, GPU: 4.9GB\n",
      "Epoch 1, Batch 47255, Loss: 3.2125, Time: 28118.8s, Step: 47256, GPU: 4.9GB\n",
      "Epoch 1, Batch 47260, Loss: 2.7757, Time: 28121.3s, Step: 47261, GPU: 4.9GB\n",
      "Epoch 1, Batch 47265, Loss: 3.0232, Time: 28124.7s, Step: 47266, GPU: 4.9GB\n",
      "Epoch 1, Batch 47270, Loss: 2.8526, Time: 28127.3s, Step: 47271, GPU: 4.9GB\n",
      "Epoch 1, Batch 47275, Loss: 3.1252, Time: 28130.7s, Step: 47276, GPU: 4.9GB\n",
      "Epoch 1, Batch 47280, Loss: 2.5558, Time: 28133.2s, Step: 47281, GPU: 4.9GB\n",
      "Epoch 1, Batch 47285, Loss: 3.0743, Time: 28136.6s, Step: 47286, GPU: 4.9GB\n",
      "Epoch 1, Batch 47290, Loss: 2.6368, Time: 28139.2s, Step: 47291, GPU: 4.9GB\n",
      "Epoch 1, Batch 47295, Loss: 2.7501, Time: 28142.5s, Step: 47296, GPU: 4.9GB\n",
      "Epoch 1, Batch 47300, Loss: 3.2753, Time: 28145.1s, Step: 47301, GPU: 4.9GB\n",
      "Epoch 1, Batch 47305, Loss: 3.4298, Time: 28148.5s, Step: 47306, GPU: 4.9GB\n",
      "Epoch 1, Batch 47310, Loss: 2.9005, Time: 28151.0s, Step: 47311, GPU: 4.9GB\n",
      "Epoch 1, Batch 47315, Loss: 2.6406, Time: 28154.4s, Step: 47316, GPU: 4.9GB\n",
      "Epoch 1, Batch 47320, Loss: 2.3173, Time: 28157.0s, Step: 47321, GPU: 4.9GB\n",
      "Epoch 1, Batch 47325, Loss: 3.0182, Time: 28160.4s, Step: 47326, GPU: 4.9GB\n",
      "Epoch 1, Batch 47330, Loss: 3.2910, Time: 28162.9s, Step: 47331, GPU: 4.9GB\n",
      "Epoch 1, Batch 47335, Loss: 3.7169, Time: 28166.3s, Step: 47336, GPU: 4.9GB\n",
      "Epoch 1, Batch 47340, Loss: 4.0006, Time: 28168.8s, Step: 47341, GPU: 4.9GB\n",
      "Epoch 1, Batch 47345, Loss: 3.3363, Time: 28172.2s, Step: 47346, GPU: 4.9GB\n",
      "Epoch 1, Batch 47350, Loss: 2.4668, Time: 28174.7s, Step: 47351, GPU: 4.9GB\n",
      "Epoch 1, Batch 47355, Loss: 2.6230, Time: 28178.2s, Step: 47356, GPU: 4.9GB\n",
      "Epoch 1, Batch 47360, Loss: 3.2538, Time: 28180.7s, Step: 47361, GPU: 4.9GB\n",
      "Epoch 1, Batch 47365, Loss: 2.2549, Time: 28184.1s, Step: 47366, GPU: 4.9GB\n",
      "Epoch 1, Batch 47370, Loss: 3.2840, Time: 28186.6s, Step: 47371, GPU: 4.9GB\n",
      "Epoch 1, Batch 47375, Loss: 3.3807, Time: 28190.0s, Step: 47376, GPU: 4.9GB\n",
      "Epoch 1, Batch 47380, Loss: 2.5394, Time: 28192.5s, Step: 47381, GPU: 4.9GB\n",
      "Epoch 1, Batch 47385, Loss: 2.6430, Time: 28195.9s, Step: 47386, GPU: 4.9GB\n",
      "Epoch 1, Batch 47390, Loss: 2.9726, Time: 28198.4s, Step: 47391, GPU: 4.9GB\n",
      "Epoch 1, Batch 47395, Loss: 3.5774, Time: 28201.8s, Step: 47396, GPU: 4.9GB\n",
      "Epoch 1, Batch 47400, Loss: 2.9962, Time: 28204.4s, Step: 47401, GPU: 4.9GB\n",
      "Epoch 1, Batch 47405, Loss: 2.8259, Time: 28207.8s, Step: 47406, GPU: 4.9GB\n",
      "Epoch 1, Batch 47410, Loss: 3.3817, Time: 28210.4s, Step: 47411, GPU: 4.9GB\n",
      "Epoch 1, Batch 47415, Loss: 3.0827, Time: 28213.8s, Step: 47416, GPU: 4.9GB\n",
      "Epoch 1, Batch 47420, Loss: 2.2797, Time: 28216.3s, Step: 47421, GPU: 4.9GB\n",
      "Epoch 1, Batch 47425, Loss: 2.7051, Time: 28219.7s, Step: 47426, GPU: 4.9GB\n",
      "Epoch 1, Batch 47430, Loss: 3.7047, Time: 28222.2s, Step: 47431, GPU: 4.9GB\n",
      "Epoch 1, Batch 47435, Loss: 3.2811, Time: 28225.6s, Step: 47436, GPU: 4.9GB\n",
      "Epoch 1, Batch 47440, Loss: 3.7935, Time: 28228.1s, Step: 47441, GPU: 4.9GB\n",
      "Epoch 1, Batch 47445, Loss: 2.5681, Time: 28231.5s, Step: 47446, GPU: 4.9GB\n",
      "Epoch 1, Batch 47450, Loss: 3.2014, Time: 28234.0s, Step: 47451, GPU: 4.9GB\n",
      "Epoch 1, Batch 47455, Loss: 3.2168, Time: 28237.5s, Step: 47456, GPU: 4.9GB\n",
      "Epoch 1, Batch 47460, Loss: 3.4784, Time: 28240.0s, Step: 47461, GPU: 4.9GB\n",
      "Epoch 1, Batch 47465, Loss: 2.2112, Time: 28243.4s, Step: 47466, GPU: 4.9GB\n",
      "Epoch 1, Batch 47470, Loss: 2.8269, Time: 28245.9s, Step: 47471, GPU: 4.9GB\n",
      "Epoch 1, Batch 47475, Loss: 2.7472, Time: 28249.3s, Step: 47476, GPU: 4.9GB\n",
      "Epoch 1, Batch 47480, Loss: 3.2839, Time: 28251.8s, Step: 47481, GPU: 4.9GB\n",
      "Epoch 1, Batch 47485, Loss: 3.7193, Time: 28255.3s, Step: 47486, GPU: 4.9GB\n",
      "Epoch 1, Batch 47490, Loss: 3.1987, Time: 28257.8s, Step: 47491, GPU: 4.9GB\n",
      "Epoch 1, Batch 47495, Loss: 3.9700, Time: 28261.3s, Step: 47496, GPU: 4.9GB\n",
      "Epoch 1, Batch 47500, Loss: 4.1415, Time: 28263.8s, Step: 47501, GPU: 4.9GB\n",
      "Epoch 1, Batch 47505, Loss: 3.1318, Time: 28267.2s, Step: 47506, GPU: 4.9GB\n",
      "Epoch 1, Batch 47510, Loss: 3.9452, Time: 28269.7s, Step: 47511, GPU: 4.9GB\n",
      "Epoch 1, Batch 47515, Loss: 2.7413, Time: 28273.2s, Step: 47516, GPU: 4.9GB\n",
      "Epoch 1, Batch 47520, Loss: 2.5226, Time: 28275.7s, Step: 47521, GPU: 4.9GB\n",
      "Epoch 1, Batch 47525, Loss: 3.2825, Time: 28279.1s, Step: 47526, GPU: 4.9GB\n",
      "Epoch 1, Batch 47530, Loss: 4.0716, Time: 28281.7s, Step: 47531, GPU: 4.9GB\n",
      "Epoch 1, Batch 47535, Loss: 2.7031, Time: 28285.1s, Step: 47536, GPU: 4.9GB\n",
      "Epoch 1, Batch 47540, Loss: 3.7472, Time: 28287.6s, Step: 47541, GPU: 4.9GB\n",
      "Epoch 1, Batch 47545, Loss: 4.2622, Time: 28291.0s, Step: 47546, GPU: 4.9GB\n",
      "Epoch 1, Batch 47550, Loss: 2.8028, Time: 28293.5s, Step: 47551, GPU: 4.9GB\n",
      "Epoch 1, Batch 47555, Loss: 3.6769, Time: 28296.9s, Step: 47556, GPU: 4.9GB\n",
      "Epoch 1, Batch 47560, Loss: 2.9857, Time: 28299.5s, Step: 47561, GPU: 4.9GB\n",
      "Epoch 1, Batch 47565, Loss: 2.7923, Time: 28302.9s, Step: 47566, GPU: 4.9GB\n",
      "Epoch 1, Batch 47570, Loss: 3.1969, Time: 28305.4s, Step: 47571, GPU: 4.9GB\n",
      "Epoch 1, Batch 47575, Loss: 3.7201, Time: 28308.8s, Step: 47576, GPU: 4.9GB\n",
      "Epoch 1, Batch 47580, Loss: 3.5012, Time: 28311.4s, Step: 47581, GPU: 4.9GB\n",
      "Epoch 1, Batch 47585, Loss: 3.5211, Time: 28314.8s, Step: 47586, GPU: 4.9GB\n",
      "Epoch 1, Batch 47590, Loss: 3.1262, Time: 28317.3s, Step: 47591, GPU: 4.9GB\n",
      "Epoch 1, Batch 47595, Loss: 2.3708, Time: 28320.7s, Step: 47596, GPU: 4.9GB\n",
      "Epoch 1, Batch 47600, Loss: 8.1056, Time: 28323.4s, Step: 47601, GPU: 4.9GB\n",
      "Epoch 1, Batch 47605, Loss: 3.0933, Time: 28326.8s, Step: 47606, GPU: 4.9GB\n",
      "Epoch 1, Batch 47610, Loss: 4.1475, Time: 28329.3s, Step: 47611, GPU: 4.9GB\n",
      "Epoch 1, Batch 47615, Loss: 2.4262, Time: 28332.7s, Step: 47616, GPU: 4.9GB\n",
      "Epoch 1, Batch 47620, Loss: 3.1394, Time: 28335.2s, Step: 47621, GPU: 4.9GB\n",
      "Epoch 1, Batch 47625, Loss: 2.7311, Time: 28338.7s, Step: 47626, GPU: 4.9GB\n",
      "Epoch 1, Batch 47630, Loss: 3.3371, Time: 28341.2s, Step: 47631, GPU: 4.9GB\n",
      "Epoch 1, Batch 47635, Loss: 3.4769, Time: 28344.6s, Step: 47636, GPU: 4.9GB\n",
      "Epoch 1, Batch 47640, Loss: 3.4864, Time: 28347.1s, Step: 47641, GPU: 4.9GB\n",
      "Epoch 1, Batch 47645, Loss: 2.9415, Time: 28350.6s, Step: 47646, GPU: 4.9GB\n",
      "Epoch 1, Batch 47650, Loss: 3.2175, Time: 28353.1s, Step: 47651, GPU: 4.9GB\n",
      "Epoch 1, Batch 47655, Loss: 3.5031, Time: 28356.5s, Step: 47656, GPU: 4.9GB\n",
      "Epoch 1, Batch 47660, Loss: 3.0415, Time: 28359.0s, Step: 47661, GPU: 4.9GB\n",
      "Epoch 1, Batch 47665, Loss: 2.9632, Time: 28362.4s, Step: 47666, GPU: 4.9GB\n",
      "Epoch 1, Batch 47670, Loss: 2.9728, Time: 28364.9s, Step: 47671, GPU: 4.9GB\n",
      "Epoch 1, Batch 47675, Loss: 2.5219, Time: 28368.4s, Step: 47676, GPU: 4.9GB\n",
      "Epoch 1, Batch 47680, Loss: 2.8326, Time: 28370.9s, Step: 47681, GPU: 4.9GB\n",
      "Epoch 1, Batch 47685, Loss: 4.1312, Time: 28374.3s, Step: 47686, GPU: 4.9GB\n",
      "Epoch 1, Batch 47690, Loss: 3.3081, Time: 28376.8s, Step: 47691, GPU: 4.9GB\n",
      "Epoch 1, Batch 47695, Loss: 3.1588, Time: 28380.3s, Step: 47696, GPU: 4.9GB\n",
      "Epoch 1, Batch 47700, Loss: 3.1467, Time: 28382.8s, Step: 47701, GPU: 4.9GB\n",
      "Epoch 1, Batch 47705, Loss: 3.3196, Time: 28386.3s, Step: 47706, GPU: 4.9GB\n",
      "Epoch 1, Batch 47710, Loss: 2.5941, Time: 28388.8s, Step: 47711, GPU: 4.9GB\n",
      "Epoch 1, Batch 47715, Loss: 2.9752, Time: 28392.2s, Step: 47716, GPU: 4.9GB\n",
      "Epoch 1, Batch 47720, Loss: 3.9126, Time: 28394.7s, Step: 47721, GPU: 4.9GB\n",
      "Epoch 1, Batch 47725, Loss: 3.2766, Time: 28398.1s, Step: 47726, GPU: 4.9GB\n",
      "Epoch 1, Batch 47730, Loss: 3.5798, Time: 28400.6s, Step: 47731, GPU: 4.9GB\n",
      "Epoch 1, Batch 47735, Loss: 2.5229, Time: 28404.0s, Step: 47736, GPU: 4.9GB\n",
      "Epoch 1, Batch 47740, Loss: 2.6573, Time: 28406.6s, Step: 47741, GPU: 4.9GB\n",
      "Epoch 1, Batch 47745, Loss: 2.7487, Time: 28410.0s, Step: 47746, GPU: 4.9GB\n",
      "Epoch 1, Batch 47750, Loss: 3.1892, Time: 28412.5s, Step: 47751, GPU: 4.9GB\n",
      "Epoch 1, Batch 47755, Loss: 3.6596, Time: 28415.9s, Step: 47756, GPU: 4.9GB\n",
      "Epoch 1, Batch 47760, Loss: 2.8351, Time: 28418.4s, Step: 47761, GPU: 4.9GB\n",
      "Epoch 1, Batch 47765, Loss: 3.0053, Time: 28421.9s, Step: 47766, GPU: 4.9GB\n",
      "Epoch 1, Batch 47770, Loss: 3.4667, Time: 28424.4s, Step: 47771, GPU: 4.9GB\n",
      "Epoch 1, Batch 47775, Loss: 3.4313, Time: 28427.8s, Step: 47776, GPU: 4.9GB\n",
      "Epoch 1, Batch 47780, Loss: 2.6516, Time: 28430.3s, Step: 47781, GPU: 4.9GB\n",
      "Epoch 1, Batch 47785, Loss: 2.6744, Time: 28433.7s, Step: 47786, GPU: 4.9GB\n",
      "Epoch 1, Batch 47790, Loss: 3.6437, Time: 28436.2s, Step: 47791, GPU: 4.9GB\n",
      "Epoch 1, Batch 47795, Loss: 3.2696, Time: 28439.6s, Step: 47796, GPU: 4.9GB\n",
      "Epoch 1, Batch 47800, Loss: 3.2708, Time: 28442.2s, Step: 47801, GPU: 4.9GB\n",
      "Epoch 1, Batch 47805, Loss: 2.8984, Time: 28445.6s, Step: 47806, GPU: 4.9GB\n",
      "Epoch 1, Batch 47810, Loss: 3.8820, Time: 28448.1s, Step: 47811, GPU: 4.9GB\n",
      "Epoch 1, Batch 47815, Loss: 3.4612, Time: 28451.5s, Step: 47816, GPU: 4.9GB\n",
      "Epoch 1, Batch 47820, Loss: 3.5192, Time: 28454.0s, Step: 47821, GPU: 4.9GB\n",
      "Epoch 1, Batch 47825, Loss: 2.3866, Time: 28457.4s, Step: 47826, GPU: 4.9GB\n",
      "Epoch 1, Batch 47830, Loss: 4.0601, Time: 28459.9s, Step: 47831, GPU: 4.9GB\n",
      "Epoch 1, Batch 47835, Loss: 2.6944, Time: 28463.3s, Step: 47836, GPU: 4.9GB\n",
      "Epoch 1, Batch 47840, Loss: 3.9247, Time: 28465.8s, Step: 47841, GPU: 4.9GB\n",
      "Epoch 1, Batch 47845, Loss: 3.6625, Time: 28469.3s, Step: 47846, GPU: 4.9GB\n",
      "Epoch 1, Batch 47850, Loss: 3.2084, Time: 28471.8s, Step: 47851, GPU: 4.9GB\n",
      "Epoch 1, Batch 47855, Loss: 3.5924, Time: 28475.2s, Step: 47856, GPU: 4.9GB\n",
      "Epoch 1, Batch 47860, Loss: 3.2377, Time: 28477.7s, Step: 47861, GPU: 4.9GB\n",
      "Epoch 1, Batch 47865, Loss: 3.4949, Time: 28481.1s, Step: 47866, GPU: 4.9GB\n",
      "Epoch 1, Batch 47870, Loss: 3.7092, Time: 28483.6s, Step: 47871, GPU: 4.9GB\n",
      "Epoch 1, Batch 47875, Loss: 2.7645, Time: 28487.0s, Step: 47876, GPU: 4.9GB\n",
      "Epoch 1, Batch 47880, Loss: 4.0908, Time: 28489.5s, Step: 47881, GPU: 4.9GB\n",
      "Epoch 1, Batch 47885, Loss: 3.8396, Time: 28492.9s, Step: 47886, GPU: 4.9GB\n",
      "Epoch 1, Batch 47890, Loss: 4.0272, Time: 28495.4s, Step: 47891, GPU: 4.9GB\n",
      "Epoch 1, Batch 47895, Loss: 2.5783, Time: 28498.8s, Step: 47896, GPU: 4.9GB\n",
      "Epoch 1, Batch 47900, Loss: 3.2434, Time: 28501.4s, Step: 47901, GPU: 4.9GB\n",
      "Epoch 1, Batch 47905, Loss: 3.4385, Time: 28504.8s, Step: 47906, GPU: 4.9GB\n",
      "Epoch 1, Batch 47910, Loss: 2.5899, Time: 28507.3s, Step: 47911, GPU: 4.9GB\n",
      "Epoch 1, Batch 47915, Loss: 3.5811, Time: 28510.7s, Step: 47916, GPU: 4.9GB\n",
      "Epoch 1, Batch 47920, Loss: 3.2184, Time: 28513.3s, Step: 47921, GPU: 4.9GB\n",
      "Epoch 1, Batch 47925, Loss: 3.6410, Time: 28516.7s, Step: 47926, GPU: 4.9GB\n",
      "Epoch 1, Batch 47930, Loss: 3.2806, Time: 28519.2s, Step: 47931, GPU: 4.9GB\n",
      "Epoch 1, Batch 47935, Loss: 3.3084, Time: 28522.7s, Step: 47936, GPU: 4.9GB\n",
      "Epoch 1, Batch 47940, Loss: 3.8260, Time: 28525.2s, Step: 47941, GPU: 4.9GB\n",
      "Epoch 1, Batch 47945, Loss: 2.5877, Time: 28528.6s, Step: 47946, GPU: 4.9GB\n",
      "Epoch 1, Batch 47950, Loss: 3.6639, Time: 28531.1s, Step: 47951, GPU: 4.9GB\n",
      "Epoch 1, Batch 47955, Loss: 3.2701, Time: 28534.5s, Step: 47956, GPU: 4.9GB\n",
      "Epoch 1, Batch 47960, Loss: 2.9081, Time: 28537.1s, Step: 47961, GPU: 4.9GB\n",
      "Epoch 1, Batch 47965, Loss: 4.3323, Time: 28540.5s, Step: 47966, GPU: 4.9GB\n",
      "Epoch 1, Batch 47970, Loss: 2.9842, Time: 28543.0s, Step: 47971, GPU: 4.9GB\n",
      "Epoch 1, Batch 47975, Loss: 3.6003, Time: 28546.4s, Step: 47976, GPU: 4.9GB\n",
      "Epoch 1, Batch 47980, Loss: 2.8307, Time: 28548.9s, Step: 47981, GPU: 4.9GB\n",
      "Epoch 1, Batch 47985, Loss: 2.8390, Time: 28552.3s, Step: 47986, GPU: 4.9GB\n",
      "Epoch 1, Batch 47990, Loss: 3.7123, Time: 28554.8s, Step: 47991, GPU: 4.9GB\n",
      "Epoch 1, Batch 47995, Loss: 2.9345, Time: 28558.2s, Step: 47996, GPU: 4.9GB\n",
      "Epoch 1, Batch 48000, Loss: 2.0787, Time: 28560.9s, Step: 48001, GPU: 4.9GB\n",
      "Epoch 1, Batch 48005, Loss: 3.4644, Time: 28564.3s, Step: 48006, GPU: 4.9GB\n",
      "Epoch 1, Batch 48010, Loss: 2.8303, Time: 28566.9s, Step: 48011, GPU: 4.9GB\n",
      "Epoch 1, Batch 48015, Loss: 3.1676, Time: 28570.3s, Step: 48016, GPU: 4.9GB\n",
      "Epoch 1, Batch 48020, Loss: 3.2937, Time: 28572.8s, Step: 48021, GPU: 4.9GB\n",
      "Epoch 1, Batch 48025, Loss: 3.5100, Time: 28576.2s, Step: 48026, GPU: 4.9GB\n",
      "Epoch 1, Batch 48030, Loss: 3.4944, Time: 28578.7s, Step: 48031, GPU: 4.9GB\n",
      "Epoch 1, Batch 48035, Loss: 2.6243, Time: 28582.1s, Step: 48036, GPU: 4.9GB\n",
      "Epoch 1, Batch 48040, Loss: 3.3812, Time: 28584.7s, Step: 48041, GPU: 4.9GB\n",
      "Epoch 1, Batch 48045, Loss: 3.4185, Time: 28588.1s, Step: 48046, GPU: 4.9GB\n",
      "Epoch 1, Batch 48050, Loss: 3.0697, Time: 28590.6s, Step: 48051, GPU: 4.9GB\n",
      "Epoch 1, Batch 48055, Loss: 3.6219, Time: 28594.0s, Step: 48056, GPU: 4.9GB\n",
      "Epoch 1, Batch 48060, Loss: 2.8812, Time: 28596.5s, Step: 48061, GPU: 4.9GB\n",
      "Epoch 1, Batch 48065, Loss: 2.8967, Time: 28600.0s, Step: 48066, GPU: 4.9GB\n",
      "Epoch 1, Batch 48070, Loss: 3.7840, Time: 28602.5s, Step: 48071, GPU: 4.9GB\n",
      "Epoch 1, Batch 48075, Loss: 3.0283, Time: 28606.0s, Step: 48076, GPU: 4.9GB\n",
      "Epoch 1, Batch 48080, Loss: 3.6444, Time: 28608.5s, Step: 48081, GPU: 4.9GB\n",
      "Epoch 1, Batch 48085, Loss: 3.5829, Time: 28611.9s, Step: 48086, GPU: 4.9GB\n",
      "Epoch 1, Batch 48090, Loss: 3.7281, Time: 28614.5s, Step: 48091, GPU: 4.9GB\n",
      "Epoch 1, Batch 48095, Loss: 3.6470, Time: 28617.9s, Step: 48096, GPU: 4.9GB\n",
      "Epoch 1, Batch 48100, Loss: 2.9440, Time: 28620.4s, Step: 48101, GPU: 4.9GB\n",
      "Epoch 1, Batch 48105, Loss: 3.4648, Time: 28623.8s, Step: 48106, GPU: 4.9GB\n",
      "Epoch 1, Batch 48110, Loss: 2.6661, Time: 28626.4s, Step: 48111, GPU: 4.9GB\n",
      "Epoch 1, Batch 48115, Loss: 4.6564, Time: 28629.8s, Step: 48116, GPU: 4.9GB\n",
      "Epoch 1, Batch 48120, Loss: 3.7854, Time: 28632.3s, Step: 48121, GPU: 4.9GB\n",
      "Epoch 1, Batch 48125, Loss: 3.1619, Time: 28635.8s, Step: 48126, GPU: 4.9GB\n",
      "Epoch 1, Batch 48130, Loss: 2.7686, Time: 28638.3s, Step: 48131, GPU: 4.9GB\n",
      "Epoch 1, Batch 48135, Loss: 3.2728, Time: 28641.7s, Step: 48136, GPU: 4.9GB\n",
      "Epoch 1, Batch 48140, Loss: 3.1012, Time: 28644.2s, Step: 48141, GPU: 4.9GB\n",
      "Epoch 1, Batch 48145, Loss: 2.7353, Time: 28647.7s, Step: 48146, GPU: 4.9GB\n",
      "Epoch 1, Batch 48150, Loss: 3.2944, Time: 28650.2s, Step: 48151, GPU: 4.9GB\n",
      "Epoch 1, Batch 48155, Loss: 3.5609, Time: 28653.6s, Step: 48156, GPU: 4.9GB\n",
      "Epoch 1, Batch 48160, Loss: 2.6505, Time: 28656.1s, Step: 48161, GPU: 4.9GB\n",
      "Epoch 1, Batch 48165, Loss: 3.6836, Time: 28659.6s, Step: 48166, GPU: 4.9GB\n",
      "Epoch 1, Batch 48170, Loss: 2.9991, Time: 28662.1s, Step: 48171, GPU: 4.9GB\n",
      "Epoch 1, Batch 48175, Loss: 3.2973, Time: 28665.6s, Step: 48176, GPU: 4.9GB\n",
      "Epoch 1, Batch 48180, Loss: 3.4880, Time: 28668.1s, Step: 48181, GPU: 4.9GB\n",
      "Epoch 1, Batch 48185, Loss: 3.4247, Time: 28671.5s, Step: 48186, GPU: 4.9GB\n",
      "Epoch 1, Batch 48190, Loss: 3.7953, Time: 28674.1s, Step: 48191, GPU: 4.9GB\n",
      "Epoch 1, Batch 48195, Loss: 2.9024, Time: 28677.5s, Step: 48196, GPU: 4.9GB\n",
      "Epoch 1, Batch 48200, Loss: 2.6859, Time: 28680.1s, Step: 48201, GPU: 4.9GB\n",
      "Epoch 1, Batch 48205, Loss: 3.4975, Time: 28683.5s, Step: 48206, GPU: 4.9GB\n",
      "Epoch 1, Batch 48210, Loss: 3.6401, Time: 28686.0s, Step: 48211, GPU: 4.9GB\n",
      "Epoch 1, Batch 48215, Loss: 3.3540, Time: 28689.4s, Step: 48216, GPU: 4.9GB\n",
      "Epoch 1, Batch 48220, Loss: 2.9725, Time: 28691.9s, Step: 48221, GPU: 4.9GB\n",
      "Epoch 1, Batch 48225, Loss: 3.9444, Time: 28695.3s, Step: 48226, GPU: 4.9GB\n",
      "Epoch 1, Batch 48230, Loss: 3.1911, Time: 28697.9s, Step: 48231, GPU: 4.9GB\n",
      "Epoch 1, Batch 48235, Loss: 2.8616, Time: 28701.3s, Step: 48236, GPU: 4.9GB\n",
      "Epoch 1, Batch 48240, Loss: 3.2068, Time: 28703.8s, Step: 48241, GPU: 4.9GB\n",
      "Epoch 1, Batch 48245, Loss: 3.0174, Time: 28707.2s, Step: 48246, GPU: 4.9GB\n",
      "Epoch 1, Batch 48250, Loss: 3.1962, Time: 28709.8s, Step: 48251, GPU: 4.9GB\n",
      "Epoch 1, Batch 48255, Loss: 2.7646, Time: 28713.3s, Step: 48256, GPU: 4.9GB\n",
      "Epoch 1, Batch 48260, Loss: 3.7216, Time: 28715.9s, Step: 48261, GPU: 4.9GB\n",
      "Epoch 1, Batch 48265, Loss: 3.5449, Time: 28719.3s, Step: 48266, GPU: 4.9GB\n",
      "Epoch 1, Batch 48270, Loss: 3.8021, Time: 28721.8s, Step: 48271, GPU: 4.9GB\n",
      "Epoch 1, Batch 48275, Loss: 3.4185, Time: 28725.2s, Step: 48276, GPU: 4.9GB\n",
      "Epoch 1, Batch 48280, Loss: 2.6997, Time: 28727.7s, Step: 48281, GPU: 4.9GB\n",
      "Epoch 1, Batch 48285, Loss: 2.9452, Time: 28731.2s, Step: 48286, GPU: 4.9GB\n",
      "Epoch 1, Batch 48290, Loss: 2.4315, Time: 28733.7s, Step: 48291, GPU: 4.9GB\n",
      "Epoch 1, Batch 48295, Loss: 3.0090, Time: 28737.1s, Step: 48296, GPU: 4.9GB\n",
      "Epoch 1, Batch 48300, Loss: 2.9494, Time: 28739.6s, Step: 48301, GPU: 4.9GB\n",
      "Epoch 1, Batch 48305, Loss: 3.3919, Time: 28743.0s, Step: 48306, GPU: 4.9GB\n",
      "Epoch 1, Batch 48310, Loss: 3.1577, Time: 28745.6s, Step: 48311, GPU: 4.9GB\n",
      "Epoch 1, Batch 48315, Loss: 4.1463, Time: 28749.0s, Step: 48316, GPU: 4.9GB\n",
      "Epoch 1, Batch 48320, Loss: 4.4282, Time: 28751.5s, Step: 48321, GPU: 4.9GB\n",
      "Epoch 1, Batch 48325, Loss: 3.6417, Time: 28754.9s, Step: 48326, GPU: 4.9GB\n",
      "Epoch 1, Batch 48330, Loss: 2.9210, Time: 28757.4s, Step: 48331, GPU: 4.9GB\n",
      "Epoch 1, Batch 48335, Loss: 3.6213, Time: 28760.8s, Step: 48336, GPU: 4.9GB\n",
      "Epoch 1, Batch 48340, Loss: 3.5463, Time: 28763.3s, Step: 48341, GPU: 4.9GB\n",
      "Epoch 1, Batch 48345, Loss: 3.2935, Time: 28766.7s, Step: 48346, GPU: 4.9GB\n",
      "Epoch 1, Batch 48350, Loss: 3.5932, Time: 28769.2s, Step: 48351, GPU: 4.9GB\n",
      "Epoch 1, Batch 48355, Loss: 3.7759, Time: 28772.7s, Step: 48356, GPU: 4.9GB\n",
      "Epoch 1, Batch 48360, Loss: 3.1318, Time: 28775.2s, Step: 48361, GPU: 4.9GB\n",
      "Epoch 1, Batch 48365, Loss: 3.7731, Time: 28778.6s, Step: 48366, GPU: 4.9GB\n",
      "Epoch 1, Batch 48370, Loss: 4.0742, Time: 28781.1s, Step: 48371, GPU: 4.9GB\n",
      "Epoch 1, Batch 48375, Loss: 3.2226, Time: 28784.5s, Step: 48376, GPU: 4.9GB\n",
      "Epoch 1, Batch 48380, Loss: 3.0293, Time: 28787.0s, Step: 48381, GPU: 4.9GB\n",
      "Epoch 1, Batch 48385, Loss: 3.3701, Time: 28790.4s, Step: 48386, GPU: 4.9GB\n",
      "Epoch 1, Batch 48390, Loss: 3.5034, Time: 28792.9s, Step: 48391, GPU: 4.9GB\n",
      "Epoch 1, Batch 48395, Loss: 3.7621, Time: 28796.3s, Step: 48396, GPU: 4.9GB\n",
      "Epoch 1, Batch 48400, Loss: 4.1651, Time: 28798.9s, Step: 48401, GPU: 4.9GB\n",
      "\n",
      "🔄 Auto-saving checkpoint at epoch 1, batch 48403...\n",
      "💾 Checkpoint saved to: ./my_model_checkpoints/auto_checkpoint_epoch_1_step_48403.pt\n",
      "✅ Checkpoint saved successfully!\n",
      "\n",
      "Epoch 1, Batch 48405, Loss: 3.3957, Time: 28804.2s, Step: 48406, GPU: 4.9GB\n",
      "Epoch 1, Batch 48410, Loss: 3.1263, Time: 28806.7s, Step: 48411, GPU: 4.9GB\n",
      "Epoch 1, Batch 48415, Loss: 3.1629, Time: 28810.1s, Step: 48416, GPU: 4.9GB\n",
      "Epoch 1, Batch 48420, Loss: 3.0284, Time: 28812.6s, Step: 48421, GPU: 4.9GB\n",
      "Epoch 1, Batch 48425, Loss: 3.0114, Time: 28816.0s, Step: 48426, GPU: 4.9GB\n",
      "Epoch 1, Batch 48430, Loss: 3.3395, Time: 28818.5s, Step: 48431, GPU: 4.9GB\n",
      "Epoch 1, Batch 48435, Loss: 2.8347, Time: 28821.9s, Step: 48436, GPU: 4.9GB\n",
      "Epoch 1, Batch 48440, Loss: 3.3333, Time: 28824.4s, Step: 48441, GPU: 4.9GB\n",
      "Epoch 1, Batch 48445, Loss: 3.6712, Time: 28827.8s, Step: 48446, GPU: 4.9GB\n",
      "Epoch 1, Batch 48450, Loss: 3.4092, Time: 28830.4s, Step: 48451, GPU: 4.9GB\n",
      "Epoch 1, Batch 48455, Loss: 2.4378, Time: 28833.8s, Step: 48456, GPU: 4.9GB\n",
      "Epoch 1, Batch 48460, Loss: 2.8553, Time: 28836.3s, Step: 48461, GPU: 4.9GB\n",
      "Epoch 1, Batch 48465, Loss: 3.2578, Time: 28839.7s, Step: 48466, GPU: 4.9GB\n",
      "Epoch 1, Batch 48470, Loss: 2.6051, Time: 28842.2s, Step: 48471, GPU: 4.9GB\n",
      "Epoch 1, Batch 48475, Loss: 3.6156, Time: 28845.6s, Step: 48476, GPU: 4.9GB\n",
      "Epoch 1, Batch 48480, Loss: 3.8923, Time: 28848.1s, Step: 48481, GPU: 4.9GB\n",
      "Epoch 1, Batch 48485, Loss: 2.8836, Time: 28851.5s, Step: 48486, GPU: 4.9GB\n",
      "Epoch 1, Batch 48490, Loss: 3.2468, Time: 28854.0s, Step: 48491, GPU: 4.9GB\n",
      "Epoch 1, Batch 48495, Loss: 3.4213, Time: 28857.4s, Step: 48496, GPU: 4.9GB\n",
      "Epoch 1, Batch 48500, Loss: 2.4586, Time: 28859.9s, Step: 48501, GPU: 4.9GB\n",
      "Epoch 1, Batch 48505, Loss: 2.6702, Time: 28863.4s, Step: 48506, GPU: 4.9GB\n",
      "Epoch 1, Batch 48510, Loss: 3.7911, Time: 28865.9s, Step: 48511, GPU: 4.9GB\n",
      "Epoch 1, Batch 48515, Loss: 3.2057, Time: 28869.3s, Step: 48516, GPU: 4.9GB\n",
      "Epoch 1, Batch 48520, Loss: 3.5096, Time: 28871.8s, Step: 48521, GPU: 4.9GB\n",
      "Epoch 1, Batch 48525, Loss: 2.8597, Time: 28875.2s, Step: 48526, GPU: 4.9GB\n",
      "Epoch 1, Batch 48530, Loss: 3.2671, Time: 28877.7s, Step: 48531, GPU: 4.9GB\n",
      "Epoch 1, Batch 48535, Loss: 3.3768, Time: 28881.2s, Step: 48536, GPU: 4.9GB\n",
      "Epoch 1, Batch 48540, Loss: 3.2479, Time: 28883.7s, Step: 48541, GPU: 4.9GB\n",
      "Epoch 1, Batch 48545, Loss: 3.8820, Time: 28887.1s, Step: 48546, GPU: 4.9GB\n",
      "Epoch 1, Batch 48550, Loss: 3.1173, Time: 28889.6s, Step: 48551, GPU: 4.9GB\n",
      "Epoch 1, Batch 48555, Loss: 2.8603, Time: 28893.0s, Step: 48556, GPU: 4.9GB\n",
      "Epoch 1, Batch 48560, Loss: 2.8639, Time: 28895.6s, Step: 48561, GPU: 4.9GB\n",
      "Epoch 1, Batch 48565, Loss: 3.7121, Time: 28899.0s, Step: 48566, GPU: 4.9GB\n",
      "Epoch 1, Batch 48570, Loss: 3.2651, Time: 28901.5s, Step: 48571, GPU: 4.9GB\n",
      "Epoch 1, Batch 48575, Loss: 3.1492, Time: 28904.9s, Step: 48576, GPU: 4.9GB\n",
      "Epoch 1, Batch 48580, Loss: 3.8580, Time: 28907.4s, Step: 48581, GPU: 4.9GB\n",
      "Epoch 1, Batch 48585, Loss: 2.2367, Time: 28910.9s, Step: 48586, GPU: 4.9GB\n",
      "Epoch 1, Batch 48590, Loss: 4.0045, Time: 28913.4s, Step: 48591, GPU: 4.9GB\n",
      "Epoch 1, Batch 48595, Loss: 2.8456, Time: 28916.8s, Step: 48596, GPU: 4.9GB\n",
      "Epoch 1, Batch 48600, Loss: 3.4711, Time: 28919.4s, Step: 48601, GPU: 4.9GB\n",
      "Epoch 1, Batch 48605, Loss: 2.9848, Time: 28922.9s, Step: 48606, GPU: 4.9GB\n",
      "Epoch 1, Batch 48610, Loss: 3.1492, Time: 28925.4s, Step: 48611, GPU: 4.9GB\n",
      "Epoch 1, Batch 48615, Loss: 3.3504, Time: 28928.8s, Step: 48616, GPU: 4.9GB\n",
      "Epoch 1, Batch 48620, Loss: 3.1621, Time: 28931.3s, Step: 48621, GPU: 4.9GB\n",
      "Epoch 1, Batch 48625, Loss: 2.8235, Time: 28934.7s, Step: 48626, GPU: 4.9GB\n",
      "Epoch 1, Batch 48630, Loss: 3.2101, Time: 28937.2s, Step: 48631, GPU: 4.9GB\n",
      "Epoch 1, Batch 48635, Loss: 2.9708, Time: 28940.6s, Step: 48636, GPU: 4.9GB\n",
      "Epoch 1, Batch 48640, Loss: 2.6190, Time: 28943.1s, Step: 48641, GPU: 4.9GB\n",
      "Epoch 1, Batch 48645, Loss: 2.3928, Time: 28946.5s, Step: 48646, GPU: 4.9GB\n",
      "Epoch 1, Batch 48650, Loss: 3.0479, Time: 28949.0s, Step: 48651, GPU: 4.9GB\n",
      "Epoch 1, Batch 48655, Loss: 2.7797, Time: 28952.4s, Step: 48656, GPU: 4.9GB\n",
      "Epoch 1, Batch 48660, Loss: 2.7246, Time: 28954.9s, Step: 48661, GPU: 4.9GB\n",
      "Epoch 1, Batch 48665, Loss: 3.6868, Time: 28958.3s, Step: 48666, GPU: 4.9GB\n",
      "Epoch 1, Batch 48670, Loss: 2.8104, Time: 28960.8s, Step: 48671, GPU: 4.9GB\n",
      "Epoch 1, Batch 48675, Loss: 2.7748, Time: 28964.3s, Step: 48676, GPU: 4.9GB\n",
      "Epoch 1, Batch 48680, Loss: 2.9516, Time: 28966.8s, Step: 48681, GPU: 4.9GB\n",
      "Epoch 1, Batch 48685, Loss: 2.7938, Time: 28970.2s, Step: 48686, GPU: 4.9GB\n",
      "Epoch 1, Batch 48690, Loss: 2.4470, Time: 28972.7s, Step: 48691, GPU: 4.9GB\n",
      "Epoch 1, Batch 48695, Loss: 3.6454, Time: 28976.1s, Step: 48696, GPU: 4.9GB\n",
      "Epoch 1, Batch 48700, Loss: 2.5178, Time: 28978.6s, Step: 48701, GPU: 4.9GB\n",
      "Epoch 1, Batch 48705, Loss: 3.3532, Time: 28982.1s, Step: 48706, GPU: 4.9GB\n",
      "Epoch 1, Batch 48710, Loss: 2.3611, Time: 28984.7s, Step: 48711, GPU: 4.9GB\n",
      "Epoch 1, Batch 48715, Loss: 2.4278, Time: 28988.2s, Step: 48716, GPU: 4.9GB\n",
      "Epoch 1, Batch 48720, Loss: 2.8834, Time: 28990.8s, Step: 48721, GPU: 4.9GB\n",
      "Epoch 1, Batch 48725, Loss: 3.0438, Time: 28994.3s, Step: 48726, GPU: 4.9GB\n",
      "Epoch 1, Batch 48730, Loss: 3.7019, Time: 28996.8s, Step: 48731, GPU: 4.9GB\n",
      "Epoch 1, Batch 48735, Loss: 3.3984, Time: 29000.4s, Step: 48736, GPU: 4.9GB\n",
      "Epoch 1, Batch 48740, Loss: 3.3831, Time: 29003.0s, Step: 48741, GPU: 4.9GB\n",
      "Epoch 1, Batch 48745, Loss: 3.3771, Time: 29006.5s, Step: 48746, GPU: 4.9GB\n",
      "Epoch 1, Batch 48750, Loss: 3.6280, Time: 29009.1s, Step: 48751, GPU: 4.9GB\n",
      "Epoch 1, Batch 48755, Loss: 3.2899, Time: 29012.6s, Step: 48756, GPU: 4.9GB\n",
      "Epoch 1, Batch 48760, Loss: 3.5087, Time: 29015.2s, Step: 48761, GPU: 4.9GB\n",
      "Epoch 1, Batch 48765, Loss: 2.5989, Time: 29018.9s, Step: 48766, GPU: 4.9GB\n",
      "Epoch 1, Batch 48770, Loss: 2.7003, Time: 29021.5s, Step: 48771, GPU: 4.9GB\n",
      "Epoch 1, Batch 48775, Loss: 3.3625, Time: 29025.0s, Step: 48776, GPU: 4.9GB\n",
      "Epoch 1, Batch 48780, Loss: 3.5148, Time: 29027.6s, Step: 48781, GPU: 4.9GB\n",
      "Epoch 1, Batch 48785, Loss: 3.0565, Time: 29031.1s, Step: 48786, GPU: 4.9GB\n",
      "Epoch 1, Batch 48790, Loss: 3.6416, Time: 29033.6s, Step: 48791, GPU: 4.9GB\n",
      "Epoch 1, Batch 48795, Loss: 3.7582, Time: 29037.1s, Step: 48796, GPU: 4.9GB\n",
      "Epoch 1, Batch 48800, Loss: 2.4299, Time: 29039.8s, Step: 48801, GPU: 4.9GB\n",
      "Epoch 1, Batch 48805, Loss: 3.9109, Time: 29043.3s, Step: 48806, GPU: 4.9GB\n",
      "Epoch 1, Batch 48810, Loss: 3.7863, Time: 29045.9s, Step: 48811, GPU: 4.9GB\n",
      "Epoch 1, Batch 48815, Loss: 3.4797, Time: 29049.4s, Step: 48816, GPU: 4.9GB\n",
      "Epoch 1, Batch 48820, Loss: 2.8345, Time: 29051.9s, Step: 48821, GPU: 4.9GB\n",
      "Epoch 1, Batch 48825, Loss: 4.6978, Time: 29055.3s, Step: 48826, GPU: 4.9GB\n",
      "Epoch 1, Batch 48830, Loss: 3.5565, Time: 29057.9s, Step: 48831, GPU: 4.9GB\n",
      "Epoch 1, Batch 48835, Loss: 3.4066, Time: 29061.4s, Step: 48836, GPU: 4.9GB\n",
      "Epoch 1, Batch 48840, Loss: 2.9616, Time: 29064.0s, Step: 48841, GPU: 4.9GB\n",
      "Epoch 1, Batch 48845, Loss: 2.7033, Time: 29067.5s, Step: 48846, GPU: 4.9GB\n",
      "Epoch 1, Batch 48850, Loss: 3.1660, Time: 29070.0s, Step: 48851, GPU: 4.9GB\n",
      "Epoch 1, Batch 48855, Loss: 3.4241, Time: 29073.6s, Step: 48856, GPU: 4.9GB\n",
      "Epoch 1, Batch 48860, Loss: 2.8010, Time: 29076.2s, Step: 48861, GPU: 4.9GB\n",
      "Epoch 1, Batch 48865, Loss: 3.1882, Time: 29079.6s, Step: 48866, GPU: 4.9GB\n",
      "Epoch 1, Batch 48870, Loss: 3.0620, Time: 29082.1s, Step: 48871, GPU: 4.9GB\n",
      "Epoch 1, Batch 48875, Loss: 2.8964, Time: 29085.6s, Step: 48876, GPU: 4.9GB\n",
      "Epoch 1, Batch 48880, Loss: 4.3213, Time: 29088.2s, Step: 48881, GPU: 4.9GB\n",
      "Epoch 1, Batch 48885, Loss: 2.6834, Time: 29091.7s, Step: 48886, GPU: 4.9GB\n",
      "Epoch 1, Batch 48890, Loss: 3.4685, Time: 29094.2s, Step: 48891, GPU: 4.9GB\n",
      "Epoch 1, Batch 48895, Loss: 2.7265, Time: 29097.7s, Step: 48896, GPU: 4.9GB\n",
      "Epoch 1, Batch 48900, Loss: 3.0311, Time: 29100.3s, Step: 48901, GPU: 4.9GB\n",
      "Epoch 1, Batch 48905, Loss: 3.0711, Time: 29103.7s, Step: 48906, GPU: 4.9GB\n",
      "Epoch 1, Batch 48910, Loss: 3.1146, Time: 29106.3s, Step: 48911, GPU: 4.9GB\n",
      "Epoch 1, Batch 48915, Loss: 3.3192, Time: 29109.9s, Step: 48916, GPU: 4.9GB\n",
      "Epoch 1, Batch 48920, Loss: 2.9125, Time: 29112.5s, Step: 48921, GPU: 4.9GB\n",
      "Epoch 1, Batch 48925, Loss: 3.5548, Time: 29116.0s, Step: 48926, GPU: 4.9GB\n",
      "Epoch 1, Batch 48930, Loss: 3.2672, Time: 29118.5s, Step: 48931, GPU: 4.9GB\n",
      "Epoch 1, Batch 48935, Loss: 3.0477, Time: 29122.3s, Step: 48936, GPU: 4.9GB\n",
      "Epoch 1, Batch 48940, Loss: 3.6043, Time: 29124.9s, Step: 48941, GPU: 4.9GB\n",
      "Epoch 1, Batch 48945, Loss: 2.6002, Time: 29128.4s, Step: 48946, GPU: 4.9GB\n",
      "Epoch 1, Batch 48950, Loss: 3.0320, Time: 29131.0s, Step: 48951, GPU: 4.9GB\n",
      "Epoch 1, Batch 48955, Loss: 3.0926, Time: 29134.4s, Step: 48956, GPU: 4.9GB\n",
      "Epoch 1, Batch 48960, Loss: 3.3507, Time: 29137.0s, Step: 48961, GPU: 4.9GB\n",
      "Epoch 1, Batch 48965, Loss: 3.2332, Time: 29140.5s, Step: 48966, GPU: 4.9GB\n",
      "Epoch 1, Batch 48970, Loss: 3.0944, Time: 29143.0s, Step: 48971, GPU: 4.9GB\n",
      "Epoch 1, Batch 48975, Loss: 3.8539, Time: 29146.6s, Step: 48976, GPU: 4.9GB\n",
      "Epoch 1, Batch 48980, Loss: 2.6460, Time: 29149.2s, Step: 48981, GPU: 4.9GB\n",
      "Epoch 1, Batch 48985, Loss: 2.7642, Time: 29152.7s, Step: 48986, GPU: 4.9GB\n",
      "Epoch 1, Batch 48990, Loss: 2.8878, Time: 29155.3s, Step: 48991, GPU: 4.9GB\n",
      "Epoch 1, Batch 48995, Loss: 2.7521, Time: 29159.0s, Step: 48996, GPU: 4.9GB\n",
      "Epoch 1, Batch 49000, Loss: 3.8734, Time: 29161.6s, Step: 49001, GPU: 4.9GB\n",
      "Epoch 1, Batch 49005, Loss: 2.6141, Time: 29165.1s, Step: 49006, GPU: 4.9GB\n",
      "Epoch 1, Batch 49010, Loss: 3.2347, Time: 29167.7s, Step: 49011, GPU: 4.9GB\n",
      "Epoch 1, Batch 49015, Loss: 3.2911, Time: 29171.2s, Step: 49016, GPU: 4.9GB\n",
      "Epoch 1, Batch 49020, Loss: 2.5149, Time: 29173.8s, Step: 49021, GPU: 4.9GB\n",
      "Epoch 1, Batch 49025, Loss: 2.4649, Time: 29177.2s, Step: 49026, GPU: 4.9GB\n",
      "Epoch 1, Batch 49030, Loss: 2.4738, Time: 29179.8s, Step: 49031, GPU: 4.9GB\n",
      "Epoch 1, Batch 49035, Loss: 3.1490, Time: 29183.3s, Step: 49036, GPU: 4.9GB\n",
      "Epoch 1, Batch 49040, Loss: 3.6741, Time: 29185.8s, Step: 49041, GPU: 4.9GB\n",
      "Epoch 1, Batch 49045, Loss: 3.5939, Time: 29189.4s, Step: 49046, GPU: 4.9GB\n",
      "Epoch 1, Batch 49050, Loss: 2.3072, Time: 29192.0s, Step: 49051, GPU: 4.9GB\n",
      "Epoch 1, Batch 49055, Loss: 2.3724, Time: 29195.4s, Step: 49056, GPU: 4.9GB\n",
      "Epoch 1, Batch 49060, Loss: 2.6649, Time: 29198.0s, Step: 49061, GPU: 4.9GB\n",
      "Epoch 1, Batch 49065, Loss: 2.9094, Time: 29201.8s, Step: 49066, GPU: 4.9GB\n",
      "Epoch 1, Batch 49070, Loss: 3.4594, Time: 29204.4s, Step: 49071, GPU: 4.9GB\n",
      "Epoch 1, Batch 49075, Loss: 3.1510, Time: 29207.8s, Step: 49076, GPU: 4.9GB\n",
      "Epoch 1, Batch 49080, Loss: 3.4050, Time: 29210.4s, Step: 49081, GPU: 4.9GB\n",
      "Epoch 1, Batch 49085, Loss: 2.9680, Time: 29213.9s, Step: 49086, GPU: 4.9GB\n",
      "Epoch 1, Batch 49090, Loss: 3.3338, Time: 29216.4s, Step: 49091, GPU: 4.9GB\n",
      "Epoch 1, Batch 49095, Loss: 3.7661, Time: 29220.0s, Step: 49096, GPU: 4.9GB\n",
      "Epoch 1, Batch 49100, Loss: 3.6627, Time: 29222.5s, Step: 49101, GPU: 4.9GB\n",
      "Epoch 1, Batch 49105, Loss: 3.5002, Time: 29226.0s, Step: 49106, GPU: 4.9GB\n",
      "Epoch 1, Batch 49110, Loss: 3.7126, Time: 29228.6s, Step: 49111, GPU: 4.9GB\n",
      "Epoch 1, Batch 49115, Loss: 2.9858, Time: 29232.1s, Step: 49116, GPU: 4.9GB\n",
      "Epoch 1, Batch 49120, Loss: 2.3995, Time: 29234.7s, Step: 49121, GPU: 4.9GB\n",
      "Epoch 1, Batch 49125, Loss: 2.9906, Time: 29238.2s, Step: 49126, GPU: 4.9GB\n",
      "Epoch 1, Batch 49130, Loss: 3.4254, Time: 29240.7s, Step: 49131, GPU: 4.9GB\n",
      "Epoch 1, Batch 49135, Loss: 3.0294, Time: 29244.3s, Step: 49136, GPU: 4.9GB\n",
      "Epoch 1, Batch 49140, Loss: 3.4206, Time: 29246.9s, Step: 49141, GPU: 4.9GB\n",
      "Epoch 1, Batch 49145, Loss: 3.2640, Time: 29250.4s, Step: 49146, GPU: 4.9GB\n",
      "Epoch 1, Batch 49150, Loss: 3.6728, Time: 29253.0s, Step: 49151, GPU: 4.9GB\n",
      "Epoch 1, Batch 49155, Loss: 2.6036, Time: 29256.4s, Step: 49156, GPU: 4.9GB\n",
      "Epoch 1, Batch 49160, Loss: 3.2652, Time: 29259.0s, Step: 49161, GPU: 4.9GB\n",
      "Epoch 1, Batch 49165, Loss: 1.9625, Time: 29262.5s, Step: 49166, GPU: 4.9GB\n",
      "Epoch 1, Batch 49170, Loss: 3.8465, Time: 29265.1s, Step: 49171, GPU: 4.9GB\n",
      "Epoch 1, Batch 49175, Loss: 3.1790, Time: 29268.6s, Step: 49176, GPU: 4.9GB\n",
      "Epoch 1, Batch 49180, Loss: 3.1018, Time: 29271.1s, Step: 49181, GPU: 4.9GB\n",
      "Epoch 1, Batch 49185, Loss: 4.1103, Time: 29274.9s, Step: 49186, GPU: 4.9GB\n",
      "Epoch 1, Batch 49190, Loss: 3.4750, Time: 29277.5s, Step: 49191, GPU: 4.9GB\n",
      "Epoch 1, Batch 49195, Loss: 3.5321, Time: 29281.2s, Step: 49196, GPU: 4.9GB\n",
      "Epoch 1, Batch 49200, Loss: 2.9973, Time: 29283.8s, Step: 49201, GPU: 4.9GB\n",
      "Epoch 1, Batch 49205, Loss: 3.2475, Time: 29287.2s, Step: 49206, GPU: 4.9GB\n",
      "Epoch 1, Batch 49210, Loss: 3.1541, Time: 29289.8s, Step: 49211, GPU: 4.9GB\n",
      "Epoch 1, Batch 49215, Loss: 2.6487, Time: 29293.2s, Step: 49216, GPU: 4.9GB\n",
      "Epoch 1, Batch 49220, Loss: 2.9102, Time: 29295.7s, Step: 49221, GPU: 4.9GB\n",
      "Epoch 1, Batch 49225, Loss: 3.7852, Time: 29299.1s, Step: 49226, GPU: 4.9GB\n",
      "Epoch 1, Batch 49230, Loss: 3.5906, Time: 29301.7s, Step: 49231, GPU: 4.9GB\n",
      "Epoch 1, Batch 49235, Loss: 3.5298, Time: 29305.1s, Step: 49236, GPU: 4.9GB\n",
      "Epoch 1, Batch 49240, Loss: 2.6720, Time: 29307.6s, Step: 49241, GPU: 4.9GB\n",
      "Epoch 1, Batch 49245, Loss: 3.3130, Time: 29311.0s, Step: 49246, GPU: 4.9GB\n",
      "Epoch 1, Batch 49250, Loss: 3.5379, Time: 29313.6s, Step: 49251, GPU: 4.9GB\n",
      "Epoch 1, Batch 49255, Loss: 3.4652, Time: 29317.0s, Step: 49256, GPU: 4.9GB\n",
      "Epoch 1, Batch 49260, Loss: 3.1070, Time: 29319.5s, Step: 49261, GPU: 4.9GB\n",
      "Epoch 1, Batch 49265, Loss: 2.8277, Time: 29322.9s, Step: 49266, GPU: 4.9GB\n",
      "Epoch 1, Batch 49270, Loss: 3.9551, Time: 29325.4s, Step: 49271, GPU: 4.9GB\n",
      "Epoch 1, Batch 49275, Loss: 3.0727, Time: 29328.8s, Step: 49276, GPU: 4.9GB\n",
      "Epoch 1, Batch 49280, Loss: 3.0727, Time: 29331.3s, Step: 49281, GPU: 4.9GB\n",
      "Epoch 1, Batch 49285, Loss: 3.2609, Time: 29334.8s, Step: 49286, GPU: 4.9GB\n",
      "Epoch 1, Batch 49290, Loss: 3.0823, Time: 29337.3s, Step: 49291, GPU: 4.9GB\n",
      "Epoch 1, Batch 49295, Loss: 2.9702, Time: 29340.7s, Step: 49296, GPU: 4.9GB\n",
      "Epoch 1, Batch 49300, Loss: 3.3897, Time: 29343.2s, Step: 49301, GPU: 4.9GB\n",
      "Epoch 1, Batch 49305, Loss: 2.5657, Time: 29346.6s, Step: 49306, GPU: 4.9GB\n",
      "Epoch 1, Batch 49310, Loss: 3.2416, Time: 29349.1s, Step: 49311, GPU: 4.9GB\n",
      "Epoch 1, Batch 49315, Loss: 3.1024, Time: 29352.5s, Step: 49316, GPU: 4.9GB\n",
      "Epoch 1, Batch 49320, Loss: 2.3875, Time: 29355.0s, Step: 49321, GPU: 4.9GB\n",
      "Epoch 1, Batch 49325, Loss: 3.0769, Time: 29358.4s, Step: 49326, GPU: 4.9GB\n",
      "Epoch 1, Batch 49330, Loss: 2.9015, Time: 29361.0s, Step: 49331, GPU: 4.9GB\n",
      "Epoch 1, Batch 49335, Loss: 2.9830, Time: 29364.4s, Step: 49336, GPU: 4.9GB\n",
      "Epoch 1, Batch 49340, Loss: 3.5823, Time: 29366.9s, Step: 49341, GPU: 4.9GB\n",
      "Epoch 1, Batch 49345, Loss: 3.0438, Time: 29370.3s, Step: 49346, GPU: 4.9GB\n",
      "Epoch 1, Batch 49350, Loss: 3.1187, Time: 29372.8s, Step: 49351, GPU: 4.9GB\n",
      "Epoch 1, Batch 49355, Loss: 2.8791, Time: 29376.2s, Step: 49356, GPU: 4.9GB\n",
      "Epoch 1, Batch 49360, Loss: 3.4933, Time: 29378.7s, Step: 49361, GPU: 4.9GB\n",
      "Epoch 1, Batch 49365, Loss: 2.8190, Time: 29382.1s, Step: 49366, GPU: 4.9GB\n",
      "Epoch 1, Batch 49370, Loss: 3.4180, Time: 29384.7s, Step: 49371, GPU: 4.9GB\n",
      "Epoch 1, Batch 49375, Loss: 3.8659, Time: 29388.1s, Step: 49376, GPU: 4.9GB\n",
      "Epoch 1, Batch 49380, Loss: 3.2325, Time: 29390.6s, Step: 49381, GPU: 4.9GB\n",
      "Epoch 1, Batch 49385, Loss: 4.0589, Time: 29394.0s, Step: 49386, GPU: 4.9GB\n",
      "Epoch 1, Batch 49390, Loss: 3.2324, Time: 29396.5s, Step: 49391, GPU: 4.9GB\n",
      "Epoch 1, Batch 49395, Loss: 3.6398, Time: 29399.9s, Step: 49396, GPU: 4.9GB\n",
      "Epoch 1, Batch 49400, Loss: 3.7528, Time: 29402.5s, Step: 49401, GPU: 4.9GB\n",
      "Epoch 1, Batch 49405, Loss: 2.1528, Time: 29406.0s, Step: 49406, GPU: 4.9GB\n",
      "Epoch 1, Batch 49410, Loss: 3.3644, Time: 29408.5s, Step: 49411, GPU: 4.9GB\n",
      "Epoch 1, Batch 49415, Loss: 3.5733, Time: 29412.0s, Step: 49416, GPU: 4.9GB\n",
      "Epoch 1, Batch 49420, Loss: 2.1904, Time: 29414.5s, Step: 49421, GPU: 4.9GB\n",
      "Epoch 1, Batch 49425, Loss: 3.3487, Time: 29417.9s, Step: 49426, GPU: 4.9GB\n",
      "Epoch 1, Batch 49430, Loss: 3.4307, Time: 29420.4s, Step: 49431, GPU: 4.9GB\n",
      "Epoch 1, Batch 49435, Loss: 3.8530, Time: 29423.8s, Step: 49436, GPU: 4.9GB\n",
      "Epoch 1, Batch 49440, Loss: 3.5383, Time: 29426.3s, Step: 49441, GPU: 4.9GB\n",
      "Epoch 1, Batch 49445, Loss: 3.0606, Time: 29429.8s, Step: 49446, GPU: 4.9GB\n",
      "Epoch 1, Batch 49450, Loss: 3.1367, Time: 29432.3s, Step: 49451, GPU: 4.9GB\n",
      "Epoch 1, Batch 49455, Loss: 2.4662, Time: 29435.7s, Step: 49456, GPU: 4.9GB\n",
      "Epoch 1, Batch 49460, Loss: 3.4911, Time: 29438.2s, Step: 49461, GPU: 4.9GB\n",
      "Epoch 1, Batch 49465, Loss: 2.8843, Time: 29441.6s, Step: 49466, GPU: 4.9GB\n",
      "Epoch 1, Batch 49470, Loss: 3.8676, Time: 29444.1s, Step: 49471, GPU: 4.9GB\n",
      "Epoch 1, Batch 49475, Loss: 3.7885, Time: 29447.6s, Step: 49476, GPU: 4.9GB\n",
      "Epoch 1, Batch 49480, Loss: 2.9069, Time: 29450.1s, Step: 49481, GPU: 4.9GB\n",
      "Epoch 1, Batch 49485, Loss: 2.8706, Time: 29453.5s, Step: 49486, GPU: 4.9GB\n",
      "Epoch 1, Batch 49490, Loss: 4.2909, Time: 29456.0s, Step: 49491, GPU: 4.9GB\n",
      "Epoch 1, Batch 49495, Loss: 3.1905, Time: 29459.4s, Step: 49496, GPU: 4.9GB\n",
      "Epoch 1, Batch 49500, Loss: 3.3966, Time: 29461.9s, Step: 49501, GPU: 4.9GB\n",
      "Epoch 1, Batch 49505, Loss: 3.4582, Time: 29465.3s, Step: 49506, GPU: 4.9GB\n",
      "Epoch 1, Batch 49510, Loss: 3.2858, Time: 29467.8s, Step: 49511, GPU: 4.9GB\n",
      "Epoch 1, Batch 49515, Loss: 3.5021, Time: 29471.2s, Step: 49516, GPU: 4.9GB\n",
      "Epoch 1, Batch 49520, Loss: 3.6513, Time: 29473.7s, Step: 49521, GPU: 4.9GB\n",
      "Epoch 1, Batch 49525, Loss: 3.3415, Time: 29477.2s, Step: 49526, GPU: 4.9GB\n",
      "Epoch 1, Batch 49530, Loss: 3.0692, Time: 29479.7s, Step: 49531, GPU: 4.9GB\n",
      "Epoch 1, Batch 49535, Loss: 3.0651, Time: 29483.1s, Step: 49536, GPU: 4.9GB\n",
      "Epoch 1, Batch 49540, Loss: 3.0422, Time: 29485.6s, Step: 49541, GPU: 4.9GB\n",
      "Epoch 1, Batch 49545, Loss: 3.0773, Time: 29489.0s, Step: 49546, GPU: 4.9GB\n",
      "Epoch 1, Batch 49550, Loss: 2.6795, Time: 29491.5s, Step: 49551, GPU: 4.9GB\n",
      "Epoch 1, Batch 49555, Loss: 3.0109, Time: 29495.0s, Step: 49556, GPU: 4.9GB\n",
      "Epoch 1, Batch 49560, Loss: 3.7627, Time: 29497.5s, Step: 49561, GPU: 4.9GB\n",
      "Epoch 1, Batch 49565, Loss: 4.0178, Time: 29500.9s, Step: 49566, GPU: 4.9GB\n",
      "Epoch 1, Batch 49570, Loss: 3.3290, Time: 29503.4s, Step: 49571, GPU: 4.9GB\n",
      "Epoch 1, Batch 49575, Loss: 3.2689, Time: 29506.8s, Step: 49576, GPU: 4.9GB\n",
      "Epoch 1, Batch 49580, Loss: 3.2026, Time: 29509.3s, Step: 49581, GPU: 4.9GB\n",
      "Epoch 1, Batch 49585, Loss: 3.4727, Time: 29512.8s, Step: 49586, GPU: 4.9GB\n",
      "Epoch 1, Batch 49590, Loss: 2.8973, Time: 29515.3s, Step: 49591, GPU: 4.9GB\n",
      "Epoch 1, Batch 49595, Loss: 2.7228, Time: 29518.7s, Step: 49596, GPU: 4.9GB\n",
      "Epoch 1, Batch 49600, Loss: 2.9803, Time: 29521.3s, Step: 49601, GPU: 4.9GB\n",
      "Epoch 1, Batch 49605, Loss: 3.1668, Time: 29524.7s, Step: 49606, GPU: 4.9GB\n",
      "Epoch 1, Batch 49610, Loss: 2.6478, Time: 29527.3s, Step: 49611, GPU: 4.9GB\n",
      "Epoch 1, Batch 49615, Loss: 4.0090, Time: 29530.7s, Step: 49616, GPU: 4.9GB\n",
      "Epoch 1, Batch 49620, Loss: 3.4078, Time: 29533.2s, Step: 49621, GPU: 4.9GB\n",
      "Epoch 1, Batch 49625, Loss: 2.1808, Time: 29536.6s, Step: 49626, GPU: 4.9GB\n",
      "Epoch 1, Batch 49630, Loss: 3.0482, Time: 29539.2s, Step: 49631, GPU: 4.9GB\n",
      "Epoch 1, Batch 49635, Loss: 2.6580, Time: 29542.6s, Step: 49636, GPU: 4.9GB\n",
      "Epoch 1, Batch 49640, Loss: 3.1466, Time: 29545.1s, Step: 49641, GPU: 4.9GB\n",
      "Epoch 1, Batch 49645, Loss: 3.2417, Time: 29548.5s, Step: 49646, GPU: 4.9GB\n",
      "Epoch 1, Batch 49650, Loss: 3.4030, Time: 29551.0s, Step: 49651, GPU: 4.9GB\n",
      "Epoch 1, Batch 49655, Loss: 2.4742, Time: 29554.5s, Step: 49656, GPU: 4.9GB\n",
      "Epoch 1, Batch 49660, Loss: 3.4496, Time: 29557.0s, Step: 49661, GPU: 4.9GB\n",
      "Epoch 1, Batch 49665, Loss: 3.2539, Time: 29560.4s, Step: 49666, GPU: 4.9GB\n",
      "Epoch 1, Batch 49670, Loss: 2.0444, Time: 29562.9s, Step: 49671, GPU: 4.9GB\n",
      "Epoch 1, Batch 49675, Loss: 3.5785, Time: 29566.3s, Step: 49676, GPU: 4.9GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 260\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Train - passing d_model explicitly\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 260\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./my_model_checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43muse_mixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 128\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataset, tokenizer, device, epochs, batch_size, lr, checkpoint_dir, resume_from_checkpoint, use_mixed_precision, use_wandb, project_name, run_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m     shift_labels \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    123\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)(\n\u001b[1;32m    124\u001b[0m         shift_logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shift_logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \n\u001b[1;32m    125\u001b[0m         shift_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    126\u001b[0m     )\n\u001b[0;32m--> 128\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    130\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train function\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "\n",
    "def train(model, dataset, tokenizer, device=\"cuda\", epochs=3, batch_size=8, lr=1e-4, \n",
    "          checkpoint_dir=\"checkpoints\", resume_from_checkpoint=None, use_mixed_precision=True,\n",
    "          use_wandb=True, project_name=\"small-model\", run_name=None):\n",
    "    \n",
    "    # Initialize WandB (unchanged)\n",
    "    if use_wandb:\n",
    "        if run_name is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            run_name = f\"decoder_training_{timestamp}\"\n",
    "        \n",
    "        wandb.init(\n",
    "            project=project_name,\n",
    "            name=run_name,\n",
    "            config={\n",
    "                # Your config here (unchanged)\n",
    "                \"model_type\": \"decoder_only_transformer\",\n",
    "                \"d_model\": 768,\n",
    "                \"num_layers\": 10,\n",
    "                \"num_heads\": 12,\n",
    "                \"num_kv_heads\": 4,\n",
    "                \"vocab_size\": len(tokenizer),\n",
    "                \"max_sequence_length\": 2048,\n",
    "                \"dropout\": 0.1,\n",
    "                \"d_ff\": 2048,\n",
    "                \"learning_rate\": lr,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"epochs\": epochs,\n",
    "                \"weight_decay\": 0.01,\n",
    "                \"gradient_clipping\": 1.0,\n",
    "                \"warmup_steps\": 500,\n",
    "                \"dataset\": \"LMSYS-Chat-1M-English\",\n",
    "                \"tokenizer_type\": \"custom_32k\",\n",
    "                \"total_conversations\": 777453,\n",
    "                \"mixed_precision\": use_mixed_precision,\n",
    "                \"device\": str(device),\n",
    "                \"architecture_features\": [\"GQA\", \"RoPE\", \"RMSNorm\"],\n",
    "            },\n",
    "            tags=[\"decoder-only\", \"conversational-ai\", \"gqa\", \"rope\"]\n",
    "        )\n",
    "        \n",
    "        wandb.watch(model, log=\"all\", log_freq=200)\n",
    "    \n",
    "    # Rest of the initialization (unchanged)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Preparing training with mixed precision: {use_mixed_precision}\")\n",
    "    train_dataset = ConversationDataset(\n",
    "        dataset[\"train\"].select(range(777453)), \n",
    "        tokenizer,\n",
    "        max_length=2048\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.98))\n",
    "    scaler = GradScaler() if use_mixed_precision else None\n",
    "    \n",
    "    start_epoch = 0\n",
    "    global_step = 0\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    if resume_from_checkpoint:\n",
    "        checkpoint = load_checkpoint(resume_from_checkpoint, model, optimizer, device)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        global_step = checkpoint['global_step']\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        if use_mixed_precision and 'scaler_state_dict' in checkpoint:\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        print(f\"Resumed training from epoch {start_epoch}, step {global_step}\")\n",
    "    \n",
    "    model.train()\n",
    "    last_checkpoint_time = time.time()\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        total_loss = 0\n",
    "        epoch_start_time = time.time()\n",
    "        batch_count = 0\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            current_time = time.time()\n",
    "            \n",
    "            if current_time - last_checkpoint_time >= 7200:\n",
    "                print(f\"\\n🔄 Auto-saving checkpoint at epoch {epoch+1}, batch {i}...\")\n",
    "                avg_loss = total_loss / max(i, 1)\n",
    "                # ✅ Call without WandB parameter\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, epoch, global_step, avg_loss, best_loss,\n",
    "                    checkpoint_dir, f\"auto_checkpoint_epoch_{epoch+1}_step_{global_step}.pt\"\n",
    "                )\n",
    "                last_checkpoint_time = current_time\n",
    "                print(f\"✅ Checkpoint saved successfully!\\n\")\n",
    "            \n",
    "            # Training step (unchanged)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_mixed_precision:\n",
    "                with autocast(device_type=device.type):\n",
    "                    embeddings = model.tgt_embed(input_ids)\n",
    "                    seq_len = input_ids.size(1)\n",
    "                    causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "                    \n",
    "                    output = embeddings\n",
    "                    for layer in model.decoder.layers:\n",
    "                        output = layer(output, causal_mask, use_cache=False)\n",
    "                    \n",
    "                    logits = model.projection_layer(output)\n",
    "                    \n",
    "                    shift_logits = logits[..., :-1, :].contiguous()\n",
    "                    shift_labels = labels[..., 1:].contiguous()\n",
    "                    \n",
    "                    loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)(\n",
    "                        shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                        shift_labels.view(-1)\n",
    "                    )\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            global_step += 1\n",
    "            batch_count += 1\n",
    "            epoch_losses.append(loss.item())\n",
    "            \n",
    "            if use_wandb:\n",
    "                log_dict = {\n",
    "                    \"train/loss\": loss.item(),\n",
    "                    \"train/epoch\": epoch + 1,\n",
    "                    \"train/global_step\": global_step,\n",
    "                    \"train/learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                }\n",
    "                if torch.cuda.is_available():\n",
    "                    gpu_memory_gb = torch.cuda.memory_allocated() / 1e9\n",
    "                    gpu_memory_reserved_gb = torch.cuda.memory_reserved() / 1e9\n",
    "                    log_dict.update({\n",
    "                        \"system/gpu_memory_allocated_gb\": gpu_memory_gb,\n",
    "                        \"system/gpu_memory_reserved_gb\": gpu_memory_reserved_gb,\n",
    "                    })\n",
    "                wandb.log(log_dict, step=global_step)\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                elapsed_time = time.time() - epoch_start_time\n",
    "                gpu_memory = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "                print(f\"Epoch {epoch+1}, Batch {i}, Loss: {loss.item():.4f}, \"\n",
    "                      f\"Time: {elapsed_time:.1f}s, Step: {global_step}, GPU: {gpu_memory:.1f}GB\")\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}, Duration: {epoch_duration:.1f}s\")\n",
    "        \n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch/avg_loss\": avg_loss,\n",
    "                \"epoch/duration_seconds\": epoch_duration,\n",
    "                \"epoch/batches_processed\": batch_count,\n",
    "                \"epoch/min_loss\": min(epoch_losses[-batch_count:]),\n",
    "                \"epoch/max_loss\": max(epoch_losses[-batch_count:]),\n",
    "            }, step=global_step)\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            print(f\"🎯 New best loss: {best_loss:.4f} - Saving best model...\")\n",
    "            # ✅ Call without WandB parameter\n",
    "            save_checkpoint(\n",
    "                model, optimizer, epoch, global_step, avg_loss, best_loss,\n",
    "                checkpoint_dir, \"best_model.pt\"\n",
    "            )\n",
    "            if use_wandb:\n",
    "                wandb.log({\"train/best_loss\": best_loss}, step=global_step)\n",
    "        \n",
    "        # ✅ Call without WandB parameter\n",
    "        save_checkpoint(\n",
    "            model, optimizer, epoch, global_step, avg_loss, best_loss,\n",
    "            checkpoint_dir, f\"epoch_{epoch+1}_checkpoint.pt\"\n",
    "        )\n",
    "    \n",
    "    print(\"🏁 Training completed! Saving final checkpoint...\")\n",
    "    # ✅ Call without WandB parameter\n",
    "    save_checkpoint(\n",
    "        model, optimizer, epochs-1, global_step, avg_loss, best_loss,\n",
    "        checkpoint_dir, \"final_model.pt\"\n",
    "    )\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Main execution\n",
    "\n",
    "# Model parameters\n",
    "print(\"Building model...\")\n",
    "d_model = 768  # Define d_model here to use in both model building and training and RMS Norm\n",
    "vocab_size = len(tokenizer)\n",
    "seq_len = 2048\n",
    "N = 10\n",
    "h = 12\n",
    "kv_h = 4\n",
    "dropout = 0.1\n",
    "d_ff = 2048\n",
    "\n",
    "# Build model\n",
    "model = build_transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    src_seq_len=seq_len,\n",
    "    tgt_seq_len=seq_len,\n",
    "    d_model=d_model,\n",
    "    N=N,\n",
    "    h=h,\n",
    "    kv_h=kv_h,\n",
    "    dropout=dropout,\n",
    "    d_ff=d_ff\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Resize token embeddings\n",
    "print(\"Resizing token embeddings...\")\n",
    "# Embedding layer: (vocab_size, d_model)\n",
    "model.tgt_embed.weight = torch.nn.Parameter(\n",
    "    torch.randn(vocab_size, d_model).to(device)\n",
    ")\n",
    "\n",
    "# Correct weight dimensions\n",
    "model.projection_layer.proj.weight = torch.nn.Parameter(\n",
    "    torch.randn(len(tokenizer), d_model).to(device)\n",
    ")\n",
    "model.projection_layer.proj.bias = torch.nn.Parameter(\n",
    "    torch.zeros(len(tokenizer)).to(device)\n",
    ")\n",
    "\n",
    "# Apply proper initialization\n",
    "torch.nn.init.xavier_uniform_(model.tgt_embed.weight)\n",
    "torch.nn.init.xavier_uniform_(model.projection_layer.proj.weight)\n",
    "torch.nn.init.zeros_(model.projection_layer.proj.bias)\n",
    "# Train - passing d_model explicitly\n",
    "print(\"Starting training...\")\n",
    "trained_model = train(model, dataset, tokenizer, device, epochs=5, batch_size=6, lr=3e-4,checkpoint_dir=\"./my_model_checkpoints\",use_mixed_precision=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mtrained_model\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_transformer.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "print(\"Saving model...\")\n",
    "torch.save(trained_model.state_dict(), \"trained_transformer.pt\")\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
